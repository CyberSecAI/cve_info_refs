=== Content from youtu.be_1642dd92_20250115_214935.html ===
[00:20] it's
[00:26] [Music]
[00:27] 9:30 I think I all thises
[00:50] next hello hello everyone so it is
[00:54] 9:31 so we're going to start and so if
[00:57] you could make your way to your seats um
[01:00] welcome to the quick working group at
[01:03] iatf
[01:08] 1119 most people are probably familiar
[01:10] at this point in the week with the note
[01:11] well but uh please do note well note
[01:15] well um this is this basically gives us
[01:20] our guidance for all of our conduct and
[01:22] you agree to the note well by
[01:23] participating in this in this working
[01:26] group uh please also note of particular
[01:29] note the code of conduct and the
[01:31] anti-harassment procedures and if you
[01:33] have any concerns about anything of that
[01:35] nature please reach out to the chairs or
[01:38] an ad or someone like
[01:43] that uh so very briefly meeting tips the
[01:46] usual type stuff make sure you use the
[01:47] on-site tool so we get credit for you
[01:50] being here it's good to have to pump our
[01:52] numbers up and uh please use meet Echo
[01:55] to join the mic que rather than just
[01:56] standing there uh and if you're going to
[01:59] use the full version uh just keep your
[02:01] audio and video off um and remote
[02:04] participants yeah you know the typical
[02:07] keep your audio and video off unless you
[02:09] are presenting and we will try to give
[02:11] you control of the slides if you prefer
[02:13] it otherwise we can also uh control it
[02:16] from
[02:22] here oh okay so everyone's favorite part
[02:26] of working group meeting is that we need
[02:27] someone to take uh notes for the minutes
[02:30] so do we have any
[02:33] volunteers for not
[02:35] taking it's a very Noble thing to do and
[02:40] we can't really thank you
[02:44] David um so let's see what else is there
[02:47] oh yeah we're going to run the we'll run
[02:49] the que so um don't worry about like
[02:51] removing yourself we can we'll dump you
[02:54] when you're when you think we're done
[02:55] you're
[02:57] done okay agenda we have a lot of things
[03:00] on the agenda so as usual we start with
[03:03] our working group items and we are
[03:04] starting with kog which will be
[03:06] presented by Robin uh remotely and then
[03:09] we're going to move on to multipath and
[03:11] 50 minutes it's a long time but it's a
[03:13] it's a big thing so then finally we're
[03:16] going to finish off with act frequency
[03:18] for the these are all working group
[03:20] items and then we're going to move on to
[03:21] other items that we think are
[03:23] interesting we have a a presentation on
[03:25] the some quick security considerations
[03:28] and recent experience with that uh then
[03:31] uh quick on streams uh the bdp frame
[03:34] some things about FEC and finally
[03:37] accurate ecn and we hope to get to all
[03:38] these things but as usual um we may not
[03:41] get to some of
[03:44] them and finally update since the last
[03:46] meeting um not a ton but there is some
[03:48] so we have a we had finished a working
[03:51] group last call for ACT frequency uh but
[03:53] we need to run another one because there
[03:55] was a bunch of issues and that's that's
[03:56] why we run working group L calls so
[03:58] that's okay um but we expect to run
[04:01] another one and we'll hear more about
[04:03] that later and then uh reliable resets
[04:06] is uh much to the uh relief of certain
[04:10] chairs is finally wait getting along and
[04:12] there's a waiting Shepherd right up so
[04:15] hopefully that will that'll be helpful
[04:16] for web transport in
[04:18] particular and uh with that oh I I guess
[04:20] I should have said does anyone want to
[04:22] bash this agenda we did a bunch of preg
[04:24] genda bashing so I expect not but if
[04:28] anyone has any bashing
[04:30] please say so
[04:32] now looks like no okay and with that
[04:35] we're going to move on to our first
[04:37] presentation which is from
[04:47] Robin
[04:49] yes can you hear me can I get the
[04:53] slides yeah I'm trying
[04:56] to do
[04:58] that
[05:15] hold Robin Robin can you request
[05:17] permission to present from the Meet Echo
[05:22] to uh I can ask slides if that's what I
[05:26] need to
[05:27] do yeah I did
[05:32] that not
[05:36] again I guess I'm going to run the
[05:38] slides for you Robin because I can't
[05:40] actually give you access for some reason
[05:43] um that's
[05:54] fine there we go so for today's update I
[05:58] wanted to go for the Dune team because
[06:00] as it turns out Australia is about 40%
[06:04] Dunes so we're all basically in arus
[06:06] right now quite cool sad I couldn't be
[06:09] there uh but so next slide please uh
[06:11] since last time we actually had a ton of
[06:14] progress um we made a lot of changes
[06:17] that we discussed last time like
[06:19] removing qack which is now done we had a
[06:21] bunch of editorial updates and then we
[06:23] also had a lot of uh additional
[06:25] clarifications and nuances that were
[06:27] fixed a lot of that was due to to a new
[06:30] character uh appearing on the scene much
[06:32] like uh characters in The Dune movie uh
[06:35] in our case this was uh Hugo landow who
[06:38] is the op SSL quick implementer and they
[06:41] also added qck support and through that
[06:43] he found quite a few
[06:45] um uh subtle nuances and issues that we
[06:48] had to tackle which we were able to do
[06:50] so a big thank you to Hugo Hugo uh for
[06:53] that next slide please another big thing
[06:56] that we merged since last time was uh
[06:59] support for uh path migration and the
[07:02] basis for multipath as you might
[07:04] remember this is a relatively simple
[07:06] proposal where we have uh string part
[07:09] IDs that can then be linked to uh
[07:12] specific part information using separate
[07:15] um events we think this is a good basis
[07:18] for multipot the goal was not to have
[07:20] full multipot support yet but we think
[07:21] this is enough for if there would ever
[07:24] be a quick multipath Q loog draft to
[07:26] build upon this which is why we also
[07:28] merged it
[07:30] um however we're not 100% sure that
[07:32] that's correct so if there are still
[07:34] people from the multipot um uh work that
[07:37] want to experiment with this or want to
[07:39] give it another review or try to
[07:40] implement this we would definitely
[07:42] welcome um additional feedback next
[07:45] slide
[07:48] please uh the main topic of today will
[07:50] be
[07:52] extensibility so uh one of the main
[07:54] goals of qog is to allow later to Define
[07:57] additional qog schemas for other
[08:00] protocols or for new documents to extend
[08:03] to add new events or new uh protocol
[08:06] mechanisms for quick and
[08:07] hcp3 and there are various ways that we
[08:10] need to add in extensibility one of the
[08:12] ways is if you have a qog file how would
[08:15] you know which protocols or which type
[08:17] of events are actually uh possibly
[08:19] represented inside of that file uh and
[08:22] so we've gone to various iterations on
[08:24] this I've been presenting this for a
[08:26] while this is the current um proposal
[08:30] that we are quite sure will work so
[08:32] basically you have two options you
[08:34] explicitly list the the event um event
[08:39] groups that you want to use in a
[08:41] specific file either through a
[08:43] pre-registered Ur So that goes through
[08:45] an aana registry um or you can also use
[08:49] a fully qualified URL uh for things that
[08:52] are for example not yet an RFC or are
[08:55] not going through the ITF um system this
[08:59] is already iterated on from previous
[09:01] feedbacks and is also something that is
[09:03] in previous rfcs so RFC 8285 for example
[09:07] also uses this so we're quite sure that
[09:09] this is a good approach um one of the
[09:11] main questions we have is how explicit
[09:14] we need to be in listing all groups of
[09:17] events so for example our quick document
[09:20] has various categories of events you if
[09:23] your general transport based events but
[09:25] you also have what we call connectivity
[09:27] events or recovery events having to do
[09:29] with congest control for example um and
[09:33] in the example on the slide what we're
[09:35] basically doing is explicitly listing
[09:38] only two of those three categories for
[09:41] quick um meaning that the other ones the
[09:44] recovery one wouldn't be present in this
[09:46] qog file this is different from HTTP 3
[09:50] where we don't explicitly list which
[09:52] categories we're using which in our
[09:54] current proposal means we're using them
[09:56] all so anything that is defined in the
[09:58] htb three document goes um so that's
[10:03] kind of the approach that we're going
[10:04] for if you don't have the hashtag with a
[10:06] specific ategory you would just import
[10:08] everything in the file and if you have
[10:11] implicit uh hashtags you import only a
[10:14] part of the file and the question
[10:16] basically is do does anyone have any
[10:18] problems with that approach with
[10:20] implicitly including the full set of
[10:23] events if you don't explicitly list them
[10:26] or if people or feel very strongly that
[10:28] you should all always explicitly list
[10:31] all of the categories that you are um
[10:34] using so if people have a very strong
[10:36] opinion on that um let us know next
[10:39] slide
[10:42] please so you want to add new T new
[10:47] stuff um but it's not always just
[10:49] completely new protocols sometimes you
[10:51] also want to add new things like new uh
[10:54] frame types in this case to for example
[10:57] quick um so approach to build the Q loog
[11:01] uh documents right now would be to say
[11:03] okay we have the existing quick frames
[11:05] let's say for example the max data frame
[11:08] and then in a packet sent event you
[11:10] would have a list of all the different
[11:11] frames that you could use uh as far as
[11:13] we know existing quick that's of course
[11:16] very inflexible because you can't add
[11:18] new types of frames um to to this
[11:21] definition you would have to redefine
[11:23] the packet sent event to add a new type
[11:26] of frame which is for example one of the
[11:28] problems we have with the ACT frame uh
[11:30] extending that for things like multibot
[11:33] act um so we want to prevent that for
[11:35] for qog using the next slide um using
[11:39] something called a cddl type socket so
[11:43] that's the dollar sign quickframe thing
[11:45] there um in the cddl language this that
[11:50] basically means it's like a I call it
[11:52] the type enum so a list of types that
[11:54] are represented by this one type name
[11:57] and you can later extend that so the
[11:59] slash equals means you add a new type to
[12:02] that type enim of quick frame and so
[12:05] later documents for example let's say
[12:07] the Act frequency draft if they would
[12:09] add Q loog support they can just extend
[12:11] that existing quick frame uh type that
[12:15] is defined in the other document and so
[12:17] if you combine the two if you combine
[12:19] the cddl definitions of both documents
[12:22] then will it will automatically work and
[12:23] you can then list act frequency frames
[12:25] in your packet set events as you would
[12:28] expect right this was actually already
[12:31] in qog for a while for some things we
[12:33] now extended it to cover many more
[12:36] things that's basically what the new PR
[12:38] is doing extending this approach to uh
[12:42] to other stuff next slide
[12:45] please so that was for if you have new
[12:47] things new frames um that you want to
[12:50] support sometimes you Al also want to
[12:52] extend existing things a good example
[12:55] here is quick transport parameters which
[12:58] we log in a an event called parameters
[13:01] set as you can see there it's a long
[13:03] list of Transport parameters that we
[13:05] have there and of course there will be
[13:07] new transport parameters in extensions
[13:10] defined and the way we used to deal with
[13:12] that in qck was to just say oh this
[13:15] event can contain any field of any
[13:19] type uh so that you can you can
[13:21] basically expect anything in there uh is
[13:24] what it was saying which is flexible
[13:26] enough to support later extensions of
[13:28] course but it's it's also completely
[13:29] useless from a schema validation
[13:31] standpoint because it's just too generic
[13:34] anything goes so um you can't really
[13:37] differentiate if it's if it's a valid
[13:38] extension or not and this is really
[13:41] something that we've only fixed very
[13:43] recently next
[13:45] slide
[13:47] um with a a different type of cddl
[13:50] socket in this case a group socket which
[13:53] is different from the type socket from
[13:54] before because it has two dollar signs
[13:57] instead of one dollar sign
[13:59] um so this is not like a type enim this
[14:03] more acts like a placeholder uh kind of
[14:05] field inside of the definition so
[14:07] basically it's saying this is a
[14:09] placeholder and whatever gets assigned
[14:11] to the placeholder later can then show
[14:13] up at this specific uh location in the
[14:16] in the event so for example here we
[14:18] would have the quick parameter set event
[14:21] in the main Quake document that we have
[14:23] now and then again if you would have act
[14:25] frequency um extension that would have q
[14:29] support you can then say oh there is a
[14:31] new transport parameter defined it's
[14:33] this field the Minag delay and it will
[14:36] just be uh slotted into that place in
[14:39] the existing event um there as
[14:43] well so basically what we're proposing
[14:46] is to add one of those extension slots
[14:49] to every single qog event so that every
[14:53] qog event can be extended with new uh
[14:56] Fields if needed by um
[14:59] um future um extensions we think this is
[15:03] a nice uh balance between verbosity and
[15:06] complexity and also allowing for
[15:09] extensibility of everything um down the
[15:11] line because it's always just a single
[15:13] line for every event that is extra even
[15:16] though some of those arguably will never
[15:18] be
[15:19] exercised next
[15:23] slide um with this we're focusing on
[15:27] making qck extensible for the RFC
[15:29] extension points so mainly the things
[15:31] that are defined as extension points in
[15:33] the existing
[15:35] rfc's which mostly means they have Anna
[15:38] registry uh we also have a few things
[15:41] that do not have that let's say quick
[15:43] packet types or uh fields in the quick
[15:46] packet header let's say if there was
[15:48] ever a loss bits extension that was
[15:50] accepted you would also want to extend
[15:52] what's in the packet header so we also
[15:54] have extension points for those um for
[15:58] those things to try try and cover our
[15:59] bases as much as possible right so we
[16:02] think we got this right
[16:05] finally um and so the question is or the
[16:08] ask is if people would be interested in
[16:11] actually trying to exercise these
[16:13] extension points for new work concretely
[16:17] things like multipot maybe or I know
[16:18] there have been some discussions in uh
[16:20] media over quick and there's actually
[16:22] already an existing draft for careful
[16:24] resume that is doing this partially uh
[16:28] they don't support
[16:29] most of it because they only add a fully
[16:32] new event which is great but we would
[16:35] need uh to exercise all the uh other uh
[16:37] extension points or at least to to an
[16:39] extent so if people are interested in
[16:41] this it doesn't have to be full-fledged
[16:42] or fully defined just to have a look at
[16:45] it and try some of it out we would be
[16:48] very happy to help you along there and
[16:50] to look over what you're trying to do
[16:52] and see if it fits with uh with what
[16:55] we're trying to do here Dad do you have
[16:57] a question you want to
[16:59] uh if you could send a note to the quick
[17:02] chair's Alias with what you think the
[17:04] extension points we ought to be
[17:06] exercising a little bit more explicitly
[17:08] that would be very
[17:09] helpful all right we'll do that thank
[17:13] you yeah
[17:15] excellent so next slide please um so
[17:20] extensibility is the main thing the only
[17:22] other big question we have for the
[17:23] working group right now uh apparently I
[17:26] said quick chairs and I meant mock
[17:28] chairs sorry
[17:29] thank you ah Mark okay that makes a
[17:32] little bit more
[17:34] sense excellent
[17:38] um so if you have signals like stream
[17:40] was finished or stream was reset or you
[17:42] get a stop sending those are usually
[17:45] received at the um uh let's say packet
[17:49] level and logged at the packet level
[17:51] sometimes quite a long time before
[17:52] they're actually communicated up to the
[17:54] application Level
[17:56] implementation right so some
[17:57] implementation for example only give you
[17:59] that signal when you try to read from a
[18:02] specific stream not when it is actually
[18:04] received um and so the question is how
[18:08] how to log that in qog when that was
[18:10] actually passed to the application uh
[18:13] implementation in that way and this
[18:15] actually has a lot of parallels but how
[18:17] we communicate how data is actually read
[18:19] how data is passed between the layers we
[18:22] have something called Data moved events
[18:24] for that and so it seemed like a natural
[18:27] fit to also have these kind of signals
[18:29] in those data moved
[18:31] events we had a lot of iterations on
[18:35] what exactly that should look like um
[18:38] but these things are so different and
[18:39] they they all have their own nuances and
[18:41] everything that in the end we just
[18:43] decided or or hoping to decide uh to say
[18:47] not to be too explicit in this event and
[18:50] just have like a a a string that
[18:53] says in this instance at this time stamp
[18:57] it was communicated to be like that
[18:59] there was Ain bit set for this stream or
[19:01] there was a stream reset received for
[19:03] this stream at some point in the past if
[19:06] you want more information look at other
[19:08] qog events that do log the full frame or
[19:11] that do log more information about what
[19:13] that might have been we think this
[19:15] strads the needle between being useful
[19:17] for debugging and not replicating too
[19:20] much of that uh information across
[19:22] different events but it is a little bit
[19:25] clunky uh which is why uh we're we're
[19:28] asking asking people if they have strong
[19:29] opinions on on whether or not we should
[19:31] do
[19:32] this and that's basically the end of the
[19:36] presentation last slide please
[19:38] um we think we are nearing the end with
[19:42] this here uh we think with extensibility
[19:44] done that was the last of the major
[19:47] design issues uh we currently have quite
[19:50] a few issues in PRS open but we should
[19:53] be limiting those very soon because a
[19:56] lot of the extensibility stuff is spread
[19:57] over a lot of the different issues and
[19:59] PRS that those are hopefully going away
[20:02] soon what I'm trying to say is if any of
[20:04] you were waiting for qog to settle down
[20:06] a little bit to start again looking into
[20:10] it and and seeing um and and helping us
[20:13] bring this to the Finish Line this might
[20:15] be a good time to start uh uh looking
[20:17] into it again as we said as well looking
[20:20] into exercising the extension
[20:23] points uh it's also starting to be a
[20:25] good time for that and so we are very
[20:28] open new qog issues new
[20:30] reviews again a big thanks to people
[20:32] like Hugo landow who did this uh if more
[20:35] people are willing to do that that would
[20:36] be very very um helpful today that's it
[20:39] for me if there are any questions
[20:42] um let's have
[20:48] them I uh I inser myself as an
[20:51] individual and author on the qog spec
[20:53] not as chair um but kind of in the lead
[20:56] up to this meeting and uh during this
[20:58] week we've he like some chats in the
[21:01] slack Etc from Mock implementers and and
[21:04] Co so to to Ted's earlier Point
[21:07] um these extension points exist I I
[21:10] don't know enough about the mock apps
[21:12] that uh to know what is useful to log so
[21:14] we've had some discussion um if if you
[21:17] just have a log of stuff that is useful
[21:20] for uh seeing what's going on with theck
[21:22] application from a performance or
[21:23] debuging perspective like we're happy to
[21:25] do some reverse engineering because we
[21:27] want to prove that the extension
[21:29] mechanisms are working um similarly we
[21:31] heard in dsv WG yesterday um that
[21:34] there's some folks uh extending qog to
[21:36] support the careful resume mechanism all
[21:38] of these are really good um I don't
[21:40] think we need to block the progress of
[21:43] this draft on doing those things those
[21:46] suggesting in the chat we do something
[21:48] like a toy application profile like pidc
[21:51] personally I don't think that provides
[21:53] much value I'd rather invest our efforts
[21:55] in uh things that actually benefiting
[21:57] real uh quick work even if that or quick
[22:00] related work even if that work is not uh
[22:03] finished or done in the idea
[22:09] yet fully
[22:12] agree any further questions or
[22:16] um
[22:20] comments seems like maybe no thank you
[22:26] Robin so our next presentation is going
[22:28] to be be on multipath
[22:57] quick good
[22:59] y should we start okay uh hello everyone
[23:03] I'm M uh today Mia and I will introduce
[23:05] the explicit passid proposal for the
[23:08] multi pass extension next slide
[23:12] please okay first SL will introduce a
[23:15] little bit about the background about
[23:17] the key issue that this proposal want to
[23:18] solve and how does this explicit passid
[23:22] works and then we'll compare this
[23:24] proposal with the 06 old one we list
[23:28] these all these pros and cons and do
[23:30] some
[23:31] comparision and then about these
[23:33] interrup reports for the new proposal
[23:36] and for the last part we will introduce
[23:39] and this all these open issues and make
[23:42] some
[23:43] discussion next slide
[23:47] please yeah uh at first I'd like to
[23:50] introduce a little bit about this key
[23:52] issue in case people has for forgotten
[23:54] this uh this is from the last uh ITF the
[23:58] key problem is that the last D6 approach
[24:01] is using an identifier for the pass
[24:05] which doesn't have the same lifetime as
[24:07] the network pass and and Martin has the
[24:12] first point out that we should use a
[24:14] separate pass ID from uh from from the
[24:18] connection IDs and a big sense for his
[24:21] great idea so what we are trying to
[24:24] introduce here is to introduce an
[24:27] explicit pass
[24:29] that stays constant even if the C ID on
[24:31] a path
[24:33] changes so next slide
[24:37] please so for this part I I try to
[24:40] introduce more details for how does this
[24:44] explicit pass ID
[24:46] Works uh I'd like to introduce it in
[24:49] three key Parts the first part is for
[24:52] the pass management it's much more
[24:55] straightforward we just use this
[24:57] explicit pass identifier for each pass
[25:00] in a connection and each it we we also
[25:05] use use it in all the multipass control
[25:09] frames such as for the pass available
[25:12] and pass abandon to address a
[25:16] pass and also we need some more CID
[25:20] management control
[25:24] frames we add this MP new CID frames
[25:28] which ties the CID to a pass ID and the
[25:31] CID sequence number is
[25:33] increasing under per pass and we also
[25:37] add this MP retire connection ID frame
[25:40] as a pair and also the retire PR 2 field
[25:46] is under
[25:48] prass and that's a difference uh from
[25:52] the
[25:53] fc9000 and the thir part is for the
[25:56] packing number space the packing number
[25:58] space now is bonded to the pass ID and
[26:02] it will remain stable when the C
[26:05] rotation
[26:07] happens and we don't need to uh we don't
[26:10] need to force a force a tradeoff between
[26:15] the performance and uh and the the and
[26:20] the packing number changes packing
[26:22] number space
[26:23] changes the next side
[26:27] please
[26:30] um for for this for this part i' like to
[26:33] try to compare with the old version and
[26:36] then list all these pros and cons here
[26:39] uh for the first part for pass
[26:41] management as pass ID the new proposal
[26:45] has a prom here uh because it's quite
[26:48] straightforward it introduced link
[26:51] between incoming package and the pass is
[26:54] unambitious you always know you always
[26:57] knows which
[26:58] which pass does this packet belong
[27:01] to but the old draft it need you need to
[27:05] treat the situations when C rotation and
[27:08] N binding happens and it will have
[27:11] ambitious situations for the second part
[27:14] for the C management uh you really need
[27:17] to do more things to maintain the Cs per
[27:21] pass and to manage the CID and the
[27:24] passid lifetime you need to retire all
[27:27] the asso ATC IDs when the pass ID is
[27:31] abandoned and the old one is just the
[27:34] same as the FC 9000 but I want to
[27:37] explain more here the new new proposal
[27:41] is fully compatible with the IFC 9,000
[27:46] because you can you always use the new
[27:49] CID frame and the retire C frame for the
[27:53] initial pass the initial pass has the
[27:56] pass identifier as
[27:59] zero the third part is for the packing
[28:02] packing number State for the loss
[28:05] recovery and conest control for this
[28:08] part explicit pass pass ID has a br here
[28:13] for the loss recovery and congest
[28:15] control can re can rely on single
[28:18] sequence number space for the duration
[28:20] of the pass it remain the same for the
[28:24] past whole
[28:25] lifetime and the old draft has a CR here
[28:28] because C rotation will triggers the
[28:33] change of this packing number space and
[28:35] you need to face a tradeoff between the
[28:38] performance or more complexity in your
[28:42] code so basically it seems we have two
[28:45] point to one point here and I like to
[28:48] introduce a little bit more about
[28:50] Hardware offloading I'm not a a expert
[28:53] for Hardware offloading but I've I have
[28:57] discussed this question with someone who
[28:59] is and uh Eric has told me that for all
[29:04] these two
[29:05] proposals from the hardware perspective
[29:09] uh they just share the same cost for
[29:12] multipass Hardware offloading because
[29:16] they all need to use the pass identifier
[29:19] to mix into the nons when when you use
[29:24] Hardware offloading with smart pass so
[29:27] they are just exactly the same
[29:31] cost next slide
[29:34] please okay for this part uh we had
[29:37] already have some interrup reports from
[29:40] the hackerone I'm very glad to see that
[29:44] even if this pro has has not been merged
[29:47] uh several implementations has already
[29:51] tested implemented in in their codes uh
[29:54] we have had this nice interrupt table uh
[29:58] we have xqu pqu RAs and
[30:02] creature and uh we we are very glad to
[30:06] find out that explicit PID management
[30:10] works quite well and you in the new
[30:12] version we have clearer logic and we
[30:16] reduce the code
[30:20] complexity yeah next slides
[30:24] please so the next big question would be
[30:27] do do we want to merge this new pro pro
[30:40] request yeah we're going to do some
[30:41] discussion first before we do any sort
[30:43] of uh asking a question so
[30:46] kazuo uh I appreciate the efforts spent
[30:50] by the authors and the people uh for did
[30:52] the interrupt I think the pro request
[30:55] produces the cost of I mean introduce
[30:58] the cost of retaining multiple c number
[31:00] spaces and it's a pain for me that said
[31:02] I understand that this is a trade-off
[31:04] that I can live with I think we have
[31:07] come to a point that uh we should just
[31:09] merge this PR and solve the remaining
[31:12] issues that might exist in the new
[31:20] approach Alexander gini uh Cloud flare
[31:23] um let's just merge
[31:26] it
[31:31] uh Jan Angar fastly I haven't read the
[31:33] pr so I'm not going to say merge it but
[31:35] generally speaking this direction seems
[31:37] to make sense to me I just had one
[31:39] observation when you are separating the
[31:43] uh packet number spaces you don't
[31:46] actually need to separate it by path ID
[31:49] because all you really care about is
[31:51] that the packet numbers are
[31:53] monotonically increasing from the sender
[31:55] perspective because ultimately the
[31:58] packet numbers just have to maintain
[32:00] send order are you proposing one packet
[32:02] number space for everything no no uh
[32:04] that's the proposal we had like NOS go
[32:06] I'm not I'm not saying that you should
[32:09] not use path ID I'm saying you don't
[32:11] have to uh uh separate all the packet
[32:13] number spaces as well because you don't
[32:15] need to you can use the path ID for
[32:17] everything that you do for condition
[32:18] control uh context for various things
[32:22] but um we don't have to spend a lot of
[32:25] time here talking about that it's a very
[32:26] minor issue but it just seems to me that
[32:28] you don't have to actually separate out
[32:29] the packet number spaces I like the
[32:31] explicit path ID for what it's worth I
[32:33] think it's the right direction to
[32:36] go Magnus won Erikson yeah also
[32:40] supporting explicit path ID let's go
[32:42] forward with that Etc
[32:46] so
[32:52] chrisan yes I mean uh obviously uh I I
[32:56] had mixed feeling at the beginning I
[32:58] would really like to have just a single
[33:00] number space like Jenna was mentioning
[33:03] but uh the truth is that you cannot do
[33:05] that with the new proposal and the
[33:07] reason you cannot do that is that um you
[33:11] cannot make that an implementation
[33:13] option because the uh when you receive a
[33:17] new pass you have a new packet that is
[33:20] arriving and you have to predict the
[33:22] 64bit number of the packet
[33:26] number and
[33:28] if the number was shared with different
[33:30] paths then you would get in a mess of
[33:34] guessing wrong and and not encryp and
[33:36] not decrypting
[33:38] correctly so jna is well as simple as
[33:42] Mia said we add actually that proposal
[33:46] back two years ago I was the prop of
[33:50] that proposal I like that proposal but
[33:52] it
[33:53] requires if you go down that R hole it
[33:57] requires a lot more code than the comp
[33:59] than the than the current
[34:01] proposal so I would say let's merge it
[34:04] and and go and and simplify it's one of
[34:07] the word not
[34:09] taken Janna J Angar I feel like I uh I'm
[34:14] I'm triggering people here by saying
[34:16] this so I'll I'll I'm not going to walk
[34:18] it back but I'm happy to take it offline
[34:20] my point here is that when you're
[34:22] talking about single packet number
[34:23] spaces only what you're using is the
[34:26] packet number as the key to to figure
[34:28] out what path you're going on whereas
[34:30] here I I I understand I understand J I
[34:34] actually have that implemented in my
[34:36] code no no no I'm
[34:38] notr hang on let me finish let me finish
[34:40] let me finish what I'm saying is use the
[34:42] path ID as the key as you're doing and
[34:45] an optimization is you don't need to use
[34:47] packet number spaces that are separate
[34:49] for congestion control or for loss
[34:50] recovery purposes because really that
[34:53] code is going to work should work uh one
[34:56] way or the other um again let's not r on
[34:59] this because yeah I mean if you want we
[35:03] can discuss that I have actually a full
[35:05] implementation of that it seems simple
[35:08] at the beginning it's get very very
[35:10] complex when you get into the details
[35:13] okay we had this on the table about two
[35:15] years ago and we made a decision that we
[35:17] go for mulp number I know what was on
[35:18] the table two years ago what I'm
[35:19] suggesting is different than that but
[35:21] like I said that's the last word I'm
[35:23] going to say on this we can talk about
[35:25] this offline
[35:26] thanks so we we're hearing um some
[35:29] support in the room we want to as chairs
[35:31] take a show of hands just so we can get
[35:34] a feel in case people don't want to be
[35:35] more vocal um so we're going to run that
[35:38] poll now um if you're not yet signed in
[35:42] please do so in the five seconds I'm
[35:45] going to take The Pusher button um and
[35:48] get
[35:51] prepared so the question we're asking
[35:53] here is should the authors merge this PR
[35:55] 292 um to go forward with with the
[35:57] explicit path IDs design uh you can see
[36:01] the options there yes no no opinion um
[36:05] we have a high number of participants
[36:09] and not many people voting
[36:14] um
[36:18] so okay we have some hands on the stage
[36:21] as well from the presenters who aren't
[36:23] currently logged
[36:26] in
[36:31] okay I think we can I think we can stop
[36:33] there
[36:36] um where do I
[36:38] stop there we go just so for the record
[36:42] there uh we had uh 23 yes hands three no
[36:48] and 14 we had 23 yes hands three no and
[36:52] 14 no opinions so I'm taking that as a
[36:55] strong signal in support of going
[36:57] forward with merging this PR we will
[36:59] take that to the list to confirm
[37:00] consensus um but otherwise uh yeah let
[37:04] so thank you for the the strong
[37:06] indicators here that's giving
[37:08] us uh a great path forward uh and means
[37:11] we can actually move on to the next
[37:13] discussion topics as our
[37:16] slides perfect um so we were hoping for
[37:19] this and that's why we prepared some um
[37:21] open issues to discuss we currently have
[37:23] 46 open issues some of them will go away
[37:26] because they might not apply to this new
[37:27] PR anymore and there's also a little bit
[37:29] redundancy in the issues so we need to
[37:31] clean that up um but we brought a couple
[37:34] of issues for today that um are
[37:36] connected to the new pass explicit pass
[37:38] ID proposal that we thought might be
[37:40] worth discussing so this slide is the
[37:42] easy one that's a proposals where we
[37:44] believe we have already a way forward so
[37:47] uh there was a discussion that a pass IB
[37:48] should not be reused and there seems to
[37:50] be agreement so we have a PR for that
[37:52] and yeah I saw that kasu has a different
[37:54] opinion so go to the
[37:56] mic
[37:59] right so the problem state statement of
[38:02] 292 was that not having a continuity of
[38:06] pocket numbers across multiple
[38:08] connection IDs is a pain and we agree
[38:11] that we solve that but now that we have
[38:15] pass Ides indicating which to which
[38:18] pcket number space each pocket belongs
[38:20] to the question becomes if we need a way
[38:23] of issuing and returing new pass idas
[38:27] and to me it seems like an unnecessary
[38:29] complication I mean if you have the
[38:32] opportunity to open for pass you can
[38:34] just use four you can just have four
[38:36] pocket number spaces across the entire
[38:38] last of the
[38:40] connection and then natur binding would
[38:42] look exactly like an explicit migration
[38:45] there would be no difference and I think
[38:47] that will be a win to
[38:49] use uh just to reuse pass ities across
[38:52] the
[38:55] migrations I mean yeah you you opened
[38:57] this issue yesterday so I didn't think
[38:59] about it so I think we just need more
[39:01] discussion on
[39:08] it um okay so yeah please comment on
[39:10] this issue either um this one or kasu is
[39:13] new issue I think they are connected um
[39:16] and as I said we will clean up the
[39:17] issues after this meeting somehow okay
[39:21] um then also hopefully an easy one uh is
[39:24] 317 so uh if a preferred pass is
[39:27] indicated during the handshake then this
[39:29] prepared pass should get a new pass ID
[39:32] uh the pass so the the original part is
[39:34] pass ID zero and the preferred pass
[39:36] would be pass id1 this is just something
[39:38] we ommitted in the current PR and
[39:40] straightforward
[39:43] hopefully okay go thank you um and then
[39:48] there was a point about the Passad is
[39:49] still not fully clear uh we already I
[39:52] think we also have a PR on this one but
[39:53] there's still discussion going on um it
[39:56] seems like people think think it is
[39:58] valuable to have to use the same pass ID
[40:00] on both sides so the client and server
[40:02] both know it's you know it's pass number
[40:04] whatever three um but also that needs
[40:06] some coordination so um that you when
[40:09] you open a new pass you actually pick a
[40:10] pass IB that wasn't taken yet and you
[40:12] don't have conflicts um so I think we
[40:14] have agreement that we want to have the
[40:16] same pass ID on both sides but then
[40:18] there are some additional stuff we need
[40:19] to discuss which is on the next
[40:23] slide okay and uh this is exactly as
[40:26] soon um as you have uh if is if if you
[40:30] have like one pass ID space basically um
[40:32] the easy solution is if we only allow
[40:34] the client to open new passes then you
[40:36] don't need to have any coordination but
[40:38] if you want to also allow the service to
[40:40] open new pass and this is a very old
[40:41] issue as you can see we never decided on
[40:43] this one um then this means you actually
[40:45] have to handle it separately and the
[40:47] proposal here is to split the pass ID
[40:50] space and as similar as we did with
[40:51] streams and the client uses the even
[40:54] numbers and the server uses the odd
[40:55] numbers but then that also means that
[40:57] all the kind of settings you provide
[40:59] about how many um passes you allowed to
[41:01] open and whatever you have to do
[41:03] basically on a server and client site um
[41:06] so yeah discussion of this one is is
[41:09] ongoing uh I don't think we have a
[41:10] decision on this one yet any thoughts
[41:13] comments
[41:18] ideas on clarification question is there
[41:21] H anybody who needs this feature for the
[41:25] use cases or is it something you know
[41:27] that's floating like a nice
[41:30] have I think people believe if you use
[41:33] the same path ID on both sides it's
[41:35] really kind of um it's less error prone
[41:40] in your implementation we have seen a
[41:41] couple of cases where the Implement did
[41:44] get it wrong right because you always at
[41:46] each frame you have to indicate which
[41:47] pass you're talking about and then it's
[41:49] always not clear are you using your pass
[41:50] ID are you using the pass ID of the
[41:52] other side and so if you just just use
[41:54] the same number then all of that you
[41:56] know comp lexity goes away yeah right so
[42:00] I think my question is around if we want
[42:02] to if we need the servers okay uh I
[42:06] think so this one um RSC 9000 doesn't
[42:09] allow the server to open um connections
[42:12] but it felt like and this is back in the
[42:14] old issue it felt like the restrictions
[42:16] we we had to make this decision are
[42:18] actually going away with multiplus so
[42:19] there would be an opportunity to just do
[42:21] it um on the other hand there's not
[42:24] probably a big use case for it because
[42:25] the server can always tell the client an
[42:27] IP address and tell the client to open
[42:29] the path right so that is like if you
[42:30] really need this you can like handle it
[42:32] this way so right right thank you so I
[42:35] think my comment would be that unless
[42:37] there's a strong use case I I'd prefer
[42:39] just not doing it or pun the decision to
[42:41] a later
[42:46] moment I mean I I see your point but
[42:49] it's also it's and why so I mean it's
[42:53] not that much complexity right it's just
[42:55] like if you can have it
[43:02] yes I mean to to answer the the same
[43:05] question I did study it and the problem
[43:09] is that if we do not do the odd even
[43:13] split
[43:15] now it will be impossible to introduce
[43:19] it as an extension
[43:21] letter because there will be a conflict
[43:24] and we have to have all kinds of
[43:27] duplication of frames and things like
[43:29] that so what is possible there is a
[43:33] potential compromise and the potential
[43:35] compromise say okay we are not going to
[43:38] authorize server initiated pass now but
[43:41] we are only going to use the even
[43:43] numbers for now with the exception of
[43:46] one for the server preferred address and
[43:49] if we do that then we do not increase
[43:53] the complexity of the current
[43:55] implementation but we leave the door
[43:57] open for the server initiated connection
[44:00] at a at a later
[44:03] stage yeah I really like this Christian
[44:06] you can write a
[44:07] PR I I I did write a text in in another
[44:10] issue but I can write
[44:18] PR Magnus yes Magnus W I think the use
[44:22] case I have for Ser initiated is the
[44:25] fact that if you're going to try to use
[44:27] for replacing STP and uh which actually
[44:30] has uh dual actually can use uh multiple
[44:36] path concurrently for and in both day
[44:39] and it's actually in a scenario like in
[44:40] a mobile network where you actually the
[44:42] client server role is not clear or
[44:45] saying it's someone need to do what's
[44:48] it's suitable for them now based on what
[44:50] information they have so um in that
[44:53] scenario it makes sense for opening up
[44:55] The Path potentially from the one that
[44:57] currently has a server role because
[44:59] they're basically equal that I think
[45:01] also applies to peer-to-peer application
[45:03] to a certain degree so and you may need
[45:07] to be able to open from the server side
[45:09] in those scenarios to be able to open a
[45:11] new path otherwise you would not get the
[45:14] right response based on middle boxes so
[45:17] I totally see that it's useful in the
[45:18] scenario but just double checking so you
[45:20] think the solution that the server would
[45:22] be able to tell the client where to open
[45:25] the PA like give an IP address to the
[45:26] client and then the client opens the
[45:27] path anyway that is not doable in the
[45:30] mobile network scenario in the mobile
[45:31] network scenario it's I would guess it's
[45:33] doable it would potentially introduce a
[45:35] latency and especially if you actually
[45:37] have a hard failure case if you drop
[45:39] both paths uh or say both paths if you
[45:43] have two paths both fails you iny cover
[45:46] up uh it might be delaying things uh I
[45:50] think it's more important in the like
[45:52] peer-to-peer quick and multip puff quick
[45:54] scenarios um where where it's a single
[45:58] path the single interface might be Ned
[46:01] and you need to be able and while you
[46:02] have a listening Port somewhere else
[46:07] so um I'll I'll speak as an individual I
[46:10] I haven't looked at this issue closely
[46:12] but here in kazuo speak about we need
[46:15] some use cases the thing that sprung to
[46:17] my mind is to allow to allow this as an
[46:19] extension and effectively in my head was
[46:21] what Christine said so um I think maybe
[46:26] allowing for that that and if people are
[46:28] really interested and think they have
[46:29] those user cases to go and work on it as
[46:31] a separate extension that maybe you know
[46:34] maybe if we get to it before multipath
[46:37] is done and we convince ourselves to
[46:39] merge it back in that could be a way to
[46:40] progress these things but that's just as
[46:43] an
[46:46] individual I think this is a good way
[46:48] forward because I don't think this is
[46:49] blocking anything else so we can also we
[46:51] should decide before we publish the RC
[46:53] but we can decide at the very end and
[46:54] and just to say in in the chat this
[46:56] seems to be some support for Christian
[46:58] suggestion
[47:02] too just wanted to point out that this
[47:05] would be potentially interesting for my
[47:07] net R versal uh extension that I
[47:09] presented last time but I have to admit
[47:12] I have to think about this issue a
[47:14] little bit more before forming an
[47:16] opinion you said it's interesting for it
[47:18] or what was the work what was it word
[47:21] you you used
[47:24] interesting so it would make it easy or
[47:27] harder
[47:29] easier maybe I need to think about this
[47:35] more uh what I I mean I would definitely
[47:38] like this if if we're g to implement
[47:41] multipath I mean this is close to half
[47:44] the value of a lot of the cases that I
[47:46] would actually want to use multipath
[47:47] right because like quick connection
[47:49] migration works pretty darn well on the
[47:51] public internet and solves an awful lot
[47:52] of use cases and so the like in order to
[47:55] do the work of doing multipath I would
[47:57] definitely prefer for either here to be
[47:59] able to change change paths like fairly
[48:02] St I mean I it didn't it took me like 10
[48:04] seconds to come with like three use
[48:05] cases so I'm like I'm sure I can come up
[48:07] with more perfect that's good input yeah
[48:11] I have another another reason to do it
[48:14] and the other reason to do it is that
[48:16] the alternative which is to have a new
[48:19] frame by which the server TS the client
[48:23] hey you can join me at IP address so and
[48:25] so
[48:27] has a slight issue that it becomes very
[48:30] easy to build a request fory attack with
[48:35] that because I mean a malevolent server
[48:39] can send the attack of a Target by that
[48:43] mechanism and the client just PS the
[48:46] target after that so the um having the
[48:51] server s its own packet doesn't have the
[48:55] same issue I mean if there an attack the
[48:57] server can always attack someone but it
[48:59] will be attacked from the server it
[49:01] won't be attacked from the client so I
[49:04] think uh the more I think of it the more
[49:06] I say we we should just put that maybe
[49:10] we have a state saying don't use it now
[49:12] until we have another extension but I
[49:16] think we should go for the even OD split
[49:18] and and put it in the
[49:21] draft okay perfect that sounds really
[49:23] good to me okay with with ch on I think
[49:26] think what I'm what I'm hearing is
[49:28] someone should go and make a PR at least
[49:31] so that the the group can give a bit
[49:33] more consideration into the the scope of
[49:35] impact on the spec itself um and that
[49:38] yeah there's maybe a little bit more
[49:39] here to
[49:41] see manifest rather than just talk about
[49:45] it yeah I think we have something like
[49:47] half a PR or something it's a little bit
[49:48] of Miss we will clean it up okay uh okay
[49:51] have one more slide if we have time we
[49:52] do have
[49:54] time um so this is about retirement this
[49:57] is still under discussion and there was
[49:59] a question uh how you actually retire a
[50:01] pass we already have the pass abandon
[50:03] frame but then the question is what do
[50:05] you do with all the connection uh IDs
[50:08] and the proposal at the table is at the
[50:10] moment that we don't imple uh don't
[50:12] introduce a new like past retire frame
[50:15] but we actually just use the ponm frame
[50:18] and then after you have AB bond in the
[50:20] pass you should also retire all the
[50:22] connection IDs of that path or did I
[50:27] miss
[50:28] anything yeah okay um yes M yeah H the
[50:34] the question is whether this is implicit
[50:36] or
[50:37] explicit exactly so any thoughts
[50:41] opinions
[50:46] Feelings by by experience I would say
[50:48] explicit implicit is better in that
[50:52] case that is if you do if you abandon a
[50:54] past you have def fact to abandon all
[50:56] the cids for that
[50:58] pass so yeah there is a lot of
[51:01] discussion on this issue already and I
[51:02] think that's a direction that people
[51:05] probably
[51:06] prefer um but this was one of the bigger
[51:09] issues so I wanted to bring it up
[51:11] here but you can also comment on the
[51:13] issue if you have any additional
[51:15] stop
[51:17] AR yeah I'm in favor of doing it
[51:19] implicitly not sending the frames as
[51:21] nice and then you also don't have to
[51:23] deal with the with the weird conditions
[51:25] where the path is B but you never
[51:28] receive the retire frames so what do you
[51:30] do then um it's just
[51:36] easier what else goes uh at the same
[51:40] time as the connection IDs are there
[51:43] other thing other resources that are
[51:44] being cleaned up at this
[51:47] point no nothing I mean the packet
[51:50] number space handling yeah so there's a
[51:52] bunch of stuff that you drop when the
[51:53] path goes away just drop it all and and
[51:56] that's just neater and cleaner
[52:01] yeah K hul just wanted to point out that
[52:03] this is one of the complications due to
[52:06] us having the concept of uh introducing
[52:09] New Path IDs and ruring them if we move
[52:12] to St path is then we don't have this
[52:14] issue at
[52:16] all yep another interconnected
[52:19] issue so um
[52:22] Magnus uh I think it's fine with
[52:25] implicit work I think it one of the
[52:27] things I stumbl on this during the
[52:29] discussion around this we had in the
[52:31] hackaton ETC has to do with more with
[52:34] the fact
[52:35] of uh is this symmetric Etc with the
[52:38] path abandon are you forcing the other
[52:39] point to also close the reverse
[52:42] path um so that's um it's more connected
[52:47] to that I think it's some cleanup needed
[52:49] there but yes so there's definitely some
[52:51] clarifications needed
[52:55] yes
[52:57] I believe that was my last slide right
[53:00] yeah I think so um yeah thank you for
[53:04] the good discussion everyone this is
[53:06] really helpful for moving forward the
[53:08] multipath work um and yeah thank you to
[53:11] the the authors thank
[53:15] [Music]
[53:19] you and our next topic is uh act
[53:23] frequency so MIA you can't go very far
[53:26] I'm sorry
[53:28] I at least saved you from getting all
[53:29] the way off the stage I thought Ian is
[53:31] doing some PowerPoint Bingo here but I
[53:34] guess
[53:39] not
[53:44] whatever um so the EG frequency draft we
[53:46] had a last call we uh got a lot of
[53:49] feedback thank you and we have a new
[53:51] version next slide should just stay here
[53:54] I
[53:55] guess thank
[53:57] you um yeah the working group last call
[54:00] concluded in November already um but we
[54:02] only submitted the next version recently
[54:04] we got uh quite a bit of feedback but we
[54:07] closed all open issues um thank you to
[54:11] for everybody Who provided feedback I
[54:12] forgot to update the acknowledgement
[54:14] section but we will do this in the next
[54:15] version next
[54:18] slide so um there was a bunch of
[54:21] editorial stuff and I'm not talking
[54:23] about this here and then there were a
[54:25] few stuff few things which were
[54:27] clarifications uh which I want to
[54:29] quickly uh name here especially the
[54:31] first two um there was still a little
[54:34] bit discussion about how often you
[54:35] should send this frames and uh we
[54:38] provide a little bit of non-normative
[54:40] guidance about it uh we updated the text
[54:42] to make it more clear what we mean um
[54:44] but I think there's still different
[54:46] opinion in the working group about how
[54:47] often you should send things and how on
[54:51] what's kind of the minimum amount of X
[54:53] you should require which is like kind of
[54:55] we a little bit recommending at least
[54:56] one act per round of time um but like
[54:59] for example gory sent another email to
[55:01] the mailing list um so we believe what
[55:04] we have in the draft right now has rough
[55:06] consensus um but just to name that that
[55:08] there was a little bit more
[55:10] discussion
[55:13] um yeah I don't think anything else that
[55:16] needs to be mentioned here next
[55:19] slide um I also wanted to point out some
[55:22] of the issues that we closed without any
[55:25] actions
[55:27] um especially I want to point out the
[55:29] one about um how CES are handled so uh
[55:34] or ecn is handled so what we're trying
[55:36] to follow here is basically what is kind
[55:38] of done in the TCP accurate accurate ecn
[55:42] draft um but there was recently an email
[55:44] on the mailing list from ingamar
[55:46] actually proposing that we could do
[55:47] smarter than that um we kind of
[55:50] basically say we leave this to like
[55:53] future Orin or whatever and didn't
[55:54] address it any further here but you know
[55:57] just to make you aware the other thing
[55:59] that we also didn't further address is
[56:00] what happens after an idle time we have
[56:02] like we added only a few versions ago we
[56:04] edited send and saying after an idle
[56:05] time you should soon send an egg uh but
[56:08] we don't specify this normatively or say
[56:10] anything further what soon means or
[56:12] whatever because we feel this is not an
[56:14] issue for this draft I mean it's of
[56:16] course impacted by this but it's more
[56:17] General issue so people want to work in
[56:19] that space probably um you know they
[56:21] should do it in
[56:23] future that's on this slide and then the
[56:26] last one is actually where I just wanted
[56:27] to make sure we're really doing the
[56:29] right thing because we had two issues
[56:30] that did impact the normative parts of
[56:32] the draft um so one was uh a
[56:37] clarification um
[56:39] about uh Max act
[56:43] delay and we we changed a must to a
[56:45] should here and that is just um
[56:47] basically I I would say that was an
[56:49] oversight because we did change a little
[56:51] bit the language and the whole draft
[56:52] saying this is not something we can
[56:54] enforce this is just like a guidance we
[56:55] give and then the sender you know can
[56:57] still do whatever they want to do
[56:58] because that's the truth and so we
[57:00] changed this must to a should and then
[57:03] the other normative part here was also
[57:04] that we changed the
[57:06] error uh type uh when you when you
[57:09] receive an invalid value because I think
[57:11] that was just the wrong error type um
[57:13] the frame encoding error is also used in
[57:16] RFC 9000 for a similar case so we just
[57:19] you know picked the hopefully right type
[57:20] now so that's the first issue and the
[57:23] second
[57:24] issue um there was a comment saying that
[57:28] uh or like there was a sentence saying
[57:29] in the draft that basically this this
[57:32] extension only changes something um if
[57:35] after you received the
[57:37] first frame of any type I guess and that
[57:41] wasn't fully true because we actually
[57:42] changed the ecn handling and so the
[57:45] solution I went here is to actually keep
[57:47] this statement and say actually nothing
[57:48] changes as long as you don't send an e
[57:51] frequency update draft um which also
[57:55] means that as long as your electing
[57:57] threshold is one you do the same as is
[58:00] written in RC 9000 and only if you send
[58:02] a new value you actually change your ecn
[58:05] handling and you know this is like it's
[58:08] kind of a little bit random you can do
[58:09] one or the other uh I just felt we need
[58:13] to clarify it and I just felt this is
[58:14] kind of a little bit the nicer solution
[58:16] because it actually doesn't change
[58:18] anything as long as you don't send an
[58:19] agency
[58:20] draft and that's actually I hope Ian you
[58:24] have read this PR but I don't know know
[58:26] if if this is the best solution for
[58:31] you yeah it's fine okay so yeah um sorry
[58:35] joh I just had a question do you just
[58:38] want to switch between CE and nonce or
[58:40] any change in the ecn bits because if
[58:41] you're path change from bleaching to
[58:43] non- bleaching for instance you probably
[58:45] want to know that too yes sorry can you
[58:47] I saying do you want to just have the
[58:49] change from CE to not CE or do you want
[58:51] to have any change in the ecn bits
[58:54] because like if you if you had a pth
[58:55] change from bleaching to non- bleaching
[58:56] presumably you'd want to know that as
[58:58] well
[59:00] right so if you if the previous packet
[59:03] was Zer one and next packet is 0 Z
[59:06] because something on the path has
[59:07] started baching ecn the question is only
[59:10] when do you want to know it right do you
[59:11] want to know it immediately or do you
[59:12] want to know it when you anyway send
[59:14] your next EG and so because this is a
[59:16] congestion signal we want to know it as
[59:18] soon as possible everything else is
[59:20] doesn't matter that
[59:22] much okay I guess so I mean again this
[59:25] is just with following just what's kind
[59:27] of specified in the uan draft at this
[59:32] point uh Lucas here speaking as an Eric
[59:35] code Enthusiast um I can't remember if I
[59:38] raised this issue or not you did
[59:40] yes uh yeah I mean I think error codes
[59:44] are important from an operational
[59:45] perspective but also they don't really
[59:47] matter so from I don't think it's a
[59:48] normative change really really anything
[59:51] cares about the the must or should is is
[59:53] as discussed I fully in suppor of um so
[59:56] yeah I just wanted to make that clear
[59:59] thanks I mean who knows what actually is
[60:01] normative and what's
[60:03] not no but I mean this is just what I
[60:05] wanted to flag because I think these are
[60:06] really the only things that kind of
[60:07] impact anything that is close to
[60:09] normative but if everybody likes it then
[60:11] we can just run another working grou
[60:12] last call yeah uh Ian in the I just
[60:15] wanted to give a huge amount of thanks
[60:17] to M for like pounding through a ton of
[60:19] changes to get this over the line um you
[60:23] know I I I enormously appreciate and as
[60:26] the quick working group um and uh yeah I
[60:29] think I mean I looked through a lot of
[60:31] text after the effect and and it all
[60:33] looked really good and in many cases I'm
[60:35] like Oh I thought that's what it said
[60:36] already oops maybe not so I I think a
[60:39] lot of the changes were of the form in
[60:41] my mind that's what I you know that's
[60:43] kind of what the authors I think
[60:44] intended I think you did a wonderful job
[60:47] um so anyway thank
[60:53] you yes thank you Maria um this is our
[60:56] longest living document in the in the
[60:57] working group this at this point so uh
[60:59] happy to see some progress on it and
[61:02] hopefully we'll we'll go forward with
[61:04] running another working group last call
[61:05] and that will be an actual working group
[61:08] last call rather than a uh first last
[61:15] call the last last call um so now we are
[61:20] going to move on to our next topic um
[61:23] which is uh Martin and talking about
[61:26] uh we're calling this quick security
[61:29] considerations I think it's what we're
[61:31] talking about AR are you okay coming
[61:46] [Music]
[61:51] up yeah in the last last couple of month
[61:54] we dealt
[61:56] with closer to the mic okay why is that
[61:59] so loud um M's not very
[62:04] tall so we we we we dealt with um two
[62:07] resource ex exhaustion attacks against
[62:10] the quick protocol itself it turned out
[62:13] because if you follow the RFC
[62:15] 9000 um by the letter then your
[62:19] implementation would be vulnerable to
[62:20] these
[62:22] attacks and a lot of implementations
[62:25] were
[62:26] affected and um we've we've now made
[62:31] these um made these attacks public and
[62:35] implementations have been fixed so now
[62:37] is a good time to talk about it and what
[62:39] we can learn from this for the design of
[62:43] new protocols uh next
[62:47] slide so this is actually the second
[62:49] attack um that we discovered but I'd
[62:53] like to discuss it first because it is
[62:55] very in instructive so let's take a look
[62:59] at how quick manages connection IDs
[63:02] during the
[63:03] handshake um one perer let's say the
[63:05] server is setting a limit for how many
[63:08] connection IDs can be um it is willing
[63:12] to store at the same time and let's say
[63:15] the limit is three so now the client is
[63:17] allowed to send fre connection IDs right
[63:20] and it does that since connection ID
[63:22] zero connection id1 connection ID 2
[63:27] however this is not where the story ends
[63:30] the client can send new connection IDs
[63:33] now as long as it retires connection IDs
[63:37] that it issued pre previously so here it
[63:40] sends connection ID number three and it
[63:43] says retire all connection IDs before
[63:45] two which means connection IDs one and
[63:48] two are now not active anymore this is
[63:51] very useful if you're running in a load
[63:53] balancer setup and your load Balan it
[63:55] like a config rotation and you can you
[63:59] know that the first two connection IDs
[64:02] will soon not be be routed anymore so
[64:05] you can tell your Pier like yeah there
[64:07] there like a small period of time where
[64:08] I'm still able to receive packets um
[64:11] with connection ID zero and one but soon
[64:14] soon they will go away so please retire
[64:17] these connection IDs and the server is
[64:19] then um required by the by the protocol
[64:23] to uh send retire connection ID frames
[64:26] for connection at Z and
[64:29] one so why is this a
[64:31] problem next
[64:34] slide and it's a problem because of
[64:38] congestion control
[64:39] basically so quick reminder here when a
[64:43] packet loss happens you're supposed to
[64:45] reduce your congestion window usually by
[64:48] a factor of like a half or something in
[64:51] in the order of
[64:52] magnitude and if um repeated packet loss
[64:55] occur
[64:57] then your congestion window is collapsed
[64:59] again and again to a minimum value of
[65:01] two two packets per round trip
[65:05] time so what an attacker can do now the
[65:08] attacker can just receive your packets
[65:12] but claim that let's say every other
[65:14] packet was actually
[65:17] lost so now if a client does that the
[65:20] server will very soon arrive at a
[65:21] congestion window of just two packets
[65:24] can basically not send any anything
[65:26] anymore it gets even worse the rtt
[65:29] measurement is also controlled by the
[65:32] client a malicious client could say like
[65:34] yeah I'm I'm acknowledging packets but
[65:37] I'm not acknowledging any any packets
[65:39] that I received in the last five seconds
[65:41] I'm only acknowledging packets that re
[65:43] received five seconds
[65:46] ago this will make the server think that
[65:48] the rtt is 5 Seconds plus the actual RTG
[65:52] so now the server is in a situation
[65:54] where it can send two packets every 5
[65:58] seconds and now the client just keeps on
[66:01] sending um new connection ID frames that
[66:05] retire old connection IDs so the server
[66:08] is now building a cue of like I need to
[66:10] send this retire connection ID frame I
[66:11] need to send this connection retire
[66:13] connection ID frame and the client can
[66:15] just keep on going sending new
[66:17] connection ID frames until the server
[66:19] runs out of
[66:21] memory next
[66:24] slide
[66:27] so the question here is like why why did
[66:30] the flow control mechanism that we try
[66:32] to build here um not work
[66:36] and how is it different from other flow
[66:39] control mechanisms that we build in
[66:40] quick that work and that are not
[66:42] vulnerable to this kind of
[66:44] attack let's look at how quick Stream
[66:47] flow control
[66:48] works so the server declares on a new
[66:51] stream I'm willing to receive let's say
[66:54] 100 bytes
[66:56] this allows the the client to send
[66:58] stream frames that carry data up to this
[67:01] uh to this up offset so let's say
[67:03] clients sends two two stream frames that
[67:06] go go up to 100 and now it's blocked at
[67:08] 100 it can't send any more new data no
[67:11] matter what the client does it can send
[67:13] more until the server sends a maxstream
[67:16] data frame granting new granting new
[67:19] credit here up to 150 now the the client
[67:22] can send 50 more
[67:24] bites so
[67:26] we can see that the main difference is
[67:28] that it
[67:30] requires requires action from the server
[67:33] to unblock the client the client has has
[67:37] there's nothing the client can do on its
[67:39] own to free up this flow control
[67:43] credit next
[67:47] slide so the attack that we've that
[67:50] we've seen on the on the connection ID
[67:52] mechanism is kind of similar to the one
[67:57] that a lot of us had to deal with last
[67:59] year which is the http2 rapid reset
[68:04] attack in the quick reminder in the
[68:07] rapid reset attack in in hp2 and it does
[68:09] not apply to Quick um the client the
[68:14] server would tell the client you're
[68:15] allowed to open 100 streams at the same
[68:19] time um and the client could open these
[68:22] 100 streams but could could then reset
[68:24] the streams immediately
[68:26] freeing up the limit again and then open
[68:29] another 100 streams reset them
[68:31] immediately and do that over and over
[68:33] again no interaction from the from the
[68:35] server
[68:36] required so why is why is quick not not
[68:40] vulner vulnerable to this attack it's
[68:42] because back in 2017 we actually fixed
[68:45] that we fixed that for a completely
[68:47] different reason we didn't know about a
[68:49] rapid reset at this point we fixed that
[68:53] because um when you're dealing with a
[68:55] transport that's not that's not ordered
[68:58] but where packet reordering can occur
[69:00] the client and the server might not have
[69:02] the same um might not have the the
[69:06] consistent picture of which stream is
[69:08] open at any given
[69:10] time so it is basically impossible to um
[69:15] to do to do accurate accounting and to
[69:17] to close a connection when a client
[69:19] violates that limit because this
[69:21] violation of the limit could just be
[69:22] have been caused by packet reordering on
[69:24] the wire and you don't want that to kill
[69:25] your connection that's why we we
[69:28] switched Quicks
[69:30] um the way that we Grant stream stream
[69:33] limits in quick to advertising a stream
[69:36] a maximum stream ID now the server says
[69:39] like you're allowed to open all streams
[69:42] up to stream ID
[69:44] 100 and there's nothing the client can
[69:46] do to increase this limit until the
[69:48] server says like okay now you're allowed
[69:50] to open open all streams up to stream ID
[69:54] 120
[69:57] uh next
[70:00] slide so looking back at um at our
[70:04] specification process like let let's say
[70:06] let's say RFC 9000 was not yet a thing
[70:09] how how would a good solution for this
[70:11] look
[70:12] like and what we could have done is we
[70:15] could have introduced a Max connection
[70:17] ID
[70:18] frame the server could declare you you
[70:21] can send me all connection IDs up to
[70:23] sequence number 10
[70:26] and then at some later Point once some
[70:27] connection IDs are retired the server
[70:29] could say okay now you can send me
[70:31] connection IDs up to sequence number
[70:34] 15 would have been nice but that's not
[70:38] the world we're living in RFC 9,000 was
[70:41] shipped and is widely
[70:44] deployed so um this will be this will
[70:48] probably a hard be be a hard fix to make
[70:51] at at at this
[70:53] point the the way that most
[70:55] implementations have actually fixed
[70:58] fixed this vulnerability is that they
[71:00] now Implement um an explicit an explicit
[71:04] check for like how many how many retire
[71:07] connection IDs do I have queued at any
[71:09] point and the good thing is that retire
[71:11] connection ID frames are really really
[71:14] small they are just a few bytes and if
[71:17] you cue let's say a thousand of them for
[71:20] a connection you're not wasting a lot of
[71:23] memory the the problem only occurs if
[71:26] you're queuing like a million or a
[71:28] billion of them then you run out of
[71:30] memory on the other hand in on on on a
[71:34] connection where you're not under attack
[71:36] you should never be you should never be
[71:38] in the situation where you have a Quee
[71:40] of like a thousand retire connection ID
[71:42] frames before you can send out a single
[71:44] packet so it seems like if you just if
[71:47] you just Implement a check where you say
[71:48] like okay if I reach if I reach this
[71:51] absurdly High number I can just kill the
[71:53] connection and that works
[71:57] uh next
[71:58] slide so now we're coming to to the the
[72:01] second attack which is actually the one
[72:03] that we found first and um a very
[72:06] similar a very similar thing applies for
[72:09] Quick's path validation mechanism
[72:11] because the client can send a path
[72:13] challenge with with some some random
[72:16] data and then the server is is required
[72:19] by by RFC 9000 to send a path response
[72:22] and RFC 9000 says like you can't you
[72:24] can't drop this
[72:26] if you receive the path challenge you
[72:28] must send the path response and
[72:30] obviously the client can do can do the
[72:32] same tricks with a collapse the
[72:34] condition window inflate the rtt and uh
[72:37] the server starts building a queue of
[72:39] path
[72:40] responses the fix in this case is pretty
[72:45] easy because there's not really anything
[72:47] that breaks in the protocol if you if
[72:50] you drop a path challenge under those
[72:52] conditions um I mean the packet
[72:55] contained the path challenge could have
[72:56] been lost um so that's that's a way to
[73:01] to mitigate this attack technically
[73:04] you're not you're not RFC 9000 compliant
[73:06] anymore so maybe maybe we should do
[73:09] something about this and um use the or
[73:13] process to um to change RC
[73:19] 9000 uh Martin Duke Google um this kind
[73:22] of gave me Deja Vu because I I thought
[73:24] we
[73:26] argued this out uh when we were doing
[73:28] 9,000 um what's the new wrinkle is this
[73:31] CEST control bit and I'm not really I I
[73:33] guess I don't follow why that creates
[73:35] the problem but um this is issue 3509 in
[73:39] base drafts and the end result of that
[73:41] after much argument and like it's a very
[73:46] long thread but we put in some some text
[73:49] at the end of section 512 of 9000 about
[73:52] uh limiting the amount of retire
[73:54] connection ID that a that a endpoint has
[73:57] to track and you can that you should you
[74:00] you you you need not put store more than
[74:03] twice the connection ID limit in terms
[74:05] of like pending retire connection IDs
[74:08] and um you can throw an error called a
[74:10] connection ID limit error uh if if
[74:13] somebody somebody's doing something
[74:14] pathological about retiring connection
[74:15] IDs
[74:17] um but again like I me you know sitting
[74:21] here at the mic I I there is this this
[74:25] um CR control wrinkle but I I don't
[74:27] really see why that supersedes what was
[74:29] in the text yeah so it's not clear if
[74:31] this text applies to connection ideas
[74:33] that get retired um because of the the
[74:36] retire prior to field um I'm I'm I'm I'm
[74:40] not I don't recall if we discussed this
[74:42] issue before we introduce retire prior
[74:44] to or or afterwards um it was very late
[74:48] the fact juming in as note take right
[74:50] quickly can you repeat the issue number
[74:52] yes it is um 3509 in the base drafts
[74:56] repo thank you the fact that out of the
[74:59] 18 quick Stacks that we that we surveyed
[75:02] um 11 um if I recall correctly were
[75:06] vulnerable to this attack
[75:08] shows that apparently what we have in
[75:11] the RFC was not sufficient to prevent
[75:14] this vulnerability from occurring in the
[75:16] wild uh I mean it is advisory there are
[75:18] like shs and and Maze I mean you
[75:22] know so so like
[75:26] this prop this has been useful because
[75:28] uh people have not taken um some advice
[75:31] in security considerations and in the
[75:33] section so thank you for doing that um I
[75:36] I'm not convinced that 9,000 is broken
[75:39] in that respect I I mean personally if I
[75:42] remember my position as food fight I
[75:43] would prefer stronger language in it but
[75:45] I think there are the tools here for for
[75:47] servers to protect themselves
[75:50] thanks yeah there there's also the the
[75:53] catch all that at any point during the
[75:55] connection you can kill the
[76:00] connection Alexander gini Cloud Flair um
[76:04] I don't know that this is like an AATA
[76:07] situation where you change the RFC
[76:11] but like we we we've basically had the
[76:14] same problem in hcp2 right for the past
[76:17] like fiveish years we've been fixing a
[76:20] bunch of
[76:21] vulnerabilities and new implementations
[76:23] of htb2 that didn't go through that kind
[76:27] of were surprised about all of the
[76:29] additional changes that they were
[76:31] supposed to do that they didn't make
[76:32] because all of those all of that work
[76:35] wasn't really collected in any
[76:37] particular place um I wonder if this
[76:41] could be a case of either having a
[76:44] separate draft that says these are all
[76:47] the gas that you need to you know be
[76:50] aware of or even like a sort of RFC 9000
[76:55] B with more clear language about all the
[77:00] but then the the other problem is we
[77:02] might you know find new issues in the
[77:05] future so like we might need to do a
[77:07] best B and best best
[77:13] B uh Eric caner I think this is a good
[77:16] thing to dig into a little bit more on
[77:19] the note about somebody manipulating
[77:21] their rtt or congestion control Etc like
[77:24] you can also just be in a place where
[77:26] somebody has an
[77:28] asymmetric path latency and you can also
[77:32] be in a place where somebody has
[77:34] asymmetric loss so the the fact that I
[77:36] can send you more than you can send to
[77:38] me is not even necessarily something
[77:41] that an attacker has to do um but it it
[77:44] would be worth digging a little bit more
[77:46] deeply like for example with path
[77:49] validation it's always expected that
[77:51] some of those may be actually
[77:52] legitimately lost um so
[77:55] yeah I don't I don't know that we're in
[77:57] a like OMG the sky is falling place but
[77:59] it'd be worth making sure that we've at
[78:01] least to Martin's Point like maybe we
[78:03] should make the language a little bit
[78:04] stronger or at least go talk to those
[78:06] implementations and say hey did you read
[78:08] this
[78:18] part David skazi infinite buffering
[78:21] Enthusiast um we actually weren't
[78:24] vulnerable to this particular attack in
[78:26] our spec and we were to some of the
[78:28] other ones and the reason for that is
[78:32] the way you we implemented this is when
[78:35] our stack needs to send a control frame
[78:38] like this we have a thing called the
[78:40] control frame manager because that was a
[78:43] good name and you just tell it oh send
[78:45] this thing and it'll send it right away
[78:47] if it can otherwise it'll buffer it and
[78:49] I don't know who the smart engineer was
[78:51] who built that thing but when they did
[78:54] um was Ryan Hamilton uh you went oh well
[78:58] you can't just keep adding to a buffer
[78:59] that could be bad so if buffer. size
[79:02] greater than a thousand close connection
[79:05] and this is the kind of guidance that
[79:07] you maybe it's a quick document maybe
[79:09] it's even a more generic thing but we
[79:11] should provide that kind of ground TRS
[79:14] because we keep shooting our toes off
[79:16] and most of the time the same attack is
[79:18] buffer. append that can be called in a
[79:23] loop
[79:30] okay uh I think that's good discuss
[79:32] discussion there um thank you
[79:36] Martin and uh as a quick Community
[79:39] Enthusiast I just want to thank Martin
[79:41] for his work on this um looking into
[79:43] things and and The Wider Community for
[79:45] handling responsible disclosure as
[79:47] mentioned this affected a few different
[79:48] implementations and could have could
[79:50] have uh had nasty side effects I say uh
[79:55] so yeah this is good I think it's a a
[79:57] good example of of how we can identify
[79:59] these issues uh in implementations um
[80:02] and get them addressed
[80:05] so kazuo would you want to come on
[80:18] up uh thank you my name I'm I'm C hul
[80:21] and I'm going to talk about quick on
[80:23] streams uh we did have a side meeting
[80:25] this morning we hope that we reflect the
[80:28] comments that we GA there in the HTTP
[80:33] working group uh that we have session on
[80:35] Friday so if you have time please come
[80:37] to that as
[80:38] well so next slide
[80:41] please uh so quick as we know quick is a
[80:45] huge success it's better than T CP and
[80:48] TLS in so many ways and we now have
[80:51] broad
[80:52] adoption like major Brothers support it
[80:55] are going to support it and it's already
[80:58] like 30% of all websites are supporting
[81:01] htb3 next
[81:03] please but TCP continues to be used at
[81:07] least as a fallback next
[81:10] please so now we have this sad state of
[81:13] application protocols where we have to
[81:17] develop and maintain two different set
[81:18] of
[81:19] stacks uh for hb3 we have for quick we
[81:24] have H 3 and for TCP we have htb2 heter
[81:28] compression are different between http2
[81:31] and
[81:31] hp3 priorities are semantically the same
[81:36] but because htb2 and htb stacks are
[81:39] different the priority logic has to be
[81:41] applied differently or I mean connected
[81:45] differently to each stack mask has two
[81:48] ways of sending UDP datagrams because
[81:52] HTP 3 has datagrams but h doesn't have
[81:55] data Ram
[81:56] frames web transport uh is like a
[82:00] completely different base for hb2 and
[82:02] hb3 and this this having two different
[82:06] Stacks uh would continue for any new
[82:10] protocol that we develop in the future
[82:12] assuming that we want to support both
[82:14] quake and ntcp as a back stop next
[82:18] please so what can we do here if you
[82:22] look at what ipv 4 and ib6 does with UDP
[82:27] and
[82:28] TCP I mean the UDP and TCP works on top
[82:32] of both ipv F Basics the only difference
[82:35] is how the addresses are being represent
[82:38] represented so if you write code for UDP
[82:42] or TCP it works on both IP V4 and V6 so
[82:46] the question is can we do the same for
[82:48] quick next
[82:51] please that's how we came up to the idea
[82:54] of of quick on streams it's essentially
[82:57] a backboard of the quick API contract on
[83:01] top of TCP so you now have quick
[83:04] streams that can be used
[83:06] on TCP and TLS so once you write the
[83:11] code that uses the quick API provided by
[83:14] quick Stacks then you can run that code
[83:18] either on quick running on top of UDP or
[83:21] running on top of quick on streams that
[83:24] runs on on top of TS and
[83:26] TCP next
[83:28] please so our goals has been to
[83:33] eliminate the need to develop new things
[83:35] on top of two protocols like hb2 and hb3
[83:37] or different protocols protocol
[83:40] protocols and the other goal has been to
[83:42] eliminate the need to deploy two
[83:43] different protocol Stacks when you
[83:45] control both sides so regardless of if
[83:48] the network passes um or blocks UDP we
[83:52] can use the same application protocol
[83:55] stack to run the
[83:57] protocol and the non goals has been to
[84:00] not spend time optimizing TCP like
[84:02] solving the head of line blocking issue
[84:04] or improving the quick frames design
[84:08] because our belief is that quick Works
[84:10] in most cases and performs better so we
[84:13] thought that quick on streams can just
[84:16] be a fallback with uh minimal effort to
[84:19] design and
[84:23] implement
[84:25] David do okay all right next
[84:28] please so the design of draft zero is
[84:31] that we just say that send quick frames
[84:34] on top of TCP NLS and there's no act
[84:36] frames all frames are implicitly
[84:39] act and we also prohibit the use of um
[84:44] frames unrelated to stream operation
[84:47] like new connection ID because we don't
[84:49] need connection IDs when we send quick
[84:51] frames on top of
[84:52] TCP and transport parameters exchange
[84:54] using the first frame called TRS
[84:56] transport parameters and for what was
[85:00] we've said that minimum maximum of frame
[85:02] size is 16 kilobytes because that's the
[85:05] size of t Circle and it provides Optimal
[85:07] Performance when you are sending stuff
[85:09] on TS and for watch was uh I succeeded
[85:14] in implementing a working uh Pro concept
[85:16] code in quickly in half a day and it
[85:19] only took a half day to use that to
[85:22] implement H3 over on streams inside h2l
[85:26] so I'd say that it's fairly easy to
[85:29] implement next
[85:32] please and this is a slide that I
[85:35] presented uh created and I'm not going
[85:37] to present it we're short on time we
[85:39] have 20 minutes in the htb agenda to
[85:41] discuss this stuff um I think we can
[85:44] move some some of what I was going to
[85:45] talk about there um I'd like to to ask
[85:48] people in the queue to come on up and
[85:50] speak uh bearing in mind we do have a
[85:52] lot more time in the hp's agenda to
[85:53] discuss stuff so if you could keep it on
[85:56] track to Quick specific matters that
[85:58] would really help us um David
[86:03] please Lucas J here can I use my Eddie
[86:07] kind of thing to actually say something
[86:09] before we start the discussion so I
[86:11] think I think this this I have talked
[86:14] with you and couple of others uh this
[86:17] things like we're trying to solve
[86:18] something obviously so my Eddie kind of
[86:21] question is like and also like those who
[86:23] will be discussing this today is what
[86:26] I'm trying what important for me to
[86:28] understand is like how this kind of um
[86:32] work um helps deploying quick in the
[86:35] Internet it's we're really doing good
[86:38] job on getting it adapted um and the one
[86:41] of the dream for for us perhaps was to
[86:44] replace TCP all together now we are
[86:46] trying to run quick over TCP I'd like to
[86:49] just understand how this impacts the
[86:52] adoption of quick and where things
[86:55] should be fixed so just think about it
[86:58] when you discussing this
[87:02] thing David skazi quick Enthusiast mask
[87:05] Enthusiast web transport Enthusiast also
[87:08] co-author of every single RFC on this
[87:10] slide
[87:13] um so thanks for this
[87:18] unfortunately I think this proposal is
[87:20] actively harmful to the adoption of
[87:23] quick um
[87:25] the fundamental concept of letting folks
[87:28] write applications over quick and saying
[87:32] don't worry we'll just hide it under the
[87:34] hood is super dangerous um
[87:39] because quick and TCP work very very
[87:42] differently right and especially when it
[87:44] comes to http datagrams and things like
[87:46] that and I think one of the goals and to
[87:50] answer zad's question that we should
[87:52] keep in mind is the overall option of
[87:54] quick and not not the overall adoption
[87:58] of quick over things like TCP because
[88:01] that's going to be a broken version of
[88:02] quick that performs incredibly poorly um
[88:05] so in particular let's say you know you
[88:08] have an application like there are very
[88:10] few applications running directly over
[88:12] quick uh and we don't want to
[88:14] necessarily tell folks to build them
[88:16] directly if they don't know what they're
[88:17] doing but let's take an example of a
[88:19] concrete one DNS over quick if you start
[88:23] running that over quick over TCP you're
[88:26] gonna have a really bad day instead of
[88:28] saying just use DNS over TLS or you know
[88:32] DNS over hgps or DNS over whatever your
[88:35] favorite thing is so I don't think the
[88:37] idea of making it easier to just deploy
[88:40] something over quick with this
[88:41] abstraction layer is the right answer
[88:44] personally so I'm not sure if I agree
[88:48] with the assumption that this you know
[88:51] uh this this has negative impact on our
[88:55] effort to deploy quick
[88:57] everywhere I mean if we think that way
[88:59] we shouldn't be developing mask for htb2
[89:03] we shouldn't have created web trans for
[89:06] htb2 but the fact is that we've accepted
[89:10] the reality that we need to support ECP
[89:13] and the the side effect that we have had
[89:17] is that that effort of uh accepting the
[89:20] reality happen at each extension that we
[89:23] develop
[89:24] and that has created duplicated efforts
[89:27] and I think that's harming us both in
[89:29] terms of writing standards as well as
[89:31] maintaining implementations and I would
[89:34] like to have a fix that applies to
[89:37] multiple approaches at the correct
[89:41] layer Eric kir to try to keep things
[89:44] reasonably quick I don't know that I'm
[89:46] deeply into the actively harmful place
[89:49] but I do very much want to solve the
[89:52] duplication of effort and the problem
[89:53] here um if you look at web transport
[89:55] over H2 that started as an effort to
[89:59] provide similar things as you could get
[90:01] from just the multiplexing layer which
[90:03] we decided was quick rather than H3 um
[90:06] for people over TCP so it would be
[90:09] interesting to have a little bit more
[90:10] discussion about how these things are
[90:13] going to layer and why we can't use some
[90:14] of those existing Solutions um a brief
[90:18] note that might be useful you mentioned
[90:20] that about 30% of servers are using
[90:22] quick we also seeing traffic from Apple
[90:25] platforms finding that about 30% of
[90:27] resources on the web are also loaded via
[90:29] quick right now so that is that number
[90:32] continues to go up the number that isn't
[90:35] going down is the number of networks
[90:37] where we think quick doesn't work um
[90:41] so do we push on that do we let this be
[90:46] the release valve and not need to push
[90:48] on that
[90:49] anymore but either way I think this is
[90:51] very much a a problem that would be good
[90:54] to solve I'm not totally sure if this is
[90:55] the solution versus any of the other
[90:58] possibilities that we have um but
[91:01] someday it would be nice to not have to
[91:03] be in in this weird dual
[91:08] World Ian s Google um thanks for
[91:11] bringing this up I'm not sure if I
[91:13] support doing this or not I need to
[91:15] think about it a lot more um although I
[91:16] will note that we kind of already did it
[91:19] technically um if you look at the web
[91:21] transport over hp2 spec um there is the
[91:24] web transport reset stream capsule the
[91:26] web transport stop in capsule web
[91:28] transport transport stream capsule I
[91:30] mean if you remove the WT underscore um
[91:34] from each of these you basically end up
[91:37] with quick over
[91:39] htb2 over TLS over T yeah I don't know
[91:44] there's a few more Turtles maybe there
[91:45] very much not an
[91:47] accident um and so I guess to some
[91:50] extent we sort of already did this which
[91:52] is not to say we shouldn't do it but
[91:53] like I'm just for for those you know and
[91:56] you know David I realize your your name
[91:58] is not an author on that but you are the
[91:59] chair so probably you noticed it
[92:00] happened
[92:02] um um I don't know so it seems like
[92:04] we're kind of already doing it so but
[92:07] but whether it needs to be a separate
[92:08] effort and quick I don't know yep I
[92:10] think that's a really good point and the
[92:13] problem is that the abstraction is not
[92:18] provided at the correct place where I
[92:21] mean so if you want to use uh web
[92:25] Transformer H2 as the abstruct of quick
[92:29] on TCP whereas using quick directly on
[92:32] UDP then you have you essentially have
[92:35] two different Stacks that you have to
[92:36] maneuver with and that's not a good
[92:40] place compared to where you just have
[92:42] one quick stack that does both TCP and
[92:45] UDP that that was actually uh taken into
[92:48] account when designing it so let's let's
[92:50] go more offline for that just in the
[92:51] issues of
[92:52] time
[92:55] yeah reminder please be brief and
[92:57] comments folks we have a lot in the
[92:59] queue we have still some agend of time
[93:02] to get through so please in in my
[93:04] previous job I I was I was layering
[93:07] protocols on top of quick that are not
[93:09] HTP and I always had to deal with the
[93:11] TCP fallback and it's just it's just
[93:13] painful just getting streams on top of
[93:16] TCP is painful all the stream
[93:17] multiplexers around there are are a big
[93:21] pain they are all terrible um so this
[93:24] this would have been very much
[93:26] appreciated if we had had this back then
[93:28] also as somebody who is implementing web
[93:30] transport over over quick I really don't
[93:33] want to want to implement web transport
[93:36] um over http2 sorry um it would be so
[93:40] much nicer if we had this and my web
[93:42] transport implementation could just
[93:44] switch to switch to quick on
[93:48] streams
[93:52] Christian
[93:55] Christine yes I'm here yeah I already
[94:00] said that in the in the email
[94:02] discussion I I understand why Kaz wants
[94:05] to do something like that
[94:09] but I'm not sure it actually solve the
[94:12] problem I mean the reason why UDP is
[94:16] blocked in various network is not
[94:20] because their Wi-Fi water cannot do UDP
[94:24] it's because some policy decision was
[94:27] made that they wanted to inspect traffic
[94:31] or protect against
[94:33] whatever and uh it's very
[94:37] unclear that those same network that
[94:40] want to protect against traffic Etc will
[94:43] let you pass quick over
[94:47] TCP that's very unclear if I if I was
[94:51] designing a solution like that say
[94:54] layers on top of
[94:56] layers I would want to do the solution
[95:00] probably either on top of web transport
[95:03] because they will resolve the problem or
[95:05] on top of web circuit sending the quick
[95:08] frames on top of web
[95:11] circuit so the I think our response
[95:15] would be that if there there are those
[95:18] midle boxes that look deep into the
[95:20] pocket then they are also likely to
[95:23] block web transport over H2 or those new
[95:25] protocol that doesn't follow but
[95:27] existing http2 does so yeah but what
[95:31] what if because there is a a wide body
[95:33] of experience of doing stuff like say
[95:36] running Skype over
[95:39] https right doing a connect for a proxy
[95:43] and that's a kind of stuff that does
[95:45] work just trying to do your own protocol
[95:47] on top of TCP typically does not work
[95:51] well so you're seeing that there's
[95:53] assumption that some endpoints look at
[95:56] the traffic pattern and they only allow
[95:58] the ones that they wanton to allow allow
[96:01] or they know that they they exist so
[96:05] yeah any new attempt uh being developed
[96:07] on top of TCP would be blocked
[96:09] regardless of it being protocol X or
[96:12] protocol y that's my
[96:15] point so but we our our procedence is
[96:19] that we expect some protocols to go
[96:21] through TCP ands that's why we continue
[96:23] to develop new protocols and so then I
[96:27] think the question is why do you think
[96:29] that quickon streams will be blocked
[96:31] while the other protocol being developed
[96:33] on top of TLS or TCP or HTTP whatever uh
[96:37] would work I don't think there's a good
[96:39] discussion in the interest of time I'd
[96:41] like to move on to to another one
[96:43] there's a good discussion please carry
[96:45] it on Martin uh Martin do Google um
[96:48] three things uh quickly first of all um
[96:50] as a protocal engineer this is super
[96:51] duper cool and like it was fun to look
[96:53] at number two um I'm concerned about NX
[96:56] kcd 927 problem uh just creating another
[97:00] permutation of things to support um uh
[97:03] like I mean I think there's a different
[97:05] working group that could give me an
[97:06] answer would be very satisfying which is
[97:08] that you know the path here is that
[97:10] eventually we will deprecate http2 and I
[97:11] can delete that code from my stack
[97:13] because this is so awesome um if it's
[97:15] not if that's not going to be the answer
[97:17] at some point in the next five to 10
[97:18] years this is again just another thing I
[97:20] have to test another thing I have to
[97:21] support and that's bad um and of course
[97:25] the other question which I think it's it
[97:26] we we can't answer yet which would be
[97:28] interesting to to to answer is what is
[97:31] the performance impact of this versus
[97:32] running over hb2 um if it's similar or
[97:35] better that is more appealing than if
[97:37] it's just worse uh ex then why would I
[97:39] do this uh if I have to keep hb2 as well
[97:43] thanks Ted uh Ted if you could go back a
[97:47] slide um and while he's going back sled
[97:49] I'll say I think you have an interesting
[97:51] idea uh but that the curent design is
[97:54] very fundamentally problematic because
[97:57] you you would like to have a common API
[97:59] but the functions beneath the common API
[98:01] are significantly different in ways that
[98:04] impact the utilization of the API and I
[98:06] think the the result of that is if you
[98:09] have a common API that you're using uh
[98:12] natively for quick and appropriately and
[98:14] you're treating the things underneath it
[98:17] when you fall back to TCP as if they
[98:19] were uh behaving as if uh they were UDP
[98:24] uh structures under quick you will find
[98:27] Corner case after Corner case where you
[98:29] have to work around it uh where you you
[98:32] can say okay we're not acknowledging it
[98:33] or we're going to treat TCP in ways that
[98:35] make it look UDP like um I just I I
[98:39] think that you're actually driving
[98:40] complexity into a different part of this
[98:42] stack uh that you'll still have to deal
[98:45] with at the end of the day because
[98:46] functionally TCP and UDP just aren't the
[98:49] same and you're not going to be able to
[98:51] get the same functions with TCP over
[98:53] streams
[98:54] um uh as you're going to be able to get
[98:56] sorry with quick over streams as you're
[98:58] going to be able to get uh for for quick
[99:00] over the the current stack just because
[99:02] of those differences so I think if you
[99:04] want to tackle this you actually have to
[99:06] tackle it by looking at what you need
[99:07] TCP to change to enable and that's a
[99:10] much bigger problem but it also has a
[99:12] much bigger payoff for potentially other
[99:15] U parts of the protocol stack
[99:18] thanks
[99:20] yav uh scor um I think there are two
[99:24] main reasons why UDP 443 or whatever
[99:26] your Port favorite Port is is uh blocked
[99:29] uh one is because people don't know
[99:31] better um just enable tcp8 TCP 443 uh
[99:35] and another reason is because uh some
[99:37] Enterprises want to inspect their
[99:38] traffic with firewalls proxies and so on
[99:41] um and if they try to inspect uh quick
[99:44] embedded in TLS and TCP things will go
[99:48] bad anyway and uh that will be a
[99:50] troubleshooting nightmares uh I think
[99:52] we've been here before um with
[99:54] websockets circuit 10 years ago when it
[99:56] was introduced it was also blocked by
[99:58] default by all those fancy proxies and
[100:01] the solution was just to deliver some
[100:03] cool Enterprise critical business
[100:05] critical applications that relied on
[100:07] websockets and now 10 years later I
[100:10] think most firewalls and proxies do
[100:13] allow web socket simply because things
[100:15] will break for Enterprises so I think
[100:18] the solution to increase quick adoption
[100:20] would be to deliver more uh great
[100:23] applications uh with multipath with web
[100:26] transport and all other great benefits
[100:28] that uh quick provides uh and I'm afraid
[100:31] this solution actually goes
[100:33] against um that uh it actually
[100:37] encourages not to enable UDP 443 to
[100:40] potentially gain those
[100:43] benefits final brief comments colen and
[100:46] Victor uh Colin Jennings I'm a David
[100:49] snazzi Enthusiast uh the the point I
[100:52] want to make here I'm very much
[100:54] interested in the the media over quick
[100:55] type use cases and they are very
[100:57] impacted by latency uh that the running
[100:59] over TCP doesn't doesn't work very well
[101:01] and the VoIP have tried all kinds of
[101:02] things I mean we have TCP Stacks where
[101:04] you act stuff that you haven't received
[101:06] to reduce the problems and stuff which
[101:08] might help this um but I think that
[101:10] we're trading off a short-term problem
[101:12] versus a long-term long-term we
[101:14] definitely agree we don't want quick to
[101:16] be blocked that would be the best thing
[101:18] and we go and ask people why are you
[101:19] blocking UDP today um there are some
[101:21] people as people have said inspection
[101:23] but I think that is almost gone at this
[101:25] point because all the VoIP major VoIP
[101:27] applications are doing strong end to-end
[101:29] encryption that's not interceptable on
[101:31] these types of proxies um so I think
[101:33] that the proxy argument has mostly gone
[101:35] away and we have reason really data to
[101:36] support that what they do say though is
[101:39] this these bizarre conversations about
[101:41] like well there was some malware virus
[101:44] that used UDP as a control Channel at
[101:46] some point and we block all of that and
[101:47] I think that those arguments are gone
[101:49] when you have a UDP connection and I
[101:52] think what we need to do that the best
[101:53] best long-term thing for us is to use
[101:54] this great application called quick and
[101:56] all the things are built on top of it to
[101:58] go argue look quick is not like your UDP
[102:01] from long ago and you can allow you
[102:03] quick through your firewall running over
[102:05] UDP and you don't need to block it and
[102:09] if we if we have a fallback for that
[102:12] that just instantly disappears there's
[102:13] no incentive for them to ever do that um
[102:15] so I don't I I I really feel like the
[102:18] long-term best solution for us is um
[102:21] let's do greasing like we do it
[102:22] everywhere else and let's say that you
[102:24] know we are going to insist that the the
[102:26] UDP version of quick Works across this
[102:29] um and I think that that will be the
[102:30] best longterm game I definitely get the
[102:32] short-term pain of everyone who's like
[102:35] oh this is such a hassle to implement
[102:36] fallback for my app and everybody has to
[102:38] do it in three different ways but I want
[102:39] to get out of that long-term you know of
[102:42] that that short-term pay so I sort of
[102:44] lean the other direction but I think
[102:45] it's a very complicated trade-off that
[102:47] we need to think about carefully and
[102:48] that we should think about the long-term
[102:50] path because the future's bigger than
[102:52] the past not the short term path
[102:55] thanks
[102:58] VI I wanted to comment on differences
[103:01] between quick and web transport web
[103:03] transport aims to provide the same
[103:06] abstractions that quick provides and in
[103:09] fact uh the Google mq
[103:12] implementation works both over Rock Wick
[103:16] and over web transfer using the same
[103:19] code because it was is intentionally
[103:22] designed to do that so uh I don't think
[103:24] the abstractions would be the issue and
[103:27] other than that from my current
[103:30] understanding of the draft it is does
[103:33] mostly the same things as web transport
[103:35] over H2
[103:38] does cool thank you very much all for
[103:40] the discussion there um thank you going
[103:44] to move on to the next agenda item which
[103:46] is the quick bdp
[103:52] frame
[104:01] you okay
[104:03] gory hi have
[104:06] you yeah
[104:08] okay wait a
[104:11] second there we go um yeah uh I've got
[104:15] quite a long delay so um I will try and
[104:18] work this with the delay um this is a
[104:21] quick bdp frame extension draft um the
[104:26] bdp
[104:27] frame is the mechanism we use to
[104:29] transport the data it's in the draft
[104:32] name it doesn't have to be the way we do
[104:34] this let's take the next slide and go
[104:37] through what we plan to do with the
[104:42] thing are you doing next or am I ah
[104:45] you're doing good okay so
[104:48] the use case for this is to transport
[104:52] what we call congestion control
[104:54] parameters CC
[104:56] prims um careful resume uses these
[105:00] for Rapid sender side U rate increase so
[105:05] it can do a fast congestion control
[105:07] startup um primary target of this is
[105:11] quick so given that it's a sender side
[105:15] change could we propose a receiver side
[105:18] change can the receiver participate in
[105:20] deciding whether a change
[105:23] in the congestion control parameters is
[105:28] a useful
[105:30] thing at a particular time are there
[105:33] trade-offs are there interactions that
[105:35] could be done with the
[105:36] applications so two different
[105:39] possibilities are obvious ways of using
[105:42] this the first one is for the client to
[105:45] tell the server please don't use careful
[105:47] resume uh because I'm not sending much
[105:50] data because I don't want to use it on
[105:52] this particular connection I want to use
[105:53] it on a later one or because I know that
[105:57] something about my path has changed and
[105:59] it really isn't a great idea it's not
[106:01] going to work out
[106:03] well the second use of this is to allow
[106:06] a the CC parameters to provide a hint to
[106:10] the upper layers to basically tell you
[106:12] what to do and the first thing not much
[106:15] above the congestion controller is it
[106:18] allows you to modify the quick flow
[106:21] credit to let you take advantage of a a
[106:23] big change in congestion control but
[106:26] there's other applications also possible
[106:28] and other parameters you can send so
[106:30] next
[106:34] slide um basically the whole thing
[106:37] starts by the client saying please can
[106:39] you tell me the congestion control
[106:42] parameter information from the server
[106:45] next
[106:48] Slide the server then sends a set of
[106:50] congestion control parameters to the
[106:52] client
[106:53] and these are authenticated so you can't
[106:57] change them at the client but you can
[106:59] read them and the set should be
[107:02] extensible because what you need now for
[107:05] congestion controller at the moment
[107:06] depends on what congestion controller
[107:08] you're using and there might well be
[107:10] good things you want to add into this
[107:13] basically once you get the information
[107:14] the client you save it next
[107:19] slide you can use it at the client but
[107:22] you also return back to the server to
[107:24] request the server to take advantage of
[107:28] this information you've given it to tell
[107:31] the server not to use it all of this is
[107:34] hints to the server so we had questions
[107:37] last time uh on the list about what
[107:39] happens to this request well it goes to
[107:42] the server the server decides to use it
[107:44] like anything else it does with
[107:46] congestion control the server is in
[107:48] control it can decide whether to use it
[107:50] or not but it gets the extra value from
[107:52] seeing the information from the client
[107:55] knowing that the client could give its
[107:57] perspective so next slide
[108:02] please
[108:04] well this draft has changed its word
[108:07] significantly since the last time uh I
[108:10] tried to present it
[108:13] uh we changed about 350 lines of text uh
[108:17] which is most of the draft to make a
[108:19] complete rewrite which says the that is
[108:22] in a more abstract way to try and bring
[108:25] this into something which could be
[108:27] extended and used for other congestion
[108:29] control
[108:31] parameters we are not particularly
[108:34] worried about um what that set of
[108:37] parameters are and we also don't really
[108:39] mind if the working group has a strong
[108:41] preference to change the methods even
[108:44] the draft name can change what we want
[108:46] to do is try and um allow the receiver
[108:49] to take part in negotia creating
[108:53] significant changes in the way in which
[108:56] the path is
[108:58] used so um do you think this is
[109:01] something that we might uh be able to
[109:03] work on in quick is this
[109:06] something that the working group has
[109:08] appeti to help us make are people
[109:12] interested in
[109:13] this here's the
[109:18] question
[109:21] m
[109:31] Martin I'm a big fan of doing careful
[109:35] resume I'm also a big fan of storing
[109:38] stuff with your token and using that on
[109:41] subsequent connections
[109:44] um these are all very useful things to
[109:47] do in your quick stack
[109:50] however I'm not sure if it's useful to
[109:54] communicate these things on the wire we
[109:57] have the token and we have the session
[109:59] ticket and the server is able to put its
[110:03] state into the token or into the session
[110:05] ticket wherever it likes uh encrypted um
[110:09] and send it to the client and when the
[110:11] connection is restored uh uh resumed it
[110:14] can restore the parameters from the
[110:16] token and resume its congestion
[110:19] controller same on the client side when
[110:21] the client saves the token or saves the
[110:24] session ticket it can attach some some
[110:26] data to it and then restore these
[110:29] parameters uh when it resumes the
[110:31] connection this is all possible right
[110:33] now we don't need any protocol for this
[110:34] this is purely an implementation
[110:36] decision what I'm worried about when we
[110:39] communicate things on the
[110:41] wire
[110:42] is
[110:44] that the congestion controller in
[110:47] use is purely an implementation
[110:51] decision client and server can use
[110:54] vastly different uh congestion
[110:56] controllers they can switch congestion
[110:57] controllers between uh between
[111:00] connections they can do all kinds of
[111:02] stuff and I'm concerned that the values
[111:06] we communicate from the server to the
[111:09] client or in the opposite direction have
[111:11] might have very different meanings to
[111:14] the congestion controllers used on both
[111:17] sides and if we do something like this
[111:20] um there's there's the implicit uh
[111:22] assumption that congestion controllers
[111:24] will work uh in the way that they do
[111:27] right
[111:29] now and I hope that there's more
[111:32] um um more Evolution going on in the
[111:35] congestion controller field and we will
[111:37] have congestion controllers that look
[111:38] very different um from what we have now
[111:41] in just a couple of
[111:43] years and I don't see how this fits into
[111:46] um this um this
[111:51] extension well I don't see any of the
[111:53] problems that you that Martin brought up
[111:55] there U I'm not
[111:57] sure that uh the details that's going to
[112:01] be revealed in this case is going to be
[112:02] the amount of capacity the amount of rtt
[112:05] and some private data which you throw
[112:08] back at the server and the server will
[112:10] decide what to use so I'm not sure that
[112:13] the uh uh giving this token or um frame
[112:18] to the client is going to stop the
[112:21] evolution of ingestion control I think
[112:24] it would help though if the client then
[112:26] have hint has hints that a particular
[112:28] interface is not available or the
[112:30] interface has changed its rate because
[112:32] the client could then tell the server
[112:35] about this so um not quite sure I by
[112:39] that but anyway go go ahead
[112:41] Ian yeah Ian Google um yeah I've been I
[112:44] mean for this has been talked about for
[112:46] a while um I've been trying to think for
[112:48] a long time about what use cases I would
[112:50] have of this um I I haven't come up with
[112:53] anything really solid I think the use
[112:57] cases I'm currently most concerned with
[112:59] are actually better addressed by scone
[113:00] Pro uh gosh that's a terrible
[113:07] name um to be completely honest um so
[113:11] um uh I I guess I would say I'm not
[113:16] interested because uh I can't find a use
[113:19] case in the different situations that
[113:21] I've looked at where this is more
[113:22] compelling than say putting something in
[113:24] the token or doing something else that I
[113:26] think works equally well um so not
[113:30] interested personally that's
[113:34] what so I'm going to jump in here both
[113:38] as uh an individual um so
[113:43] the I I think gory would it be accurate
[113:46] to say that uh not just the you're not
[113:48] just interested in potentially the the
[113:51] idea of
[113:53] sending congestion control parameters on
[113:55] the wire but the general idea of uh
[113:58] doing this kind of optimization work in
[114:00] the quick working group or or you
[114:03] because I think what we're hearing is
[114:05] push back on the particular solution um
[114:09] and I'm not sure that that's really what
[114:11] you were trying to get more clarity on
[114:12] and what we were trying to get more
[114:14] clarity
[114:16] on yeah I wasn't I mean I'm not I don't
[114:19] care about the particular solution I
[114:22] don't hugely think it matters which
[114:25] quick mechanisms we use to do this but
[114:28] um I know that we have implemented it
[114:30] using tokens and or we've implemented it
[114:32] using um bdp
[114:35] frames I was more interested in whether
[114:37] the client gets to say whether it's a
[114:40] really bad idea to do a congestion
[114:42] control change and give that hint back
[114:45] to the server so the sender side uh has
[114:49] some clue that something's changed at
[114:51] the client
[114:54] hey gory LS uh so two two points uh one
[114:58] we we tried something similar for TCP a
[115:01] long time ago it was much simpler in the
[115:02] sense that um we had this idea that when
[115:06] a client knew it was going to be
[115:09] disconnected right if if the sender
[115:11] doesn't know that then it's just
[115:13] exponentially doubling the retry time
[115:16] and then when the client is back right
[115:18] it would be nice if the client could
[115:19] tell the server hey I'm back now keep
[115:20] sending stuff and and that's sounds
[115:22] great on paper except what it does is it
[115:24] synchronizes all the clients that are
[115:26] for example on a train going into a
[115:28] tunnel and you have this massive restart
[115:31] event because all the clients say I'm
[115:33] back right so there's some subtle side
[115:34] effects here when when trying to sort of
[115:37] exchange this sort of stuff for the best
[115:39] intentions right the the other thing I
[115:40] was going to say is that this is not
[115:43] specific to this presentation but maybe
[115:44] the the theme of some of the
[115:45] presentations I said know we've designed
[115:47] quick with a lot of extensibility and
[115:49] you can just grab code points and and
[115:51] like do stuff and and there's no need to
[115:53] like with TCP to standardize ahead of
[115:55] time so so I would encourage like to
[115:58] experiment and do stuff and and then
[116:00] come here and say you know we tried this
[116:01] it seems to work pretty well here's the
[116:04] idea and here's how we want to do this
[116:05] right so I I I don't think we need a
[116:07] front load of standardization in quick
[116:09] so much as we needed to do in TCP and we
[116:10] should take advantage of
[116:18] that so um just speaking as chair this
[116:21] is some work that been presented on and
[116:23] off over the last few years there has
[116:24] been discussion on the list there been
[116:26] discussion here that's great kind of
[116:28] hearing different signals depending on
[116:31] who's in the room and those kinds of
[116:32] things so I think what would help from
[116:33] our perspective is to do a show of hands
[116:35] just to put like some numbers into
[116:37] things um to to augment the discussion
[116:41] that's happened here so I'm going to
[116:43] going to ask the question about whether
[116:44] the the working group would be uh
[116:47] interested in adopting work to solve
[116:48] this problem or this kind of use case I
[116:52] keep keeping this as as fuzzy right um
[116:55] which is annoying but that's what I want
[116:56] to do so live with it so I'm going to
[116:58] start the poll now we going to keep this
[116:59] very short so we got time for the
[117:01] clarifying question this being
[117:04] explicitly sending information like is
[117:07] described or this being the general
[117:09] congestion control exchanging
[117:12] information but not specifically the
[117:14] proposals technically on on precisely
[117:16] how to do that okay but the explicit
[117:18] sending of information which was
[117:19] something that people objected to that's
[117:21] part of the the yes great thank you for
[117:24] for clarifying um
[117:43] so
[117:46] y please please finish the poll I just
[117:49] had one quick comment after the poll
[117:51] okay okay
[117:56] for time I'm going to I'm going to close
[117:58] it there we're kind of
[118:00] seeing balance of yes no and no opinions
[118:03] there so we'll take that and we'll we'll
[118:05] follow up with some offline discussion
[118:07] thank
[118:08] you my quick comment is just going to be
[118:11] um now that there's a congestion control
[118:12] working group um I don't think it's
[118:15] unreasonable to consider discussing this
[118:19] there um
[118:24] okay thank you um thank you and Fran we
[118:29] have a couple of minutes if you would
[118:30] like to jump on to discuss FEC we can
[118:33] give you as much time as we'll have
[118:35] until we start leaving the
[118:40] room yeah I can talk a few minutes for
[118:43] sure okay um can we get the slides up M
[118:50] yeah so there's like two minutes
[118:51] remaining so try to be like really
[118:56] fast okay so next
[118:59] slide so you already know that when you
[119:03] lose some packets you have to wait one
[119:05] RT to recover it next next
[119:08] slide so with FC you can compute repair
[119:12] symbols so that you can send it with
[119:14] your packets and so you can recover the
[119:15] L packets without having to wait for the
[119:18] this rtt so it's helpful for videos or a
[119:23] short West response use case for
[119:25] instance next
[119:28] slide so we made a draft for quick and
[119:31] FC actually I spent my thesis on that uh
[119:35] we have an implementation of our last
[119:37] prototype based on clo FL skish
[119:41] implementation so here the talk is
[119:43] especially to make you aware that it
[119:45] exists that we had interesting results
[119:47] that we may want to do some stuff with
[119:50] mqt uh given the the recent discussions
[119:53] on mqt recently about FC next
[119:58] slide so some of our experiments were
[120:01] about tunneling an RTP stream and
[120:04] sending FC to protect the RTP stream and
[120:06] see how it works with with quick so we
[120:08] have an RTP stream turn it through uh
[120:12] quick with FC and look at the Sim of the
[120:15] video next
[120:18] slide uh yeah so it's videos of drone
[120:21] drone live recording next
[120:24] slide so our results um so to be quite
[120:29] quick so on the Y AIS you have the
[120:32] average Sim so the average Fidelity of
[120:35] the video and on the x-axis you have the
[120:38] playback buffer so the latency that you
[120:40] add in the
[120:41] Stream uh our implementation
[120:44] implementation is is the red box um the
[120:48] green box is the classical quick and you
[120:51] have RTP an RTP with FC in blue and
[120:54] yellow and what what we could see is
[120:56] that we had good uh average of video
[121:00] Fidelity results with different playback
[121:03] buffers when you increase the playback
[121:04] buffer so if you increase the latency
[121:07] then RR Transmissions work well and so
[121:09] classical quick works well but we low
[121:12] buffers uh our quick
[121:16] FC uh obtains good uh video fedility
[121:19] results um compared to to the the
[121:22] classical quick and even compared to
[121:23] some rtpc
[121:25] implementation uh I think you can go to
[121:27] the last
[121:30] slide yeah so here are some links so
[121:33] there is a link to my thesis I just
[121:35] wanted to make you aware that it exists
[121:37] let me know if you want to collaborate
[121:39] uh Kish is not totally compatible to
[121:42] mqt right now but it's doable because uh
[121:46] We've pushed some generic trites to make
[121:47] it work so we might want to to do that
[121:50] instead of teling AR streams So yeah
[121:53] thank you for
[121:55] listening thank you for being so brief
[121:58] Fran we we had Abby in the que I'm
[122:00] afraid we're over time so I'm gonna I'm
[122:02] GNA have to make you sit back down sorry
[122:05] uh if you have questions FR I'll take
[122:06] them offline go on the list there's some
[122:08] good stuff there um yes I just want to
[122:10] thank everyone for their time here
[122:12] especially our note takers and any other
[122:13] people there was some great discussion
[122:15] in the in the
[122:17] jabber I'll be sure to try and follow up
[122:19] on some of that too um yeah yes and
[122:23] thank you all safe travels goodbye for
[122:50] now
[122:57] [Music]
[123:20] keep
[123:34] I like the like the the spiciness in the
[123:45] chat the point of crazy is is exactly
[123:50] that
[123:52] I
[124:04] for this
[124:06] is3 it's exactly the same thing
[124:10] right ht3 is the same
[124:14] thing it's
[124:19] deliber I didn't up
[124:24] [Music]
[124:25] uh at one point I thought we might
[124:28] finish
[124:33] yeah it was really good to have it I
[124:35] want to but um I supect what happen in h
[124:39] session is a lot of what people wanted
[124:42] to
[124:50] express
[124:52] that's on record now and we come back
[124:54] and go we should have done
[125:05] this

=== Content from www.youtube.com_dbc4ea6f_20250115_180209.html ===
[00:20] it's
[00:26] [Music]
[00:27] 9:30 I think I all thises
[00:50] next hello hello everyone so it is
[00:54] 9:31 so we're going to start and so if
[00:57] you could make your way to your seats um
[01:00] welcome to the quick working group at
[01:03] iatf
[01:08] 1119 most people are probably familiar
[01:10] at this point in the week with the note
[01:11] well but uh please do note well note
[01:15] well um this is this basically gives us
[01:20] our guidance for all of our conduct and
[01:22] you agree to the note well by
[01:23] participating in this in this working
[01:26] group uh please also note of particular
[01:29] note the code of conduct and the
[01:31] anti-harassment procedures and if you
[01:33] have any concerns about anything of that
[01:35] nature please reach out to the chairs or
[01:38] an ad or someone like
[01:43] that uh so very briefly meeting tips the
[01:46] usual type stuff make sure you use the
[01:47] on-site tool so we get credit for you
[01:50] being here it's good to have to pump our
[01:52] numbers up and uh please use meet Echo
[01:55] to join the mic que rather than just
[01:56] standing there uh and if you're going to
[01:59] use the full version uh just keep your
[02:01] audio and video off um and remote
[02:04] participants yeah you know the typical
[02:07] keep your audio and video off unless you
[02:09] are presenting and we will try to give
[02:11] you control of the slides if you prefer
[02:13] it otherwise we can also uh control it
[02:16] from
[02:22] here oh okay so everyone's favorite part
[02:26] of working group meeting is that we need
[02:27] someone to take uh notes for the minutes
[02:30] so do we have any
[02:33] volunteers for not
[02:35] taking it's a very Noble thing to do and
[02:40] we can't really thank you
[02:44] David um so let's see what else is there
[02:47] oh yeah we're going to run the we'll run
[02:49] the que so um don't worry about like
[02:51] removing yourself we can we'll dump you
[02:54] when you're when you think we're done
[02:55] you're
[02:57] done okay agenda we have a lot of things
[03:00] on the agenda so as usual we start with
[03:03] our working group items and we are
[03:04] starting with kog which will be
[03:06] presented by Robin uh remotely and then
[03:09] we're going to move on to multipath and
[03:11] 50 minutes it's a long time but it's a
[03:13] it's a big thing so then finally we're
[03:16] going to finish off with act frequency
[03:18] for the these are all working group
[03:20] items and then we're going to move on to
[03:21] other items that we think are
[03:23] interesting we have a a presentation on
[03:25] the some quick security considerations
[03:28] and recent experience with that uh then
[03:31] uh quick on streams uh the bdp frame
[03:34] some things about FEC and finally
[03:37] accurate ecn and we hope to get to all
[03:38] these things but as usual um we may not
[03:41] get to some of
[03:44] them and finally update since the last
[03:46] meeting um not a ton but there is some
[03:48] so we have a we had finished a working
[03:51] group last call for ACT frequency uh but
[03:53] we need to run another one because there
[03:55] was a bunch of issues and that's that's
[03:56] why we run working group L calls so
[03:58] that's okay um but we expect to run
[04:01] another one and we'll hear more about
[04:03] that later and then uh reliable resets
[04:06] is uh much to the uh relief of certain
[04:10] chairs is finally wait getting along and
[04:12] there's a waiting Shepherd right up so
[04:15] hopefully that will that'll be helpful
[04:16] for web transport in
[04:18] particular and uh with that oh I I guess
[04:20] I should have said does anyone want to
[04:22] bash this agenda we did a bunch of preg
[04:24] genda bashing so I expect not but if
[04:28] anyone has any bashing
[04:30] please say so
[04:32] now looks like no okay and with that
[04:35] we're going to move on to our first
[04:37] presentation which is from
[04:47] Robin
[04:49] yes can you hear me can I get the
[04:53] slides yeah I'm trying
[04:56] to do
[04:58] that
[05:15] hold Robin Robin can you request
[05:17] permission to present from the Meet Echo
[05:22] to uh I can ask slides if that's what I
[05:26] need to
[05:27] do yeah I did
[05:32] that not
[05:36] again I guess I'm going to run the
[05:38] slides for you Robin because I can't
[05:40] actually give you access for some reason
[05:43] um that's
[05:54] fine there we go so for today's update I
[05:58] wanted to go for the Dune team because
[06:00] as it turns out Australia is about 40%
[06:04] Dunes so we're all basically in arus
[06:06] right now quite cool sad I couldn't be
[06:09] there uh but so next slide please uh
[06:11] since last time we actually had a ton of
[06:14] progress um we made a lot of changes
[06:17] that we discussed last time like
[06:19] removing qack which is now done we had a
[06:21] bunch of editorial updates and then we
[06:23] also had a lot of uh additional
[06:25] clarifications and nuances that were
[06:27] fixed a lot of that was due to to a new
[06:30] character uh appearing on the scene much
[06:32] like uh characters in The Dune movie uh
[06:35] in our case this was uh Hugo landow who
[06:38] is the op SSL quick implementer and they
[06:41] also added qck support and through that
[06:43] he found quite a few
[06:45] um uh subtle nuances and issues that we
[06:48] had to tackle which we were able to do
[06:50] so a big thank you to Hugo Hugo uh for
[06:53] that next slide please another big thing
[06:56] that we merged since last time was uh
[06:59] support for uh path migration and the
[07:02] basis for multipath as you might
[07:04] remember this is a relatively simple
[07:06] proposal where we have uh string part
[07:09] IDs that can then be linked to uh
[07:12] specific part information using separate
[07:15] um events we think this is a good basis
[07:18] for multipot the goal was not to have
[07:20] full multipot support yet but we think
[07:21] this is enough for if there would ever
[07:24] be a quick multipath Q loog draft to
[07:26] build upon this which is why we also
[07:28] merged it
[07:30] um however we're not 100% sure that
[07:32] that's correct so if there are still
[07:34] people from the multipot um uh work that
[07:37] want to experiment with this or want to
[07:39] give it another review or try to
[07:40] implement this we would definitely
[07:42] welcome um additional feedback next
[07:45] slide
[07:48] please uh the main topic of today will
[07:50] be
[07:52] extensibility so uh one of the main
[07:54] goals of qog is to allow later to Define
[07:57] additional qog schemas for other
[08:00] protocols or for new documents to extend
[08:03] to add new events or new uh protocol
[08:06] mechanisms for quick and
[08:07] hcp3 and there are various ways that we
[08:10] need to add in extensibility one of the
[08:12] ways is if you have a qog file how would
[08:15] you know which protocols or which type
[08:17] of events are actually uh possibly
[08:19] represented inside of that file uh and
[08:22] so we've gone to various iterations on
[08:24] this I've been presenting this for a
[08:26] while this is the current um proposal
[08:30] that we are quite sure will work so
[08:32] basically you have two options you
[08:34] explicitly list the the event um event
[08:39] groups that you want to use in a
[08:41] specific file either through a
[08:43] pre-registered Ur So that goes through
[08:45] an aana registry um or you can also use
[08:49] a fully qualified URL uh for things that
[08:52] are for example not yet an RFC or are
[08:55] not going through the ITF um system this
[08:59] is already iterated on from previous
[09:01] feedbacks and is also something that is
[09:03] in previous rfcs so RFC 8285 for example
[09:07] also uses this so we're quite sure that
[09:09] this is a good approach um one of the
[09:11] main questions we have is how explicit
[09:14] we need to be in listing all groups of
[09:17] events so for example our quick document
[09:20] has various categories of events you if
[09:23] your general transport based events but
[09:25] you also have what we call connectivity
[09:27] events or recovery events having to do
[09:29] with congest control for example um and
[09:33] in the example on the slide what we're
[09:35] basically doing is explicitly listing
[09:38] only two of those three categories for
[09:41] quick um meaning that the other ones the
[09:44] recovery one wouldn't be present in this
[09:46] qog file this is different from HTTP 3
[09:50] where we don't explicitly list which
[09:52] categories we're using which in our
[09:54] current proposal means we're using them
[09:56] all so anything that is defined in the
[09:58] htb three document goes um so that's
[10:03] kind of the approach that we're going
[10:04] for if you don't have the hashtag with a
[10:06] specific ategory you would just import
[10:08] everything in the file and if you have
[10:11] implicit uh hashtags you import only a
[10:14] part of the file and the question
[10:16] basically is do does anyone have any
[10:18] problems with that approach with
[10:20] implicitly including the full set of
[10:23] events if you don't explicitly list them
[10:26] or if people or feel very strongly that
[10:28] you should all always explicitly list
[10:31] all of the categories that you are um
[10:34] using so if people have a very strong
[10:36] opinion on that um let us know next
[10:39] slide
[10:42] please so you want to add new T new
[10:47] stuff um but it's not always just
[10:49] completely new protocols sometimes you
[10:51] also want to add new things like new uh
[10:54] frame types in this case to for example
[10:57] quick um so approach to build the Q loog
[11:01] uh documents right now would be to say
[11:03] okay we have the existing quick frames
[11:05] let's say for example the max data frame
[11:08] and then in a packet sent event you
[11:10] would have a list of all the different
[11:11] frames that you could use uh as far as
[11:13] we know existing quick that's of course
[11:16] very inflexible because you can't add
[11:18] new types of frames um to to this
[11:21] definition you would have to redefine
[11:23] the packet sent event to add a new type
[11:26] of frame which is for example one of the
[11:28] problems we have with the ACT frame uh
[11:30] extending that for things like multibot
[11:33] act um so we want to prevent that for
[11:35] for qog using the next slide um using
[11:39] something called a cddl type socket so
[11:43] that's the dollar sign quickframe thing
[11:45] there um in the cddl language this that
[11:50] basically means it's like a I call it
[11:52] the type enum so a list of types that
[11:54] are represented by this one type name
[11:57] and you can later extend that so the
[11:59] slash equals means you add a new type to
[12:02] that type enim of quick frame and so
[12:05] later documents for example let's say
[12:07] the Act frequency draft if they would
[12:09] add Q loog support they can just extend
[12:11] that existing quick frame uh type that
[12:15] is defined in the other document and so
[12:17] if you combine the two if you combine
[12:19] the cddl definitions of both documents
[12:22] then will it will automatically work and
[12:23] you can then list act frequency frames
[12:25] in your packet set events as you would
[12:28] expect right this was actually already
[12:31] in qog for a while for some things we
[12:33] now extended it to cover many more
[12:36] things that's basically what the new PR
[12:38] is doing extending this approach to uh
[12:42] to other stuff next slide
[12:45] please so that was for if you have new
[12:47] things new frames um that you want to
[12:50] support sometimes you Al also want to
[12:52] extend existing things a good example
[12:55] here is quick transport parameters which
[12:58] we log in a an event called parameters
[13:01] set as you can see there it's a long
[13:03] list of Transport parameters that we
[13:05] have there and of course there will be
[13:07] new transport parameters in extensions
[13:10] defined and the way we used to deal with
[13:12] that in qck was to just say oh this
[13:15] event can contain any field of any
[13:19] type uh so that you can you can
[13:21] basically expect anything in there uh is
[13:24] what it was saying which is flexible
[13:26] enough to support later extensions of
[13:28] course but it's it's also completely
[13:29] useless from a schema validation
[13:31] standpoint because it's just too generic
[13:34] anything goes so um you can't really
[13:37] differentiate if it's if it's a valid
[13:38] extension or not and this is really
[13:41] something that we've only fixed very
[13:43] recently next
[13:45] slide
[13:47] um with a a different type of cddl
[13:50] socket in this case a group socket which
[13:53] is different from the type socket from
[13:54] before because it has two dollar signs
[13:57] instead of one dollar sign
[13:59] um so this is not like a type enim this
[14:03] more acts like a placeholder uh kind of
[14:05] field inside of the definition so
[14:07] basically it's saying this is a
[14:09] placeholder and whatever gets assigned
[14:11] to the placeholder later can then show
[14:13] up at this specific uh location in the
[14:16] in the event so for example here we
[14:18] would have the quick parameter set event
[14:21] in the main Quake document that we have
[14:23] now and then again if you would have act
[14:25] frequency um extension that would have q
[14:29] support you can then say oh there is a
[14:31] new transport parameter defined it's
[14:33] this field the Minag delay and it will
[14:36] just be uh slotted into that place in
[14:39] the existing event um there as
[14:43] well so basically what we're proposing
[14:46] is to add one of those extension slots
[14:49] to every single qog event so that every
[14:53] qog event can be extended with new uh
[14:56] Fields if needed by um
[14:59] um future um extensions we think this is
[15:03] a nice uh balance between verbosity and
[15:06] complexity and also allowing for
[15:09] extensibility of everything um down the
[15:11] line because it's always just a single
[15:13] line for every event that is extra even
[15:16] though some of those arguably will never
[15:18] be
[15:19] exercised next
[15:23] slide um with this we're focusing on
[15:27] making qck extensible for the RFC
[15:29] extension points so mainly the things
[15:31] that are defined as extension points in
[15:33] the existing
[15:35] rfc's which mostly means they have Anna
[15:38] registry uh we also have a few things
[15:41] that do not have that let's say quick
[15:43] packet types or uh fields in the quick
[15:46] packet header let's say if there was
[15:48] ever a loss bits extension that was
[15:50] accepted you would also want to extend
[15:52] what's in the packet header so we also
[15:54] have extension points for those um for
[15:58] those things to try try and cover our
[15:59] bases as much as possible right so we
[16:02] think we got this right
[16:05] finally um and so the question is or the
[16:08] ask is if people would be interested in
[16:11] actually trying to exercise these
[16:13] extension points for new work concretely
[16:17] things like multipot maybe or I know
[16:18] there have been some discussions in uh
[16:20] media over quick and there's actually
[16:22] already an existing draft for careful
[16:24] resume that is doing this partially uh
[16:28] they don't support
[16:29] most of it because they only add a fully
[16:32] new event which is great but we would
[16:35] need uh to exercise all the uh other uh
[16:37] extension points or at least to to an
[16:39] extent so if people are interested in
[16:41] this it doesn't have to be full-fledged
[16:42] or fully defined just to have a look at
[16:45] it and try some of it out we would be
[16:48] very happy to help you along there and
[16:50] to look over what you're trying to do
[16:52] and see if it fits with uh with what
[16:55] we're trying to do here Dad do you have
[16:57] a question you want to
[16:59] uh if you could send a note to the quick
[17:02] chair's Alias with what you think the
[17:04] extension points we ought to be
[17:06] exercising a little bit more explicitly
[17:08] that would be very
[17:09] helpful all right we'll do that thank
[17:13] you yeah
[17:15] excellent so next slide please um so
[17:20] extensibility is the main thing the only
[17:22] other big question we have for the
[17:23] working group right now uh apparently I
[17:26] said quick chairs and I meant mock
[17:28] chairs sorry
[17:29] thank you ah Mark okay that makes a
[17:32] little bit more
[17:34] sense excellent
[17:38] um so if you have signals like stream
[17:40] was finished or stream was reset or you
[17:42] get a stop sending those are usually
[17:45] received at the um uh let's say packet
[17:49] level and logged at the packet level
[17:51] sometimes quite a long time before
[17:52] they're actually communicated up to the
[17:54] application Level
[17:56] implementation right so some
[17:57] implementation for example only give you
[17:59] that signal when you try to read from a
[18:02] specific stream not when it is actually
[18:04] received um and so the question is how
[18:08] how to log that in qog when that was
[18:10] actually passed to the application uh
[18:13] implementation in that way and this
[18:15] actually has a lot of parallels but how
[18:17] we communicate how data is actually read
[18:19] how data is passed between the layers we
[18:22] have something called Data moved events
[18:24] for that and so it seemed like a natural
[18:27] fit to also have these kind of signals
[18:29] in those data moved
[18:31] events we had a lot of iterations on
[18:35] what exactly that should look like um
[18:38] but these things are so different and
[18:39] they they all have their own nuances and
[18:41] everything that in the end we just
[18:43] decided or or hoping to decide uh to say
[18:47] not to be too explicit in this event and
[18:50] just have like a a a string that
[18:53] says in this instance at this time stamp
[18:57] it was communicated to be like that
[18:59] there was Ain bit set for this stream or
[19:01] there was a stream reset received for
[19:03] this stream at some point in the past if
[19:06] you want more information look at other
[19:08] qog events that do log the full frame or
[19:11] that do log more information about what
[19:13] that might have been we think this
[19:15] strads the needle between being useful
[19:17] for debugging and not replicating too
[19:20] much of that uh information across
[19:22] different events but it is a little bit
[19:25] clunky uh which is why uh we're we're
[19:28] asking asking people if they have strong
[19:29] opinions on on whether or not we should
[19:31] do
[19:32] this and that's basically the end of the
[19:36] presentation last slide please
[19:38] um we think we are nearing the end with
[19:42] this here uh we think with extensibility
[19:44] done that was the last of the major
[19:47] design issues uh we currently have quite
[19:50] a few issues in PRS open but we should
[19:53] be limiting those very soon because a
[19:56] lot of the extensibility stuff is spread
[19:57] over a lot of the different issues and
[19:59] PRS that those are hopefully going away
[20:02] soon what I'm trying to say is if any of
[20:04] you were waiting for qog to settle down
[20:06] a little bit to start again looking into
[20:10] it and and seeing um and and helping us
[20:13] bring this to the Finish Line this might
[20:15] be a good time to start uh uh looking
[20:17] into it again as we said as well looking
[20:20] into exercising the extension
[20:23] points uh it's also starting to be a
[20:25] good time for that and so we are very
[20:28] open new qog issues new
[20:30] reviews again a big thanks to people
[20:32] like Hugo landow who did this uh if more
[20:35] people are willing to do that that would
[20:36] be very very um helpful today that's it
[20:39] for me if there are any questions
[20:42] um let's have
[20:48] them I uh I inser myself as an
[20:51] individual and author on the qog spec
[20:53] not as chair um but kind of in the lead
[20:56] up to this meeting and uh during this
[20:58] week we've he like some chats in the
[21:01] slack Etc from Mock implementers and and
[21:04] Co so to to Ted's earlier Point
[21:07] um these extension points exist I I
[21:10] don't know enough about the mock apps
[21:12] that uh to know what is useful to log so
[21:14] we've had some discussion um if if you
[21:17] just have a log of stuff that is useful
[21:20] for uh seeing what's going on with theck
[21:22] application from a performance or
[21:23] debuging perspective like we're happy to
[21:25] do some reverse engineering because we
[21:27] want to prove that the extension
[21:29] mechanisms are working um similarly we
[21:31] heard in dsv WG yesterday um that
[21:34] there's some folks uh extending qog to
[21:36] support the careful resume mechanism all
[21:38] of these are really good um I don't
[21:40] think we need to block the progress of
[21:43] this draft on doing those things those
[21:46] suggesting in the chat we do something
[21:48] like a toy application profile like pidc
[21:51] personally I don't think that provides
[21:53] much value I'd rather invest our efforts
[21:55] in uh things that actually benefiting
[21:57] real uh quick work even if that or quick
[22:00] related work even if that work is not uh
[22:03] finished or done in the idea
[22:09] yet fully
[22:12] agree any further questions or
[22:16] um
[22:20] comments seems like maybe no thank you
[22:26] Robin so our next presentation is going
[22:28] to be be on multipath
[22:57] quick good
[22:59] y should we start okay uh hello everyone
[23:03] I'm M uh today Mia and I will introduce
[23:05] the explicit passid proposal for the
[23:08] multi pass extension next slide
[23:12] please okay first SL will introduce a
[23:15] little bit about the background about
[23:17] the key issue that this proposal want to
[23:18] solve and how does this explicit passid
[23:22] works and then we'll compare this
[23:24] proposal with the 06 old one we list
[23:28] these all these pros and cons and do
[23:30] some
[23:31] comparision and then about these
[23:33] interrup reports for the new proposal
[23:36] and for the last part we will introduce
[23:39] and this all these open issues and make
[23:42] some
[23:43] discussion next slide
[23:47] please yeah uh at first I'd like to
[23:50] introduce a little bit about this key
[23:52] issue in case people has for forgotten
[23:54] this uh this is from the last uh ITF the
[23:58] key problem is that the last D6 approach
[24:01] is using an identifier for the pass
[24:05] which doesn't have the same lifetime as
[24:07] the network pass and and Martin has the
[24:12] first point out that we should use a
[24:14] separate pass ID from uh from from the
[24:18] connection IDs and a big sense for his
[24:21] great idea so what we are trying to
[24:24] introduce here is to introduce an
[24:27] explicit pass
[24:29] that stays constant even if the C ID on
[24:31] a path
[24:33] changes so next slide
[24:37] please so for this part I I try to
[24:40] introduce more details for how does this
[24:44] explicit pass ID
[24:46] Works uh I'd like to introduce it in
[24:49] three key Parts the first part is for
[24:52] the pass management it's much more
[24:55] straightforward we just use this
[24:57] explicit pass identifier for each pass
[25:00] in a connection and each it we we also
[25:05] use use it in all the multipass control
[25:09] frames such as for the pass available
[25:12] and pass abandon to address a
[25:16] pass and also we need some more CID
[25:20] management control
[25:24] frames we add this MP new CID frames
[25:28] which ties the CID to a pass ID and the
[25:31] CID sequence number is
[25:33] increasing under per pass and we also
[25:37] add this MP retire connection ID frame
[25:40] as a pair and also the retire PR 2 field
[25:46] is under
[25:48] prass and that's a difference uh from
[25:52] the
[25:53] fc9000 and the thir part is for the
[25:56] packing number space the packing number
[25:58] space now is bonded to the pass ID and
[26:02] it will remain stable when the C
[26:05] rotation
[26:07] happens and we don't need to uh we don't
[26:10] need to force a force a tradeoff between
[26:15] the performance and uh and the the and
[26:20] the packing number changes packing
[26:22] number space
[26:23] changes the next side
[26:27] please
[26:30] um for for this for this part i' like to
[26:33] try to compare with the old version and
[26:36] then list all these pros and cons here
[26:39] uh for the first part for pass
[26:41] management as pass ID the new proposal
[26:45] has a prom here uh because it's quite
[26:48] straightforward it introduced link
[26:51] between incoming package and the pass is
[26:54] unambitious you always know you always
[26:57] knows which
[26:58] which pass does this packet belong
[27:01] to but the old draft it need you need to
[27:05] treat the situations when C rotation and
[27:08] N binding happens and it will have
[27:11] ambitious situations for the second part
[27:14] for the C management uh you really need
[27:17] to do more things to maintain the Cs per
[27:21] pass and to manage the CID and the
[27:24] passid lifetime you need to retire all
[27:27] the asso ATC IDs when the pass ID is
[27:31] abandoned and the old one is just the
[27:34] same as the FC 9000 but I want to
[27:37] explain more here the new new proposal
[27:41] is fully compatible with the IFC 9,000
[27:46] because you can you always use the new
[27:49] CID frame and the retire C frame for the
[27:53] initial pass the initial pass has the
[27:56] pass identifier as
[27:59] zero the third part is for the packing
[28:02] packing number State for the loss
[28:05] recovery and conest control for this
[28:08] part explicit pass pass ID has a br here
[28:13] for the loss recovery and congest
[28:15] control can re can rely on single
[28:18] sequence number space for the duration
[28:20] of the pass it remain the same for the
[28:24] past whole
[28:25] lifetime and the old draft has a CR here
[28:28] because C rotation will triggers the
[28:33] change of this packing number space and
[28:35] you need to face a tradeoff between the
[28:38] performance or more complexity in your
[28:42] code so basically it seems we have two
[28:45] point to one point here and I like to
[28:48] introduce a little bit more about
[28:50] Hardware offloading I'm not a a expert
[28:53] for Hardware offloading but I've I have
[28:57] discussed this question with someone who
[28:59] is and uh Eric has told me that for all
[29:04] these two
[29:05] proposals from the hardware perspective
[29:09] uh they just share the same cost for
[29:12] multipass Hardware offloading because
[29:16] they all need to use the pass identifier
[29:19] to mix into the nons when when you use
[29:24] Hardware offloading with smart pass so
[29:27] they are just exactly the same
[29:31] cost next slide
[29:34] please okay for this part uh we had
[29:37] already have some interrup reports from
[29:40] the hackerone I'm very glad to see that
[29:44] even if this pro has has not been merged
[29:47] uh several implementations has already
[29:51] tested implemented in in their codes uh
[29:54] we have had this nice interrupt table uh
[29:58] we have xqu pqu RAs and
[30:02] creature and uh we we are very glad to
[30:06] find out that explicit PID management
[30:10] works quite well and you in the new
[30:12] version we have clearer logic and we
[30:16] reduce the code
[30:20] complexity yeah next slides
[30:24] please so the next big question would be
[30:27] do do we want to merge this new pro pro
[30:40] request yeah we're going to do some
[30:41] discussion first before we do any sort
[30:43] of uh asking a question so
[30:46] kazuo uh I appreciate the efforts spent
[30:50] by the authors and the people uh for did
[30:52] the interrupt I think the pro request
[30:55] produces the cost of I mean introduce
[30:58] the cost of retaining multiple c number
[31:00] spaces and it's a pain for me that said
[31:02] I understand that this is a trade-off
[31:04] that I can live with I think we have
[31:07] come to a point that uh we should just
[31:09] merge this PR and solve the remaining
[31:12] issues that might exist in the new
[31:20] approach Alexander gini uh Cloud flare
[31:23] um let's just merge
[31:26] it
[31:31] uh Jan Angar fastly I haven't read the
[31:33] pr so I'm not going to say merge it but
[31:35] generally speaking this direction seems
[31:37] to make sense to me I just had one
[31:39] observation when you are separating the
[31:43] uh packet number spaces you don't
[31:46] actually need to separate it by path ID
[31:49] because all you really care about is
[31:51] that the packet numbers are
[31:53] monotonically increasing from the sender
[31:55] perspective because ultimately the
[31:58] packet numbers just have to maintain
[32:00] send order are you proposing one packet
[32:02] number space for everything no no uh
[32:04] that's the proposal we had like NOS go
[32:06] I'm not I'm not saying that you should
[32:09] not use path ID I'm saying you don't
[32:11] have to uh uh separate all the packet
[32:13] number spaces as well because you don't
[32:15] need to you can use the path ID for
[32:17] everything that you do for condition
[32:18] control uh context for various things
[32:22] but um we don't have to spend a lot of
[32:25] time here talking about that it's a very
[32:26] minor issue but it just seems to me that
[32:28] you don't have to actually separate out
[32:29] the packet number spaces I like the
[32:31] explicit path ID for what it's worth I
[32:33] think it's the right direction to
[32:36] go Magnus won Erikson yeah also
[32:40] supporting explicit path ID let's go
[32:42] forward with that Etc
[32:46] so
[32:52] chrisan yes I mean uh obviously uh I I
[32:56] had mixed feeling at the beginning I
[32:58] would really like to have just a single
[33:00] number space like Jenna was mentioning
[33:03] but uh the truth is that you cannot do
[33:05] that with the new proposal and the
[33:07] reason you cannot do that is that um you
[33:11] cannot make that an implementation
[33:13] option because the uh when you receive a
[33:17] new pass you have a new packet that is
[33:20] arriving and you have to predict the
[33:22] 64bit number of the packet
[33:26] number and
[33:28] if the number was shared with different
[33:30] paths then you would get in a mess of
[33:34] guessing wrong and and not encryp and
[33:36] not decrypting
[33:38] correctly so jna is well as simple as
[33:42] Mia said we add actually that proposal
[33:46] back two years ago I was the prop of
[33:50] that proposal I like that proposal but
[33:52] it
[33:53] requires if you go down that R hole it
[33:57] requires a lot more code than the comp
[33:59] than the than the current
[34:01] proposal so I would say let's merge it
[34:04] and and go and and simplify it's one of
[34:07] the word not
[34:09] taken Janna J Angar I feel like I uh I'm
[34:14] I'm triggering people here by saying
[34:16] this so I'll I'll I'm not going to walk
[34:18] it back but I'm happy to take it offline
[34:20] my point here is that when you're
[34:22] talking about single packet number
[34:23] spaces only what you're using is the
[34:26] packet number as the key to to figure
[34:28] out what path you're going on whereas
[34:30] here I I I understand I understand J I
[34:34] actually have that implemented in my
[34:36] code no no no I'm
[34:38] notr hang on let me finish let me finish
[34:40] let me finish what I'm saying is use the
[34:42] path ID as the key as you're doing and
[34:45] an optimization is you don't need to use
[34:47] packet number spaces that are separate
[34:49] for congestion control or for loss
[34:50] recovery purposes because really that
[34:53] code is going to work should work uh one
[34:56] way or the other um again let's not r on
[34:59] this because yeah I mean if you want we
[35:03] can discuss that I have actually a full
[35:05] implementation of that it seems simple
[35:08] at the beginning it's get very very
[35:10] complex when you get into the details
[35:13] okay we had this on the table about two
[35:15] years ago and we made a decision that we
[35:17] go for mulp number I know what was on
[35:18] the table two years ago what I'm
[35:19] suggesting is different than that but
[35:21] like I said that's the last word I'm
[35:23] going to say on this we can talk about
[35:25] this offline
[35:26] thanks so we we're hearing um some
[35:29] support in the room we want to as chairs
[35:31] take a show of hands just so we can get
[35:34] a feel in case people don't want to be
[35:35] more vocal um so we're going to run that
[35:38] poll now um if you're not yet signed in
[35:42] please do so in the five seconds I'm
[35:45] going to take The Pusher button um and
[35:48] get
[35:51] prepared so the question we're asking
[35:53] here is should the authors merge this PR
[35:55] 292 um to go forward with with the
[35:57] explicit path IDs design uh you can see
[36:01] the options there yes no no opinion um
[36:05] we have a high number of participants
[36:09] and not many people voting
[36:14] um
[36:18] so okay we have some hands on the stage
[36:21] as well from the presenters who aren't
[36:23] currently logged
[36:26] in
[36:31] okay I think we can I think we can stop
[36:33] there
[36:36] um where do I
[36:38] stop there we go just so for the record
[36:42] there uh we had uh 23 yes hands three no
[36:48] and 14 we had 23 yes hands three no and
[36:52] 14 no opinions so I'm taking that as a
[36:55] strong signal in support of going
[36:57] forward with merging this PR we will
[36:59] take that to the list to confirm
[37:00] consensus um but otherwise uh yeah let
[37:04] so thank you for the the strong
[37:06] indicators here that's giving
[37:08] us uh a great path forward uh and means
[37:11] we can actually move on to the next
[37:13] discussion topics as our
[37:16] slides perfect um so we were hoping for
[37:19] this and that's why we prepared some um
[37:21] open issues to discuss we currently have
[37:23] 46 open issues some of them will go away
[37:26] because they might not apply to this new
[37:27] PR anymore and there's also a little bit
[37:29] redundancy in the issues so we need to
[37:31] clean that up um but we brought a couple
[37:34] of issues for today that um are
[37:36] connected to the new pass explicit pass
[37:38] ID proposal that we thought might be
[37:40] worth discussing so this slide is the
[37:42] easy one that's a proposals where we
[37:44] believe we have already a way forward so
[37:47] uh there was a discussion that a pass IB
[37:48] should not be reused and there seems to
[37:50] be agreement so we have a PR for that
[37:52] and yeah I saw that kasu has a different
[37:54] opinion so go to the
[37:56] mic
[37:59] right so the problem state statement of
[38:02] 292 was that not having a continuity of
[38:06] pocket numbers across multiple
[38:08] connection IDs is a pain and we agree
[38:11] that we solve that but now that we have
[38:15] pass Ides indicating which to which
[38:18] pcket number space each pocket belongs
[38:20] to the question becomes if we need a way
[38:23] of issuing and returing new pass idas
[38:27] and to me it seems like an unnecessary
[38:29] complication I mean if you have the
[38:32] opportunity to open for pass you can
[38:34] just use four you can just have four
[38:36] pocket number spaces across the entire
[38:38] last of the
[38:40] connection and then natur binding would
[38:42] look exactly like an explicit migration
[38:45] there would be no difference and I think
[38:47] that will be a win to
[38:49] use uh just to reuse pass ities across
[38:52] the
[38:55] migrations I mean yeah you you opened
[38:57] this issue yesterday so I didn't think
[38:59] about it so I think we just need more
[39:01] discussion on
[39:08] it um okay so yeah please comment on
[39:10] this issue either um this one or kasu is
[39:13] new issue I think they are connected um
[39:16] and as I said we will clean up the
[39:17] issues after this meeting somehow okay
[39:21] um then also hopefully an easy one uh is
[39:24] 317 so uh if a preferred pass is
[39:27] indicated during the handshake then this
[39:29] prepared pass should get a new pass ID
[39:32] uh the pass so the the original part is
[39:34] pass ID zero and the preferred pass
[39:36] would be pass id1 this is just something
[39:38] we ommitted in the current PR and
[39:40] straightforward
[39:43] hopefully okay go thank you um and then
[39:48] there was a point about the Passad is
[39:49] still not fully clear uh we already I
[39:52] think we also have a PR on this one but
[39:53] there's still discussion going on um it
[39:56] seems like people think think it is
[39:58] valuable to have to use the same pass ID
[40:00] on both sides so the client and server
[40:02] both know it's you know it's pass number
[40:04] whatever three um but also that needs
[40:06] some coordination so um that you when
[40:09] you open a new pass you actually pick a
[40:10] pass IB that wasn't taken yet and you
[40:12] don't have conflicts um so I think we
[40:14] have agreement that we want to have the
[40:16] same pass ID on both sides but then
[40:18] there are some additional stuff we need
[40:19] to discuss which is on the next
[40:23] slide okay and uh this is exactly as
[40:26] soon um as you have uh if is if if you
[40:30] have like one pass ID space basically um
[40:32] the easy solution is if we only allow
[40:34] the client to open new passes then you
[40:36] don't need to have any coordination but
[40:38] if you want to also allow the service to
[40:40] open new pass and this is a very old
[40:41] issue as you can see we never decided on
[40:43] this one um then this means you actually
[40:45] have to handle it separately and the
[40:47] proposal here is to split the pass ID
[40:50] space and as similar as we did with
[40:51] streams and the client uses the even
[40:54] numbers and the server uses the odd
[40:55] numbers but then that also means that
[40:57] all the kind of settings you provide
[40:59] about how many um passes you allowed to
[41:01] open and whatever you have to do
[41:03] basically on a server and client site um
[41:06] so yeah discussion of this one is is
[41:09] ongoing uh I don't think we have a
[41:10] decision on this one yet any thoughts
[41:13] comments
[41:18] ideas on clarification question is there
[41:21] H anybody who needs this feature for the
[41:25] use cases or is it something you know
[41:27] that's floating like a nice
[41:30] have I think people believe if you use
[41:33] the same path ID on both sides it's
[41:35] really kind of um it's less error prone
[41:40] in your implementation we have seen a
[41:41] couple of cases where the Implement did
[41:44] get it wrong right because you always at
[41:46] each frame you have to indicate which
[41:47] pass you're talking about and then it's
[41:49] always not clear are you using your pass
[41:50] ID are you using the pass ID of the
[41:52] other side and so if you just just use
[41:54] the same number then all of that you
[41:56] know comp lexity goes away yeah right so
[42:00] I think my question is around if we want
[42:02] to if we need the servers okay uh I
[42:06] think so this one um RSC 9000 doesn't
[42:09] allow the server to open um connections
[42:12] but it felt like and this is back in the
[42:14] old issue it felt like the restrictions
[42:16] we we had to make this decision are
[42:18] actually going away with multiplus so
[42:19] there would be an opportunity to just do
[42:21] it um on the other hand there's not
[42:24] probably a big use case for it because
[42:25] the server can always tell the client an
[42:27] IP address and tell the client to open
[42:29] the path right so that is like if you
[42:30] really need this you can like handle it
[42:32] this way so right right thank you so I
[42:35] think my comment would be that unless
[42:37] there's a strong use case I I'd prefer
[42:39] just not doing it or pun the decision to
[42:41] a later
[42:46] moment I mean I I see your point but
[42:49] it's also it's and why so I mean it's
[42:53] not that much complexity right it's just
[42:55] like if you can have it
[43:02] yes I mean to to answer the the same
[43:05] question I did study it and the problem
[43:09] is that if we do not do the odd even
[43:13] split
[43:15] now it will be impossible to introduce
[43:19] it as an extension
[43:21] letter because there will be a conflict
[43:24] and we have to have all kinds of
[43:27] duplication of frames and things like
[43:29] that so what is possible there is a
[43:33] potential compromise and the potential
[43:35] compromise say okay we are not going to
[43:38] authorize server initiated pass now but
[43:41] we are only going to use the even
[43:43] numbers for now with the exception of
[43:46] one for the server preferred address and
[43:49] if we do that then we do not increase
[43:53] the complexity of the current
[43:55] implementation but we leave the door
[43:57] open for the server initiated connection
[44:00] at a at a later
[44:03] stage yeah I really like this Christian
[44:06] you can write a
[44:07] PR I I I did write a text in in another
[44:10] issue but I can write
[44:18] PR Magnus yes Magnus W I think the use
[44:22] case I have for Ser initiated is the
[44:25] fact that if you're going to try to use
[44:27] for replacing STP and uh which actually
[44:30] has uh dual actually can use uh multiple
[44:36] path concurrently for and in both day
[44:39] and it's actually in a scenario like in
[44:40] a mobile network where you actually the
[44:42] client server role is not clear or
[44:45] saying it's someone need to do what's
[44:48] it's suitable for them now based on what
[44:50] information they have so um in that
[44:53] scenario it makes sense for opening up
[44:55] The Path potentially from the one that
[44:57] currently has a server role because
[44:59] they're basically equal that I think
[45:01] also applies to peer-to-peer application
[45:03] to a certain degree so and you may need
[45:07] to be able to open from the server side
[45:09] in those scenarios to be able to open a
[45:11] new path otherwise you would not get the
[45:14] right response based on middle boxes so
[45:17] I totally see that it's useful in the
[45:18] scenario but just double checking so you
[45:20] think the solution that the server would
[45:22] be able to tell the client where to open
[45:25] the PA like give an IP address to the
[45:26] client and then the client opens the
[45:27] path anyway that is not doable in the
[45:30] mobile network scenario in the mobile
[45:31] network scenario it's I would guess it's
[45:33] doable it would potentially introduce a
[45:35] latency and especially if you actually
[45:37] have a hard failure case if you drop
[45:39] both paths uh or say both paths if you
[45:43] have two paths both fails you iny cover
[45:46] up uh it might be delaying things uh I
[45:50] think it's more important in the like
[45:52] peer-to-peer quick and multip puff quick
[45:54] scenarios um where where it's a single
[45:58] path the single interface might be Ned
[46:01] and you need to be able and while you
[46:02] have a listening Port somewhere else
[46:07] so um I'll I'll speak as an individual I
[46:10] I haven't looked at this issue closely
[46:12] but here in kazuo speak about we need
[46:15] some use cases the thing that sprung to
[46:17] my mind is to allow to allow this as an
[46:19] extension and effectively in my head was
[46:21] what Christine said so um I think maybe
[46:26] allowing for that that and if people are
[46:28] really interested and think they have
[46:29] those user cases to go and work on it as
[46:31] a separate extension that maybe you know
[46:34] maybe if we get to it before multipath
[46:37] is done and we convince ourselves to
[46:39] merge it back in that could be a way to
[46:40] progress these things but that's just as
[46:43] an
[46:46] individual I think this is a good way
[46:48] forward because I don't think this is
[46:49] blocking anything else so we can also we
[46:51] should decide before we publish the RC
[46:53] but we can decide at the very end and
[46:54] and just to say in in the chat this
[46:56] seems to be some support for Christian
[46:58] suggestion
[47:02] too just wanted to point out that this
[47:05] would be potentially interesting for my
[47:07] net R versal uh extension that I
[47:09] presented last time but I have to admit
[47:12] I have to think about this issue a
[47:14] little bit more before forming an
[47:16] opinion you said it's interesting for it
[47:18] or what was the work what was it word
[47:21] you you used
[47:24] interesting so it would make it easy or
[47:27] harder
[47:29] easier maybe I need to think about this
[47:35] more uh what I I mean I would definitely
[47:38] like this if if we're g to implement
[47:41] multipath I mean this is close to half
[47:44] the value of a lot of the cases that I
[47:46] would actually want to use multipath
[47:47] right because like quick connection
[47:49] migration works pretty darn well on the
[47:51] public internet and solves an awful lot
[47:52] of use cases and so the like in order to
[47:55] do the work of doing multipath I would
[47:57] definitely prefer for either here to be
[47:59] able to change change paths like fairly
[48:02] St I mean I it didn't it took me like 10
[48:04] seconds to come with like three use
[48:05] cases so I'm like I'm sure I can come up
[48:07] with more perfect that's good input yeah
[48:11] I have another another reason to do it
[48:14] and the other reason to do it is that
[48:16] the alternative which is to have a new
[48:19] frame by which the server TS the client
[48:23] hey you can join me at IP address so and
[48:25] so
[48:27] has a slight issue that it becomes very
[48:30] easy to build a request fory attack with
[48:35] that because I mean a malevolent server
[48:39] can send the attack of a Target by that
[48:43] mechanism and the client just PS the
[48:46] target after that so the um having the
[48:51] server s its own packet doesn't have the
[48:55] same issue I mean if there an attack the
[48:57] server can always attack someone but it
[48:59] will be attacked from the server it
[49:01] won't be attacked from the client so I
[49:04] think uh the more I think of it the more
[49:06] I say we we should just put that maybe
[49:10] we have a state saying don't use it now
[49:12] until we have another extension but I
[49:16] think we should go for the even OD split
[49:18] and and put it in the
[49:21] draft okay perfect that sounds really
[49:23] good to me okay with with ch on I think
[49:26] think what I'm what I'm hearing is
[49:28] someone should go and make a PR at least
[49:31] so that the the group can give a bit
[49:33] more consideration into the the scope of
[49:35] impact on the spec itself um and that
[49:38] yeah there's maybe a little bit more
[49:39] here to
[49:41] see manifest rather than just talk about
[49:45] it yeah I think we have something like
[49:47] half a PR or something it's a little bit
[49:48] of Miss we will clean it up okay uh okay
[49:51] have one more slide if we have time we
[49:52] do have
[49:54] time um so this is about retirement this
[49:57] is still under discussion and there was
[49:59] a question uh how you actually retire a
[50:01] pass we already have the pass abandon
[50:03] frame but then the question is what do
[50:05] you do with all the connection uh IDs
[50:08] and the proposal at the table is at the
[50:10] moment that we don't imple uh don't
[50:12] introduce a new like past retire frame
[50:15] but we actually just use the ponm frame
[50:18] and then after you have AB bond in the
[50:20] pass you should also retire all the
[50:22] connection IDs of that path or did I
[50:27] miss
[50:28] anything yeah okay um yes M yeah H the
[50:34] the question is whether this is implicit
[50:36] or
[50:37] explicit exactly so any thoughts
[50:41] opinions
[50:46] Feelings by by experience I would say
[50:48] explicit implicit is better in that
[50:52] case that is if you do if you abandon a
[50:54] past you have def fact to abandon all
[50:56] the cids for that
[50:58] pass so yeah there is a lot of
[51:01] discussion on this issue already and I
[51:02] think that's a direction that people
[51:05] probably
[51:06] prefer um but this was one of the bigger
[51:09] issues so I wanted to bring it up
[51:11] here but you can also comment on the
[51:13] issue if you have any additional
[51:15] stop
[51:17] AR yeah I'm in favor of doing it
[51:19] implicitly not sending the frames as
[51:21] nice and then you also don't have to
[51:23] deal with the with the weird conditions
[51:25] where the path is B but you never
[51:28] receive the retire frames so what do you
[51:30] do then um it's just
[51:36] easier what else goes uh at the same
[51:40] time as the connection IDs are there
[51:43] other thing other resources that are
[51:44] being cleaned up at this
[51:47] point no nothing I mean the packet
[51:50] number space handling yeah so there's a
[51:52] bunch of stuff that you drop when the
[51:53] path goes away just drop it all and and
[51:56] that's just neater and cleaner
[52:01] yeah K hul just wanted to point out that
[52:03] this is one of the complications due to
[52:06] us having the concept of uh introducing
[52:09] New Path IDs and ruring them if we move
[52:12] to St path is then we don't have this
[52:14] issue at
[52:16] all yep another interconnected
[52:19] issue so um
[52:22] Magnus uh I think it's fine with
[52:25] implicit work I think it one of the
[52:27] things I stumbl on this during the
[52:29] discussion around this we had in the
[52:31] hackaton ETC has to do with more with
[52:34] the fact
[52:35] of uh is this symmetric Etc with the
[52:38] path abandon are you forcing the other
[52:39] point to also close the reverse
[52:42] path um so that's um it's more connected
[52:47] to that I think it's some cleanup needed
[52:49] there but yes so there's definitely some
[52:51] clarifications needed
[52:55] yes
[52:57] I believe that was my last slide right
[53:00] yeah I think so um yeah thank you for
[53:04] the good discussion everyone this is
[53:06] really helpful for moving forward the
[53:08] multipath work um and yeah thank you to
[53:11] the the authors thank
[53:15] [Music]
[53:19] you and our next topic is uh act
[53:23] frequency so MIA you can't go very far
[53:26] I'm sorry
[53:28] I at least saved you from getting all
[53:29] the way off the stage I thought Ian is
[53:31] doing some PowerPoint Bingo here but I
[53:34] guess
[53:39] not
[53:44] whatever um so the EG frequency draft we
[53:46] had a last call we uh got a lot of
[53:49] feedback thank you and we have a new
[53:51] version next slide should just stay here
[53:54] I
[53:55] guess thank
[53:57] you um yeah the working group last call
[54:00] concluded in November already um but we
[54:02] only submitted the next version recently
[54:04] we got uh quite a bit of feedback but we
[54:07] closed all open issues um thank you to
[54:11] for everybody Who provided feedback I
[54:12] forgot to update the acknowledgement
[54:14] section but we will do this in the next
[54:15] version next
[54:18] slide so um there was a bunch of
[54:21] editorial stuff and I'm not talking
[54:23] about this here and then there were a
[54:25] few stuff few things which were
[54:27] clarifications uh which I want to
[54:29] quickly uh name here especially the
[54:31] first two um there was still a little
[54:34] bit discussion about how often you
[54:35] should send this frames and uh we
[54:38] provide a little bit of non-normative
[54:40] guidance about it uh we updated the text
[54:42] to make it more clear what we mean um
[54:44] but I think there's still different
[54:46] opinion in the working group about how
[54:47] often you should send things and how on
[54:51] what's kind of the minimum amount of X
[54:53] you should require which is like kind of
[54:55] we a little bit recommending at least
[54:56] one act per round of time um but like
[54:59] for example gory sent another email to
[55:01] the mailing list um so we believe what
[55:04] we have in the draft right now has rough
[55:06] consensus um but just to name that that
[55:08] there was a little bit more
[55:10] discussion
[55:13] um yeah I don't think anything else that
[55:16] needs to be mentioned here next
[55:19] slide um I also wanted to point out some
[55:22] of the issues that we closed without any
[55:25] actions
[55:27] um especially I want to point out the
[55:29] one about um how CES are handled so uh
[55:34] or ecn is handled so what we're trying
[55:36] to follow here is basically what is kind
[55:38] of done in the TCP accurate accurate ecn
[55:42] draft um but there was recently an email
[55:44] on the mailing list from ingamar
[55:46] actually proposing that we could do
[55:47] smarter than that um we kind of
[55:50] basically say we leave this to like
[55:53] future Orin or whatever and didn't
[55:54] address it any further here but you know
[55:57] just to make you aware the other thing
[55:59] that we also didn't further address is
[56:00] what happens after an idle time we have
[56:02] like we added only a few versions ago we
[56:04] edited send and saying after an idle
[56:05] time you should soon send an egg uh but
[56:08] we don't specify this normatively or say
[56:10] anything further what soon means or
[56:12] whatever because we feel this is not an
[56:14] issue for this draft I mean it's of
[56:16] course impacted by this but it's more
[56:17] General issue so people want to work in
[56:19] that space probably um you know they
[56:21] should do it in
[56:23] future that's on this slide and then the
[56:26] last one is actually where I just wanted
[56:27] to make sure we're really doing the
[56:29] right thing because we had two issues
[56:30] that did impact the normative parts of
[56:32] the draft um so one was uh a
[56:37] clarification um
[56:39] about uh Max act
[56:43] delay and we we changed a must to a
[56:45] should here and that is just um
[56:47] basically I I would say that was an
[56:49] oversight because we did change a little
[56:51] bit the language and the whole draft
[56:52] saying this is not something we can
[56:54] enforce this is just like a guidance we
[56:55] give and then the sender you know can
[56:57] still do whatever they want to do
[56:58] because that's the truth and so we
[57:00] changed this must to a should and then
[57:03] the other normative part here was also
[57:04] that we changed the
[57:06] error uh type uh when you when you
[57:09] receive an invalid value because I think
[57:11] that was just the wrong error type um
[57:13] the frame encoding error is also used in
[57:16] RFC 9000 for a similar case so we just
[57:19] you know picked the hopefully right type
[57:20] now so that's the first issue and the
[57:23] second
[57:24] issue um there was a comment saying that
[57:28] uh or like there was a sentence saying
[57:29] in the draft that basically this this
[57:32] extension only changes something um if
[57:35] after you received the
[57:37] first frame of any type I guess and that
[57:41] wasn't fully true because we actually
[57:42] changed the ecn handling and so the
[57:45] solution I went here is to actually keep
[57:47] this statement and say actually nothing
[57:48] changes as long as you don't send an e
[57:51] frequency update draft um which also
[57:55] means that as long as your electing
[57:57] threshold is one you do the same as is
[58:00] written in RC 9000 and only if you send
[58:02] a new value you actually change your ecn
[58:05] handling and you know this is like it's
[58:08] kind of a little bit random you can do
[58:09] one or the other uh I just felt we need
[58:13] to clarify it and I just felt this is
[58:14] kind of a little bit the nicer solution
[58:16] because it actually doesn't change
[58:18] anything as long as you don't send an
[58:19] agency
[58:20] draft and that's actually I hope Ian you
[58:24] have read this PR but I don't know know
[58:26] if if this is the best solution for
[58:31] you yeah it's fine okay so yeah um sorry
[58:35] joh I just had a question do you just
[58:38] want to switch between CE and nonce or
[58:40] any change in the ecn bits because if
[58:41] you're path change from bleaching to
[58:43] non- bleaching for instance you probably
[58:45] want to know that too yes sorry can you
[58:47] I saying do you want to just have the
[58:49] change from CE to not CE or do you want
[58:51] to have any change in the ecn bits
[58:54] because like if you if you had a pth
[58:55] change from bleaching to non- bleaching
[58:56] presumably you'd want to know that as
[58:58] well
[59:00] right so if you if the previous packet
[59:03] was Zer one and next packet is 0 Z
[59:06] because something on the path has
[59:07] started baching ecn the question is only
[59:10] when do you want to know it right do you
[59:11] want to know it immediately or do you
[59:12] want to know it when you anyway send
[59:14] your next EG and so because this is a
[59:16] congestion signal we want to know it as
[59:18] soon as possible everything else is
[59:20] doesn't matter that
[59:22] much okay I guess so I mean again this
[59:25] is just with following just what's kind
[59:27] of specified in the uan draft at this
[59:32] point uh Lucas here speaking as an Eric
[59:35] code Enthusiast um I can't remember if I
[59:38] raised this issue or not you did
[59:40] yes uh yeah I mean I think error codes
[59:44] are important from an operational
[59:45] perspective but also they don't really
[59:47] matter so from I don't think it's a
[59:48] normative change really really anything
[59:51] cares about the the must or should is is
[59:53] as discussed I fully in suppor of um so
[59:56] yeah I just wanted to make that clear
[59:59] thanks I mean who knows what actually is
[60:01] normative and what's
[60:03] not no but I mean this is just what I
[60:05] wanted to flag because I think these are
[60:06] really the only things that kind of
[60:07] impact anything that is close to
[60:09] normative but if everybody likes it then
[60:11] we can just run another working grou
[60:12] last call yeah uh Ian in the I just
[60:15] wanted to give a huge amount of thanks
[60:17] to M for like pounding through a ton of
[60:19] changes to get this over the line um you
[60:23] know I I I enormously appreciate and as
[60:26] the quick working group um and uh yeah I
[60:29] think I mean I looked through a lot of
[60:31] text after the effect and and it all
[60:33] looked really good and in many cases I'm
[60:35] like Oh I thought that's what it said
[60:36] already oops maybe not so I I think a
[60:39] lot of the changes were of the form in
[60:41] my mind that's what I you know that's
[60:43] kind of what the authors I think
[60:44] intended I think you did a wonderful job
[60:47] um so anyway thank
[60:53] you yes thank you Maria um this is our
[60:56] longest living document in the in the
[60:57] working group this at this point so uh
[60:59] happy to see some progress on it and
[61:02] hopefully we'll we'll go forward with
[61:04] running another working group last call
[61:05] and that will be an actual working group
[61:08] last call rather than a uh first last
[61:15] call the last last call um so now we are
[61:20] going to move on to our next topic um
[61:23] which is uh Martin and talking about
[61:26] uh we're calling this quick security
[61:29] considerations I think it's what we're
[61:31] talking about AR are you okay coming
[61:46] [Music]
[61:51] up yeah in the last last couple of month
[61:54] we dealt
[61:56] with closer to the mic okay why is that
[61:59] so loud um M's not very
[62:04] tall so we we we we dealt with um two
[62:07] resource ex exhaustion attacks against
[62:10] the quick protocol itself it turned out
[62:13] because if you follow the RFC
[62:15] 9000 um by the letter then your
[62:19] implementation would be vulnerable to
[62:20] these
[62:22] attacks and a lot of implementations
[62:25] were
[62:26] affected and um we've we've now made
[62:31] these um made these attacks public and
[62:35] implementations have been fixed so now
[62:37] is a good time to talk about it and what
[62:39] we can learn from this for the design of
[62:43] new protocols uh next
[62:47] slide so this is actually the second
[62:49] attack um that we discovered but I'd
[62:53] like to discuss it first because it is
[62:55] very in instructive so let's take a look
[62:59] at how quick manages connection IDs
[63:02] during the
[63:03] handshake um one perer let's say the
[63:05] server is setting a limit for how many
[63:08] connection IDs can be um it is willing
[63:12] to store at the same time and let's say
[63:15] the limit is three so now the client is
[63:17] allowed to send fre connection IDs right
[63:20] and it does that since connection ID
[63:22] zero connection id1 connection ID 2
[63:27] however this is not where the story ends
[63:30] the client can send new connection IDs
[63:33] now as long as it retires connection IDs
[63:37] that it issued pre previously so here it
[63:40] sends connection ID number three and it
[63:43] says retire all connection IDs before
[63:45] two which means connection IDs one and
[63:48] two are now not active anymore this is
[63:51] very useful if you're running in a load
[63:53] balancer setup and your load Balan it
[63:55] like a config rotation and you can you
[63:59] know that the first two connection IDs
[64:02] will soon not be be routed anymore so
[64:05] you can tell your Pier like yeah there
[64:07] there like a small period of time where
[64:08] I'm still able to receive packets um
[64:11] with connection ID zero and one but soon
[64:14] soon they will go away so please retire
[64:17] these connection IDs and the server is
[64:19] then um required by the by the protocol
[64:23] to uh send retire connection ID frames
[64:26] for connection at Z and
[64:29] one so why is this a
[64:31] problem next
[64:34] slide and it's a problem because of
[64:38] congestion control
[64:39] basically so quick reminder here when a
[64:43] packet loss happens you're supposed to
[64:45] reduce your congestion window usually by
[64:48] a factor of like a half or something in
[64:51] in the order of
[64:52] magnitude and if um repeated packet loss
[64:55] occur
[64:57] then your congestion window is collapsed
[64:59] again and again to a minimum value of
[65:01] two two packets per round trip
[65:05] time so what an attacker can do now the
[65:08] attacker can just receive your packets
[65:12] but claim that let's say every other
[65:14] packet was actually
[65:17] lost so now if a client does that the
[65:20] server will very soon arrive at a
[65:21] congestion window of just two packets
[65:24] can basically not send any anything
[65:26] anymore it gets even worse the rtt
[65:29] measurement is also controlled by the
[65:32] client a malicious client could say like
[65:34] yeah I'm I'm acknowledging packets but
[65:37] I'm not acknowledging any any packets
[65:39] that I received in the last five seconds
[65:41] I'm only acknowledging packets that re
[65:43] received five seconds
[65:46] ago this will make the server think that
[65:48] the rtt is 5 Seconds plus the actual RTG
[65:52] so now the server is in a situation
[65:54] where it can send two packets every 5
[65:58] seconds and now the client just keeps on
[66:01] sending um new connection ID frames that
[66:05] retire old connection IDs so the server
[66:08] is now building a cue of like I need to
[66:10] send this retire connection ID frame I
[66:11] need to send this connection retire
[66:13] connection ID frame and the client can
[66:15] just keep on going sending new
[66:17] connection ID frames until the server
[66:19] runs out of
[66:21] memory next
[66:24] slide
[66:27] so the question here is like why why did
[66:30] the flow control mechanism that we try
[66:32] to build here um not work
[66:36] and how is it different from other flow
[66:39] control mechanisms that we build in
[66:40] quick that work and that are not
[66:42] vulnerable to this kind of
[66:44] attack let's look at how quick Stream
[66:47] flow control
[66:48] works so the server declares on a new
[66:51] stream I'm willing to receive let's say
[66:54] 100 bytes
[66:56] this allows the the client to send
[66:58] stream frames that carry data up to this
[67:01] uh to this up offset so let's say
[67:03] clients sends two two stream frames that
[67:06] go go up to 100 and now it's blocked at
[67:08] 100 it can't send any more new data no
[67:11] matter what the client does it can send
[67:13] more until the server sends a maxstream
[67:16] data frame granting new granting new
[67:19] credit here up to 150 now the the client
[67:22] can send 50 more
[67:24] bites so
[67:26] we can see that the main difference is
[67:28] that it
[67:30] requires requires action from the server
[67:33] to unblock the client the client has has
[67:37] there's nothing the client can do on its
[67:39] own to free up this flow control
[67:43] credit next
[67:47] slide so the attack that we've that
[67:50] we've seen on the on the connection ID
[67:52] mechanism is kind of similar to the one
[67:57] that a lot of us had to deal with last
[67:59] year which is the http2 rapid reset
[68:04] attack in the quick reminder in the
[68:07] rapid reset attack in in hp2 and it does
[68:09] not apply to Quick um the client the
[68:14] server would tell the client you're
[68:15] allowed to open 100 streams at the same
[68:19] time um and the client could open these
[68:22] 100 streams but could could then reset
[68:24] the streams immediately
[68:26] freeing up the limit again and then open
[68:29] another 100 streams reset them
[68:31] immediately and do that over and over
[68:33] again no interaction from the from the
[68:35] server
[68:36] required so why is why is quick not not
[68:40] vulner vulnerable to this attack it's
[68:42] because back in 2017 we actually fixed
[68:45] that we fixed that for a completely
[68:47] different reason we didn't know about a
[68:49] rapid reset at this point we fixed that
[68:53] because um when you're dealing with a
[68:55] transport that's not that's not ordered
[68:58] but where packet reordering can occur
[69:00] the client and the server might not have
[69:02] the same um might not have the the
[69:06] consistent picture of which stream is
[69:08] open at any given
[69:10] time so it is basically impossible to um
[69:15] to do to do accurate accounting and to
[69:17] to close a connection when a client
[69:19] violates that limit because this
[69:21] violation of the limit could just be
[69:22] have been caused by packet reordering on
[69:24] the wire and you don't want that to kill
[69:25] your connection that's why we we
[69:28] switched Quicks
[69:30] um the way that we Grant stream stream
[69:33] limits in quick to advertising a stream
[69:36] a maximum stream ID now the server says
[69:39] like you're allowed to open all streams
[69:42] up to stream ID
[69:44] 100 and there's nothing the client can
[69:46] do to increase this limit until the
[69:48] server says like okay now you're allowed
[69:50] to open open all streams up to stream ID
[69:54] 120
[69:57] uh next
[70:00] slide so looking back at um at our
[70:04] specification process like let let's say
[70:06] let's say RFC 9000 was not yet a thing
[70:09] how how would a good solution for this
[70:11] look
[70:12] like and what we could have done is we
[70:15] could have introduced a Max connection
[70:17] ID
[70:18] frame the server could declare you you
[70:21] can send me all connection IDs up to
[70:23] sequence number 10
[70:26] and then at some later Point once some
[70:27] connection IDs are retired the server
[70:29] could say okay now you can send me
[70:31] connection IDs up to sequence number
[70:34] 15 would have been nice but that's not
[70:38] the world we're living in RFC 9,000 was
[70:41] shipped and is widely
[70:44] deployed so um this will be this will
[70:48] probably a hard be be a hard fix to make
[70:51] at at at this
[70:53] point the the way that most
[70:55] implementations have actually fixed
[70:58] fixed this vulnerability is that they
[71:00] now Implement um an explicit an explicit
[71:04] check for like how many how many retire
[71:07] connection IDs do I have queued at any
[71:09] point and the good thing is that retire
[71:11] connection ID frames are really really
[71:14] small they are just a few bytes and if
[71:17] you cue let's say a thousand of them for
[71:20] a connection you're not wasting a lot of
[71:23] memory the the problem only occurs if
[71:26] you're queuing like a million or a
[71:28] billion of them then you run out of
[71:30] memory on the other hand in on on on a
[71:34] connection where you're not under attack
[71:36] you should never be you should never be
[71:38] in the situation where you have a Quee
[71:40] of like a thousand retire connection ID
[71:42] frames before you can send out a single
[71:44] packet so it seems like if you just if
[71:47] you just Implement a check where you say
[71:48] like okay if I reach if I reach this
[71:51] absurdly High number I can just kill the
[71:53] connection and that works
[71:57] uh next
[71:58] slide so now we're coming to to the the
[72:01] second attack which is actually the one
[72:03] that we found first and um a very
[72:06] similar a very similar thing applies for
[72:09] Quick's path validation mechanism
[72:11] because the client can send a path
[72:13] challenge with with some some random
[72:16] data and then the server is is required
[72:19] by by RFC 9000 to send a path response
[72:22] and RFC 9000 says like you can't you
[72:24] can't drop this
[72:26] if you receive the path challenge you
[72:28] must send the path response and
[72:30] obviously the client can do can do the
[72:32] same tricks with a collapse the
[72:34] condition window inflate the rtt and uh
[72:37] the server starts building a queue of
[72:39] path
[72:40] responses the fix in this case is pretty
[72:45] easy because there's not really anything
[72:47] that breaks in the protocol if you if
[72:50] you drop a path challenge under those
[72:52] conditions um I mean the packet
[72:55] contained the path challenge could have
[72:56] been lost um so that's that's a way to
[73:01] to mitigate this attack technically
[73:04] you're not you're not RFC 9000 compliant
[73:06] anymore so maybe maybe we should do
[73:09] something about this and um use the or
[73:13] process to um to change RC
[73:19] 9000 uh Martin Duke Google um this kind
[73:22] of gave me Deja Vu because I I thought
[73:24] we
[73:26] argued this out uh when we were doing
[73:28] 9,000 um what's the new wrinkle is this
[73:31] CEST control bit and I'm not really I I
[73:33] guess I don't follow why that creates
[73:35] the problem but um this is issue 3509 in
[73:39] base drafts and the end result of that
[73:41] after much argument and like it's a very
[73:46] long thread but we put in some some text
[73:49] at the end of section 512 of 9000 about
[73:52] uh limiting the amount of retire
[73:54] connection ID that a that a endpoint has
[73:57] to track and you can that you should you
[74:00] you you you need not put store more than
[74:03] twice the connection ID limit in terms
[74:05] of like pending retire connection IDs
[74:08] and um you can throw an error called a
[74:10] connection ID limit error uh if if
[74:13] somebody somebody's doing something
[74:14] pathological about retiring connection
[74:15] IDs
[74:17] um but again like I me you know sitting
[74:21] here at the mic I I there is this this
[74:25] um CR control wrinkle but I I don't
[74:27] really see why that supersedes what was
[74:29] in the text yeah so it's not clear if
[74:31] this text applies to connection ideas
[74:33] that get retired um because of the the
[74:36] retire prior to field um I'm I'm I'm I'm
[74:40] not I don't recall if we discussed this
[74:42] issue before we introduce retire prior
[74:44] to or or afterwards um it was very late
[74:48] the fact juming in as note take right
[74:50] quickly can you repeat the issue number
[74:52] yes it is um 3509 in the base drafts
[74:56] repo thank you the fact that out of the
[74:59] 18 quick Stacks that we that we surveyed
[75:02] um 11 um if I recall correctly were
[75:06] vulnerable to this attack
[75:08] shows that apparently what we have in
[75:11] the RFC was not sufficient to prevent
[75:14] this vulnerability from occurring in the
[75:16] wild uh I mean it is advisory there are
[75:18] like shs and and Maze I mean you
[75:22] know so so like
[75:26] this prop this has been useful because
[75:28] uh people have not taken um some advice
[75:31] in security considerations and in the
[75:33] section so thank you for doing that um I
[75:36] I'm not convinced that 9,000 is broken
[75:39] in that respect I I mean personally if I
[75:42] remember my position as food fight I
[75:43] would prefer stronger language in it but
[75:45] I think there are the tools here for for
[75:47] servers to protect themselves
[75:50] thanks yeah there there's also the the
[75:53] catch all that at any point during the
[75:55] connection you can kill the
[76:00] connection Alexander gini Cloud Flair um
[76:04] I don't know that this is like an AATA
[76:07] situation where you change the RFC
[76:11] but like we we we've basically had the
[76:14] same problem in hcp2 right for the past
[76:17] like fiveish years we've been fixing a
[76:20] bunch of
[76:21] vulnerabilities and new implementations
[76:23] of htb2 that didn't go through that kind
[76:27] of were surprised about all of the
[76:29] additional changes that they were
[76:31] supposed to do that they didn't make
[76:32] because all of those all of that work
[76:35] wasn't really collected in any
[76:37] particular place um I wonder if this
[76:41] could be a case of either having a
[76:44] separate draft that says these are all
[76:47] the gas that you need to you know be
[76:50] aware of or even like a sort of RFC 9000
[76:55] B with more clear language about all the
[77:00] but then the the other problem is we
[77:02] might you know find new issues in the
[77:05] future so like we might need to do a
[77:07] best B and best best
[77:13] B uh Eric caner I think this is a good
[77:16] thing to dig into a little bit more on
[77:19] the note about somebody manipulating
[77:21] their rtt or congestion control Etc like
[77:24] you can also just be in a place where
[77:26] somebody has an
[77:28] asymmetric path latency and you can also
[77:32] be in a place where somebody has
[77:34] asymmetric loss so the the fact that I
[77:36] can send you more than you can send to
[77:38] me is not even necessarily something
[77:41] that an attacker has to do um but it it
[77:44] would be worth digging a little bit more
[77:46] deeply like for example with path
[77:49] validation it's always expected that
[77:51] some of those may be actually
[77:52] legitimately lost um so
[77:55] yeah I don't I don't know that we're in
[77:57] a like OMG the sky is falling place but
[77:59] it'd be worth making sure that we've at
[78:01] least to Martin's Point like maybe we
[78:03] should make the language a little bit
[78:04] stronger or at least go talk to those
[78:06] implementations and say hey did you read
[78:08] this
[78:18] part David skazi infinite buffering
[78:21] Enthusiast um we actually weren't
[78:24] vulnerable to this particular attack in
[78:26] our spec and we were to some of the
[78:28] other ones and the reason for that is
[78:32] the way you we implemented this is when
[78:35] our stack needs to send a control frame
[78:38] like this we have a thing called the
[78:40] control frame manager because that was a
[78:43] good name and you just tell it oh send
[78:45] this thing and it'll send it right away
[78:47] if it can otherwise it'll buffer it and
[78:49] I don't know who the smart engineer was
[78:51] who built that thing but when they did
[78:54] um was Ryan Hamilton uh you went oh well
[78:58] you can't just keep adding to a buffer
[78:59] that could be bad so if buffer. size
[79:02] greater than a thousand close connection
[79:05] and this is the kind of guidance that
[79:07] you maybe it's a quick document maybe
[79:09] it's even a more generic thing but we
[79:11] should provide that kind of ground TRS
[79:14] because we keep shooting our toes off
[79:16] and most of the time the same attack is
[79:18] buffer. append that can be called in a
[79:23] loop
[79:30] okay uh I think that's good discuss
[79:32] discussion there um thank you
[79:36] Martin and uh as a quick Community
[79:39] Enthusiast I just want to thank Martin
[79:41] for his work on this um looking into
[79:43] things and and The Wider Community for
[79:45] handling responsible disclosure as
[79:47] mentioned this affected a few different
[79:48] implementations and could have could
[79:50] have uh had nasty side effects I say uh
[79:55] so yeah this is good I think it's a a
[79:57] good example of of how we can identify
[79:59] these issues uh in implementations um
[80:02] and get them addressed
[80:05] so kazuo would you want to come on
[80:18] up uh thank you my name I'm I'm C hul
[80:21] and I'm going to talk about quick on
[80:23] streams uh we did have a side meeting
[80:25] this morning we hope that we reflect the
[80:28] comments that we GA there in the HTTP
[80:33] working group uh that we have session on
[80:35] Friday so if you have time please come
[80:37] to that as
[80:38] well so next slide
[80:41] please uh so quick as we know quick is a
[80:45] huge success it's better than T CP and
[80:48] TLS in so many ways and we now have
[80:51] broad
[80:52] adoption like major Brothers support it
[80:55] are going to support it and it's already
[80:58] like 30% of all websites are supporting
[81:01] htb3 next
[81:03] please but TCP continues to be used at
[81:07] least as a fallback next
[81:10] please so now we have this sad state of
[81:13] application protocols where we have to
[81:17] develop and maintain two different set
[81:18] of
[81:19] stacks uh for hb3 we have for quick we
[81:24] have H 3 and for TCP we have htb2 heter
[81:28] compression are different between http2
[81:31] and
[81:31] hp3 priorities are semantically the same
[81:36] but because htb2 and htb stacks are
[81:39] different the priority logic has to be
[81:41] applied differently or I mean connected
[81:45] differently to each stack mask has two
[81:48] ways of sending UDP datagrams because
[81:52] HTP 3 has datagrams but h doesn't have
[81:55] data Ram
[81:56] frames web transport uh is like a
[82:00] completely different base for hb2 and
[82:02] hb3 and this this having two different
[82:06] Stacks uh would continue for any new
[82:10] protocol that we develop in the future
[82:12] assuming that we want to support both
[82:14] quake and ntcp as a back stop next
[82:18] please so what can we do here if you
[82:22] look at what ipv 4 and ib6 does with UDP
[82:27] and
[82:28] TCP I mean the UDP and TCP works on top
[82:32] of both ipv F Basics the only difference
[82:35] is how the addresses are being represent
[82:38] represented so if you write code for UDP
[82:42] or TCP it works on both IP V4 and V6 so
[82:46] the question is can we do the same for
[82:48] quick next
[82:51] please that's how we came up to the idea
[82:54] of of quick on streams it's essentially
[82:57] a backboard of the quick API contract on
[83:01] top of TCP so you now have quick
[83:04] streams that can be used
[83:06] on TCP and TLS so once you write the
[83:11] code that uses the quick API provided by
[83:14] quick Stacks then you can run that code
[83:18] either on quick running on top of UDP or
[83:21] running on top of quick on streams that
[83:24] runs on on top of TS and
[83:26] TCP next
[83:28] please so our goals has been to
[83:33] eliminate the need to develop new things
[83:35] on top of two protocols like hb2 and hb3
[83:37] or different protocols protocol
[83:40] protocols and the other goal has been to
[83:42] eliminate the need to deploy two
[83:43] different protocol Stacks when you
[83:45] control both sides so regardless of if
[83:48] the network passes um or blocks UDP we
[83:52] can use the same application protocol
[83:55] stack to run the
[83:57] protocol and the non goals has been to
[84:00] not spend time optimizing TCP like
[84:02] solving the head of line blocking issue
[84:04] or improving the quick frames design
[84:08] because our belief is that quick Works
[84:10] in most cases and performs better so we
[84:13] thought that quick on streams can just
[84:16] be a fallback with uh minimal effort to
[84:19] design and
[84:23] implement
[84:25] David do okay all right next
[84:28] please so the design of draft zero is
[84:31] that we just say that send quick frames
[84:34] on top of TCP NLS and there's no act
[84:36] frames all frames are implicitly
[84:39] act and we also prohibit the use of um
[84:44] frames unrelated to stream operation
[84:47] like new connection ID because we don't
[84:49] need connection IDs when we send quick
[84:51] frames on top of
[84:52] TCP and transport parameters exchange
[84:54] using the first frame called TRS
[84:56] transport parameters and for what was
[85:00] we've said that minimum maximum of frame
[85:02] size is 16 kilobytes because that's the
[85:05] size of t Circle and it provides Optimal
[85:07] Performance when you are sending stuff
[85:09] on TS and for watch was uh I succeeded
[85:14] in implementing a working uh Pro concept
[85:16] code in quickly in half a day and it
[85:19] only took a half day to use that to
[85:22] implement H3 over on streams inside h2l
[85:26] so I'd say that it's fairly easy to
[85:29] implement next
[85:32] please and this is a slide that I
[85:35] presented uh created and I'm not going
[85:37] to present it we're short on time we
[85:39] have 20 minutes in the htb agenda to
[85:41] discuss this stuff um I think we can
[85:44] move some some of what I was going to
[85:45] talk about there um I'd like to to ask
[85:48] people in the queue to come on up and
[85:50] speak uh bearing in mind we do have a
[85:52] lot more time in the hp's agenda to
[85:53] discuss stuff so if you could keep it on
[85:56] track to Quick specific matters that
[85:58] would really help us um David
[86:03] please Lucas J here can I use my Eddie
[86:07] kind of thing to actually say something
[86:09] before we start the discussion so I
[86:11] think I think this this I have talked
[86:14] with you and couple of others uh this
[86:17] things like we're trying to solve
[86:18] something obviously so my Eddie kind of
[86:21] question is like and also like those who
[86:23] will be discussing this today is what
[86:26] I'm trying what important for me to
[86:28] understand is like how this kind of um
[86:32] work um helps deploying quick in the
[86:35] Internet it's we're really doing good
[86:38] job on getting it adapted um and the one
[86:41] of the dream for for us perhaps was to
[86:44] replace TCP all together now we are
[86:46] trying to run quick over TCP I'd like to
[86:49] just understand how this impacts the
[86:52] adoption of quick and where things
[86:55] should be fixed so just think about it
[86:58] when you discussing this
[87:02] thing David skazi quick Enthusiast mask
[87:05] Enthusiast web transport Enthusiast also
[87:08] co-author of every single RFC on this
[87:10] slide
[87:13] um so thanks for this
[87:18] unfortunately I think this proposal is
[87:20] actively harmful to the adoption of
[87:23] quick um
[87:25] the fundamental concept of letting folks
[87:28] write applications over quick and saying
[87:32] don't worry we'll just hide it under the
[87:34] hood is super dangerous um
[87:39] because quick and TCP work very very
[87:42] differently right and especially when it
[87:44] comes to http datagrams and things like
[87:46] that and I think one of the goals and to
[87:50] answer zad's question that we should
[87:52] keep in mind is the overall option of
[87:54] quick and not not the overall adoption
[87:58] of quick over things like TCP because
[88:01] that's going to be a broken version of
[88:02] quick that performs incredibly poorly um
[88:05] so in particular let's say you know you
[88:08] have an application like there are very
[88:10] few applications running directly over
[88:12] quick uh and we don't want to
[88:14] necessarily tell folks to build them
[88:16] directly if they don't know what they're
[88:17] doing but let's take an example of a
[88:19] concrete one DNS over quick if you start
[88:23] running that over quick over TCP you're
[88:26] gonna have a really bad day instead of
[88:28] saying just use DNS over TLS or you know
[88:32] DNS over hgps or DNS over whatever your
[88:35] favorite thing is so I don't think the
[88:37] idea of making it easier to just deploy
[88:40] something over quick with this
[88:41] abstraction layer is the right answer
[88:44] personally so I'm not sure if I agree
[88:48] with the assumption that this you know
[88:51] uh this this has negative impact on our
[88:55] effort to deploy quick
[88:57] everywhere I mean if we think that way
[88:59] we shouldn't be developing mask for htb2
[89:03] we shouldn't have created web trans for
[89:06] htb2 but the fact is that we've accepted
[89:10] the reality that we need to support ECP
[89:13] and the the side effect that we have had
[89:17] is that that effort of uh accepting the
[89:20] reality happen at each extension that we
[89:23] develop
[89:24] and that has created duplicated efforts
[89:27] and I think that's harming us both in
[89:29] terms of writing standards as well as
[89:31] maintaining implementations and I would
[89:34] like to have a fix that applies to
[89:37] multiple approaches at the correct
[89:41] layer Eric kir to try to keep things
[89:44] reasonably quick I don't know that I'm
[89:46] deeply into the actively harmful place
[89:49] but I do very much want to solve the
[89:52] duplication of effort and the problem
[89:53] here um if you look at web transport
[89:55] over H2 that started as an effort to
[89:59] provide similar things as you could get
[90:01] from just the multiplexing layer which
[90:03] we decided was quick rather than H3 um
[90:06] for people over TCP so it would be
[90:09] interesting to have a little bit more
[90:10] discussion about how these things are
[90:13] going to layer and why we can't use some
[90:14] of those existing Solutions um a brief
[90:18] note that might be useful you mentioned
[90:20] that about 30% of servers are using
[90:22] quick we also seeing traffic from Apple
[90:25] platforms finding that about 30% of
[90:27] resources on the web are also loaded via
[90:29] quick right now so that is that number
[90:32] continues to go up the number that isn't
[90:35] going down is the number of networks
[90:37] where we think quick doesn't work um
[90:41] so do we push on that do we let this be
[90:46] the release valve and not need to push
[90:48] on that
[90:49] anymore but either way I think this is
[90:51] very much a a problem that would be good
[90:54] to solve I'm not totally sure if this is
[90:55] the solution versus any of the other
[90:58] possibilities that we have um but
[91:01] someday it would be nice to not have to
[91:03] be in in this weird dual
[91:08] World Ian s Google um thanks for
[91:11] bringing this up I'm not sure if I
[91:13] support doing this or not I need to
[91:15] think about it a lot more um although I
[91:16] will note that we kind of already did it
[91:19] technically um if you look at the web
[91:21] transport over hp2 spec um there is the
[91:24] web transport reset stream capsule the
[91:26] web transport stop in capsule web
[91:28] transport transport stream capsule I
[91:30] mean if you remove the WT underscore um
[91:34] from each of these you basically end up
[91:37] with quick over
[91:39] htb2 over TLS over T yeah I don't know
[91:44] there's a few more Turtles maybe there
[91:45] very much not an
[91:47] accident um and so I guess to some
[91:50] extent we sort of already did this which
[91:52] is not to say we shouldn't do it but
[91:53] like I'm just for for those you know and
[91:56] you know David I realize your your name
[91:58] is not an author on that but you are the
[91:59] chair so probably you noticed it
[92:00] happened
[92:02] um um I don't know so it seems like
[92:04] we're kind of already doing it so but
[92:07] but whether it needs to be a separate
[92:08] effort and quick I don't know yep I
[92:10] think that's a really good point and the
[92:13] problem is that the abstraction is not
[92:18] provided at the correct place where I
[92:21] mean so if you want to use uh web
[92:25] Transformer H2 as the abstruct of quick
[92:29] on TCP whereas using quick directly on
[92:32] UDP then you have you essentially have
[92:35] two different Stacks that you have to
[92:36] maneuver with and that's not a good
[92:40] place compared to where you just have
[92:42] one quick stack that does both TCP and
[92:45] UDP that that was actually uh taken into
[92:48] account when designing it so let's let's
[92:50] go more offline for that just in the
[92:51] issues of
[92:52] time
[92:55] yeah reminder please be brief and
[92:57] comments folks we have a lot in the
[92:59] queue we have still some agend of time
[93:02] to get through so please in in my
[93:04] previous job I I was I was layering
[93:07] protocols on top of quick that are not
[93:09] HTP and I always had to deal with the
[93:11] TCP fallback and it's just it's just
[93:13] painful just getting streams on top of
[93:16] TCP is painful all the stream
[93:17] multiplexers around there are are a big
[93:21] pain they are all terrible um so this
[93:24] this would have been very much
[93:26] appreciated if we had had this back then
[93:28] also as somebody who is implementing web
[93:30] transport over over quick I really don't
[93:33] want to want to implement web transport
[93:36] um over http2 sorry um it would be so
[93:40] much nicer if we had this and my web
[93:42] transport implementation could just
[93:44] switch to switch to quick on
[93:48] streams
[93:52] Christian
[93:55] Christine yes I'm here yeah I already
[94:00] said that in the in the email
[94:02] discussion I I understand why Kaz wants
[94:05] to do something like that
[94:09] but I'm not sure it actually solve the
[94:12] problem I mean the reason why UDP is
[94:16] blocked in various network is not
[94:20] because their Wi-Fi water cannot do UDP
[94:24] it's because some policy decision was
[94:27] made that they wanted to inspect traffic
[94:31] or protect against
[94:33] whatever and uh it's very
[94:37] unclear that those same network that
[94:40] want to protect against traffic Etc will
[94:43] let you pass quick over
[94:47] TCP that's very unclear if I if I was
[94:51] designing a solution like that say
[94:54] layers on top of
[94:56] layers I would want to do the solution
[95:00] probably either on top of web transport
[95:03] because they will resolve the problem or
[95:05] on top of web circuit sending the quick
[95:08] frames on top of web
[95:11] circuit so the I think our response
[95:15] would be that if there there are those
[95:18] midle boxes that look deep into the
[95:20] pocket then they are also likely to
[95:23] block web transport over H2 or those new
[95:25] protocol that doesn't follow but
[95:27] existing http2 does so yeah but what
[95:31] what if because there is a a wide body
[95:33] of experience of doing stuff like say
[95:36] running Skype over
[95:39] https right doing a connect for a proxy
[95:43] and that's a kind of stuff that does
[95:45] work just trying to do your own protocol
[95:47] on top of TCP typically does not work
[95:51] well so you're seeing that there's
[95:53] assumption that some endpoints look at
[95:56] the traffic pattern and they only allow
[95:58] the ones that they wanton to allow allow
[96:01] or they know that they they exist so
[96:05] yeah any new attempt uh being developed
[96:07] on top of TCP would be blocked
[96:09] regardless of it being protocol X or
[96:12] protocol y that's my
[96:15] point so but we our our procedence is
[96:19] that we expect some protocols to go
[96:21] through TCP ands that's why we continue
[96:23] to develop new protocols and so then I
[96:27] think the question is why do you think
[96:29] that quickon streams will be blocked
[96:31] while the other protocol being developed
[96:33] on top of TLS or TCP or HTTP whatever uh
[96:37] would work I don't think there's a good
[96:39] discussion in the interest of time I'd
[96:41] like to move on to to another one
[96:43] there's a good discussion please carry
[96:45] it on Martin uh Martin do Google um
[96:48] three things uh quickly first of all um
[96:50] as a protocal engineer this is super
[96:51] duper cool and like it was fun to look
[96:53] at number two um I'm concerned about NX
[96:56] kcd 927 problem uh just creating another
[97:00] permutation of things to support um uh
[97:03] like I mean I think there's a different
[97:05] working group that could give me an
[97:06] answer would be very satisfying which is
[97:08] that you know the path here is that
[97:10] eventually we will deprecate http2 and I
[97:11] can delete that code from my stack
[97:13] because this is so awesome um if it's
[97:15] not if that's not going to be the answer
[97:17] at some point in the next five to 10
[97:18] years this is again just another thing I
[97:20] have to test another thing I have to
[97:21] support and that's bad um and of course
[97:25] the other question which I think it's it
[97:26] we we can't answer yet which would be
[97:28] interesting to to to answer is what is
[97:31] the performance impact of this versus
[97:32] running over hb2 um if it's similar or
[97:35] better that is more appealing than if
[97:37] it's just worse uh ex then why would I
[97:39] do this uh if I have to keep hb2 as well
[97:43] thanks Ted uh Ted if you could go back a
[97:47] slide um and while he's going back sled
[97:49] I'll say I think you have an interesting
[97:51] idea uh but that the curent design is
[97:54] very fundamentally problematic because
[97:57] you you would like to have a common API
[97:59] but the functions beneath the common API
[98:01] are significantly different in ways that
[98:04] impact the utilization of the API and I
[98:06] think the the result of that is if you
[98:09] have a common API that you're using uh
[98:12] natively for quick and appropriately and
[98:14] you're treating the things underneath it
[98:17] when you fall back to TCP as if they
[98:19] were uh behaving as if uh they were UDP
[98:24] uh structures under quick you will find
[98:27] Corner case after Corner case where you
[98:29] have to work around it uh where you you
[98:32] can say okay we're not acknowledging it
[98:33] or we're going to treat TCP in ways that
[98:35] make it look UDP like um I just I I
[98:39] think that you're actually driving
[98:40] complexity into a different part of this
[98:42] stack uh that you'll still have to deal
[98:45] with at the end of the day because
[98:46] functionally TCP and UDP just aren't the
[98:49] same and you're not going to be able to
[98:51] get the same functions with TCP over
[98:53] streams
[98:54] um uh as you're going to be able to get
[98:56] sorry with quick over streams as you're
[98:58] going to be able to get uh for for quick
[99:00] over the the current stack just because
[99:02] of those differences so I think if you
[99:04] want to tackle this you actually have to
[99:06] tackle it by looking at what you need
[99:07] TCP to change to enable and that's a
[99:10] much bigger problem but it also has a
[99:12] much bigger payoff for potentially other
[99:15] U parts of the protocol stack
[99:18] thanks
[99:20] yav uh scor um I think there are two
[99:24] main reasons why UDP 443 or whatever
[99:26] your Port favorite Port is is uh blocked
[99:29] uh one is because people don't know
[99:31] better um just enable tcp8 TCP 443 uh
[99:35] and another reason is because uh some
[99:37] Enterprises want to inspect their
[99:38] traffic with firewalls proxies and so on
[99:41] um and if they try to inspect uh quick
[99:44] embedded in TLS and TCP things will go
[99:48] bad anyway and uh that will be a
[99:50] troubleshooting nightmares uh I think
[99:52] we've been here before um with
[99:54] websockets circuit 10 years ago when it
[99:56] was introduced it was also blocked by
[99:58] default by all those fancy proxies and
[100:01] the solution was just to deliver some
[100:03] cool Enterprise critical business
[100:05] critical applications that relied on
[100:07] websockets and now 10 years later I
[100:10] think most firewalls and proxies do
[100:13] allow web socket simply because things
[100:15] will break for Enterprises so I think
[100:18] the solution to increase quick adoption
[100:20] would be to deliver more uh great
[100:23] applications uh with multipath with web
[100:26] transport and all other great benefits
[100:28] that uh quick provides uh and I'm afraid
[100:31] this solution actually goes
[100:33] against um that uh it actually
[100:37] encourages not to enable UDP 443 to
[100:40] potentially gain those
[100:43] benefits final brief comments colen and
[100:46] Victor uh Colin Jennings I'm a David
[100:49] snazzi Enthusiast uh the the point I
[100:52] want to make here I'm very much
[100:54] interested in the the media over quick
[100:55] type use cases and they are very
[100:57] impacted by latency uh that the running
[100:59] over TCP doesn't doesn't work very well
[101:01] and the VoIP have tried all kinds of
[101:02] things I mean we have TCP Stacks where
[101:04] you act stuff that you haven't received
[101:06] to reduce the problems and stuff which
[101:08] might help this um but I think that
[101:10] we're trading off a short-term problem
[101:12] versus a long-term long-term we
[101:14] definitely agree we don't want quick to
[101:16] be blocked that would be the best thing
[101:18] and we go and ask people why are you
[101:19] blocking UDP today um there are some
[101:21] people as people have said inspection
[101:23] but I think that is almost gone at this
[101:25] point because all the VoIP major VoIP
[101:27] applications are doing strong end to-end
[101:29] encryption that's not interceptable on
[101:31] these types of proxies um so I think
[101:33] that the proxy argument has mostly gone
[101:35] away and we have reason really data to
[101:36] support that what they do say though is
[101:39] this these bizarre conversations about
[101:41] like well there was some malware virus
[101:44] that used UDP as a control Channel at
[101:46] some point and we block all of that and
[101:47] I think that those arguments are gone
[101:49] when you have a UDP connection and I
[101:52] think what we need to do that the best
[101:53] best long-term thing for us is to use
[101:54] this great application called quick and
[101:56] all the things are built on top of it to
[101:58] go argue look quick is not like your UDP
[102:01] from long ago and you can allow you
[102:03] quick through your firewall running over
[102:05] UDP and you don't need to block it and
[102:09] if we if we have a fallback for that
[102:12] that just instantly disappears there's
[102:13] no incentive for them to ever do that um
[102:15] so I don't I I I really feel like the
[102:18] long-term best solution for us is um
[102:21] let's do greasing like we do it
[102:22] everywhere else and let's say that you
[102:24] know we are going to insist that the the
[102:26] UDP version of quick Works across this
[102:29] um and I think that that will be the
[102:30] best longterm game I definitely get the
[102:32] short-term pain of everyone who's like
[102:35] oh this is such a hassle to implement
[102:36] fallback for my app and everybody has to
[102:38] do it in three different ways but I want
[102:39] to get out of that long-term you know of
[102:42] that that short-term pay so I sort of
[102:44] lean the other direction but I think
[102:45] it's a very complicated trade-off that
[102:47] we need to think about carefully and
[102:48] that we should think about the long-term
[102:50] path because the future's bigger than
[102:52] the past not the short term path
[102:55] thanks
[102:58] VI I wanted to comment on differences
[103:01] between quick and web transport web
[103:03] transport aims to provide the same
[103:06] abstractions that quick provides and in
[103:09] fact uh the Google mq
[103:12] implementation works both over Rock Wick
[103:16] and over web transfer using the same
[103:19] code because it was is intentionally
[103:22] designed to do that so uh I don't think
[103:24] the abstractions would be the issue and
[103:27] other than that from my current
[103:30] understanding of the draft it is does
[103:33] mostly the same things as web transport
[103:35] over H2
[103:38] does cool thank you very much all for
[103:40] the discussion there um thank you going
[103:44] to move on to the next agenda item which
[103:46] is the quick bdp
[103:52] frame
[104:01] you okay
[104:03] gory hi have
[104:06] you yeah
[104:08] okay wait a
[104:11] second there we go um yeah uh I've got
[104:15] quite a long delay so um I will try and
[104:18] work this with the delay um this is a
[104:21] quick bdp frame extension draft um the
[104:26] bdp
[104:27] frame is the mechanism we use to
[104:29] transport the data it's in the draft
[104:32] name it doesn't have to be the way we do
[104:34] this let's take the next slide and go
[104:37] through what we plan to do with the
[104:42] thing are you doing next or am I ah
[104:45] you're doing good okay so
[104:48] the use case for this is to transport
[104:52] what we call congestion control
[104:54] parameters CC
[104:56] prims um careful resume uses these
[105:00] for Rapid sender side U rate increase so
[105:05] it can do a fast congestion control
[105:07] startup um primary target of this is
[105:11] quick so given that it's a sender side
[105:15] change could we propose a receiver side
[105:18] change can the receiver participate in
[105:20] deciding whether a change
[105:23] in the congestion control parameters is
[105:28] a useful
[105:30] thing at a particular time are there
[105:33] trade-offs are there interactions that
[105:35] could be done with the
[105:36] applications so two different
[105:39] possibilities are obvious ways of using
[105:42] this the first one is for the client to
[105:45] tell the server please don't use careful
[105:47] resume uh because I'm not sending much
[105:50] data because I don't want to use it on
[105:52] this particular connection I want to use
[105:53] it on a later one or because I know that
[105:57] something about my path has changed and
[105:59] it really isn't a great idea it's not
[106:01] going to work out
[106:03] well the second use of this is to allow
[106:06] a the CC parameters to provide a hint to
[106:10] the upper layers to basically tell you
[106:12] what to do and the first thing not much
[106:15] above the congestion controller is it
[106:18] allows you to modify the quick flow
[106:21] credit to let you take advantage of a a
[106:23] big change in congestion control but
[106:26] there's other applications also possible
[106:28] and other parameters you can send so
[106:30] next
[106:34] slide um basically the whole thing
[106:37] starts by the client saying please can
[106:39] you tell me the congestion control
[106:42] parameter information from the server
[106:45] next
[106:48] Slide the server then sends a set of
[106:50] congestion control parameters to the
[106:52] client
[106:53] and these are authenticated so you can't
[106:57] change them at the client but you can
[106:59] read them and the set should be
[107:02] extensible because what you need now for
[107:05] congestion controller at the moment
[107:06] depends on what congestion controller
[107:08] you're using and there might well be
[107:10] good things you want to add into this
[107:13] basically once you get the information
[107:14] the client you save it next
[107:19] slide you can use it at the client but
[107:22] you also return back to the server to
[107:24] request the server to take advantage of
[107:28] this information you've given it to tell
[107:31] the server not to use it all of this is
[107:34] hints to the server so we had questions
[107:37] last time uh on the list about what
[107:39] happens to this request well it goes to
[107:42] the server the server decides to use it
[107:44] like anything else it does with
[107:46] congestion control the server is in
[107:48] control it can decide whether to use it
[107:50] or not but it gets the extra value from
[107:52] seeing the information from the client
[107:55] knowing that the client could give its
[107:57] perspective so next slide
[108:02] please
[108:04] well this draft has changed its word
[108:07] significantly since the last time uh I
[108:10] tried to present it
[108:13] uh we changed about 350 lines of text uh
[108:17] which is most of the draft to make a
[108:19] complete rewrite which says the that is
[108:22] in a more abstract way to try and bring
[108:25] this into something which could be
[108:27] extended and used for other congestion
[108:29] control
[108:31] parameters we are not particularly
[108:34] worried about um what that set of
[108:37] parameters are and we also don't really
[108:39] mind if the working group has a strong
[108:41] preference to change the methods even
[108:44] the draft name can change what we want
[108:46] to do is try and um allow the receiver
[108:49] to take part in negotia creating
[108:53] significant changes in the way in which
[108:56] the path is
[108:58] used so um do you think this is
[109:01] something that we might uh be able to
[109:03] work on in quick is this
[109:06] something that the working group has
[109:08] appeti to help us make are people
[109:12] interested in
[109:13] this here's the
[109:18] question
[109:21] m
[109:31] Martin I'm a big fan of doing careful
[109:35] resume I'm also a big fan of storing
[109:38] stuff with your token and using that on
[109:41] subsequent connections
[109:44] um these are all very useful things to
[109:47] do in your quick stack
[109:50] however I'm not sure if it's useful to
[109:54] communicate these things on the wire we
[109:57] have the token and we have the session
[109:59] ticket and the server is able to put its
[110:03] state into the token or into the session
[110:05] ticket wherever it likes uh encrypted um
[110:09] and send it to the client and when the
[110:11] connection is restored uh uh resumed it
[110:14] can restore the parameters from the
[110:16] token and resume its congestion
[110:19] controller same on the client side when
[110:21] the client saves the token or saves the
[110:24] session ticket it can attach some some
[110:26] data to it and then restore these
[110:29] parameters uh when it resumes the
[110:31] connection this is all possible right
[110:33] now we don't need any protocol for this
[110:34] this is purely an implementation
[110:36] decision what I'm worried about when we
[110:39] communicate things on the
[110:41] wire
[110:42] is
[110:44] that the congestion controller in
[110:47] use is purely an implementation
[110:51] decision client and server can use
[110:54] vastly different uh congestion
[110:56] controllers they can switch congestion
[110:57] controllers between uh between
[111:00] connections they can do all kinds of
[111:02] stuff and I'm concerned that the values
[111:06] we communicate from the server to the
[111:09] client or in the opposite direction have
[111:11] might have very different meanings to
[111:14] the congestion controllers used on both
[111:17] sides and if we do something like this
[111:20] um there's there's the implicit uh
[111:22] assumption that congestion controllers
[111:24] will work uh in the way that they do
[111:27] right
[111:29] now and I hope that there's more
[111:32] um um more Evolution going on in the
[111:35] congestion controller field and we will
[111:37] have congestion controllers that look
[111:38] very different um from what we have now
[111:41] in just a couple of
[111:43] years and I don't see how this fits into
[111:46] um this um this
[111:51] extension well I don't see any of the
[111:53] problems that you that Martin brought up
[111:55] there U I'm not
[111:57] sure that uh the details that's going to
[112:01] be revealed in this case is going to be
[112:02] the amount of capacity the amount of rtt
[112:05] and some private data which you throw
[112:08] back at the server and the server will
[112:10] decide what to use so I'm not sure that
[112:13] the uh uh giving this token or um frame
[112:18] to the client is going to stop the
[112:21] evolution of ingestion control I think
[112:24] it would help though if the client then
[112:26] have hint has hints that a particular
[112:28] interface is not available or the
[112:30] interface has changed its rate because
[112:32] the client could then tell the server
[112:35] about this so um not quite sure I by
[112:39] that but anyway go go ahead
[112:41] Ian yeah Ian Google um yeah I've been I
[112:44] mean for this has been talked about for
[112:46] a while um I've been trying to think for
[112:48] a long time about what use cases I would
[112:50] have of this um I I haven't come up with
[112:53] anything really solid I think the use
[112:57] cases I'm currently most concerned with
[112:59] are actually better addressed by scone
[113:00] Pro uh gosh that's a terrible
[113:07] name um to be completely honest um so
[113:11] um uh I I guess I would say I'm not
[113:16] interested because uh I can't find a use
[113:19] case in the different situations that
[113:21] I've looked at where this is more
[113:22] compelling than say putting something in
[113:24] the token or doing something else that I
[113:26] think works equally well um so not
[113:30] interested personally that's
[113:34] what so I'm going to jump in here both
[113:38] as uh an individual um so
[113:43] the I I think gory would it be accurate
[113:46] to say that uh not just the you're not
[113:48] just interested in potentially the the
[113:51] idea of
[113:53] sending congestion control parameters on
[113:55] the wire but the general idea of uh
[113:58] doing this kind of optimization work in
[114:00] the quick working group or or you
[114:03] because I think what we're hearing is
[114:05] push back on the particular solution um
[114:09] and I'm not sure that that's really what
[114:11] you were trying to get more clarity on
[114:12] and what we were trying to get more
[114:14] clarity
[114:16] on yeah I wasn't I mean I'm not I don't
[114:19] care about the particular solution I
[114:22] don't hugely think it matters which
[114:25] quick mechanisms we use to do this but
[114:28] um I know that we have implemented it
[114:30] using tokens and or we've implemented it
[114:32] using um bdp
[114:35] frames I was more interested in whether
[114:37] the client gets to say whether it's a
[114:40] really bad idea to do a congestion
[114:42] control change and give that hint back
[114:45] to the server so the sender side uh has
[114:49] some clue that something's changed at
[114:51] the client
[114:54] hey gory LS uh so two two points uh one
[114:58] we we tried something similar for TCP a
[115:01] long time ago it was much simpler in the
[115:02] sense that um we had this idea that when
[115:06] a client knew it was going to be
[115:09] disconnected right if if the sender
[115:11] doesn't know that then it's just
[115:13] exponentially doubling the retry time
[115:16] and then when the client is back right
[115:18] it would be nice if the client could
[115:19] tell the server hey I'm back now keep
[115:20] sending stuff and and that's sounds
[115:22] great on paper except what it does is it
[115:24] synchronizes all the clients that are
[115:26] for example on a train going into a
[115:28] tunnel and you have this massive restart
[115:31] event because all the clients say I'm
[115:33] back right so there's some subtle side
[115:34] effects here when when trying to sort of
[115:37] exchange this sort of stuff for the best
[115:39] intentions right the the other thing I
[115:40] was going to say is that this is not
[115:43] specific to this presentation but maybe
[115:44] the the theme of some of the
[115:45] presentations I said know we've designed
[115:47] quick with a lot of extensibility and
[115:49] you can just grab code points and and
[115:51] like do stuff and and there's no need to
[115:53] like with TCP to standardize ahead of
[115:55] time so so I would encourage like to
[115:58] experiment and do stuff and and then
[116:00] come here and say you know we tried this
[116:01] it seems to work pretty well here's the
[116:04] idea and here's how we want to do this
[116:05] right so I I I don't think we need a
[116:07] front load of standardization in quick
[116:09] so much as we needed to do in TCP and we
[116:10] should take advantage of
[116:18] that so um just speaking as chair this
[116:21] is some work that been presented on and
[116:23] off over the last few years there has
[116:24] been discussion on the list there been
[116:26] discussion here that's great kind of
[116:28] hearing different signals depending on
[116:31] who's in the room and those kinds of
[116:32] things so I think what would help from
[116:33] our perspective is to do a show of hands
[116:35] just to put like some numbers into
[116:37] things um to to augment the discussion
[116:41] that's happened here so I'm going to
[116:43] going to ask the question about whether
[116:44] the the working group would be uh
[116:47] interested in adopting work to solve
[116:48] this problem or this kind of use case I
[116:52] keep keeping this as as fuzzy right um
[116:55] which is annoying but that's what I want
[116:56] to do so live with it so I'm going to
[116:58] start the poll now we going to keep this
[116:59] very short so we got time for the
[117:01] clarifying question this being
[117:04] explicitly sending information like is
[117:07] described or this being the general
[117:09] congestion control exchanging
[117:12] information but not specifically the
[117:14] proposals technically on on precisely
[117:16] how to do that okay but the explicit
[117:18] sending of information which was
[117:19] something that people objected to that's
[117:21] part of the the yes great thank you for
[117:24] for clarifying um
[117:43] so
[117:46] y please please finish the poll I just
[117:49] had one quick comment after the poll
[117:51] okay okay
[117:56] for time I'm going to I'm going to close
[117:58] it there we're kind of
[118:00] seeing balance of yes no and no opinions
[118:03] there so we'll take that and we'll we'll
[118:05] follow up with some offline discussion
[118:07] thank
[118:08] you my quick comment is just going to be
[118:11] um now that there's a congestion control
[118:12] working group um I don't think it's
[118:15] unreasonable to consider discussing this
[118:19] there um
[118:24] okay thank you um thank you and Fran we
[118:29] have a couple of minutes if you would
[118:30] like to jump on to discuss FEC we can
[118:33] give you as much time as we'll have
[118:35] until we start leaving the
[118:40] room yeah I can talk a few minutes for
[118:43] sure okay um can we get the slides up M
[118:50] yeah so there's like two minutes
[118:51] remaining so try to be like really
[118:56] fast okay so next
[118:59] slide so you already know that when you
[119:03] lose some packets you have to wait one
[119:05] RT to recover it next next
[119:08] slide so with FC you can compute repair
[119:12] symbols so that you can send it with
[119:14] your packets and so you can recover the
[119:15] L packets without having to wait for the
[119:18] this rtt so it's helpful for videos or a
[119:23] short West response use case for
[119:25] instance next
[119:28] slide so we made a draft for quick and
[119:31] FC actually I spent my thesis on that uh
[119:35] we have an implementation of our last
[119:37] prototype based on clo FL skish
[119:41] implementation so here the talk is
[119:43] especially to make you aware that it
[119:45] exists that we had interesting results
[119:47] that we may want to do some stuff with
[119:50] mqt uh given the the recent discussions
[119:53] on mqt recently about FC next
[119:58] slide so some of our experiments were
[120:01] about tunneling an RTP stream and
[120:04] sending FC to protect the RTP stream and
[120:06] see how it works with with quick so we
[120:08] have an RTP stream turn it through uh
[120:12] quick with FC and look at the Sim of the
[120:15] video next
[120:18] slide uh yeah so it's videos of drone
[120:21] drone live recording next
[120:24] slide so our results um so to be quite
[120:29] quick so on the Y AIS you have the
[120:32] average Sim so the average Fidelity of
[120:35] the video and on the x-axis you have the
[120:38] playback buffer so the latency that you
[120:40] add in the
[120:41] Stream uh our implementation
[120:44] implementation is is the red box um the
[120:48] green box is the classical quick and you
[120:51] have RTP an RTP with FC in blue and
[120:54] yellow and what what we could see is
[120:56] that we had good uh average of video
[121:00] Fidelity results with different playback
[121:03] buffers when you increase the playback
[121:04] buffer so if you increase the latency
[121:07] then RR Transmissions work well and so
[121:09] classical quick works well but we low
[121:12] buffers uh our quick
[121:16] FC uh obtains good uh video fedility
[121:19] results um compared to to the the
[121:22] classical quick and even compared to
[121:23] some rtpc
[121:25] implementation uh I think you can go to
[121:27] the last
[121:30] slide yeah so here are some links so
[121:33] there is a link to my thesis I just
[121:35] wanted to make you aware that it exists
[121:37] let me know if you want to collaborate
[121:39] uh Kish is not totally compatible to
[121:42] mqt right now but it's doable because uh
[121:46] We've pushed some generic trites to make
[121:47] it work so we might want to to do that
[121:50] instead of teling AR streams So yeah
[121:53] thank you for
[121:55] listening thank you for being so brief
[121:58] Fran we we had Abby in the que I'm
[122:00] afraid we're over time so I'm gonna I'm
[122:02] GNA have to make you sit back down sorry
[122:05] uh if you have questions FR I'll take
[122:06] them offline go on the list there's some
[122:08] good stuff there um yes I just want to
[122:10] thank everyone for their time here
[122:12] especially our note takers and any other
[122:13] people there was some great discussion
[122:15] in the in the
[122:17] jabber I'll be sure to try and follow up
[122:19] on some of that too um yeah yes and
[122:23] thank you all safe travels goodbye for
[122:50] now
[122:57] [Music]
[123:20] keep
[123:34] I like the like the the spiciness in the
[123:45] chat the point of crazy is is exactly
[123:50] that
[123:52] I
[124:04] for this
[124:06] is3 it's exactly the same thing
[124:10] right ht3 is the same
[124:14] thing it's
[124:19] deliber I didn't up
[124:24] [Music]
[124:25] uh at one point I thought we might
[124:28] finish
[124:33] yeah it was really good to have it I
[124:35] want to but um I supect what happen in h
[124:39] session is a lot of what people wanted
[124:42] to
[124:50] express
[124:52] that's on record now and we come back
[124:54] and go we should have done
[125:05] this

=== Content from github.com_f7c79950_20250115_180208.html ===

[Skip to content](#start-of-content)

## Navigation Menu

Toggle navigation

[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fquic-go%2Fquic-go%2Fcommit%2F4a99b816ae3ab03ae5449d15aac45147c85ed47a)

* Product

  + [GitHub Copilot
    Write better code with AI](https://github.com/features/copilot)
  + [Security
    Find and fix vulnerabilities](https://github.com/features/security)
  + [Actions
    Automate any workflow](https://github.com/features/actions)
  + [Codespaces
    Instant dev environments](https://github.com/features/codespaces)
  + [Issues
    Plan and track work](https://github.com/features/issues)
  + [Code Review
    Manage code changes](https://github.com/features/code-review)
  + [Discussions
    Collaborate outside of code](https://github.com/features/discussions)
  + [Code Search
    Find more, search less](https://github.com/features/code-search)

  Explore
  + [All features](https://github.com/features)
  + [Documentation](https://docs.github.com)
  + [GitHub Skills](https://skills.github.com)
  + [Blog](https://github.blog)
* Solutions

  By company size
  + [Enterprises](https://github.com/enterprise)
  + [Small and medium teams](https://github.com/team)
  + [Startups](https://github.com/enterprise/startups)
  By use case
  + [DevSecOps](/solutions/use-case/devsecops)
  + [DevOps](/solutions/use-case/devops)
  + [CI/CD](/solutions/use-case/ci-cd)
  + [View all use cases](/solutions/use-case)

  By industry
  + [Healthcare](/solutions/industry/healthcare)
  + [Financial services](/solutions/industry/financial-services)
  + [Manufacturing](/solutions/industry/manufacturing)
  + [Government](/solutions/industry/government)
  + [View all industries](/solutions/industry)

  [View all solutions](/solutions)
* Resources

  Topics
  + [AI](/resources/articles/ai)
  + [DevOps](/resources/articles/devops)
  + [Security](/resources/articles/security)
  + [Software Development](/resources/articles/software-development)
  + [View all](/resources/articles)

  Explore
  + [Learning Pathways](https://resources.github.com/learn/pathways)
  + [White papers, Ebooks, Webinars](https://resources.github.com)
  + [Customer Stories](https://github.com/customer-stories)
  + [Partners](https://partner.github.com)
  + [Executive Insights](https://github.com/solutions/executive-insights)
* Open Source

  + [GitHub Sponsors
    Fund open source developers](/sponsors)
  + [The ReadME Project
    GitHub community articles](https://github.com/readme)
  Repositories
  + [Topics](https://github.com/topics)
  + [Trending](https://github.com/trending)
  + [Collections](https://github.com/collections)
* Enterprise

  + [Enterprise platform
    AI-powered developer platform](/enterprise)
  Available add-ons
  + [Advanced Security
    Enterprise-grade security features](https://github.com/enterprise/advanced-security)
  + [GitHub Copilot
    Enterprise-grade AI features](/features/copilot#enterprise)
  + [Premium Support
    Enterprise-grade 24/7 support](/premium-support)
* [Pricing](https://github.com/pricing)

Search or jump to...

# Search code, repositories, users, issues, pull requests...

Search

Clear

[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)

# Provide feedback

We read every piece of feedback, and take your input very seriously.

Include my email address so I can be contacted

  Cancel

 Submit feedback

# Saved searches

## Use saved searches to filter your results more quickly

Name

Query

To see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).

  Cancel

 Create saved search

[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fquic-go%2Fquic-go%2Fcommit%2F4a99b816ae3ab03ae5449d15aac45147c85ed47a)

[Sign up](/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E%2Fvoltron%2Fcommit_fragments%2Frepo_layout&source=header-repo&source_repo=quic-go%2Fquic-go)
Reseting focus

You signed in with another tab or window. Reload to refresh your session.
You signed out in another tab or window. Reload to refresh your session.
You switched accounts on another tab or window. Reload to refresh your session.

Dismiss alert

{{ message }}

[quic-go](/quic-go)
/
**[quic-go](/quic-go/quic-go)**
Public

* [Notifications](/login?return_to=%2Fquic-go%2Fquic-go) You must be signed in to change notification settings
* [Fork
  1.3k](/login?return_to=%2Fquic-go%2Fquic-go)
* [Star
   10.3k](/login?return_to=%2Fquic-go%2Fquic-go)

* [Code](/quic-go/quic-go)
* [Issues
  160](/quic-go/quic-go/issues)
* [Pull requests
  38](/quic-go/quic-go/pulls)
* [Actions](/quic-go/quic-go/actions)
* [Wiki](/quic-go/quic-go/wiki)
* [Security](/quic-go/quic-go/security)
* [Insights](/quic-go/quic-go/pulse)

Additional navigation options

* [Code](/quic-go/quic-go)
* [Issues](/quic-go/quic-go/issues)
* [Pull requests](/quic-go/quic-go/pulls)
* [Actions](/quic-go/quic-go/actions)
* [Wiki](/quic-go/quic-go/wiki)
* [Security](/quic-go/quic-go/security)
* [Insights](/quic-go/quic-go/pulse)

## Commit

[Permalink](/quic-go/quic-go/commit/4a99b816ae3ab03ae5449d15aac45147c85ed47a)

This commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.

close connection when an abnormally large number of frames are queued ([…](https://github.com/quic-go/quic-go/pull/4369)

[Browse files](/quic-go/quic-go/tree/4a99b816ae3ab03ae5449d15aac45147c85ed47a)
Browse the repository at this point in the history

```
[…#4369](https://github.com/quic-go/quic-go/pull/4369))

Under normal circumstances, we should be able to send out control frames
right away, so we don't expect any queue to build up. To defend against
resource exhaustion attacks, we limit the control frame queue to 16384
elements.
```

* Loading branch information

[![@marten-seemann](https://avatars.githubusercontent.com/u/1478487?s=40&v=4)](/marten-seemann)

[marten-seemann](/quic-go/quic-go/commits?author=marten-seemann "View all commits by marten-seemann")
authored
Mar 18, 2024

1 parent
[9971fed](/quic-go/quic-go/commit/9971fedd42e9cb853d05fa94809d84743ffa010f)

commit 4a99b81

 Show file tree

 Hide file tree

Showing
**3 changed files**
with
**44 additions**
and
**4 deletions**.

* Whitespace
* Ignore whitespace

* Split
* Unified

* connection.go
  [connection.go](#diff-25a7cf08cc8fc8eb57475835cc380524de2fd1215cda9c6915132ae86dbd80a7)
* framer.go
  [framer.go](#diff-a97a2544cb1bf86661368aeb59921e0629ab259d2728c61715e856a7ab5c1f6f)
* framer\_test.go
  [framer\_test.go](#diff-44d4e5f3567272c5d38adad20085dc713b183d678b6d1832bd455ea97bf88d25)

## There are no files selected for viewing

3 changes: 3 additions & 0 deletions

3
[connection.go](#diff-25a7cf08cc8fc8eb57475835cc380524de2fd1215cda9c6915132ae86dbd80a7 "connection.go")

Show comments

[View file](/quic-go/quic-go/blob/4a99b816ae3ab03ae5449d15aac45147c85ed47a/connection.go)
Edit file

Delete file

This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters.
[Learn more about bidirectional Unicode characters](https://github.co/hiddenchars)

  [Show hidden characters](%7B%7B%20revealButtonHref%20%7D%7D)

| Original file line number | Diff line number | Diff line change |
| --- | --- | --- |
| Expand Up | | @@ -521,6 +521,9 @@ func (s \*connection) run() error { |
|  |  |  |
|  |  | runLoop: |
|  |  | for { |
|  |  | if s.framer.QueuedTooManyControlFrames() { |
|  |  | s.closeLocal(&qerr.TransportError{ErrorCode: InternalError}) |
|  |  | } |
|  |  | // Close immediately if requested |
|  |  | select { |
|  |  | case closeErr = <-s.closeChan: |
| Expand Down | |  |

28 changes: 24 additions & 4 deletions

28
[framer.go](#diff-a97a2544cb1bf86661368aeb59921e0629ab259d2728c61715e856a7ab5c1f6f "framer.go")

Show comments

[View file](/quic-go/quic-go/blob/4a99b816ae3ab03ae5449d15aac45147c85ed47a/framer.go)
Edit file

Delete file

This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters.
[Learn more about bidirectional Unicode characters](https://github.co/hiddenchars)

  [Show hidden characters](%7B%7B%20revealButtonHref%20%7D%7D)

| Original file line number | Diff line number | Diff line change |
| --- | --- | --- |
| Expand Up | | @@ -21,9 +21,19 @@ type framer interface { |
|  |  | AppendStreamFrames([]ackhandler.StreamFrame, protocol.ByteCount, protocol.Version) ([]ackhandler.StreamFrame, protocol.ByteCount) |
|  |  |  |
|  |  | Handle0RTTRejection() error |
|  |  |  |
|  |  | // QueuedTooManyControlFrames says if the control frame queue exceeded its maximum queue length. |
|  |  | // This is a hack. |
|  |  | // It is easier to implement than propagating an error return value in QueueControlFrame. |
|  |  | // The correct solution would be to queue frames with their respective structs. |
|  |  | // See https://github.com/quic-go/quic-go/issues/4271 for the queueing of stream-related control frames. |
|  |  | QueuedTooManyControlFrames() bool |
|  |  | } |
|  |  |  |
|  |  | const maxPathResponses = 256 |
|  |  | const ( |
|  |  | maxPathResponses = 256 |
|  |  | maxControlFrames = 16 << 10 |
|  |  | ) |
|  |  |  |
|  |  | type framerI struct { |
|  |  | mutex sync.Mutex |
| Expand All | | @@ -33,9 +43,10 @@ type framerI struct { |
|  |  | activeStreams map[protocol.StreamID]struct{} |
|  |  | streamQueue ringbuffer.RingBuffer[protocol.StreamID] |
|  |  |  |
|  |  | controlFrameMutex sync.Mutex |
|  |  | controlFrames []wire.Frame |
|  |  | pathResponses []\*wire.PathResponseFrame |
|  |  | controlFrameMutex sync.Mutex |
|  |  | controlFrames []wire.Frame |
|  |  | pathResponses []\*wire.PathResponseFrame |
|  |  | queuedTooManyControlFrames bool |
|  |  | } |
|  |  |  |
|  |  | var \_ framer = &framerI{} |
| Expand Down  Expand Up | | @@ -73,6 +84,11 @@ func (f \*framerI) QueueControlFrame(frame wire.Frame) { |
|  |  | f.pathResponses = append(f.pathResponses, pr) |
|  |  | return |
|  |  | } |
|  |  | // This is a hack. |
|  |  | if len(f.controlFrames) >= maxControlFrames { |
|  |  | f.queuedTooManyControlFrames = true |
|  |  | return |
|  |  | } |
|  |  | f.controlFrames = append(f.controlFrames, frame) |
|  |  | } |
|  |  |  |
| Expand Down  Expand Up | | @@ -105,6 +121,10 @@ func (f \*framerI) AppendControlFrames(frames []ackhandler.Frame, maxLen protocol |
|  |  | return frames, length |
|  |  | } |
|  |  |  |
|  |  | func (f \*framerI) QueuedTooManyControlFrames() bool { |
|  |  | return f.queuedTooManyControlFrames |
|  |  | } |
|  |  |  |
|  |  | func (f \*framerI) AddActiveStream(id protocol.StreamID) { |
|  |  | f.mutex.Lock() |
|  |  | if \_, ok := f.activeStreams[id]; !ok { |
| Expand Down | |  |

17 changes: 17 additions & 0 deletions

17
[framer\_test.go](#diff-44d4e5f3567272c5d38adad20085dc713b183d678b6d1832bd455ea97bf88d25 "framer_test.go")

Show comments

[View file](/quic-go/quic-go/blob/4a99b816ae3ab03ae5449d15aac45147c85ed47a/framer_test.go)
Edit file

Delete file

This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters.
[Learn more about bidirectional Unicode characters](https://github.co/hiddenchars)

  [Show hidden characters](%7B%7B%20revealButtonHref%20%7D%7D)

| Original file line number | Diff line number | Diff line change |
| --- | --- | --- |
| Expand Up | | @@ -109,6 +109,23 @@ var \_ = Describe("Framer", func() { |
|  |  | Expect(fs).To(HaveLen(2)) |
|  |  | Expect(length).To(Equal(ping.Length(version) + ncid.Length(version))) |
|  |  | }) |
|  |  |  |
|  |  | It("detects when too many frames are queued", func() { |
|  |  | for i := 0; i < maxControlFrames-1; i++ { |
|  |  | framer.QueueControlFrame(&wire.PingFrame{}) |
|  |  | framer.QueueControlFrame(&wire.PingFrame{}) |
|  |  | Expect(framer.QueuedTooManyControlFrames()).To(BeFalse()) |
|  |  | frames, \_ := framer.AppendControlFrames([]ackhandler.Frame{}, 1, protocol.Version1) |
|  |  | Expect(frames).To(HaveLen(1)) |
|  |  | Expect(framer.(\*framerI).controlFrames).To(HaveLen(i + 1)) |
|  |  | } |
|  |  | framer.QueueControlFrame(&wire.PingFrame{}) |
|  |  | Expect(framer.QueuedTooManyControlFrames()).To(BeFalse()) |
|  |  | Expect(framer.(\*framerI).controlFrames).To(HaveLen(maxControlFrames)) |
|  |  | framer.QueueControlFrame(&wire.PingFrame{}) |
|  |  | Expect(framer.QueuedTooManyControlFrames()).To(BeTrue()) |
|  |  | Expect(framer.(\*framerI).controlFrames).To(HaveLen(maxControlFrames)) |
|  |  | }) |
|  |  | }) |
|  |  |  |
|  |  | Context("handling PATH\_RESPONSE frames", func() { |
| Expand Down | |  |

Toggle all file notes
Toggle all file annotations

### 0 comments on commit `4a99b81`

Please
[sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fquic-go%2Fquic-go%2Fcommit%2F4a99b816ae3ab03ae5449d15aac45147c85ed47a) to comment.

## Footer

© 2025 GitHub, Inc.

### Footer navigation

* [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)
* [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)
* [Security](https://github.com/security)
* [Status](https://www.githubstatus.com/)
* [Docs](https://docs.github.com/)
* [Contact](https://support.github.com?tags=dotcom-footer)
* Manage cookies
* Do not share my personal information

You can’t perform that action at this time.



=== Content from github.com_154f2735_20250115_180208.html ===

[Skip to content](#start-of-content)

## Navigation Menu

Toggle navigation

[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fquic-go%2Fquic-go%2Fsecurity%2Fadvisories%2FGHSA-c33x-xqrf-c478)

* Product

  + [GitHub Copilot
    Write better code with AI](https://github.com/features/copilot)
  + [Security
    Find and fix vulnerabilities](https://github.com/features/security)
  + [Actions
    Automate any workflow](https://github.com/features/actions)
  + [Codespaces
    Instant dev environments](https://github.com/features/codespaces)
  + [Issues
    Plan and track work](https://github.com/features/issues)
  + [Code Review
    Manage code changes](https://github.com/features/code-review)
  + [Discussions
    Collaborate outside of code](https://github.com/features/discussions)
  + [Code Search
    Find more, search less](https://github.com/features/code-search)

  Explore
  + [All features](https://github.com/features)
  + [Documentation](https://docs.github.com)
  + [GitHub Skills](https://skills.github.com)
  + [Blog](https://github.blog)
* Solutions

  By company size
  + [Enterprises](https://github.com/enterprise)
  + [Small and medium teams](https://github.com/team)
  + [Startups](https://github.com/enterprise/startups)
  By use case
  + [DevSecOps](/solutions/use-case/devsecops)
  + [DevOps](/solutions/use-case/devops)
  + [CI/CD](/solutions/use-case/ci-cd)
  + [View all use cases](/solutions/use-case)

  By industry
  + [Healthcare](/solutions/industry/healthcare)
  + [Financial services](/solutions/industry/financial-services)
  + [Manufacturing](/solutions/industry/manufacturing)
  + [Government](/solutions/industry/government)
  + [View all industries](/solutions/industry)

  [View all solutions](/solutions)
* Resources

  Topics
  + [AI](/resources/articles/ai)
  + [DevOps](/resources/articles/devops)
  + [Security](/resources/articles/security)
  + [Software Development](/resources/articles/software-development)
  + [View all](/resources/articles)

  Explore
  + [Learning Pathways](https://resources.github.com/learn/pathways)
  + [White papers, Ebooks, Webinars](https://resources.github.com)
  + [Customer Stories](https://github.com/customer-stories)
  + [Partners](https://partner.github.com)
  + [Executive Insights](https://github.com/solutions/executive-insights)
* Open Source

  + [GitHub Sponsors
    Fund open source developers](/sponsors)
  + [The ReadME Project
    GitHub community articles](https://github.com/readme)
  Repositories
  + [Topics](https://github.com/topics)
  + [Trending](https://github.com/trending)
  + [Collections](https://github.com/collections)
* Enterprise

  + [Enterprise platform
    AI-powered developer platform](/enterprise)
  Available add-ons
  + [Advanced Security
    Enterprise-grade security features](https://github.com/enterprise/advanced-security)
  + [GitHub Copilot
    Enterprise-grade AI features](/features/copilot#enterprise)
  + [Premium Support
    Enterprise-grade 24/7 support](/premium-support)
* [Pricing](https://github.com/pricing)

Search or jump to...

# Search code, repositories, users, issues, pull requests...

Search

Clear

[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)

# Provide feedback

We read every piece of feedback, and take your input very seriously.

Include my email address so I can be contacted

  Cancel

 Submit feedback

# Saved searches

## Use saved searches to filter your results more quickly

Name

Query

To see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).

  Cancel

 Create saved search

[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fquic-go%2Fquic-go%2Fsecurity%2Fadvisories%2FGHSA-c33x-xqrf-c478)

[Sign up](/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E%2Frepos%2Fadvisories%2Fshow&source=header-repo&source_repo=quic-go%2Fquic-go)
Reseting focus

You signed in with another tab or window. Reload to refresh your session.
You signed out in another tab or window. Reload to refresh your session.
You switched accounts on another tab or window. Reload to refresh your session.

Dismiss alert

{{ message }}

[quic-go](/quic-go)
/
**[quic-go](/quic-go/quic-go)**
Public

* [Notifications](/login?return_to=%2Fquic-go%2Fquic-go) You must be signed in to change notification settings
* [Fork
  1.3k](/login?return_to=%2Fquic-go%2Fquic-go)
* [Star
   10.3k](/login?return_to=%2Fquic-go%2Fquic-go)

* [Code](/quic-go/quic-go)
* [Issues
  160](/quic-go/quic-go/issues)
* [Pull requests
  38](/quic-go/quic-go/pulls)
* [Actions](/quic-go/quic-go/actions)
* [Wiki](/quic-go/quic-go/wiki)
* [Security](/quic-go/quic-go/security)
* [Insights](/quic-go/quic-go/pulse)

Additional navigation options

* [Code](/quic-go/quic-go)
* [Issues](/quic-go/quic-go/issues)
* [Pull requests](/quic-go/quic-go/pulls)
* [Actions](/quic-go/quic-go/actions)
* [Wiki](/quic-go/quic-go/wiki)
* [Security](/quic-go/quic-go/security)
* [Insights](/quic-go/quic-go/pulse)

# Memory Exhaustion Attack against QUIC's Connection ID Mechanism

Moderate

[marten-seemann](/marten-seemann)
published
GHSA-c33x-xqrf-c478
Apr 2, 2024

## Package

gomod

quic-go
([Go](/advisories?query=ecosystem%3Ago))

## Affected versions

< v0.42.0

## Patched versions

v0.42.0

## Description

An attacker can cause its peer to run out of memory sending a large number of NEW\_CONNECTION\_ID frames that retire old connection IDs. The receiver is supposed to respond to each retirement frame with a RETIRE\_CONNECTION\_ID frame. The attacker can prevent the receiver from sending out (the vast majority of) these RETIRE\_CONNECTION\_ID frames by collapsing the peers congestion window (by selectively acknowledging received packets) and by manipulating the peer's RTT estimate.

I published a more detailed description of the attack and its mitigation in this blog post: <https://seemann.io/posts/2024-03-19-exploiting-quics-connection-id-management/>.

I also presented this attack in the IETF QUIC working group session at IETF 119: <https://youtu.be/JqXtYcZAtIA?si=nJ31QKLBSTRXY35U&t=3683>

There's no way to mitigate this attack, please update quic-go to a version that contains the fix.

### Severity

Moderate

### CVE ID

CVE-2024-22189

### Weaknesses

No CWEs

### Credits

* [![@marten-seemann](https://avatars.githubusercontent.com/u/1478487?s=40&v=4)](/marten-seemann)
  [marten-seemann](/marten-seemann)
  Finder

## Footer

© 2025 GitHub, Inc.

### Footer navigation

* [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)
* [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)
* [Security](https://github.com/security)
* [Status](https://www.githubstatus.com/)
* [Docs](https://docs.github.com/)
* [Contact](https://support.github.com?tags=dotcom-footer)
* Manage cookies
* Do not share my personal information

You can’t perform that action at this time.



=== Content from avatars.githubusercontent.com_9068e71c_20250115_214934.html ===
���� �  

 $.' ",#(7),01444'9=82<.342 
2!!22222222222222222222222222222222222222222222222222��  ( (" ���          
   } !1AQa"q2���#B��R��$3br�
%&'()\*456789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz�������������������������������������������������������������������������       
  w !1AQaq"2�B���� #3R�br�
$4�%�&'()\*56789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz��������������������������������������������������������������������������   ? �k��>��>?���o�%�\*$ԝw�+= �>���|����^W��Jk��A�D��xڣ��S7eqS�3��k�OM1�j��Aݳh+��q��/���Gm,7P�7��y�+Џ�������Ī�� v�Z�W�������;v�c!�>�k8�m���)(����3A��L
����k�|{c�MZ�W �Q��#���ѵ��M:��7�1[½^W
?Z�� x�/x�i�S��"���|�x;�}1���Ҫ�+�t[RV5�7xWT�h�n鴯�[8��^�k,sG�ƁF�t� �ז�bMj+ֵ�a���,���]\_��m���A �[O�yr��
���5�%��^wV:�h�39�5�r8�֧��u��Is/mǅ� �
ʷ���c���6�:�>�T��c})�4{F���gE���Q�|v��q t���%��Fp�#�ޭh���� ס� ё�/�/�TS��v.srJ�|9�#^[��u!i������~��]�kƾ�ܟ��J�J�f��
