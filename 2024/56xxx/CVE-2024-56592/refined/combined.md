=== Content from git.kernel.org_bc278fe6_20250115_100030.html ===


| [cgit logo](/) | [index](/) : [kernel/git/stable/linux.git](/pub/scm/linux/kernel/git/stable/linux.git/) | linux-2.6.11.y linux-2.6.12.y linux-2.6.13.y linux-2.6.14.y linux-2.6.15.y linux-2.6.16.y linux-2.6.17.y linux-2.6.18.y linux-2.6.19.y linux-2.6.20.y linux-2.6.21.y linux-2.6.22.y linux-2.6.23.y linux-2.6.24.y linux-2.6.25.y linux-2.6.26.y linux-2.6.27.y linux-2.6.28.y linux-2.6.29.y linux-2.6.30.y linux-2.6.31.y linux-2.6.32.y linux-2.6.33.y linux-2.6.34.y linux-2.6.35.y linux-2.6.36.y linux-2.6.37.y linux-2.6.38.y linux-2.6.39.y linux-3.0.y linux-3.1.y linux-3.10.y linux-3.11.y linux-3.12.y linux-3.13.y linux-3.14.y linux-3.15.y linux-3.16.y linux-3.17.y linux-3.18.y linux-3.19.y linux-3.2.y linux-3.3.y linux-3.4.y linux-3.5.y linux-3.6.y linux-3.7.y linux-3.8.y linux-3.9.y linux-4.0.y linux-4.1.y linux-4.10.y linux-4.11.y linux-4.12.y linux-4.13.y linux-4.14.y linux-4.15.y linux-4.16.y linux-4.17.y linux-4.18.y linux-4.19.y linux-4.2.y linux-4.20.y linux-4.3.y linux-4.4.y linux-4.5.y linux-4.6.y linux-4.7.y linux-4.8.y linux-4.9.y linux-5.0.y linux-5.1.y linux-5.10.y linux-5.11.y linux-5.12.y linux-5.13.y linux-5.14.y linux-5.15.y linux-5.16.y linux-5.17.y linux-5.18.y linux-5.19.y linux-5.2.y linux-5.3.y linux-5.4.y linux-5.5.y linux-5.6.y linux-5.7.y linux-5.8.y linux-5.9.y linux-6.0.y linux-6.1.y linux-6.10.y linux-6.11.y linux-6.12.y linux-6.2.y linux-6.3.y linux-6.4.y linux-6.5.y linux-6.6.y linux-6.7.y linux-6.8.y linux-6.9.y linux-rolling-lts linux-rolling-stable master |
| --- | --- | --- |
| Linux kernel stable tree | Stable Group |

| [about](/pub/scm/linux/kernel/git/stable/linux.git/about/)[summary](/pub/scm/linux/kernel/git/stable/linux.git/)[refs](/pub/scm/linux/kernel/git/stable/linux.git/refs/?id=10e8a2dec9ff1b81de8e892b0850924038adbc6d)[log](/pub/scm/linux/kernel/git/stable/linux.git/log/)[tree](/pub/scm/linux/kernel/git/stable/linux.git/tree/?id=10e8a2dec9ff1b81de8e892b0850924038adbc6d)[commit](/pub/scm/linux/kernel/git/stable/linux.git/commit/?id=10e8a2dec9ff1b81de8e892b0850924038adbc6d)[diff](/pub/scm/linux/kernel/git/stable/linux.git/diff/?id=10e8a2dec9ff1b81de8e892b0850924038adbc6d)[stats](/pub/scm/linux/kernel/git/stable/linux.git/stats/) | log msg author committer range |
| --- | --- |

**diff options**

|  | |
| --- | --- |
| context: | 12345678910152025303540 |
| space: | includeignore |
| mode: | unifiedssdiffstat only |
|  |  |

| author | Hou Tao <houtao1@huawei.com> | 2024-11-06 14:35:40 +0800 |
| --- | --- | --- |
| committer | Greg Kroah-Hartman <gregkh@linuxfoundation.org> | 2024-12-14 20:00:08 +0100 |
| commit | [10e8a2dec9ff1b81de8e892b0850924038adbc6d](/pub/scm/linux/kernel/git/stable/linux.git/commit/?id=10e8a2dec9ff1b81de8e892b0850924038adbc6d) ([patch](/pub/scm/linux/kernel/git/stable/linux.git/patch/?id=10e8a2dec9ff1b81de8e892b0850924038adbc6d)) | |
| tree | [a40a521304433484553a6ec6cdedc674e68a9460](/pub/scm/linux/kernel/git/stable/linux.git/tree/?id=10e8a2dec9ff1b81de8e892b0850924038adbc6d) | |
| parent | [07c020c6d14d29e5a3ea4e4576b8ecf956a80834](/pub/scm/linux/kernel/git/stable/linux.git/commit/?id=07c020c6d14d29e5a3ea4e4576b8ecf956a80834) ([diff](/pub/scm/linux/kernel/git/stable/linux.git/diff/?id=10e8a2dec9ff1b81de8e892b0850924038adbc6d&id2=07c020c6d14d29e5a3ea4e4576b8ecf956a80834)) | |
| download | [linux-10e8a2dec9ff1b81de8e892b0850924038adbc6d.tar.gz](/pub/scm/linux/kernel/git/stable/linux.git/snapshot/linux-10e8a2dec9ff1b81de8e892b0850924038adbc6d.tar.gz) | |

bpf: Call free\_htab\_elem() after htab\_unlock\_bucket()[ Upstream commit b9e9ed90b10c82a4e9d4d70a2890f06bfcdd3b78 ]
For htab of maps, when the map is removed from the htab, it may hold the
last reference of the map. bpf\_map\_fd\_put\_ptr() will invoke
bpf\_map\_free\_id() to free the id of the removed map element. However,
bpf\_map\_fd\_put\_ptr() is invoked while holding a bucket lock
(raw\_spin\_lock\_t), and bpf\_map\_free\_id() attempts to acquire map\_idr\_lock
(spinlock\_t), triggering the following lockdep warning:
=============================
[ BUG: Invalid wait context ]
6.11.0-rc4+ #49 Not tainted
-----------------------------
test\_maps/4881 is trying to lock:
ffffffff84884578 (map\_idr\_lock){+...}-{3:3}, at: bpf\_map\_free\_id.part.0+0x21/0x70
other info that might help us debug this:
context-{5:5}
2 locks held by test\_maps/4881:
#0: ffffffff846caf60 (rcu\_read\_lock){....}-{1:3}, at: bpf\_fd\_htab\_map\_update\_elem+0xf9/0x270
#1: ffff888149ced148 (&htab->lockdep\_key#2){....}-{2:2}, at: htab\_map\_update\_elem+0x178/0xa80
stack backtrace:
CPU: 0 UID: 0 PID: 4881 Comm: test\_maps Not tainted 6.11.0-rc4+ #49
Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), ...
Call Trace:
<TASK>
dump\_stack\_lvl+0x6e/0xb0
dump\_stack+0x10/0x20
\_\_lock\_acquire+0x73e/0x36c0
lock\_acquire+0x182/0x450
\_raw\_spin\_lock\_irqsave+0x43/0x70
bpf\_map\_free\_id.part.0+0x21/0x70
bpf\_map\_put+0xcf/0x110
bpf\_map\_fd\_put\_ptr+0x9a/0xb0
free\_htab\_elem+0x69/0xe0
htab\_map\_update\_elem+0x50f/0xa80
bpf\_fd\_htab\_map\_update\_elem+0x131/0x270
htab\_map\_update\_elem+0x50f/0xa80
bpf\_fd\_htab\_map\_update\_elem+0x131/0x270
bpf\_map\_update\_value+0x266/0x380
\_\_sys\_bpf+0x21bb/0x36b0
\_\_x64\_sys\_bpf+0x45/0x60
x64\_sys\_call+0x1b2a/0x20d0
do\_syscall\_64+0x5d/0x100
entry\_SYSCALL\_64\_after\_hwframe+0x76/0x7e
One way to fix the lockdep warning is using raw\_spinlock\_t for
map\_idr\_lock as well. However, bpf\_map\_alloc\_id() invokes
idr\_alloc\_cyclic() after acquiring map\_idr\_lock, it will trigger a
similar lockdep warning because the slab's lock (s->cpu\_slab->lock) is
still a spinlock.
Instead of changing map\_idr\_lock's type, fix the issue by invoking
htab\_put\_fd\_value() after htab\_unlock\_bucket(). However, only deferring
the invocation of htab\_put\_fd\_value() is not enough, because the old map
pointers in htab of maps can not be saved during batched deletion.
Therefore, also defer the invocation of free\_htab\_elem(), so these
to-be-freed elements could be linked together similar to lru map.
There are four callers for ->map\_fd\_put\_ptr:
(1) alloc\_htab\_elem() (through htab\_put\_fd\_value())
It invokes ->map\_fd\_put\_ptr() under a raw\_spinlock\_t. The invocation of
htab\_put\_fd\_value() can not simply move after htab\_unlock\_bucket(),
because the old element has already been stashed in htab->extra\_elems.
It may be reused immediately after htab\_unlock\_bucket() and the
invocation of htab\_put\_fd\_value() after htab\_unlock\_bucket() may release
the newly-added element incorrectly. Therefore, saving the map pointer
of the old element for htab of maps before unlocking the bucket and
releasing the map\_ptr after unlock. Beside the map pointer in the old
element, should do the same thing for the special fields in the old
element as well.
(2) free\_htab\_elem() (through htab\_put\_fd\_value())
Its caller includes \_\_htab\_map\_lookup\_and\_delete\_elem(),
htab\_map\_delete\_elem() and \_\_htab\_map\_lookup\_and\_delete\_batch().
For htab\_map\_delete\_elem(), simply invoke free\_htab\_elem() after
htab\_unlock\_bucket(). For \_\_htab\_map\_lookup\_and\_delete\_batch(), just
like lru map, linking the to-be-freed element into node\_to\_free list
and invoking free\_htab\_elem() for these element after unlock. It is safe
to reuse batch\_flink as the link for node\_to\_free, because these
elements have been removed from the hash llist.
Because htab of maps doesn't support lookup\_and\_delete operation,
\_\_htab\_map\_lookup\_and\_delete\_elem() doesn't have the problem, so kept
it as is.
(3) fd\_htab\_map\_free()
It invokes ->map\_fd\_put\_ptr without raw\_spinlock\_t.
(4) bpf\_fd\_htab\_map\_update\_elem()
It invokes ->map\_fd\_put\_ptr without raw\_spinlock\_t.
After moving free\_htab\_elem() outside htab bucket lock scope, using
pcpu\_freelist\_push() instead of \_\_pcpu\_freelist\_push() to disable
the irq before freeing elements, and protecting the invocations of
bpf\_mem\_cache\_free() with migrate\_{disable|enable} pair.
Signed-off-by: Hou Tao <houtao1@huawei.com>
Link: [https://lore.kernel.org/r/20241106063542.357743-2-houtao@huaweicloud.com](https://lore.kernel.org/r/20241106063542.357743-2-houtao%40huaweicloud.com)
Signed-off-by: Alexei Starovoitov <ast@kernel.org>
Signed-off-by: Andrii Nakryiko <andrii@kernel.org>
Signed-off-by: Sasha Levin <sashal@kernel.org>
[Diffstat](/pub/scm/linux/kernel/git/stable/linux.git/diff/?id=10e8a2dec9ff1b81de8e892b0850924038adbc6d)

| -rw-r--r-- | [kernel/bpf/hashtab.c](/pub/scm/linux/kernel/git/stable/linux.git/diff/kernel/bpf/hashtab.c?id=10e8a2dec9ff1b81de8e892b0850924038adbc6d) | 56 | |  |  |  | | --- | --- | --- | |
| --- | --- | --- | --- | --- | --- | --- |

1 files changed, 39 insertions, 17 deletions

| diff --git a/kernel/bpf/hashtab.c b/kernel/bpf/hashtab.cindex 7c64ad4f3732be..fc34f72702cc40 100644--- a/[kernel/bpf/hashtab.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/kernel/bpf/hashtab.c?id=07c020c6d14d29e5a3ea4e4576b8ecf956a80834)+++ b/[kernel/bpf/hashtab.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/kernel/bpf/hashtab.c?id=10e8a2dec9ff1b81de8e892b0850924038adbc6d)@@ -892,9 +892,12 @@ find\_first\_elem: static void htab\_elem\_free(struct bpf\_htab \*htab, struct htab\_elem \*l) { check\_and\_free\_fields(htab, l);++ migrate\_disable(); if (htab->map.map\_type == BPF\_MAP\_TYPE\_PERCPU\_HASH) bpf\_mem\_cache\_free(&htab->pcpu\_ma, l->ptr\_to\_pptr); bpf\_mem\_cache\_free(&htab->ma, l);+ migrate\_enable(); }  static void htab\_put\_fd\_value(struct bpf\_htab \*htab, struct htab\_elem \*l)@@ -944,7 +947,7 @@ static void free\_htab\_elem(struct bpf\_htab \*htab, struct htab\_elem \*l) if (htab\_is\_prealloc(htab)) { bpf\_map\_dec\_elem\_count(&htab->map); check\_and\_free\_fields(htab, l);- \_\_pcpu\_freelist\_push(&htab->freelist, &l->fnode);+ pcpu\_freelist\_push(&htab->freelist, &l->fnode); } else { dec\_elem\_count(htab); htab\_elem\_free(htab, l);@@ -1014,7 +1017,6 @@ static struct htab\_elem \*alloc\_htab\_elem(struct bpf\_htab \*htab, void \*key, \*/ pl\_new = this\_cpu\_ptr(htab->extra\_elems); l\_new = \*pl\_new;- htab\_put\_fd\_value(htab, old\_elem); \*pl\_new = old\_elem; } else { struct pcpu\_freelist\_node \*l;@@ -1100,6 +1102,7 @@ static long htab\_map\_update\_elem(struct bpf\_map \*map, void \*key, void \*value, struct htab\_elem \*l\_new = NULL, \*l\_old; struct hlist\_nulls\_head \*head; unsigned long flags;+ void \*old\_map\_ptr; struct bucket \*b; u32 key\_size, hash; int ret;@@ -1178,12 +1181,27 @@ static long htab\_map\_update\_elem(struct bpf\_map \*map, void \*key, void \*value, hlist\_nulls\_add\_head\_rcu(&l\_new->hash\_node, head); if (l\_old) { hlist\_nulls\_del\_rcu(&l\_old->hash\_node);++ /\* l\_old has already been stashed in htab->extra\_elems, free+ \* its special fields before it is available for reuse. Also+ \* save the old map pointer in htab of maps before unlock+ \* and release it after unlock.+ \*/+ old\_map\_ptr = NULL;+ if (htab\_is\_prealloc(htab)) {+ if (map->ops->map\_fd\_put\_ptr)+ old\_map\_ptr = fd\_htab\_map\_get\_ptr(map, l\_old);+ check\_and\_free\_fields(htab, l\_old);+ }+ }+ htab\_unlock\_bucket(htab, b, hash, flags);+ if (l\_old) {+ if (old\_map\_ptr)+ map->ops->map\_fd\_put\_ptr(map, old\_map\_ptr, true); if (!htab\_is\_prealloc(htab)) free\_htab\_elem(htab, l\_old);- else- check\_and\_free\_fields(htab, l\_old); }- ret = 0;+ return 0; err: htab\_unlock\_bucket(htab, b, hash, flags); return ret;@@ -1427,15 +1445,15 @@ static long htab\_map\_delete\_elem(struct bpf\_map \*map, void \*key) return ret;  l = lookup\_elem\_raw(head, hash, key, key\_size);-- if (l) {+ if (l) hlist\_nulls\_del\_rcu(&l->hash\_node);- free\_htab\_elem(htab, l);- } else {+ else ret = -ENOENT;- }  htab\_unlock\_bucket(htab, b, hash, flags);++ if (l)+ free\_htab\_elem(htab, l); return ret; } @@ -1842,13 +1860,14 @@ again\_nocopy: \* may cause deadlock. See comments in function \* prealloc\_lru\_pop(). Let us do bpf\_lru\_push\_free() \* after releasing the bucket lock.+ \*+ \* For htab of maps, htab\_put\_fd\_value() in+ \* free\_htab\_elem() may acquire a spinlock with bucket+ \* lock being held and it violates the lock rule, so+ \* invoke free\_htab\_elem() after unlock as well. \*/- if (is\_lru\_map) {- l->batch\_flink = node\_to\_free;- node\_to\_free = l;- } else {- free\_htab\_elem(htab, l);- }+ l->batch\_flink = node\_to\_free;+ node\_to\_free = l; } dst\_key += key\_size; dst\_val += value\_size;@@ -1860,7 +1879,10 @@ again\_nocopy: while (node\_to\_free) { l = node\_to\_free; node\_to\_free = node\_to\_free->batch\_flink;- htab\_lru\_push\_free(htab, l);+ if (is\_lru\_map)+ htab\_lru\_push\_free(htab, l);+ else+ free\_htab\_elem(htab, l); }  next\_batch: |
| --- |

generated by [cgit 1.2.3-korg](https://git.zx2c4.com/cgit/about/) ([git 2.43.0](https://git-scm.com/)) at 2025-01-15 09:59:07 +0000



=== Content from git.kernel.org_14d65550_20250115_100034.html ===


| [cgit logo](/) | [index](/) : [kernel/git/stable/linux.git](/pub/scm/linux/kernel/git/stable/linux.git/) | linux-2.6.11.y linux-2.6.12.y linux-2.6.13.y linux-2.6.14.y linux-2.6.15.y linux-2.6.16.y linux-2.6.17.y linux-2.6.18.y linux-2.6.19.y linux-2.6.20.y linux-2.6.21.y linux-2.6.22.y linux-2.6.23.y linux-2.6.24.y linux-2.6.25.y linux-2.6.26.y linux-2.6.27.y linux-2.6.28.y linux-2.6.29.y linux-2.6.30.y linux-2.6.31.y linux-2.6.32.y linux-2.6.33.y linux-2.6.34.y linux-2.6.35.y linux-2.6.36.y linux-2.6.37.y linux-2.6.38.y linux-2.6.39.y linux-3.0.y linux-3.1.y linux-3.10.y linux-3.11.y linux-3.12.y linux-3.13.y linux-3.14.y linux-3.15.y linux-3.16.y linux-3.17.y linux-3.18.y linux-3.19.y linux-3.2.y linux-3.3.y linux-3.4.y linux-3.5.y linux-3.6.y linux-3.7.y linux-3.8.y linux-3.9.y linux-4.0.y linux-4.1.y linux-4.10.y linux-4.11.y linux-4.12.y linux-4.13.y linux-4.14.y linux-4.15.y linux-4.16.y linux-4.17.y linux-4.18.y linux-4.19.y linux-4.2.y linux-4.20.y linux-4.3.y linux-4.4.y linux-4.5.y linux-4.6.y linux-4.7.y linux-4.8.y linux-4.9.y linux-5.0.y linux-5.1.y linux-5.10.y linux-5.11.y linux-5.12.y linux-5.13.y linux-5.14.y linux-5.15.y linux-5.16.y linux-5.17.y linux-5.18.y linux-5.19.y linux-5.2.y linux-5.3.y linux-5.4.y linux-5.5.y linux-5.6.y linux-5.7.y linux-5.8.y linux-5.9.y linux-6.0.y linux-6.1.y linux-6.10.y linux-6.11.y linux-6.12.y linux-6.2.y linux-6.3.y linux-6.4.y linux-6.5.y linux-6.6.y linux-6.7.y linux-6.8.y linux-6.9.y linux-rolling-lts linux-rolling-stable master |
| --- | --- | --- |
| Linux kernel stable tree | Stable Group |

| [about](/pub/scm/linux/kernel/git/stable/linux.git/about/)[summary](/pub/scm/linux/kernel/git/stable/linux.git/)[refs](/pub/scm/linux/kernel/git/stable/linux.git/refs/?id=b9e9ed90b10c82a4e9d4d70a2890f06bfcdd3b78)[log](/pub/scm/linux/kernel/git/stable/linux.git/log/)[tree](/pub/scm/linux/kernel/git/stable/linux.git/tree/?id=b9e9ed90b10c82a4e9d4d70a2890f06bfcdd3b78)[commit](/pub/scm/linux/kernel/git/stable/linux.git/commit/?id=b9e9ed90b10c82a4e9d4d70a2890f06bfcdd3b78)[diff](/pub/scm/linux/kernel/git/stable/linux.git/diff/?id=b9e9ed90b10c82a4e9d4d70a2890f06bfcdd3b78)[stats](/pub/scm/linux/kernel/git/stable/linux.git/stats/) | log msg author committer range |
| --- | --- |

**diff options**

|  | |
| --- | --- |
| context: | 12345678910152025303540 |
| space: | includeignore |
| mode: | unifiedssdiffstat only |
|  |  |

| author | Hou Tao <houtao1@huawei.com> | 2024-11-06 14:35:40 +0800 |
| --- | --- | --- |
| committer | Andrii Nakryiko <andrii@kernel.org> | 2024-11-11 08:18:30 -0800 |
| commit | [b9e9ed90b10c82a4e9d4d70a2890f06bfcdd3b78](/pub/scm/linux/kernel/git/stable/linux.git/commit/?id=b9e9ed90b10c82a4e9d4d70a2890f06bfcdd3b78) ([patch](/pub/scm/linux/kernel/git/stable/linux.git/patch/?id=b9e9ed90b10c82a4e9d4d70a2890f06bfcdd3b78)) | |
| tree | [67659823f2b0765eefc113f7f4c2228fdfd7eebe](/pub/scm/linux/kernel/git/stable/linux.git/tree/?id=b9e9ed90b10c82a4e9d4d70a2890f06bfcdd3b78) | |
| parent | [269e7c97cac8e19117518056e9f4bd3a1dfe9362](/pub/scm/linux/kernel/git/stable/linux.git/commit/?id=269e7c97cac8e19117518056e9f4bd3a1dfe9362) ([diff](/pub/scm/linux/kernel/git/stable/linux.git/diff/?id=b9e9ed90b10c82a4e9d4d70a2890f06bfcdd3b78&id2=269e7c97cac8e19117518056e9f4bd3a1dfe9362)) | |
| download | [linux-b9e9ed90b10c82a4e9d4d70a2890f06bfcdd3b78.tar.gz](/pub/scm/linux/kernel/git/stable/linux.git/snapshot/linux-b9e9ed90b10c82a4e9d4d70a2890f06bfcdd3b78.tar.gz) | |

bpf: Call free\_htab\_elem() after htab\_unlock\_bucket()For htab of maps, when the map is removed from the htab, it may hold the
last reference of the map. bpf\_map\_fd\_put\_ptr() will invoke
bpf\_map\_free\_id() to free the id of the removed map element. However,
bpf\_map\_fd\_put\_ptr() is invoked while holding a bucket lock
(raw\_spin\_lock\_t), and bpf\_map\_free\_id() attempts to acquire map\_idr\_lock
(spinlock\_t), triggering the following lockdep warning:
=============================
[ BUG: Invalid wait context ]
6.11.0-rc4+ #49 Not tainted
-----------------------------
test\_maps/4881 is trying to lock:
ffffffff84884578 (map\_idr\_lock){+...}-{3:3}, at: bpf\_map\_free\_id.part.0+0x21/0x70
other info that might help us debug this:
context-{5:5}
2 locks held by test\_maps/4881:
#0: ffffffff846caf60 (rcu\_read\_lock){....}-{1:3}, at: bpf\_fd\_htab\_map\_update\_elem+0xf9/0x270
#1: ffff888149ced148 (&htab->lockdep\_key#2){....}-{2:2}, at: htab\_map\_update\_elem+0x178/0xa80
stack backtrace:
CPU: 0 UID: 0 PID: 4881 Comm: test\_maps Not tainted 6.11.0-rc4+ #49
Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), ...
Call Trace:
<TASK>
dump\_stack\_lvl+0x6e/0xb0
dump\_stack+0x10/0x20
\_\_lock\_acquire+0x73e/0x36c0
lock\_acquire+0x182/0x450
\_raw\_spin\_lock\_irqsave+0x43/0x70
bpf\_map\_free\_id.part.0+0x21/0x70
bpf\_map\_put+0xcf/0x110
bpf\_map\_fd\_put\_ptr+0x9a/0xb0
free\_htab\_elem+0x69/0xe0
htab\_map\_update\_elem+0x50f/0xa80
bpf\_fd\_htab\_map\_update\_elem+0x131/0x270
htab\_map\_update\_elem+0x50f/0xa80
bpf\_fd\_htab\_map\_update\_elem+0x131/0x270
bpf\_map\_update\_value+0x266/0x380
\_\_sys\_bpf+0x21bb/0x36b0
\_\_x64\_sys\_bpf+0x45/0x60
x64\_sys\_call+0x1b2a/0x20d0
do\_syscall\_64+0x5d/0x100
entry\_SYSCALL\_64\_after\_hwframe+0x76/0x7e
One way to fix the lockdep warning is using raw\_spinlock\_t for
map\_idr\_lock as well. However, bpf\_map\_alloc\_id() invokes
idr\_alloc\_cyclic() after acquiring map\_idr\_lock, it will trigger a
similar lockdep warning because the slab's lock (s->cpu\_slab->lock) is
still a spinlock.
Instead of changing map\_idr\_lock's type, fix the issue by invoking
htab\_put\_fd\_value() after htab\_unlock\_bucket(). However, only deferring
the invocation of htab\_put\_fd\_value() is not enough, because the old map
pointers in htab of maps can not be saved during batched deletion.
Therefore, also defer the invocation of free\_htab\_elem(), so these
to-be-freed elements could be linked together similar to lru map.
There are four callers for ->map\_fd\_put\_ptr:
(1) alloc\_htab\_elem() (through htab\_put\_fd\_value())
It invokes ->map\_fd\_put\_ptr() under a raw\_spinlock\_t. The invocation of
htab\_put\_fd\_value() can not simply move after htab\_unlock\_bucket(),
because the old element has already been stashed in htab->extra\_elems.
It may be reused immediately after htab\_unlock\_bucket() and the
invocation of htab\_put\_fd\_value() after htab\_unlock\_bucket() may release
the newly-added element incorrectly. Therefore, saving the map pointer
of the old element for htab of maps before unlocking the bucket and
releasing the map\_ptr after unlock. Beside the map pointer in the old
element, should do the same thing for the special fields in the old
element as well.
(2) free\_htab\_elem() (through htab\_put\_fd\_value())
Its caller includes \_\_htab\_map\_lookup\_and\_delete\_elem(),
htab\_map\_delete\_elem() and \_\_htab\_map\_lookup\_and\_delete\_batch().
For htab\_map\_delete\_elem(), simply invoke free\_htab\_elem() after
htab\_unlock\_bucket(). For \_\_htab\_map\_lookup\_and\_delete\_batch(), just
like lru map, linking the to-be-freed element into node\_to\_free list
and invoking free\_htab\_elem() for these element after unlock. It is safe
to reuse batch\_flink as the link for node\_to\_free, because these
elements have been removed from the hash llist.
Because htab of maps doesn't support lookup\_and\_delete operation,
\_\_htab\_map\_lookup\_and\_delete\_elem() doesn't have the problem, so kept
it as is.
(3) fd\_htab\_map\_free()
It invokes ->map\_fd\_put\_ptr without raw\_spinlock\_t.
(4) bpf\_fd\_htab\_map\_update\_elem()
It invokes ->map\_fd\_put\_ptr without raw\_spinlock\_t.
After moving free\_htab\_elem() outside htab bucket lock scope, using
pcpu\_freelist\_push() instead of \_\_pcpu\_freelist\_push() to disable
the irq before freeing elements, and protecting the invocations of
bpf\_mem\_cache\_free() with migrate\_{disable|enable} pair.
Signed-off-by: Hou Tao <houtao1@huawei.com>
Link: [https://lore.kernel.org/r/20241106063542.357743-2-houtao@huaweicloud.com](https://lore.kernel.org/r/20241106063542.357743-2-houtao%40huaweicloud.com)
Signed-off-by: Alexei Starovoitov <ast@kernel.org>
Signed-off-by: Andrii Nakryiko <andrii@kernel.org>
[Diffstat](/pub/scm/linux/kernel/git/stable/linux.git/diff/?id=b9e9ed90b10c82a4e9d4d70a2890f06bfcdd3b78)

| -rw-r--r-- | [kernel/bpf/hashtab.c](/pub/scm/linux/kernel/git/stable/linux.git/diff/kernel/bpf/hashtab.c?id=b9e9ed90b10c82a4e9d4d70a2890f06bfcdd3b78) | 56 | |  |  |  | | --- | --- | --- | |
| --- | --- | --- | --- | --- | --- | --- |

1 files changed, 39 insertions, 17 deletions

| diff --git a/kernel/bpf/hashtab.c b/kernel/bpf/hashtab.cindex b14b87463ee04e..3ec941a0ea41c5 100644--- a/[kernel/bpf/hashtab.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/kernel/bpf/hashtab.c?id=269e7c97cac8e19117518056e9f4bd3a1dfe9362)+++ b/[kernel/bpf/hashtab.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/kernel/bpf/hashtab.c?id=b9e9ed90b10c82a4e9d4d70a2890f06bfcdd3b78)@@ -896,9 +896,12 @@ find\_first\_elem: static void htab\_elem\_free(struct bpf\_htab \*htab, struct htab\_elem \*l) { check\_and\_free\_fields(htab, l);++ migrate\_disable(); if (htab->map.map\_type == BPF\_MAP\_TYPE\_PERCPU\_HASH) bpf\_mem\_cache\_free(&htab->pcpu\_ma, l->ptr\_to\_pptr); bpf\_mem\_cache\_free(&htab->ma, l);+ migrate\_enable(); }  static void htab\_put\_fd\_value(struct bpf\_htab \*htab, struct htab\_elem \*l)@@ -948,7 +951,7 @@ static void free\_htab\_elem(struct bpf\_htab \*htab, struct htab\_elem \*l) if (htab\_is\_prealloc(htab)) { bpf\_map\_dec\_elem\_count(&htab->map); check\_and\_free\_fields(htab, l);- \_\_pcpu\_freelist\_push(&htab->freelist, &l->fnode);+ pcpu\_freelist\_push(&htab->freelist, &l->fnode); } else { dec\_elem\_count(htab); htab\_elem\_free(htab, l);@@ -1018,7 +1021,6 @@ static struct htab\_elem \*alloc\_htab\_elem(struct bpf\_htab \*htab, void \*key, \*/ pl\_new = this\_cpu\_ptr(htab->extra\_elems); l\_new = \*pl\_new;- htab\_put\_fd\_value(htab, old\_elem); \*pl\_new = old\_elem; } else { struct pcpu\_freelist\_node \*l;@@ -1105,6 +1107,7 @@ static long htab\_map\_update\_elem(struct bpf\_map \*map, void \*key, void \*value, struct htab\_elem \*l\_new = NULL, \*l\_old; struct hlist\_nulls\_head \*head; unsigned long flags;+ void \*old\_map\_ptr; struct bucket \*b; u32 key\_size, hash; int ret;@@ -1183,12 +1186,27 @@ static long htab\_map\_update\_elem(struct bpf\_map \*map, void \*key, void \*value, hlist\_nulls\_add\_head\_rcu(&l\_new->hash\_node, head); if (l\_old) { hlist\_nulls\_del\_rcu(&l\_old->hash\_node);++ /\* l\_old has already been stashed in htab->extra\_elems, free+ \* its special fields before it is available for reuse. Also+ \* save the old map pointer in htab of maps before unlock+ \* and release it after unlock.+ \*/+ old\_map\_ptr = NULL;+ if (htab\_is\_prealloc(htab)) {+ if (map->ops->map\_fd\_put\_ptr)+ old\_map\_ptr = fd\_htab\_map\_get\_ptr(map, l\_old);+ check\_and\_free\_fields(htab, l\_old);+ }+ }+ htab\_unlock\_bucket(htab, b, hash, flags);+ if (l\_old) {+ if (old\_map\_ptr)+ map->ops->map\_fd\_put\_ptr(map, old\_map\_ptr, true); if (!htab\_is\_prealloc(htab)) free\_htab\_elem(htab, l\_old);- else- check\_and\_free\_fields(htab, l\_old); }- ret = 0;+ return 0; err: htab\_unlock\_bucket(htab, b, hash, flags); return ret;@@ -1432,15 +1450,15 @@ static long htab\_map\_delete\_elem(struct bpf\_map \*map, void \*key) return ret;  l = lookup\_elem\_raw(head, hash, key, key\_size);-- if (l) {+ if (l) hlist\_nulls\_del\_rcu(&l->hash\_node);- free\_htab\_elem(htab, l);- } else {+ else ret = -ENOENT;- }  htab\_unlock\_bucket(htab, b, hash, flags);++ if (l)+ free\_htab\_elem(htab, l); return ret; } @@ -1853,13 +1871,14 @@ again\_nocopy: \* may cause deadlock. See comments in function \* prealloc\_lru\_pop(). Let us do bpf\_lru\_push\_free() \* after releasing the bucket lock.+ \*+ \* For htab of maps, htab\_put\_fd\_value() in+ \* free\_htab\_elem() may acquire a spinlock with bucket+ \* lock being held and it violates the lock rule, so+ \* invoke free\_htab\_elem() after unlock as well. \*/- if (is\_lru\_map) {- l->batch\_flink = node\_to\_free;- node\_to\_free = l;- } else {- free\_htab\_elem(htab, l);- }+ l->batch\_flink = node\_to\_free;+ node\_to\_free = l; } dst\_key += key\_size; dst\_val += value\_size;@@ -1871,7 +1890,10 @@ again\_nocopy: while (node\_to\_free) { l = node\_to\_free; node\_to\_free = node\_to\_free->batch\_flink;- htab\_lru\_push\_free(htab, l);+ if (is\_lru\_map)+ htab\_lru\_push\_free(htab, l);+ else+ free\_htab\_elem(htab, l); }  next\_batch: |
| --- |

generated by [cgit 1.2.3-korg](https://git.zx2c4.com/cgit/about/) ([git 2.43.0](https://git-scm.com/)) at 2025-01-15 09:59:11 +0000



=== Content from git.kernel.org_75e8ab4b_20250115_100032.html ===


| [cgit logo](/) | [index](/) : [kernel/git/stable/linux.git](/pub/scm/linux/kernel/git/stable/linux.git/) | linux-2.6.11.y linux-2.6.12.y linux-2.6.13.y linux-2.6.14.y linux-2.6.15.y linux-2.6.16.y linux-2.6.17.y linux-2.6.18.y linux-2.6.19.y linux-2.6.20.y linux-2.6.21.y linux-2.6.22.y linux-2.6.23.y linux-2.6.24.y linux-2.6.25.y linux-2.6.26.y linux-2.6.27.y linux-2.6.28.y linux-2.6.29.y linux-2.6.30.y linux-2.6.31.y linux-2.6.32.y linux-2.6.33.y linux-2.6.34.y linux-2.6.35.y linux-2.6.36.y linux-2.6.37.y linux-2.6.38.y linux-2.6.39.y linux-3.0.y linux-3.1.y linux-3.10.y linux-3.11.y linux-3.12.y linux-3.13.y linux-3.14.y linux-3.15.y linux-3.16.y linux-3.17.y linux-3.18.y linux-3.19.y linux-3.2.y linux-3.3.y linux-3.4.y linux-3.5.y linux-3.6.y linux-3.7.y linux-3.8.y linux-3.9.y linux-4.0.y linux-4.1.y linux-4.10.y linux-4.11.y linux-4.12.y linux-4.13.y linux-4.14.y linux-4.15.y linux-4.16.y linux-4.17.y linux-4.18.y linux-4.19.y linux-4.2.y linux-4.20.y linux-4.3.y linux-4.4.y linux-4.5.y linux-4.6.y linux-4.7.y linux-4.8.y linux-4.9.y linux-5.0.y linux-5.1.y linux-5.10.y linux-5.11.y linux-5.12.y linux-5.13.y linux-5.14.y linux-5.15.y linux-5.16.y linux-5.17.y linux-5.18.y linux-5.19.y linux-5.2.y linux-5.3.y linux-5.4.y linux-5.5.y linux-5.6.y linux-5.7.y linux-5.8.y linux-5.9.y linux-6.0.y linux-6.1.y linux-6.10.y linux-6.11.y linux-6.12.y linux-6.2.y linux-6.3.y linux-6.4.y linux-6.5.y linux-6.6.y linux-6.7.y linux-6.8.y linux-6.9.y linux-rolling-lts linux-rolling-stable master |
| --- | --- | --- |
| Linux kernel stable tree | Stable Group |

| [about](/pub/scm/linux/kernel/git/stable/linux.git/about/)[summary](/pub/scm/linux/kernel/git/stable/linux.git/)[refs](/pub/scm/linux/kernel/git/stable/linux.git/refs/?id=a50b4aa3007e63a590d501341f304676ebc74b3b)[log](/pub/scm/linux/kernel/git/stable/linux.git/log/)[tree](/pub/scm/linux/kernel/git/stable/linux.git/tree/?id=a50b4aa3007e63a590d501341f304676ebc74b3b)[commit](/pub/scm/linux/kernel/git/stable/linux.git/commit/?id=a50b4aa3007e63a590d501341f304676ebc74b3b)[diff](/pub/scm/linux/kernel/git/stable/linux.git/diff/?id=a50b4aa3007e63a590d501341f304676ebc74b3b)[stats](/pub/scm/linux/kernel/git/stable/linux.git/stats/) | log msg author committer range |
| --- | --- |

**diff options**

|  | |
| --- | --- |
| context: | 12345678910152025303540 |
| space: | includeignore |
| mode: | unifiedssdiffstat only |
|  |  |

| author | Hou Tao <houtao1@huawei.com> | 2024-11-06 14:35:40 +0800 |
| --- | --- | --- |
| committer | Greg Kroah-Hartman <gregkh@linuxfoundation.org> | 2024-12-14 20:03:55 +0100 |
| commit | [a50b4aa3007e63a590d501341f304676ebc74b3b](/pub/scm/linux/kernel/git/stable/linux.git/commit/?id=a50b4aa3007e63a590d501341f304676ebc74b3b) ([patch](/pub/scm/linux/kernel/git/stable/linux.git/patch/?id=a50b4aa3007e63a590d501341f304676ebc74b3b)) | |
| tree | [8f79b26ff09c01919051a3ed4904d94ff474a242](/pub/scm/linux/kernel/git/stable/linux.git/tree/?id=a50b4aa3007e63a590d501341f304676ebc74b3b) | |
| parent | [34941321b516bd7c6103bd01287d71a1804d19d3](/pub/scm/linux/kernel/git/stable/linux.git/commit/?id=34941321b516bd7c6103bd01287d71a1804d19d3) ([diff](/pub/scm/linux/kernel/git/stable/linux.git/diff/?id=a50b4aa3007e63a590d501341f304676ebc74b3b&id2=34941321b516bd7c6103bd01287d71a1804d19d3)) | |
| download | [linux-a50b4aa3007e63a590d501341f304676ebc74b3b.tar.gz](/pub/scm/linux/kernel/git/stable/linux.git/snapshot/linux-a50b4aa3007e63a590d501341f304676ebc74b3b.tar.gz) | |

bpf: Call free\_htab\_elem() after htab\_unlock\_bucket()[ Upstream commit b9e9ed90b10c82a4e9d4d70a2890f06bfcdd3b78 ]
For htab of maps, when the map is removed from the htab, it may hold the
last reference of the map. bpf\_map\_fd\_put\_ptr() will invoke
bpf\_map\_free\_id() to free the id of the removed map element. However,
bpf\_map\_fd\_put\_ptr() is invoked while holding a bucket lock
(raw\_spin\_lock\_t), and bpf\_map\_free\_id() attempts to acquire map\_idr\_lock
(spinlock\_t), triggering the following lockdep warning:
=============================
[ BUG: Invalid wait context ]
6.11.0-rc4+ #49 Not tainted
-----------------------------
test\_maps/4881 is trying to lock:
ffffffff84884578 (map\_idr\_lock){+...}-{3:3}, at: bpf\_map\_free\_id.part.0+0x21/0x70
other info that might help us debug this:
context-{5:5}
2 locks held by test\_maps/4881:
#0: ffffffff846caf60 (rcu\_read\_lock){....}-{1:3}, at: bpf\_fd\_htab\_map\_update\_elem+0xf9/0x270
#1: ffff888149ced148 (&htab->lockdep\_key#2){....}-{2:2}, at: htab\_map\_update\_elem+0x178/0xa80
stack backtrace:
CPU: 0 UID: 0 PID: 4881 Comm: test\_maps Not tainted 6.11.0-rc4+ #49
Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), ...
Call Trace:
<TASK>
dump\_stack\_lvl+0x6e/0xb0
dump\_stack+0x10/0x20
\_\_lock\_acquire+0x73e/0x36c0
lock\_acquire+0x182/0x450
\_raw\_spin\_lock\_irqsave+0x43/0x70
bpf\_map\_free\_id.part.0+0x21/0x70
bpf\_map\_put+0xcf/0x110
bpf\_map\_fd\_put\_ptr+0x9a/0xb0
free\_htab\_elem+0x69/0xe0
htab\_map\_update\_elem+0x50f/0xa80
bpf\_fd\_htab\_map\_update\_elem+0x131/0x270
htab\_map\_update\_elem+0x50f/0xa80
bpf\_fd\_htab\_map\_update\_elem+0x131/0x270
bpf\_map\_update\_value+0x266/0x380
\_\_sys\_bpf+0x21bb/0x36b0
\_\_x64\_sys\_bpf+0x45/0x60
x64\_sys\_call+0x1b2a/0x20d0
do\_syscall\_64+0x5d/0x100
entry\_SYSCALL\_64\_after\_hwframe+0x76/0x7e
One way to fix the lockdep warning is using raw\_spinlock\_t for
map\_idr\_lock as well. However, bpf\_map\_alloc\_id() invokes
idr\_alloc\_cyclic() after acquiring map\_idr\_lock, it will trigger a
similar lockdep warning because the slab's lock (s->cpu\_slab->lock) is
still a spinlock.
Instead of changing map\_idr\_lock's type, fix the issue by invoking
htab\_put\_fd\_value() after htab\_unlock\_bucket(). However, only deferring
the invocation of htab\_put\_fd\_value() is not enough, because the old map
pointers in htab of maps can not be saved during batched deletion.
Therefore, also defer the invocation of free\_htab\_elem(), so these
to-be-freed elements could be linked together similar to lru map.
There are four callers for ->map\_fd\_put\_ptr:
(1) alloc\_htab\_elem() (through htab\_put\_fd\_value())
It invokes ->map\_fd\_put\_ptr() under a raw\_spinlock\_t. The invocation of
htab\_put\_fd\_value() can not simply move after htab\_unlock\_bucket(),
because the old element has already been stashed in htab->extra\_elems.
It may be reused immediately after htab\_unlock\_bucket() and the
invocation of htab\_put\_fd\_value() after htab\_unlock\_bucket() may release
the newly-added element incorrectly. Therefore, saving the map pointer
of the old element for htab of maps before unlocking the bucket and
releasing the map\_ptr after unlock. Beside the map pointer in the old
element, should do the same thing for the special fields in the old
element as well.
(2) free\_htab\_elem() (through htab\_put\_fd\_value())
Its caller includes \_\_htab\_map\_lookup\_and\_delete\_elem(),
htab\_map\_delete\_elem() and \_\_htab\_map\_lookup\_and\_delete\_batch().
For htab\_map\_delete\_elem(), simply invoke free\_htab\_elem() after
htab\_unlock\_bucket(). For \_\_htab\_map\_lookup\_and\_delete\_batch(), just
like lru map, linking the to-be-freed element into node\_to\_free list
and invoking free\_htab\_elem() for these element after unlock. It is safe
to reuse batch\_flink as the link for node\_to\_free, because these
elements have been removed from the hash llist.
Because htab of maps doesn't support lookup\_and\_delete operation,
\_\_htab\_map\_lookup\_and\_delete\_elem() doesn't have the problem, so kept
it as is.
(3) fd\_htab\_map\_free()
It invokes ->map\_fd\_put\_ptr without raw\_spinlock\_t.
(4) bpf\_fd\_htab\_map\_update\_elem()
It invokes ->map\_fd\_put\_ptr without raw\_spinlock\_t.
After moving free\_htab\_elem() outside htab bucket lock scope, using
pcpu\_freelist\_push() instead of \_\_pcpu\_freelist\_push() to disable
the irq before freeing elements, and protecting the invocations of
bpf\_mem\_cache\_free() with migrate\_{disable|enable} pair.
Signed-off-by: Hou Tao <houtao1@huawei.com>
Link: [https://lore.kernel.org/r/20241106063542.357743-2-houtao@huaweicloud.com](https://lore.kernel.org/r/20241106063542.357743-2-houtao%40huaweicloud.com)
Signed-off-by: Alexei Starovoitov <ast@kernel.org>
Signed-off-by: Andrii Nakryiko <andrii@kernel.org>
Signed-off-by: Sasha Levin <sashal@kernel.org>
[Diffstat](/pub/scm/linux/kernel/git/stable/linux.git/diff/?id=a50b4aa3007e63a590d501341f304676ebc74b3b)

| -rw-r--r-- | [kernel/bpf/hashtab.c](/pub/scm/linux/kernel/git/stable/linux.git/diff/kernel/bpf/hashtab.c?id=a50b4aa3007e63a590d501341f304676ebc74b3b) | 56 | |  |  |  | | --- | --- | --- | |
| --- | --- | --- | --- | --- | --- | --- |

1 files changed, 39 insertions, 17 deletions

| diff --git a/kernel/bpf/hashtab.c b/kernel/bpf/hashtab.cindex b14b87463ee04e..3ec941a0ea41c5 100644--- a/[kernel/bpf/hashtab.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/kernel/bpf/hashtab.c?id=34941321b516bd7c6103bd01287d71a1804d19d3)+++ b/[kernel/bpf/hashtab.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/kernel/bpf/hashtab.c?id=a50b4aa3007e63a590d501341f304676ebc74b3b)@@ -896,9 +896,12 @@ find\_first\_elem: static void htab\_elem\_free(struct bpf\_htab \*htab, struct htab\_elem \*l) { check\_and\_free\_fields(htab, l);++ migrate\_disable(); if (htab->map.map\_type == BPF\_MAP\_TYPE\_PERCPU\_HASH) bpf\_mem\_cache\_free(&htab->pcpu\_ma, l->ptr\_to\_pptr); bpf\_mem\_cache\_free(&htab->ma, l);+ migrate\_enable(); }  static void htab\_put\_fd\_value(struct bpf\_htab \*htab, struct htab\_elem \*l)@@ -948,7 +951,7 @@ static void free\_htab\_elem(struct bpf\_htab \*htab, struct htab\_elem \*l) if (htab\_is\_prealloc(htab)) { bpf\_map\_dec\_elem\_count(&htab->map); check\_and\_free\_fields(htab, l);- \_\_pcpu\_freelist\_push(&htab->freelist, &l->fnode);+ pcpu\_freelist\_push(&htab->freelist, &l->fnode); } else { dec\_elem\_count(htab); htab\_elem\_free(htab, l);@@ -1018,7 +1021,6 @@ static struct htab\_elem \*alloc\_htab\_elem(struct bpf\_htab \*htab, void \*key, \*/ pl\_new = this\_cpu\_ptr(htab->extra\_elems); l\_new = \*pl\_new;- htab\_put\_fd\_value(htab, old\_elem); \*pl\_new = old\_elem; } else { struct pcpu\_freelist\_node \*l;@@ -1105,6 +1107,7 @@ static long htab\_map\_update\_elem(struct bpf\_map \*map, void \*key, void \*value, struct htab\_elem \*l\_new = NULL, \*l\_old; struct hlist\_nulls\_head \*head; unsigned long flags;+ void \*old\_map\_ptr; struct bucket \*b; u32 key\_size, hash; int ret;@@ -1183,12 +1186,27 @@ static long htab\_map\_update\_elem(struct bpf\_map \*map, void \*key, void \*value, hlist\_nulls\_add\_head\_rcu(&l\_new->hash\_node, head); if (l\_old) { hlist\_nulls\_del\_rcu(&l\_old->hash\_node);++ /\* l\_old has already been stashed in htab->extra\_elems, free+ \* its special fields before it is available for reuse. Also+ \* save the old map pointer in htab of maps before unlock+ \* and release it after unlock.+ \*/+ old\_map\_ptr = NULL;+ if (htab\_is\_prealloc(htab)) {+ if (map->ops->map\_fd\_put\_ptr)+ old\_map\_ptr = fd\_htab\_map\_get\_ptr(map, l\_old);+ check\_and\_free\_fields(htab, l\_old);+ }+ }+ htab\_unlock\_bucket(htab, b, hash, flags);+ if (l\_old) {+ if (old\_map\_ptr)+ map->ops->map\_fd\_put\_ptr(map, old\_map\_ptr, true); if (!htab\_is\_prealloc(htab)) free\_htab\_elem(htab, l\_old);- else- check\_and\_free\_fields(htab, l\_old); }- ret = 0;+ return 0; err: htab\_unlock\_bucket(htab, b, hash, flags); return ret;@@ -1432,15 +1450,15 @@ static long htab\_map\_delete\_elem(struct bpf\_map \*map, void \*key) return ret;  l = lookup\_elem\_raw(head, hash, key, key\_size);-- if (l) {+ if (l) hlist\_nulls\_del\_rcu(&l->hash\_node);- free\_htab\_elem(htab, l);- } else {+ else ret = -ENOENT;- }  htab\_unlock\_bucket(htab, b, hash, flags);++ if (l)+ free\_htab\_elem(htab, l); return ret; } @@ -1853,13 +1871,14 @@ again\_nocopy: \* may cause deadlock. See comments in function \* prealloc\_lru\_pop(). Let us do bpf\_lru\_push\_free() \* after releasing the bucket lock.+ \*+ \* For htab of maps, htab\_put\_fd\_value() in+ \* free\_htab\_elem() may acquire a spinlock with bucket+ \* lock being held and it violates the lock rule, so+ \* invoke free\_htab\_elem() after unlock as well. \*/- if (is\_lru\_map) {- l->batch\_flink = node\_to\_free;- node\_to\_free = l;- } else {- free\_htab\_elem(htab, l);- }+ l->batch\_flink = node\_to\_free;+ node\_to\_free = l; } dst\_key += key\_size; dst\_val += value\_size;@@ -1871,7 +1890,10 @@ again\_nocopy: while (node\_to\_free) { l = node\_to\_free; node\_to\_free = node\_to\_free->batch\_flink;- htab\_lru\_push\_free(htab, l);+ if (is\_lru\_map)+ htab\_lru\_push\_free(htab, l);+ else+ free\_htab\_elem(htab, l); }  next\_batch: |
| --- |

generated by [cgit 1.2.3-korg](https://git.zx2c4.com/cgit/about/) ([git 2.43.0](https://git-scm.com/)) at 2025-01-15 09:59:09 +0000


