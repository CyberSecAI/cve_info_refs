=== Content from git.kernel.org_35d2916d_20250115_080606.html ===


| [cgit logo](/) | [index](/) : [kernel/git/stable/linux.git](/pub/scm/linux/kernel/git/stable/linux.git/) | linux-2.6.11.y linux-2.6.12.y linux-2.6.13.y linux-2.6.14.y linux-2.6.15.y linux-2.6.16.y linux-2.6.17.y linux-2.6.18.y linux-2.6.19.y linux-2.6.20.y linux-2.6.21.y linux-2.6.22.y linux-2.6.23.y linux-2.6.24.y linux-2.6.25.y linux-2.6.26.y linux-2.6.27.y linux-2.6.28.y linux-2.6.29.y linux-2.6.30.y linux-2.6.31.y linux-2.6.32.y linux-2.6.33.y linux-2.6.34.y linux-2.6.35.y linux-2.6.36.y linux-2.6.37.y linux-2.6.38.y linux-2.6.39.y linux-3.0.y linux-3.1.y linux-3.10.y linux-3.11.y linux-3.12.y linux-3.13.y linux-3.14.y linux-3.15.y linux-3.16.y linux-3.17.y linux-3.18.y linux-3.19.y linux-3.2.y linux-3.3.y linux-3.4.y linux-3.5.y linux-3.6.y linux-3.7.y linux-3.8.y linux-3.9.y linux-4.0.y linux-4.1.y linux-4.10.y linux-4.11.y linux-4.12.y linux-4.13.y linux-4.14.y linux-4.15.y linux-4.16.y linux-4.17.y linux-4.18.y linux-4.19.y linux-4.2.y linux-4.20.y linux-4.3.y linux-4.4.y linux-4.5.y linux-4.6.y linux-4.7.y linux-4.8.y linux-4.9.y linux-5.0.y linux-5.1.y linux-5.10.y linux-5.11.y linux-5.12.y linux-5.13.y linux-5.14.y linux-5.15.y linux-5.16.y linux-5.17.y linux-5.18.y linux-5.19.y linux-5.2.y linux-5.3.y linux-5.4.y linux-5.5.y linux-5.6.y linux-5.7.y linux-5.8.y linux-5.9.y linux-6.0.y linux-6.1.y linux-6.10.y linux-6.11.y linux-6.12.y linux-6.2.y linux-6.3.y linux-6.4.y linux-6.5.y linux-6.6.y linux-6.7.y linux-6.8.y linux-6.9.y linux-rolling-lts linux-rolling-stable master |
| --- | --- | --- |
| Linux kernel stable tree | Stable Group |

| [about](/pub/scm/linux/kernel/git/stable/linux.git/about/)[summary](/pub/scm/linux/kernel/git/stable/linux.git/)[refs](/pub/scm/linux/kernel/git/stable/linux.git/refs/?id=fc4951c3e3358dd82ea508e893695b916c813f17)[log](/pub/scm/linux/kernel/git/stable/linux.git/log/)[tree](/pub/scm/linux/kernel/git/stable/linux.git/tree/?id=fc4951c3e3358dd82ea508e893695b916c813f17)[commit](/pub/scm/linux/kernel/git/stable/linux.git/commit/?id=fc4951c3e3358dd82ea508e893695b916c813f17)[diff](/pub/scm/linux/kernel/git/stable/linux.git/diff/?id=fc4951c3e3358dd82ea508e893695b916c813f17)[stats](/pub/scm/linux/kernel/git/stable/linux.git/stats/) | log msg author committer range |
| --- | --- |

**diff options**

|  | |
| --- | --- |
| context: | 12345678910152025303540 |
| space: | includeignore |
| mode: | unifiedssdiffstat only |
|  |  |

| author | Hugh Dickins <hughd@google.com> | 2024-10-27 13:02:13 -0700 |
| --- | --- | --- |
| committer | Greg Kroah-Hartman <gregkh@linuxfoundation.org> | 2024-11-17 15:08:59 +0100 |
| commit | [fc4951c3e3358dd82ea508e893695b916c813f17](/pub/scm/linux/kernel/git/stable/linux.git/commit/?id=fc4951c3e3358dd82ea508e893695b916c813f17) ([patch](/pub/scm/linux/kernel/git/stable/linux.git/patch/?id=fc4951c3e3358dd82ea508e893695b916c813f17)) | |
| tree | [ffb6e9e04dab80bc932febb9d89cc392200fc316](/pub/scm/linux/kernel/git/stable/linux.git/tree/?id=fc4951c3e3358dd82ea508e893695b916c813f17) | |
| parent | [eb6b6d3e1f1e5bb97ef9d15bb21236d514bc0006](/pub/scm/linux/kernel/git/stable/linux.git/commit/?id=eb6b6d3e1f1e5bb97ef9d15bb21236d514bc0006) ([diff](/pub/scm/linux/kernel/git/stable/linux.git/diff/?id=fc4951c3e3358dd82ea508e893695b916c813f17&id2=eb6b6d3e1f1e5bb97ef9d15bb21236d514bc0006)) | |
| download | [linux-fc4951c3e3358dd82ea508e893695b916c813f17.tar.gz](/pub/scm/linux/kernel/git/stable/linux.git/snapshot/linux-fc4951c3e3358dd82ea508e893695b916c813f17.tar.gz) | |

mm/thp: fix deferred split unqueue naming and lockingcommit f8f931bba0f92052cf842b7e30917b1afcc77d5a upstream.
Recent changes are putting more pressure on THP deferred split queues:
under load revealing long-standing races, causing list\_del corruptions,
"Bad page state"s and worse (I keep BUGs in both of those, so usually
don't get to see how badly they end up without). The relevant recent
changes being 6.8's mTHP, 6.10's mTHP swapout, and 6.12's mTHP swapin,
improved swap allocation, and underused THP splitting.
Before fixing locking: rename misleading folio\_undo\_large\_rmappable(),
which does not undo large\_rmappable, to folio\_unqueue\_deferred\_split(),
which is what it does. But that and its out-of-line \_\_callee are mm
internals of very limited usability: add comment and WARN\_ON\_ONCEs to
check usage; and return a bool to say if a deferred split was unqueued,
which can then be used in WARN\_ON\_ONCEs around safety checks (sparing
callers the arcane conditionals in \_\_folio\_unqueue\_deferred\_split()).
Just omit the folio\_unqueue\_deferred\_split() from free\_unref\_folios(), all
of whose callers now call it beforehand (and if any forget then bad\_page()
will tell) - except for its caller put\_pages\_list(), which itself no
longer has any callers (and will be deleted separately).
Swapout: mem\_cgroup\_swapout() has been resetting folio->memcg\_data 0
without checking and unqueueing a THP folio from deferred split list;
which is unfortunate, since the split\_queue\_lock depends on the memcg
(when memcg is enabled); so swapout has been unqueueing such THPs later,
when freeing the folio, using the pgdat's lock instead: potentially
corrupting the memcg's list. \_\_remove\_mapping() has frozen refcount to 0
here, so no problem with calling folio\_unqueue\_deferred\_split() before
resetting memcg\_data.
That goes back to 5.4 commit 87eaceb3faa5 ("mm: thp: make deferred split
shrinker memcg aware"): which included a check on swapcache before adding
to deferred queue, but no check on deferred queue before adding THP to
swapcache. That worked fine with the usual sequence of events in reclaim
(though there were a couple of rare ways in which a THP on deferred queue
could have been swapped out), but 6.12 commit dafff3f4c850 ("mm: split
underused THPs") avoids splitting underused THPs in reclaim, which makes
swapcache THPs on deferred queue commonplace.
Keep the check on swapcache before adding to deferred queue? Yes: it is
no longer essential, but preserves the existing behaviour, and is likely
to be a worthwhile optimization (vmstat showed much more traffic on the
queue under swapping load if the check was removed); update its comment.
Memcg-v1 move (deprecated): mem\_cgroup\_move\_account() has been changing
folio->memcg\_data without checking and unqueueing a THP folio from the
deferred list, sometimes corrupting "from" memcg's list, like swapout.
Refcount is non-zero here, so folio\_unqueue\_deferred\_split() can only be
used in a WARN\_ON\_ONCE to validate the fix, which must be done earlier:
mem\_cgroup\_move\_charge\_pte\_range() first try to split the THP (splitting
of course unqueues), or skip it if that fails. Not ideal, but moving
charge has been requested, and khugepaged should repair the THP later:
nobody wants new custom unqueueing code just for this deprecated case.
The 87eaceb3faa5 commit did have the code to move from one deferred list
to another (but was not conscious of its unsafety while refcount non-0);
but that was removed by 5.6 commit fac0516b5534 ("mm: thp: don't need care
deferred split queue in memcg charge move path"), which argued that the
existence of a PMD mapping guarantees that the THP cannot be on a deferred
list. As above, false in rare cases, and now commonly false.
Backport to 6.11 should be straightforward. Earlier backports must take
care that other \_deferred\_list fixes and dependencies are included. There
is not a strong case for backports, but they can fix cornercases.
Link: [https://lkml.kernel.org/r/8dc111ae-f6db-2da7-b25c-7a20b1effe3b@google.com](https://lkml.kernel.org/r/8dc111ae-f6db-2da7-b25c-7a20b1effe3b%40google.com)
Fixes: 87eaceb3faa5 ("mm: thp: make deferred split shrinker memcg aware")
Fixes: dafff3f4c850 ("mm: split underused THPs")
Signed-off-by: Hugh Dickins <hughd@google.com>
Acked-by: David Hildenbrand <david@redhat.com>
Reviewed-by: Yang Shi <shy828301@gmail.com>
Cc: Baolin Wang <baolin.wang@linux.alibaba.com>
Cc: Barry Song <baohua@kernel.org>
Cc: Chris Li <chrisl@kernel.org>
Cc: Johannes Weiner <hannes@cmpxchg.org>
Cc: Kefeng Wang <wangkefeng.wang@huawei.com>
Cc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
Cc: Matthew Wilcox (Oracle) <willy@infradead.org>
Cc: Nhat Pham <nphamcs@gmail.com>
Cc: Ryan Roberts <ryan.roberts@arm.com>
Cc: Shakeel Butt <shakeel.butt@linux.dev>
Cc: Usama Arif <usamaarif642@gmail.com>
Cc: Wei Yang <richard.weiyang@gmail.com>
Cc: Zi Yan <ziy@nvidia.com>
Cc: <stable@vger.kernel.org>
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
[ Upstream commit itself does not apply cleanly, because there
are fewer calls to folio\_undo\_large\_rmappable() in this tree
(in particular, folio migration does not migrate memcg charge),
and mm/memcontrol-v1.c has not been split out of mm/memcontrol.c. ]
Signed-off-by: Hugh Dickins <hughd@google.com>
Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
[Diffstat](/pub/scm/linux/kernel/git/stable/linux.git/diff/?id=fc4951c3e3358dd82ea508e893695b916c813f17)

| -rw-r--r-- | [mm/huge\_memory.c](/pub/scm/linux/kernel/git/stable/linux.git/diff/mm/huge_memory.c?id=fc4951c3e3358dd82ea508e893695b916c813f17) | 35 | |  |  |  | | --- | --- | --- | |
| --- | --- | --- | --- | --- | --- | --- |
| -rw-r--r-- | [mm/internal.h](/pub/scm/linux/kernel/git/stable/linux.git/diff/mm/internal.h?id=fc4951c3e3358dd82ea508e893695b916c813f17) | 10 | |  |  |  | | --- | --- | --- | |
| -rw-r--r-- | [mm/memcontrol.c](/pub/scm/linux/kernel/git/stable/linux.git/diff/mm/memcontrol.c?id=fc4951c3e3358dd82ea508e893695b916c813f17) | 32 | |  |  |  | | --- | --- | --- | |
| -rw-r--r-- | [mm/page\_alloc.c](/pub/scm/linux/kernel/git/stable/linux.git/diff/mm/page_alloc.c?id=fc4951c3e3358dd82ea508e893695b916c813f17) | 2 | |  |  |  | | --- | --- | --- | |

4 files changed, 61 insertions, 18 deletions

| diff --git a/mm/huge\_memory.c b/mm/huge\_memory.cindex 8d5a6141b951da..635f0f0f6860e8 100644--- a/[mm/huge\_memory.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/mm/huge_memory.c?id=eb6b6d3e1f1e5bb97ef9d15bb21236d514bc0006)+++ b/[mm/huge\_memory.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/mm/huge_memory.c?id=fc4951c3e3358dd82ea508e893695b916c813f17)@@ -2767,18 +2767,38 @@ out: return ret; } -void \_\_folio\_undo\_large\_rmappable(struct folio \*folio)+/\*+ \* \_\_folio\_unqueue\_deferred\_split() is not to be called directly:+ \* the folio\_unqueue\_deferred\_split() inline wrapper in mm/internal.h+ \* limits its calls to those folios which may have a \_deferred\_list for+ \* queueing THP splits, and that list is (racily observed to be) non-empty.+ \*+ \* It is unsafe to call folio\_unqueue\_deferred\_split() until folio refcount is+ \* zero: because even when split\_queue\_lock is held, a non-empty \_deferred\_list+ \* might be in use on deferred\_split\_scan()'s unlocked on-stack list.+ \*+ \* If memory cgroups are enabled, split\_queue\_lock is in the mem\_cgroup: it is+ \* therefore important to unqueue deferred split before changing folio memcg.+ \*/+bool \_\_folio\_unqueue\_deferred\_split(struct folio \*folio) { struct deferred\_split \*ds\_queue; unsigned long flags;+ bool unqueued = false;++ WARN\_ON\_ONCE(folio\_ref\_count(folio));+ WARN\_ON\_ONCE(!mem\_cgroup\_disabled() && !folio\_memcg(folio));  ds\_queue = get\_deferred\_split\_queue(folio); spin\_lock\_irqsave(&ds\_queue->split\_queue\_lock, flags); if (!list\_empty(&folio->\_deferred\_list)) { ds\_queue->split\_queue\_len--; list\_del\_init(&folio->\_deferred\_list);+ unqueued = true; } spin\_unlock\_irqrestore(&ds\_queue->split\_queue\_lock, flags);++ return unqueued; /\* useful for debug warnings \*/ }  void deferred\_split\_folio(struct folio \*folio)@@ -2797,14 +2817,11 @@ void deferred\_split\_folio(struct folio \*folio) return;  /\*- \* The try\_to\_unmap() in page reclaim path might reach here too,- \* this may cause a race condition to corrupt deferred split queue.- \* And, if page reclaim is already handling the same folio, it is- \* unnecessary to handle it again in shrinker.- \*- \* Check the swapcache flag to determine if the folio is being- \* handled by page reclaim since THP swap would add the folio into- \* swap cache before calling try\_to\_unmap().+ \* Exclude swapcache: originally to avoid a corrupt deferred split+ \* queue. Nowadays that is fully prevented by mem\_cgroup\_swapout();+ \* but if page reclaim is already handling the same folio, it is+ \* unnecessary to handle it again in the shrinker, so excluding+ \* swapcache here may still be a useful optimization. \*/ if (folio\_test\_swapcache(folio)) return;diff --git a/mm/internal.h b/mm/internal.hindex 78db278c126de8..b30907537801cc 100644--- a/[mm/internal.h](/pub/scm/linux/kernel/git/stable/linux.git/tree/mm/internal.h?id=eb6b6d3e1f1e5bb97ef9d15bb21236d514bc0006)+++ b/[mm/internal.h](/pub/scm/linux/kernel/git/stable/linux.git/tree/mm/internal.h?id=fc4951c3e3358dd82ea508e893695b916c813f17)@@ -413,11 +413,11 @@ static inline void folio\_set\_order(struct folio \*folio, unsigned int order) #endif } -void \_\_folio\_undo\_large\_rmappable(struct folio \*folio);-static inline void folio\_undo\_large\_rmappable(struct folio \*folio)+bool \_\_folio\_unqueue\_deferred\_split(struct folio \*folio);+static inline bool folio\_unqueue\_deferred\_split(struct folio \*folio) { if (folio\_order(folio) <= 1 || !folio\_test\_large\_rmappable(folio))- return;+ return false;  /\* \* At this point, there is no one trying to add the folio to@@ -425,9 +425,9 @@ static inline void folio\_undo\_large\_rmappable(struct folio \*folio) \* to check without acquiring the split\_queue\_lock. \*/ if (data\_race(list\_empty(&folio->\_deferred\_list)))- return;+ return false; - \_\_folio\_undo\_large\_rmappable(folio);+ return \_\_folio\_unqueue\_deferred\_split(folio); }  static inline struct folio \*page\_rmappable\_folio(struct page \*page)diff --git a/mm/memcontrol.c b/mm/memcontrol.cindex a7a6a1a23c7de5..d2ceadd11b1004 100644--- a/[mm/memcontrol.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/mm/memcontrol.c?id=eb6b6d3e1f1e5bb97ef9d15bb21236d514bc0006)+++ b/[mm/memcontrol.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/mm/memcontrol.c?id=fc4951c3e3358dd82ea508e893695b916c813f17)@@ -5873,6 +5873,8 @@ static int mem\_cgroup\_move\_account(struct page \*page, css\_get(&to->css); css\_put(&from->css); + /\* Warning should never happen, so don't worry about refcount non-0 \*/+ WARN\_ON\_ONCE(folio\_unqueue\_deferred\_split(folio)); folio->memcg\_data = (unsigned long)to;  \_\_folio\_memcg\_unlock(from);@@ -6237,7 +6239,10 @@ static int mem\_cgroup\_move\_charge\_pte\_range(pmd\_t \*pmd, enum mc\_target\_type target\_type; union mc\_target target; struct page \*page;+ struct folio \*folio;+ bool tried\_split\_before = false; +retry\_pmd: ptl = pmd\_trans\_huge\_lock(pmd, vma); if (ptl) { if (mc.precharge < HPAGE\_PMD\_NR) {@@ -6247,6 +6252,28 @@ static int mem\_cgroup\_move\_charge\_pte\_range(pmd\_t \*pmd, target\_type = get\_mctgt\_type\_thp(vma, addr, \*pmd, &target); if (target\_type == MC\_TARGET\_PAGE) { page = target.page;+ folio = page\_folio(page);+ /\*+ \* Deferred split queue locking depends on memcg,+ \* and unqueue is unsafe unless folio refcount is 0:+ \* split or skip if on the queue? first try to split.+ \*/+ if (!list\_empty(&folio->\_deferred\_list)) {+ spin\_unlock(ptl);+ if (!tried\_split\_before)+ split\_folio(folio);+ folio\_unlock(folio);+ folio\_put(folio);+ if (tried\_split\_before)+ return 0;+ tried\_split\_before = true;+ goto retry\_pmd;+ }+ /\*+ \* So long as that pmd lock is held, the folio cannot+ \* be racily added to the \_deferred\_list, because+ \* page\_remove\_rmap() will find it still pmdmapped.+ \*/ if (isolate\_lru\_page(page)) { if (!mem\_cgroup\_move\_account(page, true, mc.from, mc.to)) {@@ -7153,9 +7180,6 @@ static void uncharge\_folio(struct folio \*folio, struct uncharge\_gather \*ug) struct obj\_cgroup \*objcg;  VM\_BUG\_ON\_FOLIO(folio\_test\_lru(folio), folio);- VM\_BUG\_ON\_FOLIO(folio\_order(folio) > 1 &&- !folio\_test\_hugetlb(folio) &&- !list\_empty(&folio->\_deferred\_list), folio);  /\* \* Nobody should be changing or seriously looking at@@ -7202,6 +7226,7 @@ static void uncharge\_folio(struct folio \*folio, struct uncharge\_gather \*ug) ug->nr\_memory += nr\_pages; ug->pgpgout++; + WARN\_ON\_ONCE(folio\_unqueue\_deferred\_split(folio)); folio->memcg\_data = 0; } @@ -7495,6 +7520,7 @@ void mem\_cgroup\_swapout(struct folio \*folio, swp\_entry\_t entry) VM\_BUG\_ON\_FOLIO(oldid, folio); mod\_memcg\_state(swap\_memcg, MEMCG\_SWAP, nr\_entries); + folio\_unqueue\_deferred\_split(folio); folio->memcg\_data = 0;  if (!mem\_cgroup\_is\_root(memcg))diff --git a/mm/page\_alloc.c b/mm/page\_alloc.cindex fd9d9afbe1da75..7272a922b83831 100644--- a/[mm/page\_alloc.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/mm/page_alloc.c?id=eb6b6d3e1f1e5bb97ef9d15bb21236d514bc0006)+++ b/[mm/page\_alloc.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/mm/page_alloc.c?id=fc4951c3e3358dd82ea508e893695b916c813f17)@@ -600,7 +600,7 @@ void destroy\_large\_folio(struct folio \*folio) return; } - folio\_undo\_large\_rmappable(folio);+ folio\_unqueue\_deferred\_split(folio); mem\_cgroup\_uncharge(folio); free\_the\_page(&folio->page, folio\_order(folio)); } |
| --- |

generated by [cgit 1.2.3-korg](https://git.zx2c4.com/cgit/about/) ([git 2.43.0](https://git-scm.com/)) at 2025-01-15 08:04:44 +0000



=== Content from git.kernel.org_25fe2223_20250115_080605.html ===


| [cgit logo](/) | [index](/) : [kernel/git/stable/linux.git](/pub/scm/linux/kernel/git/stable/linux.git/) | linux-2.6.11.y linux-2.6.12.y linux-2.6.13.y linux-2.6.14.y linux-2.6.15.y linux-2.6.16.y linux-2.6.17.y linux-2.6.18.y linux-2.6.19.y linux-2.6.20.y linux-2.6.21.y linux-2.6.22.y linux-2.6.23.y linux-2.6.24.y linux-2.6.25.y linux-2.6.26.y linux-2.6.27.y linux-2.6.28.y linux-2.6.29.y linux-2.6.30.y linux-2.6.31.y linux-2.6.32.y linux-2.6.33.y linux-2.6.34.y linux-2.6.35.y linux-2.6.36.y linux-2.6.37.y linux-2.6.38.y linux-2.6.39.y linux-3.0.y linux-3.1.y linux-3.10.y linux-3.11.y linux-3.12.y linux-3.13.y linux-3.14.y linux-3.15.y linux-3.16.y linux-3.17.y linux-3.18.y linux-3.19.y linux-3.2.y linux-3.3.y linux-3.4.y linux-3.5.y linux-3.6.y linux-3.7.y linux-3.8.y linux-3.9.y linux-4.0.y linux-4.1.y linux-4.10.y linux-4.11.y linux-4.12.y linux-4.13.y linux-4.14.y linux-4.15.y linux-4.16.y linux-4.17.y linux-4.18.y linux-4.19.y linux-4.2.y linux-4.20.y linux-4.3.y linux-4.4.y linux-4.5.y linux-4.6.y linux-4.7.y linux-4.8.y linux-4.9.y linux-5.0.y linux-5.1.y linux-5.10.y linux-5.11.y linux-5.12.y linux-5.13.y linux-5.14.y linux-5.15.y linux-5.16.y linux-5.17.y linux-5.18.y linux-5.19.y linux-5.2.y linux-5.3.y linux-5.4.y linux-5.5.y linux-5.6.y linux-5.7.y linux-5.8.y linux-5.9.y linux-6.0.y linux-6.1.y linux-6.10.y linux-6.11.y linux-6.12.y linux-6.2.y linux-6.3.y linux-6.4.y linux-6.5.y linux-6.6.y linux-6.7.y linux-6.8.y linux-6.9.y linux-rolling-lts linux-rolling-stable master |
| --- | --- | --- |
| Linux kernel stable tree | Stable Group |

| [about](/pub/scm/linux/kernel/git/stable/linux.git/about/)[summary](/pub/scm/linux/kernel/git/stable/linux.git/)[refs](/pub/scm/linux/kernel/git/stable/linux.git/refs/?id=f8f931bba0f92052cf842b7e30917b1afcc77d5a)[log](/pub/scm/linux/kernel/git/stable/linux.git/log/)[tree](/pub/scm/linux/kernel/git/stable/linux.git/tree/?id=f8f931bba0f92052cf842b7e30917b1afcc77d5a)[commit](/pub/scm/linux/kernel/git/stable/linux.git/commit/?id=f8f931bba0f92052cf842b7e30917b1afcc77d5a)[diff](/pub/scm/linux/kernel/git/stable/linux.git/diff/?id=f8f931bba0f92052cf842b7e30917b1afcc77d5a)[stats](/pub/scm/linux/kernel/git/stable/linux.git/stats/) | log msg author committer range |
| --- | --- |

**diff options**

|  | |
| --- | --- |
| context: | 12345678910152025303540 |
| space: | includeignore |
| mode: | unifiedssdiffstat only |
|  |  |

| author | Hugh Dickins <hughd@google.com> | 2024-10-27 13:02:13 -0700 |
| --- | --- | --- |
| committer | Andrew Morton <akpm@linux-foundation.org> | 2024-11-05 16:49:54 -0800 |
| commit | [f8f931bba0f92052cf842b7e30917b1afcc77d5a](/pub/scm/linux/kernel/git/stable/linux.git/commit/?id=f8f931bba0f92052cf842b7e30917b1afcc77d5a) ([patch](/pub/scm/linux/kernel/git/stable/linux.git/patch/?id=f8f931bba0f92052cf842b7e30917b1afcc77d5a)) | |
| tree | [ab63ba54c531863caa17d2fa3e4d52672e6ca7f9](/pub/scm/linux/kernel/git/stable/linux.git/tree/?id=f8f931bba0f92052cf842b7e30917b1afcc77d5a) | |
| parent | [e66f3185fa04ccb807c6fbf0ea066574f4308831](/pub/scm/linux/kernel/git/stable/linux.git/commit/?id=e66f3185fa04ccb807c6fbf0ea066574f4308831) ([diff](/pub/scm/linux/kernel/git/stable/linux.git/diff/?id=f8f931bba0f92052cf842b7e30917b1afcc77d5a&id2=e66f3185fa04ccb807c6fbf0ea066574f4308831)) | |
| download | [linux-f8f931bba0f92052cf842b7e30917b1afcc77d5a.tar.gz](/pub/scm/linux/kernel/git/stable/linux.git/snapshot/linux-f8f931bba0f92052cf842b7e30917b1afcc77d5a.tar.gz) | |

mm/thp: fix deferred split unqueue naming and lockingRecent changes are putting more pressure on THP deferred split queues:
under load revealing long-standing races, causing list\_del corruptions,
"Bad page state"s and worse (I keep BUGs in both of those, so usually
don't get to see how badly they end up without). The relevant recent
changes being 6.8's mTHP, 6.10's mTHP swapout, and 6.12's mTHP swapin,
improved swap allocation, and underused THP splitting.
Before fixing locking: rename misleading folio\_undo\_large\_rmappable(),
which does not undo large\_rmappable, to folio\_unqueue\_deferred\_split(),
which is what it does. But that and its out-of-line \_\_callee are mm
internals of very limited usability: add comment and WARN\_ON\_ONCEs to
check usage; and return a bool to say if a deferred split was unqueued,
which can then be used in WARN\_ON\_ONCEs around safety checks (sparing
callers the arcane conditionals in \_\_folio\_unqueue\_deferred\_split()).
Just omit the folio\_unqueue\_deferred\_split() from free\_unref\_folios(), all
of whose callers now call it beforehand (and if any forget then bad\_page()
will tell) - except for its caller put\_pages\_list(), which itself no
longer has any callers (and will be deleted separately).
Swapout: mem\_cgroup\_swapout() has been resetting folio->memcg\_data 0
without checking and unqueueing a THP folio from deferred split list;
which is unfortunate, since the split\_queue\_lock depends on the memcg
(when memcg is enabled); so swapout has been unqueueing such THPs later,
when freeing the folio, using the pgdat's lock instead: potentially
corrupting the memcg's list. \_\_remove\_mapping() has frozen refcount to 0
here, so no problem with calling folio\_unqueue\_deferred\_split() before
resetting memcg\_data.
That goes back to 5.4 commit 87eaceb3faa5 ("mm: thp: make deferred split
shrinker memcg aware"): which included a check on swapcache before adding
to deferred queue, but no check on deferred queue before adding THP to
swapcache. That worked fine with the usual sequence of events in reclaim
(though there were a couple of rare ways in which a THP on deferred queue
could have been swapped out), but 6.12 commit dafff3f4c850 ("mm: split
underused THPs") avoids splitting underused THPs in reclaim, which makes
swapcache THPs on deferred queue commonplace.
Keep the check on swapcache before adding to deferred queue? Yes: it is
no longer essential, but preserves the existing behaviour, and is likely
to be a worthwhile optimization (vmstat showed much more traffic on the
queue under swapping load if the check was removed); update its comment.
Memcg-v1 move (deprecated): mem\_cgroup\_move\_account() has been changing
folio->memcg\_data without checking and unqueueing a THP folio from the
deferred list, sometimes corrupting "from" memcg's list, like swapout.
Refcount is non-zero here, so folio\_unqueue\_deferred\_split() can only be
used in a WARN\_ON\_ONCE to validate the fix, which must be done earlier:
mem\_cgroup\_move\_charge\_pte\_range() first try to split the THP (splitting
of course unqueues), or skip it if that fails. Not ideal, but moving
charge has been requested, and khugepaged should repair the THP later:
nobody wants new custom unqueueing code just for this deprecated case.
The 87eaceb3faa5 commit did have the code to move from one deferred list
to another (but was not conscious of its unsafety while refcount non-0);
but that was removed by 5.6 commit fac0516b5534 ("mm: thp: don't need care
deferred split queue in memcg charge move path"), which argued that the
existence of a PMD mapping guarantees that the THP cannot be on a deferred
list. As above, false in rare cases, and now commonly false.
Backport to 6.11 should be straightforward. Earlier backports must take
care that other \_deferred\_list fixes and dependencies are included. There
is not a strong case for backports, but they can fix cornercases.
Link: [https://lkml.kernel.org/r/8dc111ae-f6db-2da7-b25c-7a20b1effe3b@google.com](https://lkml.kernel.org/r/8dc111ae-f6db-2da7-b25c-7a20b1effe3b%40google.com)
Fixes: 87eaceb3faa5 ("mm: thp: make deferred split shrinker memcg aware")
Fixes: dafff3f4c850 ("mm: split underused THPs")
Signed-off-by: Hugh Dickins <hughd@google.com>
Acked-by: David Hildenbrand <david@redhat.com>
Reviewed-by: Yang Shi <shy828301@gmail.com>
Cc: Baolin Wang <baolin.wang@linux.alibaba.com>
Cc: Barry Song <baohua@kernel.org>
Cc: Chris Li <chrisl@kernel.org>
Cc: Johannes Weiner <hannes@cmpxchg.org>
Cc: Kefeng Wang <wangkefeng.wang@huawei.com>
Cc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
Cc: Matthew Wilcox (Oracle) <willy@infradead.org>
Cc: Nhat Pham <nphamcs@gmail.com>
Cc: Ryan Roberts <ryan.roberts@arm.com>
Cc: Shakeel Butt <shakeel.butt@linux.dev>
Cc: Usama Arif <usamaarif642@gmail.com>
Cc: Wei Yang <richard.weiyang@gmail.com>
Cc: Zi Yan <ziy@nvidia.com>
Cc: <stable@vger.kernel.org>
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
[Diffstat](/pub/scm/linux/kernel/git/stable/linux.git/diff/?id=f8f931bba0f92052cf842b7e30917b1afcc77d5a)

| -rw-r--r-- | [mm/huge\_memory.c](/pub/scm/linux/kernel/git/stable/linux.git/diff/mm/huge_memory.c?id=f8f931bba0f92052cf842b7e30917b1afcc77d5a) | 35 | |  |  |  | | --- | --- | --- | |
| --- | --- | --- | --- | --- | --- | --- |
| -rw-r--r-- | [mm/internal.h](/pub/scm/linux/kernel/git/stable/linux.git/diff/mm/internal.h?id=f8f931bba0f92052cf842b7e30917b1afcc77d5a) | 10 | |  |  |  | | --- | --- | --- | |
| -rw-r--r-- | [mm/memcontrol-v1.c](/pub/scm/linux/kernel/git/stable/linux.git/diff/mm/memcontrol-v1.c?id=f8f931bba0f92052cf842b7e30917b1afcc77d5a) | 25 | |  |  |  | | --- | --- | --- | |
| -rw-r--r-- | [mm/memcontrol.c](/pub/scm/linux/kernel/git/stable/linux.git/diff/mm/memcontrol.c?id=f8f931bba0f92052cf842b7e30917b1afcc77d5a) | 8 | |  |  |  | | --- | --- | --- | |
| -rw-r--r-- | [mm/migrate.c](/pub/scm/linux/kernel/git/stable/linux.git/diff/mm/migrate.c?id=f8f931bba0f92052cf842b7e30917b1afcc77d5a) | 4 | |  |  |  | | --- | --- | --- | |
| -rw-r--r-- | [mm/page\_alloc.c](/pub/scm/linux/kernel/git/stable/linux.git/diff/mm/page_alloc.c?id=f8f931bba0f92052cf842b7e30917b1afcc77d5a) | 1 | |  |  |  | | --- | --- | --- | |
| -rw-r--r-- | [mm/swap.c](/pub/scm/linux/kernel/git/stable/linux.git/diff/mm/swap.c?id=f8f931bba0f92052cf842b7e30917b1afcc77d5a) | 4 | |  |  |  | | --- | --- | --- | |
| -rw-r--r-- | [mm/vmscan.c](/pub/scm/linux/kernel/git/stable/linux.git/diff/mm/vmscan.c?id=f8f931bba0f92052cf842b7e30917b1afcc77d5a) | 4 | |  |  |  | | --- | --- | --- | |

8 files changed, 67 insertions, 24 deletions

| diff --git a/mm/huge\_memory.c b/mm/huge\_memory.cindex a1d345f1680cb1..03fd4bc39ea156 100644--- a/[mm/huge\_memory.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/mm/huge_memory.c?id=e66f3185fa04ccb807c6fbf0ea066574f4308831)+++ b/[mm/huge\_memory.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/mm/huge_memory.c?id=f8f931bba0f92052cf842b7e30917b1afcc77d5a)@@ -3588,10 +3588,27 @@ int split\_folio\_to\_list(struct folio \*folio, struct list\_head \*list) return split\_huge\_page\_to\_list\_to\_order(&folio->page, list, ret); } -void \_\_folio\_undo\_large\_rmappable(struct folio \*folio)+/\*+ \* \_\_folio\_unqueue\_deferred\_split() is not to be called directly:+ \* the folio\_unqueue\_deferred\_split() inline wrapper in mm/internal.h+ \* limits its calls to those folios which may have a \_deferred\_list for+ \* queueing THP splits, and that list is (racily observed to be) non-empty.+ \*+ \* It is unsafe to call folio\_unqueue\_deferred\_split() until folio refcount is+ \* zero: because even when split\_queue\_lock is held, a non-empty \_deferred\_list+ \* might be in use on deferred\_split\_scan()'s unlocked on-stack list.+ \*+ \* If memory cgroups are enabled, split\_queue\_lock is in the mem\_cgroup: it is+ \* therefore important to unqueue deferred split before changing folio memcg.+ \*/+bool \_\_folio\_unqueue\_deferred\_split(struct folio \*folio) { struct deferred\_split \*ds\_queue; unsigned long flags;+ bool unqueued = false;++ WARN\_ON\_ONCE(folio\_ref\_count(folio));+ WARN\_ON\_ONCE(!mem\_cgroup\_disabled() && !folio\_memcg(folio));  ds\_queue = get\_deferred\_split\_queue(folio); spin\_lock\_irqsave(&ds\_queue->split\_queue\_lock, flags);@@ -3603,8 +3620,11 @@ void \_\_folio\_undo\_large\_rmappable(struct folio \*folio) MTHP\_STAT\_NR\_ANON\_PARTIALLY\_MAPPED, -1); } list\_del\_init(&folio->\_deferred\_list);+ unqueued = true; } spin\_unlock\_irqrestore(&ds\_queue->split\_queue\_lock, flags);++ return unqueued; /\* useful for debug warnings \*/ }  /\* partially\_mapped=false won't clear PG\_partially\_mapped folio flag \*/@@ -3627,14 +3647,11 @@ void deferred\_split\_folio(struct folio \*folio, bool partially\_mapped) return;  /\*- \* The try\_to\_unmap() in page reclaim path might reach here too,- \* this may cause a race condition to corrupt deferred split queue.- \* And, if page reclaim is already handling the same folio, it is- \* unnecessary to handle it again in shrinker.- \*- \* Check the swapcache flag to determine if the folio is being- \* handled by page reclaim since THP swap would add the folio into- \* swap cache before calling try\_to\_unmap().+ \* Exclude swapcache: originally to avoid a corrupt deferred split+ \* queue. Nowadays that is fully prevented by mem\_cgroup\_swapout();+ \* but if page reclaim is already handling the same folio, it is+ \* unnecessary to handle it again in the shrinker, so excluding+ \* swapcache here may still be a useful optimization. \*/ if (folio\_test\_swapcache(folio)) return;diff --git a/mm/internal.h b/mm/internal.hindex 93083bbeeefae3..16c1f3cd599e56 100644--- a/[mm/internal.h](/pub/scm/linux/kernel/git/stable/linux.git/tree/mm/internal.h?id=e66f3185fa04ccb807c6fbf0ea066574f4308831)+++ b/[mm/internal.h](/pub/scm/linux/kernel/git/stable/linux.git/tree/mm/internal.h?id=f8f931bba0f92052cf842b7e30917b1afcc77d5a)@@ -639,11 +639,11 @@ static inline void folio\_set\_order(struct folio \*folio, unsigned int order) #endif } -void \_\_folio\_undo\_large\_rmappable(struct folio \*folio);-static inline void folio\_undo\_large\_rmappable(struct folio \*folio)+bool \_\_folio\_unqueue\_deferred\_split(struct folio \*folio);+static inline bool folio\_unqueue\_deferred\_split(struct folio \*folio) { if (folio\_order(folio) <= 1 || !folio\_test\_large\_rmappable(folio))- return;+ return false;  /\* \* At this point, there is no one trying to add the folio to@@ -651,9 +651,9 @@ static inline void folio\_undo\_large\_rmappable(struct folio \*folio) \* to check without acquiring the split\_queue\_lock. \*/ if (data\_race(list\_empty(&folio->\_deferred\_list)))- return;+ return false; - \_\_folio\_undo\_large\_rmappable(folio);+ return \_\_folio\_unqueue\_deferred\_split(folio); }  static inline struct folio \*page\_rmappable\_folio(struct page \*page)diff --git a/mm/memcontrol-v1.c b/mm/memcontrol-v1.cindex 81d8819f13cdbf..f8744f5630bb97 100644--- a/[mm/memcontrol-v1.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/mm/memcontrol-v1.c?id=e66f3185fa04ccb807c6fbf0ea066574f4308831)+++ b/[mm/memcontrol-v1.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/mm/memcontrol-v1.c?id=f8f931bba0f92052cf842b7e30917b1afcc77d5a)@@ -848,6 +848,8 @@ static int mem\_cgroup\_move\_account(struct folio \*folio, css\_get(&to->css); css\_put(&from->css); + /\* Warning should never happen, so don't worry about refcount non-0 \*/+ WARN\_ON\_ONCE(folio\_unqueue\_deferred\_split(folio)); folio->memcg\_data = (unsigned long)to;  \_\_folio\_memcg\_unlock(from);@@ -1217,7 +1219,9 @@ static int mem\_cgroup\_move\_charge\_pte\_range(pmd\_t \*pmd, enum mc\_target\_type target\_type; union mc\_target target; struct folio \*folio;+ bool tried\_split\_before = false; +retry\_pmd: ptl = pmd\_trans\_huge\_lock(pmd, vma); if (ptl) { if (mc.precharge < HPAGE\_PMD\_NR) {@@ -1227,6 +1231,27 @@ static int mem\_cgroup\_move\_charge\_pte\_range(pmd\_t \*pmd, target\_type = get\_mctgt\_type\_thp(vma, addr, \*pmd, &target); if (target\_type == MC\_TARGET\_PAGE) { folio = target.folio;+ /\*+ \* Deferred split queue locking depends on memcg,+ \* and unqueue is unsafe unless folio refcount is 0:+ \* split or skip if on the queue? first try to split.+ \*/+ if (!list\_empty(&folio->\_deferred\_list)) {+ spin\_unlock(ptl);+ if (!tried\_split\_before)+ split\_folio(folio);+ folio\_unlock(folio);+ folio\_put(folio);+ if (tried\_split\_before)+ return 0;+ tried\_split\_before = true;+ goto retry\_pmd;+ }+ /\*+ \* So long as that pmd lock is held, the folio cannot+ \* be racily added to the \_deferred\_list, because+ \* \_\_folio\_remove\_rmap() will find !partially\_mapped.+ \*/ if (folio\_isolate\_lru(folio)) { if (!mem\_cgroup\_move\_account(folio, true, mc.from, mc.to)) {diff --git a/mm/memcontrol.c b/mm/memcontrol.cindex 2703227cce88ba..06df2af9741592 100644--- a/[mm/memcontrol.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/mm/memcontrol.c?id=e66f3185fa04ccb807c6fbf0ea066574f4308831)+++ b/[mm/memcontrol.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/mm/memcontrol.c?id=f8f931bba0f92052cf842b7e30917b1afcc77d5a)@@ -4629,9 +4629,6 @@ static void uncharge\_folio(struct folio \*folio, struct uncharge\_gather \*ug) struct obj\_cgroup \*objcg;  VM\_BUG\_ON\_FOLIO(folio\_test\_lru(folio), folio);- VM\_BUG\_ON\_FOLIO(folio\_order(folio) > 1 &&- !folio\_test\_hugetlb(folio) &&- !list\_empty(&folio->\_deferred\_list), folio);  /\* \* Nobody should be changing or seriously looking at@@ -4678,6 +4675,7 @@ static void uncharge\_folio(struct folio \*folio, struct uncharge\_gather \*ug) ug->nr\_memory += nr\_pages; ug->pgpgout++; + WARN\_ON\_ONCE(folio\_unqueue\_deferred\_split(folio)); folio->memcg\_data = 0; } @@ -4789,6 +4787,9 @@ void mem\_cgroup\_migrate(struct folio \*old, struct folio \*new)  /\* Transfer the charge and the css ref \*/ commit\_charge(new, memcg);++ /\* Warning should never happen, so don't worry about refcount non-0 \*/+ WARN\_ON\_ONCE(folio\_unqueue\_deferred\_split(old)); old->memcg\_data = 0; } @@ -4975,6 +4976,7 @@ void mem\_cgroup\_swapout(struct folio \*folio, swp\_entry\_t entry) VM\_BUG\_ON\_FOLIO(oldid, folio); mod\_memcg\_state(swap\_memcg, MEMCG\_SWAP, nr\_entries); + folio\_unqueue\_deferred\_split(folio); folio->memcg\_data = 0;  if (!mem\_cgroup\_is\_root(memcg))diff --git a/mm/migrate.c b/mm/migrate.cindex fab84a7760889c..dfa24e41e8f956 100644--- a/[mm/migrate.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/mm/migrate.c?id=e66f3185fa04ccb807c6fbf0ea066574f4308831)+++ b/[mm/migrate.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/mm/migrate.c?id=f8f931bba0f92052cf842b7e30917b1afcc77d5a)@@ -490,7 +490,7 @@ static int \_\_folio\_migrate\_mapping(struct address\_space \*mapping, folio\_test\_large\_rmappable(folio)) { if (!folio\_ref\_freeze(folio, expected\_count)) return -EAGAIN;- folio\_undo\_large\_rmappable(folio);+ folio\_unqueue\_deferred\_split(folio); folio\_ref\_unfreeze(folio, expected\_count); } @@ -515,7 +515,7 @@ static int \_\_folio\_migrate\_mapping(struct address\_space \*mapping, }  /\* Take off deferred split queue while frozen and memcg set \*/- folio\_undo\_large\_rmappable(folio);+ folio\_unqueue\_deferred\_split(folio);  /\* \* Now we know that no one else is looking at the folio:diff --git a/mm/page\_alloc.c b/mm/page\_alloc.cindex 5e108ae755cc94..8ad38cd5e574b6 100644--- a/[mm/page\_alloc.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/mm/page_alloc.c?id=e66f3185fa04ccb807c6fbf0ea066574f4308831)+++ b/[mm/page\_alloc.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/mm/page_alloc.c?id=f8f931bba0f92052cf842b7e30917b1afcc77d5a)@@ -2681,7 +2681,6 @@ void free\_unref\_folios(struct folio\_batch \*folios) unsigned long pfn = folio\_pfn(folio); unsigned int order = folio\_order(folio); - folio\_undo\_large\_rmappable(folio); if (!free\_pages\_prepare(&folio->page, order)) continue; /\*diff --git a/mm/swap.c b/mm/swap.cindex 835bdf324b76e8..b8e3259ea2c472 100644--- a/[mm/swap.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/mm/swap.c?id=e66f3185fa04ccb807c6fbf0ea066574f4308831)+++ b/[mm/swap.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/mm/swap.c?id=f8f931bba0f92052cf842b7e30917b1afcc77d5a)@@ -121,7 +121,7 @@ void \_\_folio\_put(struct folio \*folio) }  page\_cache\_release(folio);- folio\_undo\_large\_rmappable(folio);+ folio\_unqueue\_deferred\_split(folio); mem\_cgroup\_uncharge(folio); free\_unref\_page(&folio->page, folio\_order(folio)); }@@ -988,7 +988,7 @@ void folios\_put\_refs(struct folio\_batch \*folios, unsigned int \*refs) free\_huge\_folio(folio); continue; }- folio\_undo\_large\_rmappable(folio);+ folio\_unqueue\_deferred\_split(folio); \_\_page\_cache\_release(folio, &lruvec, &flags);  if (j != i)diff --git a/mm/vmscan.c b/mm/vmscan.cindex ddaaff67642e17..28ba2b06fc7dc2 100644--- a/[mm/vmscan.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/mm/vmscan.c?id=e66f3185fa04ccb807c6fbf0ea066574f4308831)+++ b/[mm/vmscan.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/mm/vmscan.c?id=f8f931bba0f92052cf842b7e30917b1afcc77d5a)@@ -1476,7 +1476,7 @@ free\_it: \*/ nr\_reclaimed += nr\_pages; - folio\_undo\_large\_rmappable(folio);+ folio\_unqueue\_deferred\_split(folio); if (folio\_batch\_add(&free\_folios, folio) == 0) { mem\_cgroup\_uncharge\_folios(&free\_folios); try\_to\_unmap\_flush();@@ -1864,7 +1864,7 @@ static unsigned int move\_folios\_to\_lru(struct lruvec \*lruvec, if (unlikely(folio\_put\_testzero(folio))) { \_\_folio\_clear\_lru\_flags(folio); - folio\_undo\_large\_rmappable(folio);+ folio\_unqueue\_deferred\_split(folio); if (folio\_batch\_add(&free\_folios, folio) == 0) { spin\_unlock\_irq(&lruvec->lru\_lock); mem\_cgroup\_uncharge\_folios(&free\_folios); |
| --- |

generated by [cgit 1.2.3-korg](https://git.zx2c4.com/cgit/about/) ([git 2.43.0](https://git-scm.com/)) at 2025-01-15 08:04:43 +0000



=== Content from git.kernel.org_40225a5e_20250115_080604.html ===


| [cgit logo](/) | [index](/) : [kernel/git/stable/linux.git](/pub/scm/linux/kernel/git/stable/linux.git/) | linux-2.6.11.y linux-2.6.12.y linux-2.6.13.y linux-2.6.14.y linux-2.6.15.y linux-2.6.16.y linux-2.6.17.y linux-2.6.18.y linux-2.6.19.y linux-2.6.20.y linux-2.6.21.y linux-2.6.22.y linux-2.6.23.y linux-2.6.24.y linux-2.6.25.y linux-2.6.26.y linux-2.6.27.y linux-2.6.28.y linux-2.6.29.y linux-2.6.30.y linux-2.6.31.y linux-2.6.32.y linux-2.6.33.y linux-2.6.34.y linux-2.6.35.y linux-2.6.36.y linux-2.6.37.y linux-2.6.38.y linux-2.6.39.y linux-3.0.y linux-3.1.y linux-3.10.y linux-3.11.y linux-3.12.y linux-3.13.y linux-3.14.y linux-3.15.y linux-3.16.y linux-3.17.y linux-3.18.y linux-3.19.y linux-3.2.y linux-3.3.y linux-3.4.y linux-3.5.y linux-3.6.y linux-3.7.y linux-3.8.y linux-3.9.y linux-4.0.y linux-4.1.y linux-4.10.y linux-4.11.y linux-4.12.y linux-4.13.y linux-4.14.y linux-4.15.y linux-4.16.y linux-4.17.y linux-4.18.y linux-4.19.y linux-4.2.y linux-4.20.y linux-4.3.y linux-4.4.y linux-4.5.y linux-4.6.y linux-4.7.y linux-4.8.y linux-4.9.y linux-5.0.y linux-5.1.y linux-5.10.y linux-5.11.y linux-5.12.y linux-5.13.y linux-5.14.y linux-5.15.y linux-5.16.y linux-5.17.y linux-5.18.y linux-5.19.y linux-5.2.y linux-5.3.y linux-5.4.y linux-5.5.y linux-5.6.y linux-5.7.y linux-5.8.y linux-5.9.y linux-6.0.y linux-6.1.y linux-6.10.y linux-6.11.y linux-6.12.y linux-6.2.y linux-6.3.y linux-6.4.y linux-6.5.y linux-6.6.y linux-6.7.y linux-6.8.y linux-6.9.y linux-rolling-lts linux-rolling-stable master |
| --- | --- | --- |
| Linux kernel stable tree | Stable Group |

| [about](/pub/scm/linux/kernel/git/stable/linux.git/about/)[summary](/pub/scm/linux/kernel/git/stable/linux.git/)[refs](/pub/scm/linux/kernel/git/stable/linux.git/refs/?id=afb1352d06b1b6b2cfd1f901c766a430c87078b3)[log](/pub/scm/linux/kernel/git/stable/linux.git/log/)[tree](/pub/scm/linux/kernel/git/stable/linux.git/tree/?id=afb1352d06b1b6b2cfd1f901c766a430c87078b3)[commit](/pub/scm/linux/kernel/git/stable/linux.git/commit/?id=afb1352d06b1b6b2cfd1f901c766a430c87078b3)[diff](/pub/scm/linux/kernel/git/stable/linux.git/diff/?id=afb1352d06b1b6b2cfd1f901c766a430c87078b3)[stats](/pub/scm/linux/kernel/git/stable/linux.git/stats/) | log msg author committer range |
| --- | --- |

**diff options**

|  | |
| --- | --- |
| context: | 12345678910152025303540 |
| space: | includeignore |
| mode: | unifiedssdiffstat only |
|  |  |

| author | Hugh Dickins <hughd@google.com> | 2024-10-27 13:02:13 -0700 |
| --- | --- | --- |
| committer | Greg Kroah-Hartman <gregkh@linuxfoundation.org> | 2024-11-14 13:21:12 +0100 |
| commit | [afb1352d06b1b6b2cfd1f901c766a430c87078b3](/pub/scm/linux/kernel/git/stable/linux.git/commit/?id=afb1352d06b1b6b2cfd1f901c766a430c87078b3) ([patch](/pub/scm/linux/kernel/git/stable/linux.git/patch/?id=afb1352d06b1b6b2cfd1f901c766a430c87078b3)) | |
| tree | [4c7f123d9019512af8775ae3e98c16ab7dbe259c](/pub/scm/linux/kernel/git/stable/linux.git/tree/?id=afb1352d06b1b6b2cfd1f901c766a430c87078b3) | |
| parent | [1e58fe6ad0b286462c27533b055d3a9c40dfd1e7](/pub/scm/linux/kernel/git/stable/linux.git/commit/?id=1e58fe6ad0b286462c27533b055d3a9c40dfd1e7) ([diff](/pub/scm/linux/kernel/git/stable/linux.git/diff/?id=afb1352d06b1b6b2cfd1f901c766a430c87078b3&id2=1e58fe6ad0b286462c27533b055d3a9c40dfd1e7)) | |
| download | [linux-afb1352d06b1b6b2cfd1f901c766a430c87078b3.tar.gz](/pub/scm/linux/kernel/git/stable/linux.git/snapshot/linux-afb1352d06b1b6b2cfd1f901c766a430c87078b3.tar.gz) | |

mm/thp: fix deferred split unqueue naming and lockingcommit f8f931bba0f92052cf842b7e30917b1afcc77d5a upstream.
Recent changes are putting more pressure on THP deferred split queues:
under load revealing long-standing races, causing list\_del corruptions,
"Bad page state"s and worse (I keep BUGs in both of those, so usually
don't get to see how badly they end up without). The relevant recent
changes being 6.8's mTHP, 6.10's mTHP swapout, and 6.12's mTHP swapin,
improved swap allocation, and underused THP splitting.
Before fixing locking: rename misleading folio\_undo\_large\_rmappable(),
which does not undo large\_rmappable, to folio\_unqueue\_deferred\_split(),
which is what it does. But that and its out-of-line \_\_callee are mm
internals of very limited usability: add comment and WARN\_ON\_ONCEs to
check usage; and return a bool to say if a deferred split was unqueued,
which can then be used in WARN\_ON\_ONCEs around safety checks (sparing
callers the arcane conditionals in \_\_folio\_unqueue\_deferred\_split()).
Just omit the folio\_unqueue\_deferred\_split() from free\_unref\_folios(), all
of whose callers now call it beforehand (and if any forget then bad\_page()
will tell) - except for its caller put\_pages\_list(), which itself no
longer has any callers (and will be deleted separately).
Swapout: mem\_cgroup\_swapout() has been resetting folio->memcg\_data 0
without checking and unqueueing a THP folio from deferred split list;
which is unfortunate, since the split\_queue\_lock depends on the memcg
(when memcg is enabled); so swapout has been unqueueing such THPs later,
when freeing the folio, using the pgdat's lock instead: potentially
corrupting the memcg's list. \_\_remove\_mapping() has frozen refcount to 0
here, so no problem with calling folio\_unqueue\_deferred\_split() before
resetting memcg\_data.
That goes back to 5.4 commit 87eaceb3faa5 ("mm: thp: make deferred split
shrinker memcg aware"): which included a check on swapcache before adding
to deferred queue, but no check on deferred queue before adding THP to
swapcache. That worked fine with the usual sequence of events in reclaim
(though there were a couple of rare ways in which a THP on deferred queue
could have been swapped out), but 6.12 commit dafff3f4c850 ("mm: split
underused THPs") avoids splitting underused THPs in reclaim, which makes
swapcache THPs on deferred queue commonplace.
Keep the check on swapcache before adding to deferred queue? Yes: it is
no longer essential, but preserves the existing behaviour, and is likely
to be a worthwhile optimization (vmstat showed much more traffic on the
queue under swapping load if the check was removed); update its comment.
Memcg-v1 move (deprecated): mem\_cgroup\_move\_account() has been changing
folio->memcg\_data without checking and unqueueing a THP folio from the
deferred list, sometimes corrupting "from" memcg's list, like swapout.
Refcount is non-zero here, so folio\_unqueue\_deferred\_split() can only be
used in a WARN\_ON\_ONCE to validate the fix, which must be done earlier:
mem\_cgroup\_move\_charge\_pte\_range() first try to split the THP (splitting
of course unqueues), or skip it if that fails. Not ideal, but moving
charge has been requested, and khugepaged should repair the THP later:
nobody wants new custom unqueueing code just for this deprecated case.
The 87eaceb3faa5 commit did have the code to move from one deferred list
to another (but was not conscious of its unsafety while refcount non-0);
but that was removed by 5.6 commit fac0516b5534 ("mm: thp: don't need care
deferred split queue in memcg charge move path"), which argued that the
existence of a PMD mapping guarantees that the THP cannot be on a deferred
list. As above, false in rare cases, and now commonly false.
Backport to 6.11 should be straightforward. Earlier backports must take
care that other \_deferred\_list fixes and dependencies are included. There
is not a strong case for backports, but they can fix cornercases.
Link: [https://lkml.kernel.org/r/8dc111ae-f6db-2da7-b25c-7a20b1effe3b@google.com](https://lkml.kernel.org/r/8dc111ae-f6db-2da7-b25c-7a20b1effe3b%40google.com)
Fixes: 87eaceb3faa5 ("mm: thp: make deferred split shrinker memcg aware")
Fixes: dafff3f4c850 ("mm: split underused THPs")
Signed-off-by: Hugh Dickins <hughd@google.com>
Acked-by: David Hildenbrand <david@redhat.com>
Reviewed-by: Yang Shi <shy828301@gmail.com>
Cc: Baolin Wang <baolin.wang@linux.alibaba.com>
Cc: Barry Song <baohua@kernel.org>
Cc: Chris Li <chrisl@kernel.org>
Cc: Johannes Weiner <hannes@cmpxchg.org>
Cc: Kefeng Wang <wangkefeng.wang@huawei.com>
Cc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
Cc: Matthew Wilcox (Oracle) <willy@infradead.org>
Cc: Nhat Pham <nphamcs@gmail.com>
Cc: Ryan Roberts <ryan.roberts@arm.com>
Cc: Shakeel Butt <shakeel.butt@linux.dev>
Cc: Usama Arif <usamaarif642@gmail.com>
Cc: Wei Yang <richard.weiyang@gmail.com>
Cc: Zi Yan <ziy@nvidia.com>
Cc: <stable@vger.kernel.org>
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
[Diffstat](/pub/scm/linux/kernel/git/stable/linux.git/diff/?id=afb1352d06b1b6b2cfd1f901c766a430c87078b3)

| -rw-r--r-- | [mm/huge\_memory.c](/pub/scm/linux/kernel/git/stable/linux.git/diff/mm/huge_memory.c?id=afb1352d06b1b6b2cfd1f901c766a430c87078b3) | 35 | |  |  |  | | --- | --- | --- | |
| --- | --- | --- | --- | --- | --- | --- |
| -rw-r--r-- | [mm/internal.h](/pub/scm/linux/kernel/git/stable/linux.git/diff/mm/internal.h?id=afb1352d06b1b6b2cfd1f901c766a430c87078b3) | 10 | |  |  |  | | --- | --- | --- | |
| -rw-r--r-- | [mm/memcontrol-v1.c](/pub/scm/linux/kernel/git/stable/linux.git/diff/mm/memcontrol-v1.c?id=afb1352d06b1b6b2cfd1f901c766a430c87078b3) | 25 | |  |  |  | | --- | --- | --- | |
| -rw-r--r-- | [mm/memcontrol.c](/pub/scm/linux/kernel/git/stable/linux.git/diff/mm/memcontrol.c?id=afb1352d06b1b6b2cfd1f901c766a430c87078b3) | 8 | |  |  |  | | --- | --- | --- | |
| -rw-r--r-- | [mm/migrate.c](/pub/scm/linux/kernel/git/stable/linux.git/diff/mm/migrate.c?id=afb1352d06b1b6b2cfd1f901c766a430c87078b3) | 4 | |  |  |  | | --- | --- | --- | |
| -rw-r--r-- | [mm/page\_alloc.c](/pub/scm/linux/kernel/git/stable/linux.git/diff/mm/page_alloc.c?id=afb1352d06b1b6b2cfd1f901c766a430c87078b3) | 1 | |  |  |  | | --- | --- | --- | |
| -rw-r--r-- | [mm/swap.c](/pub/scm/linux/kernel/git/stable/linux.git/diff/mm/swap.c?id=afb1352d06b1b6b2cfd1f901c766a430c87078b3) | 4 | |  |  |  | | --- | --- | --- | |
| -rw-r--r-- | [mm/vmscan.c](/pub/scm/linux/kernel/git/stable/linux.git/diff/mm/vmscan.c?id=afb1352d06b1b6b2cfd1f901c766a430c87078b3) | 4 | |  |  |  | | --- | --- | --- | |

8 files changed, 67 insertions, 24 deletions

| diff --git a/mm/huge\_memory.c b/mm/huge\_memory.cindex e44508e46e8979..a4d0dbb04ea764 100644--- a/[mm/huge\_memory.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/mm/huge_memory.c?id=1e58fe6ad0b286462c27533b055d3a9c40dfd1e7)+++ b/[mm/huge\_memory.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/mm/huge_memory.c?id=afb1352d06b1b6b2cfd1f901c766a430c87078b3)@@ -3268,18 +3268,38 @@ out: return ret; } -void \_\_folio\_undo\_large\_rmappable(struct folio \*folio)+/\*+ \* \_\_folio\_unqueue\_deferred\_split() is not to be called directly:+ \* the folio\_unqueue\_deferred\_split() inline wrapper in mm/internal.h+ \* limits its calls to those folios which may have a \_deferred\_list for+ \* queueing THP splits, and that list is (racily observed to be) non-empty.+ \*+ \* It is unsafe to call folio\_unqueue\_deferred\_split() until folio refcount is+ \* zero: because even when split\_queue\_lock is held, a non-empty \_deferred\_list+ \* might be in use on deferred\_split\_scan()'s unlocked on-stack list.+ \*+ \* If memory cgroups are enabled, split\_queue\_lock is in the mem\_cgroup: it is+ \* therefore important to unqueue deferred split before changing folio memcg.+ \*/+bool \_\_folio\_unqueue\_deferred\_split(struct folio \*folio) { struct deferred\_split \*ds\_queue; unsigned long flags;+ bool unqueued = false;++ WARN\_ON\_ONCE(folio\_ref\_count(folio));+ WARN\_ON\_ONCE(!mem\_cgroup\_disabled() && !folio\_memcg(folio));  ds\_queue = get\_deferred\_split\_queue(folio); spin\_lock\_irqsave(&ds\_queue->split\_queue\_lock, flags); if (!list\_empty(&folio->\_deferred\_list)) { ds\_queue->split\_queue\_len--; list\_del\_init(&folio->\_deferred\_list);+ unqueued = true; } spin\_unlock\_irqrestore(&ds\_queue->split\_queue\_lock, flags);++ return unqueued; /\* useful for debug warnings \*/ }  void deferred\_split\_folio(struct folio \*folio)@@ -3298,14 +3318,11 @@ void deferred\_split\_folio(struct folio \*folio) return;  /\*- \* The try\_to\_unmap() in page reclaim path might reach here too,- \* this may cause a race condition to corrupt deferred split queue.- \* And, if page reclaim is already handling the same folio, it is- \* unnecessary to handle it again in shrinker.- \*- \* Check the swapcache flag to determine if the folio is being- \* handled by page reclaim since THP swap would add the folio into- \* swap cache before calling try\_to\_unmap().+ \* Exclude swapcache: originally to avoid a corrupt deferred split+ \* queue. Nowadays that is fully prevented by mem\_cgroup\_swapout();+ \* but if page reclaim is already handling the same folio, it is+ \* unnecessary to handle it again in the shrinker, so excluding+ \* swapcache here may still be a useful optimization. \*/ if (folio\_test\_swapcache(folio)) return;diff --git a/mm/internal.h b/mm/internal.hindex a963f67d3452ad..7da580dfae6c5a 100644--- a/[mm/internal.h](/pub/scm/linux/kernel/git/stable/linux.git/tree/mm/internal.h?id=1e58fe6ad0b286462c27533b055d3a9c40dfd1e7)+++ b/[mm/internal.h](/pub/scm/linux/kernel/git/stable/linux.git/tree/mm/internal.h?id=afb1352d06b1b6b2cfd1f901c766a430c87078b3)@@ -631,11 +631,11 @@ static inline void folio\_set\_order(struct folio \*folio, unsigned int order) #endif } -void \_\_folio\_undo\_large\_rmappable(struct folio \*folio);-static inline void folio\_undo\_large\_rmappable(struct folio \*folio)+bool \_\_folio\_unqueue\_deferred\_split(struct folio \*folio);+static inline bool folio\_unqueue\_deferred\_split(struct folio \*folio) { if (folio\_order(folio) <= 1 || !folio\_test\_large\_rmappable(folio))- return;+ return false;  /\* \* At this point, there is no one trying to add the folio to@@ -643,9 +643,9 @@ static inline void folio\_undo\_large\_rmappable(struct folio \*folio) \* to check without acquiring the split\_queue\_lock. \*/ if (data\_race(list\_empty(&folio->\_deferred\_list)))- return;+ return false; - \_\_folio\_undo\_large\_rmappable(folio);+ return \_\_folio\_unqueue\_deferred\_split(folio); }  static inline struct folio \*page\_rmappable\_folio(struct page \*page)diff --git a/mm/memcontrol-v1.c b/mm/memcontrol-v1.cindex 417c96f2da28e8..103c5fe41c68e3 100644--- a/[mm/memcontrol-v1.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/mm/memcontrol-v1.c?id=1e58fe6ad0b286462c27533b055d3a9c40dfd1e7)+++ b/[mm/memcontrol-v1.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/mm/memcontrol-v1.c?id=afb1352d06b1b6b2cfd1f901c766a430c87078b3)@@ -845,6 +845,8 @@ static int mem\_cgroup\_move\_account(struct folio \*folio, css\_get(&to->css); css\_put(&from->css); + /\* Warning should never happen, so don't worry about refcount non-0 \*/+ WARN\_ON\_ONCE(folio\_unqueue\_deferred\_split(folio)); folio->memcg\_data = (unsigned long)to;  \_\_folio\_memcg\_unlock(from);@@ -1214,7 +1216,9 @@ static int mem\_cgroup\_move\_charge\_pte\_range(pmd\_t \*pmd, enum mc\_target\_type target\_type; union mc\_target target; struct folio \*folio;+ bool tried\_split\_before = false; +retry\_pmd: ptl = pmd\_trans\_huge\_lock(pmd, vma); if (ptl) { if (mc.precharge < HPAGE\_PMD\_NR) {@@ -1224,6 +1228,27 @@ static int mem\_cgroup\_move\_charge\_pte\_range(pmd\_t \*pmd, target\_type = get\_mctgt\_type\_thp(vma, addr, \*pmd, &target); if (target\_type == MC\_TARGET\_PAGE) { folio = target.folio;+ /\*+ \* Deferred split queue locking depends on memcg,+ \* and unqueue is unsafe unless folio refcount is 0:+ \* split or skip if on the queue? first try to split.+ \*/+ if (!list\_empty(&folio->\_deferred\_list)) {+ spin\_unlock(ptl);+ if (!tried\_split\_before)+ split\_folio(folio);+ folio\_unlock(folio);+ folio\_put(folio);+ if (tried\_split\_before)+ return 0;+ tried\_split\_before = true;+ goto retry\_pmd;+ }+ /\*+ \* So long as that pmd lock is held, the folio cannot+ \* be racily added to the \_deferred\_list, because+ \* \_\_folio\_remove\_rmap() will find !partially\_mapped.+ \*/ if (folio\_isolate\_lru(folio)) { if (!mem\_cgroup\_move\_account(folio, true, mc.from, mc.to)) {diff --git a/mm/memcontrol.c b/mm/memcontrol.cindex d563fb515766bc..9b0a6a77a7b219 100644--- a/[mm/memcontrol.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/mm/memcontrol.c?id=1e58fe6ad0b286462c27533b055d3a9c40dfd1e7)+++ b/[mm/memcontrol.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/mm/memcontrol.c?id=afb1352d06b1b6b2cfd1f901c766a430c87078b3)@@ -4604,9 +4604,6 @@ static void uncharge\_folio(struct folio \*folio, struct uncharge\_gather \*ug) struct obj\_cgroup \*objcg;  VM\_BUG\_ON\_FOLIO(folio\_test\_lru(folio), folio);- VM\_BUG\_ON\_FOLIO(folio\_order(folio) > 1 &&- !folio\_test\_hugetlb(folio) &&- !list\_empty(&folio->\_deferred\_list), folio);  /\* \* Nobody should be changing or seriously looking at@@ -4653,6 +4650,7 @@ static void uncharge\_folio(struct folio \*folio, struct uncharge\_gather \*ug) ug->nr\_memory += nr\_pages; ug->pgpgout++; + WARN\_ON\_ONCE(folio\_unqueue\_deferred\_split(folio)); folio->memcg\_data = 0; } @@ -4769,6 +4767,9 @@ void mem\_cgroup\_migrate(struct folio \*old, struct folio \*new)  /\* Transfer the charge and the css ref \*/ commit\_charge(new, memcg);++ /\* Warning should never happen, so don't worry about refcount non-0 \*/+ WARN\_ON\_ONCE(folio\_unqueue\_deferred\_split(old)); old->memcg\_data = 0; } @@ -4955,6 +4956,7 @@ void mem\_cgroup\_swapout(struct folio \*folio, swp\_entry\_t entry) VM\_BUG\_ON\_FOLIO(oldid, folio); mod\_memcg\_state(swap\_memcg, MEMCG\_SWAP, nr\_entries); + folio\_unqueue\_deferred\_split(folio); folio->memcg\_data = 0;  if (!mem\_cgroup\_is\_root(memcg))diff --git a/mm/migrate.c b/mm/migrate.cindex 75b858bd6aa58f..5028f3788b67ad 100644--- a/[mm/migrate.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/mm/migrate.c?id=1e58fe6ad0b286462c27533b055d3a9c40dfd1e7)+++ b/[mm/migrate.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/mm/migrate.c?id=afb1352d06b1b6b2cfd1f901c766a430c87078b3)@@ -415,7 +415,7 @@ static int \_\_folio\_migrate\_mapping(struct address\_space \*mapping, folio\_test\_large\_rmappable(folio)) { if (!folio\_ref\_freeze(folio, expected\_count)) return -EAGAIN;- folio\_undo\_large\_rmappable(folio);+ folio\_unqueue\_deferred\_split(folio); folio\_ref\_unfreeze(folio, expected\_count); } @@ -438,7 +438,7 @@ static int \_\_folio\_migrate\_mapping(struct address\_space \*mapping, }  /\* Take off deferred split queue while frozen and memcg set \*/- folio\_undo\_large\_rmappable(folio);+ folio\_unqueue\_deferred\_split(folio);  /\* \* Now we know that no one else is looking at the folio:diff --git a/mm/page\_alloc.c b/mm/page\_alloc.cindex ec459522c29349..f9111356d1047b 100644--- a/[mm/page\_alloc.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/mm/page_alloc.c?id=1e58fe6ad0b286462c27533b055d3a9c40dfd1e7)+++ b/[mm/page\_alloc.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/mm/page_alloc.c?id=afb1352d06b1b6b2cfd1f901c766a430c87078b3)@@ -2663,7 +2663,6 @@ void free\_unref\_folios(struct folio\_batch \*folios) unsigned long pfn = folio\_pfn(folio); unsigned int order = folio\_order(folio); - folio\_undo\_large\_rmappable(folio); if (!free\_pages\_prepare(&folio->page, order)) continue; /\*diff --git a/mm/swap.c b/mm/swap.cindex 9caf6b017cf0ab..1e734a5a6453e2 100644--- a/[mm/swap.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/mm/swap.c?id=1e58fe6ad0b286462c27533b055d3a9c40dfd1e7)+++ b/[mm/swap.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/mm/swap.c?id=afb1352d06b1b6b2cfd1f901c766a430c87078b3)@@ -123,7 +123,7 @@ void \_\_folio\_put(struct folio \*folio) }  page\_cache\_release(folio);- folio\_undo\_large\_rmappable(folio);+ folio\_unqueue\_deferred\_split(folio); mem\_cgroup\_uncharge(folio); free\_unref\_page(&folio->page, folio\_order(folio)); }@@ -1020,7 +1020,7 @@ void folios\_put\_refs(struct folio\_batch \*folios, unsigned int \*refs) free\_huge\_folio(folio); continue; }- folio\_undo\_large\_rmappable(folio);+ folio\_unqueue\_deferred\_split(folio); \_\_page\_cache\_release(folio, &lruvec, &flags);  if (j != i)diff --git a/mm/vmscan.c b/mm/vmscan.cindex f5bcd08527ae0f..b7f326f87363a2 100644--- a/[mm/vmscan.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/mm/vmscan.c?id=1e58fe6ad0b286462c27533b055d3a9c40dfd1e7)+++ b/[mm/vmscan.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/mm/vmscan.c?id=afb1352d06b1b6b2cfd1f901c766a430c87078b3)@@ -1462,7 +1462,7 @@ free\_it: \*/ nr\_reclaimed += nr\_pages; - folio\_undo\_large\_rmappable(folio);+ folio\_unqueue\_deferred\_split(folio); if (folio\_batch\_add(&free\_folios, folio) == 0) { mem\_cgroup\_uncharge\_folios(&free\_folios); try\_to\_unmap\_flush();@@ -1849,7 +1849,7 @@ static unsigned int move\_folios\_to\_lru(struct lruvec \*lruvec, if (unlikely(folio\_put\_testzero(folio))) { \_\_folio\_clear\_lru\_flags(folio); - folio\_undo\_large\_rmappable(folio);+ folio\_unqueue\_deferred\_split(folio); if (folio\_batch\_add(&free\_folios, folio) == 0) { spin\_unlock\_irq(&lruvec->lru\_lock); mem\_cgroup\_uncharge\_folios(&free\_folios); |
| --- |

generated by [cgit 1.2.3-korg](https://git.zx2c4.com/cgit/about/) ([git 2.43.0](https://git-scm.com/)) at 2025-01-15 08:04:42 +0000


