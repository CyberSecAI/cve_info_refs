=== Content from www.cs.wisc.edu_28aa7d67_20250124_184330.html ===
Backtracking Algorithmic Complexity Attacks Against a NIDS

Randy Smith Cristian Estan Somesh Jha
Computer Sciences Department
University of Wisconsin-Madison
{smithr,estan,jha}@cs.wisc.edu

Abstract

Network Intrusion Detection Systems (NIDS) have be-
come crucial to securing modern networks. To be effective,
a NIDS must be able to counter evasion attempts and oper-
ate at or near wire-speed. Failure to do so allows malicious
packets to slip through a NIDS undetected. In this paper, we
explore NIDS evasion through algorithmic complexity at-
tacks. We present a highly effective attack against the Snort
NIDS, and we provide a practical algorithmic solution that
successfully thwarts the attack. This attack exploits the be-
havior of rule matching, yielding inspection times that are
up to 1.5 million times slower than that of benign packets.
Our analysis shows that this attack is applicable to many
rules in Snort’s ruleset, rendering vulnerable the thousands
of networks protected by it. Our countermeasure conﬁnes
the inspection time to within one order of magnitude of be-
nign packets. Experimental results using a live system show
that an attacker needs only 4.0 kbps of bandwidth to perpet-
ually disable an unmodiﬁed NIDS, whereas all intrusions
are detected when our countermeasure is used.

1. Introduction

Network Intrusion Detection Systems (NIDS) and Intru-
sion Prevention Systems (IPS) have become crucial to se-
curing today’s networks. Typically, a NIDS residing on the
edge of a network performs deep packet inspection on every
packet that enters the protected domain. When a packet is
matched against a signature, an alert is raised, indicating an
attempted intrusion or other misuse.

To be effective in an online environment, packet inspec-
tion must be performed at or near wire speed. The con-
sequences of not doing so can be dire: an intrusion detec-
tion system that fails to perform packet inspection at the
required rate will allow packets to enter the network unde-
tected. Worse, an inline intrusion prevention system that
fails to keep up can cause excessive packet loss.

A NIDS must also guard against evasion attempts which

often succeed by exploiting ambiguities in a protocol def-
inition itself. For example, attack mechanisms have relied
on ambiguities in TCP to develop evasion techniques us-
ing overlapping IP fragments, TTL manipulation, and other
transformations [10, 15, 18].

In this paper, we explore NIDS evasion through the use
of algorithmic complexity attacks [9]. Given an algorithm
whose worst-case performance is signiﬁcantly worse than
its average case performance, an algorithmic complexity at-
tack occurs when an attacker is able to trigger worst-case
or near worst-case behavior. To mount evasion attempts in
NIDS, two attack vectors are required. The ﬁrst is the true
attack that targets a host inside the network. The second is
aimed squarely at the NIDS and serves as a cover by slow-
ing it down so that incoming packets (including the true at-
tack) are able to slip through undetected. Evasion is most
successful when the true attack enters the network, and nei-
ther it nor the second attack is detected by the NIDS.

We present an algorithmic complexity attack that ex-
ploits worst-case signature matching behavior in a NIDS.
By carefully constructing packet payloads, our attack forces
the signature matcher to repeatedly backtrack during in-
spection, yielding packet processing rates that are up to 1.5
million times slower than average. We term this type of
algorithmic complexity attack a backtracking attack. Our
experiments show that hundreds of intrusions can success-
fully enter the network undetected during the course of a
backtracking attack against a NIDS. Further, the backtrack-
ing attack itself requires very little bandwidth; i.e., a single
attack packet sent once every three seconds is enough to
perpetually disable a NIDS.

Our countermeasure to the backtracking attack is an al-
gorithmic, semantics-preserving enhancement to signature
matching based on the concept of memoization. The core
idea is straightforward: whereas the backtracking attack ex-
ploits the need of a signature matcher to evaluate signatures
at all successful string match offsets, a memoization table
can be used to store intermediate state that must otherwise
be recomputed. Our defense against the backtracking at-
tack relies on the use of better algorithms that reduce the

disparity between worst and average case without changing
functionality. Empirical results show that this solution con-
ﬁnes the processing times of attack packets to within one
order of magnitude of benign packets.

Our result applies directly to Snort [17], a popular open-
source package that provides both NIDS and IPS function-
ality and claims more than 150,000 active users. Snort uses
a signature-based architecture in which each signature is
composed of a sequence of operations, such as string or
regular expression matching, that together identify a dis-
tinct misuse. In our experiments, we use Snort over both
traces and live trafﬁc. In addition, we provide a practical
implementation of the defense by extending Snort’s signa-
ture matching functionality directly.

In summary, our contributions are two-fold. First, we
discuss NIDS evasion through algorithmic complexity at-
tacks. We present a highly effective real attack, the back-
tracking attack, that yields slowdowns of up to six orders
of magnitude and is feasible against the (estimated) tens
of thousands of networks monitored by Snort. Second, we
present an algorithmic defense, based on the principle of
memoization, that conﬁnes the slowdown to less than one
order of magnitude in general and to less than a factor of
two in most cases. We provide a practical implementation
of this solution and show its efﬁcacy in a live setup.1

We organize the remainder of this paper as follows: Sec-
tion 2 provides a summary of related work, and Section 3
describes the rule-matching architecture of Snort. In Sec-
tions 4 and 5 we present the backtracking attack and the
countermeasure, respectively. Section 6 details our experi-
mental results, and Section 7 considers other types of com-
plexity attacks. Section 8 concludes.

2. Related work

To our knowledge, Crosby and Wallach [8, 9] were the
ﬁrst to provide an algorithmic basis for denial of service
attacks. They exploit weaknesses in hash function imple-
mentations and backtracking behavior in common regular
expression libraries to produce worst-case behavior that is
signiﬁcantly more expensive than the average case. The re-
sult is denial of service in general and evasion in our con-
text. For their examples, the authors observe that algorith-
mic attacks against hash tables and regular expressions can
be thwarted by better algorithm and data structure selec-
tions. Our defense also relies on algorithmic improvements.
The backtracking attack we present falls within the gene-
ral family of algorithmic attacks, although to the best of our
knowledge our method of achieving evasion through back-
tracking is novel.

1We have presented our ﬁndings to the Snort developers, who have con-
ﬁrmed the efﬁcacy of the evasion attack and have integrated the solution
into their NIDS.

In a systems-oriented approach to addressing resource
consumption and other attacks, Lee et al. [12] dynamically
divide the workload among multiple modules, making ad-
justments as necessary to maintain performance. Load-
shedding is performed as necessary to distribute the load
to different modules, or to lower its priority. Alternatively,
Kruegel et al. [11] have proposed achieving high speed in-
trusion detection by distributing the load across several sen-
sors, using a scatterer to distribute the load and slicers and
reassemblers to provide stateful detection. Still other ap-
proaches seek to provide better performance by splitting up
(and possibly replicating) a sensor onto multiple cores or
processors [6, 26]. These approaches show that allocating
more hardware can better protect large networks with large
amounts of trafﬁc, but they are not a cost effective way of
dealing with algorithmic complexity attacks.

The use of custom hardware has also been proposed for
performing high-speed matching [3,5,20,24,25]. The back-
tracking attack is probably not applicable to these solutions.
As our focus is on software-based systems, we do not con-
sider hardware solutions further in this paper.

Both [12] and [13] propose the use of monitors to track
the resource usage and performance history of a NIDS.
In [12], if a monitor discovers abnormally long process-
ing times, the current operations are aborted and optionally
transferred to a lower priority process. For [13], on the other
hand, the monitor simply triggers a restart of the NIDS.
In the general case, such techniques may provide a useful
mechanism for ensuring guaranteed minimum performance
rates at the cost of decreased detection accuracy. However,
such mechanisms result in periodic lapses in detection ca-
pability. Our solution is semantics-preserving, in the sense
that it does not sacriﬁce detection to maintain performance.
Finally, NIDS evasion has been extensively studied in
the literature. The earliest work describing evasion was
presented by Paxson [13] and Ptacek and Newsham [15].
Handley et al. [10] show that normalization combined with
stateful analysis to remove protocol ambiguities can foil
evasion attempts, although it may affect stream semantics.
Shankar and Paxson [19] address semantics by providing
an online database of network attributes, such as the hop
count from the NIDS to a protected host, that provides the
same beneﬁts as normalization without the risk of chang-
ing stream semantics. These solutions are orthogonal to the
problem discussed in this paper.

3. Rule matching in Snort

Our work is performed in the context of the Snort NIDS.
Snort employs a signature-based approach to intrusion de-
tection, deﬁning distinct signatures, or rules, for each mis-
use to be searched for. Each signature is in turn composed
of a sequence of predicates, that describe the operations that

Description

Predicate
content:< str > Searches for occurrence of < str > in payload
pcre:/regex/
byte test
byte jump

Matches regular expression /regex/ against payload
Performs bitwise or logical tests on speciﬁed payload bytes
Jumps to an offset speciﬁed by given payload bytes

Type
multiple-match
multiple-match
single-match
single-match

Table 1. Subset of Snort predicates used for packet inspection. Multiple-match predicates may need
to be applied to a packet several times.

alert tcp $EXT NET any -> $HOME NET 99
(msg:"AudioPlayer jukebox exploit";

content:"fmt=";
pcre:"/ˆ(mp3|ogg)/",relative;
content:"player=";
pcre:"/.exe|.com/",relative;
content:"overflow",relative;
sid:5678)

//P1
//P2
//P3
//P4
//P5

Figure 1. Rule with simpliﬁed Snort syntax
describing a ﬁctional vulnerability.

the signature must perform. Section 3.1 gives an overview
of the language used to specify these rules. Section 3.2 de-
scribes the algorithm used to match rules against packets.

3.1. Expressing rules in Snort

Snort’s rules are composed of a header and a body. The
header speciﬁes the ports and IP addresses to which the rule
should apply and is used during the classiﬁcation stage. The
body has a sequence of predicates that express conditions
that need to succeed for the rule to match. A rule matches a
packet only if all predicates evaluated in sequence succeed.
Of the predicates that are part of Snort’s rule language, we
focus on those used to analyze the packet payloads. Table 1
summarizes the relevant rules.

Figure 1 depicts a signature using a simpliﬁed version of
Snort’s rule language. The header of the rule instructs Snort
to match this signature against all TCP trafﬁc from external
sources to servers in the home network running on port 99.
The body of the rule contains three content predicates,
two pcre [14] predicates, and two terms, msg and sid,
used for notiﬁcation and bookkeeping. The rule matches
packets that contain the string fmt= followed immediately
by mp3 or ogg, and also contain the string player=, fol-
lowed by .exe or .com, followed by overflow.

Predicates have one important side effect: during rule
matching a predicate records the position in the payload at
which it succeeded. Further, when a predicate contains a
relative modiﬁer, that predicate inspects the packet be-
ginning at the position at which the previous predicate suc-
ceeded, rather than the start of the payload. For example,
if predicate P3 from Figure 1 ﬁnds the string player=
at offset i in the payload, the subsequent pcre predicate
(P4) succeeds only if it matches the packet payload after
position i.

3.2. Matching signatures

When matching a rule against a packet, Snort evalu-
ates the predicates in the order they are presented in the
rule, and concludes that the packet does not match the
rule when it reaches a predicate that fails. To ensure cor-
rectness, Snort potentially needs to consider all payload
offsets at which content or pcre predicates can suc-
ceed. We term these multiple-match predicates.
In con-
trast, predicates byte test and byte jump are single-
match, meaning that any distinct predicate invocation eval-
uates the payload once.

In the presence of a multiple-match predicate P, Snort
must also retry all subsequent predicates that either directly
or indirectly depend on the match position of P. For ex-
ample, consider matching the rule in Figure 1 against the
payload in Figure 2. The caret (ˆ) in P2 indicates that P2
must ﬁnd a match in the payload immediately after the pre-
vious predicate’s match position.
If Snort considers only
P1’s ﬁrst match at offset 4, then P2 will fail since P2 is
looking for mp3 or ogg but ﬁnds aac instead. However,
if Snort also considers P1’s second match at offset 28, P2
will succeed and further predicates from the rule will be
evaluated. Snort explores possible matches by backtracking
until either it ﬁnds a set of matches for all predicates or it
determines that such a set does not exist.

Figure 3 presents a simpliﬁed version of the algorithm
used by Snort to match rules against packets.2 All predi-
cates support three operations. When a predicate is evalu-
ated, the algorithm calls getNewInstance to do the re-
quired initializations. The previous match’s offset is passed
to this function. The getNextMatch function checks
whether the predicate can be satisﬁed, and it sets the offset
of the match returned by calls to the getMatchOffset
predicate. Further invocations of getNextMatch return
true as long as more matches are found. For each of these
matches, all subsequent predicates are re-evaluated, be-
cause their outcome can depend on the offset of the match.
The rule matching stops when the last predicate succeeds,
or when all possible matches of the predicates have been ex-
plored. Figure 2 shows the stack at each stage of the algo-

2The Snort implementation uses tail calls and loops to link predicate
functions together and to perform the functionality described in Figure 3.
The algorithm presented here describes the behavior that is distributed
throughout these functions.

Payload
Offset

fmt=aac player=play 000 fmt=mp3 rate=14kbps player=cmd.exe?overflow
01234567890123456789012345678901234567890123456789012345678901234567

1

2

3

4

5

6

(P2, 4, f)
(P1, 0, 4) (P1, 0, 4)

(P2,28,31)
(P1, 0,28)
Figure 2. Packet payload matching the rule in Figure 1 and corresponding stack trace after each call
to getNextMatch on line 3 of Figure 3.

(P1, 0,28)

(P3,31,51)
(P2,28,31)
(P1, 0,28)

(P4,51,59)
(P3,31,51)
(P2,28,31)
(P1, 0,28)

(P5,59,67)
(P4,51,59)
(P3,31,51)
(P2,28,31)
(P1, 0,28)

MatchRule(P reds):
Stack ← (P reds[0].getNewInstance(0));
while Stack.size > 0 do
if Stack.top.getNextMatch() then
if Stack.size == P reds.size then return T rue;
ofst ← Stack .top.getMatchOffset();
Push(Stack, P reds[Stack.size].getNewInstance(ofst));
else Pop(Stack);
return F alse;

1
2
3
4
5
6

7

8

Figure 3. Rule matching in Snort. The algo-
rithm returns T rue only if all predicates suc-
ceed.

rithm. Each stack record contains three elements: the pred-
icate identiﬁer, the offset passed to getNewInstance
at record creation, and the offset of the match found by
getNextMatch (f if no match is found). In this exam-
ple, the algorithm concludes that the rule matches.

4. NIDS evasion via backtracking

The use of backtracking to cover all possible string or
regular expression matches exposes a matching algorithm
to severe denial of service attacks. By carefully crafting
packets sent to a host on a network that the NIDS is mon-
itoring, an attacker can trigger worst-case backtracking be-
havior that forces a NIDS to spend seconds trying to match
the targeted rule against the packet before eventually con-
cluding that the packet does not match. For the rule from
Figure 1, P2 will be evaluated for every occurrence of the
string fmt= in the packet payload. Furthermore, when-
ever this string is followed by mp3, P2 will succeed and
the matcher will evaluate P3, and if P3 succeeds it will
evaluate P4. If fmt=mp3 appears n1 times, P3 is evalu-
ated n1 times. If there are n2 occurrences of player=, P4
will be evaluated n2 times for each evaluation of P3, which
gives us a total of n1 · n2 evaluations for P4. Similarly, if
these occurrences are followed by n3 repetitions of .exe
or .com, P5 is evaluated n1 · n2 · n3 times. Figure 4 shows
a packet that has n1 = n2 = n3 = 3 repetitions. Figure 5
shows the evaluation tree representing the predicates eval-

uated by the algorithm as it explores all possible matches
when matching Figure 1 against the payloads in Figure 2
and in Figure 4. Our experiments show that with packets
constructed in this manner, it is possible to force the algo-
rithm to evaluate some predicates hundreds of millions of
times while matching a single rule against a single packet.

The amount of processing a backtracking attack can
cause depends strongly on the rule. Let n be the size of
a packet in bytes. If the rule has k unconstrained multiple-
match predicates that perform O(n) work in the worst case,
an attacker can force a rule-matching algorithm to perform
O(nk) work. Thus the following three factors determine the
power of a backtracking attack against a rule.

1. The number of backtracking-causing multiple-match
content and pcre predicates k. The rule from
Figure 1 has k = 4 because it has 4 backtracking-causing
multiple-match predicates (including P5 which does not
match the attack packet, but still needs to traverse the
packet before failing). Note that not all contents and
pcres can be used to trigger excessive backtracking.
Often, predicates that have constraints on the positions
they match cannot be used by an attacker to cause back-
tracking. An example of such a predicate is the ﬁrst
pcre from Figure 1, predicate P2, which has to match
immediately after the ﬁrst content.

2. The size of the attack packets n. We can use Snort’s re-
assembly module to amplify the effect of backtracking
attacks beyond that of a single maximum sized packet.
The rule from Figure 1 is open to attacks of complexity
O(n4). When Snort combines two attack packets into a
virtual packet and feeds it to the rule-matching engine, n
doubles, and the rule-matcher does 16 times more work
than for either packet alone.

3. The total length of the strings needed to match the k
predicates. If these strings are short, the attacker can re-
peat them many times in a single packet. This inﬂuences
the constants hidden by the O-notation. Let s1,. . . ,sk be
the lengths of the strings that can cause matches for the
k predicates. If we make their contribution to the pro-
cessing time explicit we can compute for each string the
exact number of repetitions. If we divide the packet into

Payload
Offset

fmt=mp3fmt=mp3fmt=mp3player=player=player=.exe.exe.exe
0123456789012345678901234567890123456789012345678901234

1

2

3

4

5

Figure 4. A packet payload that causes rule matching to backtrack excessively.

P1

4

P2

28

P2

31

P3

51

P4

59

46

4

P2

7

P3

P1

11

P2

14

P3

18

P2

21

P3

28

35

42

28

35

42

28

35

42

P4

50

54

46

P4

50

54

46

P4

50

54

46

P4

50

54

46

P4

50

54

46

P4

50

54

46

P4

50

54

46

P4

50

P4

54

46

54

50

P5

P5

P5

P5

P5

P5

P5

P5

P5

P5

P5

P5

P5

P5

P5

P5

P5

P5

P5

P5

P5

P5

P5

P5

P5

P5

P5

P5

67

Match found!

Figure 5. Predicate evaluation trees in Snort. The left tree represents the 6 predicate evaluations
performed on the payload in Figure 2, and the right tree shows the 43 evaluations performed for the
payload in Figure 4. Numbers on edges indicate payload offsets where a predicate matched.

(cid:1)k

(cid:1)k

i=1 ni) = O(nk/(kk

k equal-sized portions, each ﬁlled with repetitions of one
of these strings, we obtain ni = (cid:1)(cid:1)n/k(cid:2)/si(cid:2). The cost
of the attack is O(
i=1 si)).
Other factors such as the amount of overlap between
these strings, the length of the strings needed to match
predicates that do not cause backtracking, and the details
of the processing costs of the predicates also inﬂuence
the processing cost. These factors remain hidden by the
constants inside the O-notation.
Approximately 8% of the 3800+ rules in our ruleset were
susceptible to backtracking attacks to some degree. Our fo-
cus is on the most egregious attacks, which typically yielded
slowdowns ranging from three to ﬁve orders of magnitude.
We quantify the strength of these attacks experimentally in
Section 6.

MemoizedMatchRule(P reds):
Stack ← (P reds[0].getNewInstance(0));
M emoizationT able ← ∅;
while Stack.size > 0 do
if Stack.top.getNextMatch() then
if Stack.size == P reds.size then return T rue;
ofst ← Stack .top.getMatchOﬀset();
if (Stack.top, ofst) /∈ MemoizationTable then
M emoizationT able ←
M emoizationT able ∪ {(Stack.top, ofst)};
Push(Stack, P reds[Stack.size].getNewInstance(ofst));

1
2
3
4
5
6
7
8

9

10

11

else Pop(Stack);
return F alse;

Figure 6. The memoization-enhanced rule-
matching algorithm. Lines 2, 7, and 8 have
been added.

5. Memoization, a remedy for backtracking

As illustrated above, rule-matching engines are open to
backtracking attacks if they retain no memory of inter-
mediate results, which for Snort are predicate evaluations
that have already been determined to fail. Thus, match-
ing engines can be forced to unnecessarily evaluate the
same doomed-for-failure predicates over and over again, as
Figure 5 indicates.

Figure 6 shows our revised algorithm for rule matching
that uses memoization [7,16]. It is based on the observation
that the outcome of evaluating a sequence of predicates de-
pends only on the payload and the offset at which process-
ing starts. The memoization table holds (predicate, oﬀset )
pairs indicating for all predicates, except the ﬁrst, the offsets
at which they have been evaluated thus far. Before evaluat-

ing a predicate, the algorithm checks whether it has already
been evaluated at the given offset (line 7). If the predicate
has been evaluated before, it must have ultimately led to
failure, so it is not evaluated again unnecessarily. Other-
wise, the (predicate, oﬀset) pair is added to the memoiza-
tion table (line 8) and the predicate is evaluated (line 9).
Note that memoization ensures that no predicate is evalu-
ated more than n times. Thus, if a rule has k(cid:1) predicates per-
forming work at most linear in the packet size n, memoiza-
tion ensures that the amount of work performed by the rule
matching algorithm is at most O(k(cid:1) · n · n) = O(k(cid:1)n2). Fig-
ure 7 updates Figure 5 to reﬂect the effects of memoization.
The greyed out nodes in the large tree from Figure 7 corre-
spond to the predicates that would not be re-evaluated when
using memoization. For the most damaging backtracking

4

Monotonicity

P2

7

P3

P1

11

P2

14

P3

18

CPS

P2

21

P3

28

35

42

28

35

42

28

35

42

P4

50

46

54

46

P4

50

54

46

P4

50

54

46

P4

50

54

46

P4

50

54

46

P4

50

54

46

P4

50

54

46

P4

50

P4

54

46

54

50

P5

P5

P5

P5

P5

P5

P5

P5

P5

P5

P5

P5

P5

P5

P5

P5

P5

P5

P5

P5

P5

P5

P5

P5

P5

P5

P5

Figure 7. The memoization algorithm performs only 13 predicate evaluations instead of 43 as it
avoids the grayed-out nodes. The CPS optimization reduces the number of predicate evaluations
to 9, and the monotonicity optimization further reduces the evaluations to 5.

attacks against rules in Snort’s default rule set, memoization
can reduce the time spent matching a rule against the packet
by more than four orders of magnitude (with the optimiza-
tions from Section 5.1, more than ﬁve orders of magnitude).
To implement memoization, we used pre-allocated bit-
maps for the memoization table, with a separate bitmap for
each predicate except the ﬁrst. The size of the bitmaps (in
bits) is the same as the size v (in bytes) of the largest virtual
packet. Thus if the largest number of predicates in a rule is
m, the memory cost of memoization is v(m − 1)/8 bytes.
In our experiments, memoization increases the amount of
memory used in Snort by less than 0.1%.

A naive implementation of memoization would need to
initialize these bitmaps for every rule evaluated. We avoid
this cost by creating a small array that holds up to 5 offsets
and an index into the array. When a rule is to be evaluated,
only the index into the array needs to be initialized to 0. If
the number of offsets a predicate is evaluated at exceeds 5,
we switch to a bitmap (and pay the cost of initializing it). It
is extremely rare that packets not speciﬁcally constructed to
trigger backtracking incur the cost of initializing the bitmap.

5.1. Further optimizations

We present three optimizations to the basic memoiza-
tion algorithm: detecting constrained predicate sequences,
monotonicity-aware memoization, and avoiding unneces-
sary memoization after single-match predicates. The ﬁrst
two of these signiﬁcantly reduce worst case processing
time, and all optimizations we use reduce the memory re-
quired to perform memoization. Most importantly, all three
optimizations are sound when appropriately applied; none
of them changes the semantics of rule matching.

Constrained predicate sequences: We use the name
marker for predicates that ignore the value of the offset pa-
rameter. The outcome of a marker and of all predicates
subsequent to the marker are independent of where pred-
icates preceding the marker matched. As a result, mark-
ers break a rule into sequences of predicates that are inde-
pendent of each other. We use the name constrained pred-

icate sequence (CPS) for a sequence of predicates begin-
ning at one marker and ending just before the next marker.
For example, P3 in Figure 1 looks for the string player=
in the entire payload, not just after the offset where the
previous predicate matches because P3 does not have the
relative modiﬁer. Thus the rule can be broken into two
CPSes: P1-P2 and P3-P4-P5.

Instead of invoking the rule-matching algorithm on the
entire rule, we invoke it separately for individual CPSes and
fail whenever we ﬁnd a CPS that cannot be matched against
the packet. The algorithm does not need to backtrack across
CPS boundaries. Less backtracking is performed because
the ﬁrst predicate in each CPS is invoked at most once. For
the example in Figure 7, detecting CPSes causes the algo-
rithm not to revisit P1 and P2 once P2 has matched, thus
reducing the number of predicate invocations from 13 to 9.
Monotone predicates: Some expensive multiple-match
predicates used by Snort have the monotonicity property
which we deﬁne below. For these predicates we use the
more aggressive lowest-offset memoization.
In this opti-
mization, we skip calls to a monotone predicate if it has
previously been evaluated at an offset smaller than the offset
for the current instance. For example, say we ﬁrst evaluate
a monotone content predicate starting at offset 100 that
does not lead to a match of the entire rule. Later we evalu-
ate the same predicate starting at offset 200. The second in-
stance is guaranteed to ﬁnd only matches that have already
been explored by the ﬁrst instance. With basic memoiza-
tion, after each of these matches of the second instance we
check the memoization table and do not evaluate the next
predicate because we know it will lead to failure. But, the
content predicate itself is evaluated unnecessarily. With
monotonicity-aware memoization, we do not even evaluate
the content predicate at offset 200.

The monotonicity property generalizes to some regular
expressions too, and it can be deﬁned formally as follows:
let S1 be the set of matches obtained when predicate p is
evaluated at offset o1, and S2 the matches for starting offset
o2. If for all packets and ∀o1 ≤ o2 we have S2 ⊂ S1, then p
is monotone. In our example from Figure 1, all contents

and pcres are monotone with the exception of the ﬁrst
pcre, P2, because it matches at most once immediately
after the position where the previous predicate matched.

Lowest-offset memoization helps reduce worst case pro-
cessing because for some predicates the number of worst-
case invocations is reduced from O(n) to 1. For the exam-
ple in Figure 7, this optimization would have eliminated the
second and third evaluations for predicates P4, and P5 (and
for P3 also if CPSes are not detected). This further reduces
the number of predicate instances evaluated from 9 to 5.

Unnecessary memoization: Basic memoization guar-
antees that no predicate is evaluated more than n times.
For some rules with single-match predicates we can pro-
vide the same guarantee even if we omit memoizing some
predicates. If we employ memoization before evaluating a
single-match predicate, but not before evaluating its suc-
cessor, we can still guarantee that the successor will not
be evaluated more than n times (at most once for every
evaluation of our single-match predicate). Also, if we have
chains of single-match predicates it is enough to memoize
only before the ﬁrst one to ensure that none is evaluated
more than n times. Thus, our third optimization is not to
perform memoization after single-match predicates, such as
byte test and byte jump (see Table 1), except when
they are followed by a monotone predicate. For our rule set,
this optimization reduces by a factor of two the amount of
memory used for memoization.

6. Experimental results

We performed empirical evaluations with traces and in
a live setting.
In Section 6.1, we present measurements
comparing backtracking attack packets with traces of typ-
ical network trafﬁc. Our results show that three to six or-
ders of magnitude slowdowns achieved with the backtrack-
ing attack are reduced to less than one order of magnitude
slowdown under memoization. In Section 6.2, we show ac-
tual evasion using a non-memoized implementation, and the
resulting recovery with the memoized version.

For our experiments we used the Snort NIDS, version
2.4.3, conﬁgured to use the Aho-Corasick [2] string match-
ing algorithm. Snort is run on a 2.0 GHz Pentium 4 pro-
cessor and is loaded with a total of 3812 rules. We instru-
mented Snort using cycle-accurate Pentium performance
counters. When enabled, instrumentation introduced less
than 2% overhead to the observed quantities of interest. We
found that our measured observations were consistent with
the instrumentation results collected in [4].

6.1. Trace-based results

For benign trafﬁc, we obtained two groups of three traces
each captured on different days at distinct times. The ﬁrst

benign traffic, unmodified Snort
attack traffic, no memoization
attack traffic, w/ memoization+opt

1000000

100000

)
e
l
a
c
s

g
o
l
(

n
w
o
d
w
o
l
S

10000

1000

100

10

1

(IMAP)
1755

(IRC)
1382

(MS-SQL) (NetBIOS)

2003

2403
Targeted Rule ID

(Oracle)
2611

(SMTP)
3682

(SMTP)
2087

Figure 8. Relative processing times for be-
nign and attack trafﬁc, and attack trafﬁc
with memoization. Memoization conﬁnes the
slowdown to less than one order of magni-
tude.

group of traces were captured on the link between a univer-
sity campus and a departmental network with 1,200 desk-
top and laptop computers, a number of high-trafﬁc servers
(web, ftp, ntp), and scientiﬁc computing clusters generating
high volumes of trafﬁc. These traces are 7 minutes long and
range in size from 3.1 GB to just over 8 GB. The second
group of traces were captured in front of a few instructional
laboratories totaling 150 desktop clients. They are also 7
minutes long and range in size from 816 MB to 2.6 GB.

We created attack trafﬁc by generating ﬂows correspond-
ing to several protocols and supplying payloads that are
constructed in a similar manner to the payload construction
outlined in Section 4.

In the trace-based experiments, we fed the benign trafﬁc
and attack trafﬁc traces into Snort and observed the perfor-
mance. We performed these experiments with and without
memoization enabled. Figure 8 shows the slowdowns expe-
rienced due to backtracking attacks targeting several rules
and the corresponding defense rates. It summarizes the in-
formation in Table 2. In each group, the leftmost bar repre-
sents the cost of packet processing for the speciﬁed protocol
relative to 20.6 s/GB, the combined average packet process-
ing rate in all our traces. For Rule 1382 (IRC), the rate is
less than 1, reﬂecting the fact that the average trafﬁc pro-
cessing time for IRC trafﬁc is less than the baseline.

The central bar in each group shows the slowdown ob-
served by packets crafted to target the speciﬁc rules indi-
cated at the base of each group. The attacks result in pro-
cessing times that are typically several orders of magnitude
slower than the baseline, with the most egregious attack
coming in at a factor of 1.5 million times slower. Finally,
in the rightmost bar of each group we see the result of each
attack repeated with the memoization defense deployed. In

Protocol

Rule
ID

Trace
trafﬁc

IMAP
IRC
MS-SQL
NetBIOS
Oracle
SMTP

1755
1382
2003
2403
2611
3682

200.6
14.6
119.3
729.7
110.5
132.8
SMTP 3682, w/o reassembly
132.8

SMTP

2087

Processing time (seconds/gigabyte)
Backtracking attack

Slowdown
w.r.t. avg trafﬁc

Slowdown
w.r.t. same protocol

Original
89,181
1,956,858
18,206
357,777
6,220,768
30,933,874
1,986,624
175,657

Basic Memo. Memo+Opt.
91.9
87.6
140.4
122.0
174.0
126.4
103.1
164.5

1,802
1,170
715
57,173
3,666
2,192
903
5,123

Original

4,329×
94,993×
884×
17,368×
301,979×
1,501,644×
96,438×
8,527×

Original
Memo+Opt
4.46×
444×
4.25× 134,031×
152×
6.82×
490×
5.92×
8.45×
56,296×
6.14× 232,936×
14,960×
5.00×
1,323×
7.99×

Memo+Opt
0.46×
6.00×
1.17×
0.17×
1.57×
0.95×
0.78×
1.24×

Table 2. Strength of the backtracking attack and feasibility of the memoization defense. Columns
7-8 shows the overall slowdown under attack when memoization is not and is used. Columns 9-10
shows similar slowdowns with respect to the same protocol.

most cases, Snort performance when under attack is com-
parable if not better than when not under attack.

Table 2 details the attacks and the defenses quantitatively
for several different protocols. For each attack, Columns 1
and 2 give the protocol and the targeted Rule ID to which
the attack belongs, respectively. Column 3 shows the aver-
age processing time for each protocol. Columns 4 through
6 show the raw processing times for attack packets under an
unmodiﬁed Snort, Snort with basic memoization, and Snort
with fully optimized memoization. Columns 7-8 give over-
all slowdowns and Columns 9-10 supply the slowdowns
on a per-protocol basis. The backtracking attack achieves
slowdowns between 3 and 5 orders of magnitude for rules
from many protocols. When memoization is employed, the
overall slowdown is conﬁned to within one order of mag-
nitude. Per-protocol, memoization conﬁnes most attacks to
within a factor of two of their normal processing time.

Rows 7 and 8 highlight the impact that reassembly has on
the processing time. In this experiment, when reassembly
is performed the size of the virtual packet fed to the rule-
matching engine is only twice the size of a non-reassembled
packet, but the processing time is almost 16× longer.

The effects of the three memoization optimizations can
be seen by comparing Columns 5 and 6 in Table 2. The
strength of the optimizations varies by protocol, ranging
from just under a factor of 10 to just over a factor of 30, ex-
cluding the NetBIOS outlier. In the Snort rule set, NetBIOS
rules contain many predicates that can be decomposed into
constrained predicate sequences. These rules beneﬁt con-
siderably from the optimizations. The accompanying tech-
nical report [21] contains the individual contributions of
each optimization to the reduction in processing time.

Recall that the attacks applied are all low-bandwidth at-
tacks. Even though the overall slowdown rate using mem-
oization is up to an order of magnitude slower, these rates
apply only to the attack packets (which are few in number)
and not to the overall performance of Snort. Under mem-
oization, processing times for attack packets fall within the
normal variation exhibited by benign packets.

In the rightmost column, slowdowns less than 1.0 indi-
cate that with all the optimizations included, Snort was able
to process backtracking attack packets more quickly than it
could process legitimate trafﬁc. In other words, our opti-
mizations allowed Snort to reject these attack packets more
quickly than it otherwise was able since fewer overall pred-
icate evaluations are performed.

6.2. Evading a live Snort

In this section we demonstrate the efﬁcacy of the back-
tracking attack by applying it to a live Snort installation. We
ﬁrst show successful evasion by applying the attack under a
variety of conditions. We then show that with memoization,
all the formerly undetected attacks are observed.

Figure 9 shows the topology used for testing evasion for
this experiment. To induce denial of service in Snort, we use
an SMTP backtracking attack that connects to a Sendmail
SMTP server in the protected network. We are using this
attack to mask a Nimda [1] exploit normally recognized by
Snort. Both the Nimda exploit and its SMTP cover are sent
from the same attacking computer. Each Nimda exploit is
sent one byte at a time in packets spaced 1 second apart. To
simulate real world conditions, we used the Harpoon trafﬁc
generator [23] to continuously generate background trafﬁc
at 10 Mbps during the experiments.

We measure the effectiveness of the backtracking attack
by the number of malicious exploits that can slip by Snort
undetected over various time frames. We initiated a new
Nimda exploit attempt every second for 5 minutes, yield-
ing 300 overlapping intrusion attempts. Table 3 shows the
results. Test 1 is the control: when the backtracking ex-
ploit is not performed, Snort recognizes and reports all 300
exploits despite our fragmenting them. In Test 2, we sent
two backtracking attack packets every 60 seconds for the
duration of the experiment. Snort missed only one-third of
the attacks, detecting 222 out of 300 intrusion attempts. In
Test 3, we increased the frequency of the backtracking at-
tacks to 2 packets every 15 seconds, dropping the detection

External Network

Protected Network

Attacker

Background
Traffic
Generator

(cid:1)(cid:1)(cid:1)
(cid:0)(cid:0)(cid:0)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)

(cid:1)(cid:1)(cid:1)
(cid:0)(cid:0)(cid:0)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:1)(cid:1)(cid:1)
(cid:0)(cid:0)(cid:0)
(cid:0)
(cid:0)
(cid:1)
(cid:1)

Snort 2.4.3

2 GHz P4

Hub

Fast Eth Hub

Victim

HTTP Server
Sendmail Server

Background
Traffic
Generator

(cid:1)(cid:1)(cid:1)
(cid:0)(cid:0)(cid:0)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)

(cid:1)(cid:1)(cid:1)
(cid:0)(cid:0)(cid:0)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)

Figure 9. Live Snort evasion environment.
Snort monitors a network composed of web
and mail servers.

rate to just 2% of the transmitted exploits. Test 4 decreased
the detection rate even further, and in Tests 5 and 6 the at-
tacker successfully transmitted all 300 exploits without de-
tection. Aside from high CPU utilization during the attacks
and an occasional, sporadic port scan warning directed at
the SMTP attack, Snort gave no indication of any abnormal
activity or intrusion attempt.

These experiments show that

the transmission rate
needed to successfully penetrate a network undetected is
quite low, with both tests 5 and 6 requiring no more than
4.0 kbps of bandwidth. Test 5, in particular, suggests that
perpetual evasion can be achieved through regular, repeated
transmissions of backtracking attack packets.

Tests 7 and 8 demonstrate the effectiveness of memoiza-
tion. These tests repeat Tests 5 and 6 with memoization
enabled (including all optimizations). With memoization,
Snort successfully detected all intrusions in both tests.

In summary, these experiments validate the results of our
trace-based experiments and illustrate the real-world appli-
cability of the backtracking attack. Using carefully crafted
and timed packets, we can perpetually disable an IPS with-
out triggering any alarms, using at most 4 kilobits per sec-
ond of trafﬁc. Correspondingly, the memoization defense
can effectively be used to counter such attacks.

7. Discussion

Often, algorithmic complexity attacks and their solutions
seem obvious once they have been properly described. Nev-
ertheless, software is still written that is vulnerable to such
attacks, which begs the question–how can a NIDS or IPS
designer defend against complexity attacks that she has not
yet seen? A possible ﬁrst step is to explicitly consider
worst-case performance in critical algorithms and to look
at whether it is signiﬁcantly slower than average case and
can be exploited. For example, [9] has shown that in the
Bro NIDS, failure to consider worst-case time complexity
of hash functions leads to denial of service. With this mind-
set, we brieﬂy consider mechanisms employed by existing

Test

1
2
3
4
5
6
7

8

Description of backtrack
attack
Control; no attack
two packets every 60 sec.
two packets every 15 sec.
one packet every 5 sec.
one packet every 3 sec.
twenty packets initially
one packet every 3 sec.

(memoization enabled)

Exploits
detected
300/300
220/300
6/300
4/300
0/300
0/300
300/300

Required
rate (kbps)
N/A
0.4
1.6
2.4
4.0
0.8
N/A

twenty packets initially

300/300

N/A

(memoization enabled)

Table 3. Summary of
live Snort experi-
ments. Without memoization, 300 intru-
sions pass into the network undetected.

NIDS with an eye towards triggering the worst case.

• Deterministic ﬁnite automata (DFA) systems can experi-
ence exponential memory requirements when DFA’s cor-
responding to individual rules are combined.
In some
cases, automata are built incrementally [22] to reduce the
footprint of a DFA that cannot otherwise ﬁt in memory.
Because each byte of trafﬁc is examined exactly once in
a DFA, backtracking does not occur. However, it may be
possible for an adversary to construct packets that trig-
ger incremental state creation on each byte of payload,
resulting in consistently increased computation costs and
potentially leading to memory exhaustion.

• Nondeterministic ﬁnite atomata (NFA) systems reduce
the memory requirement costs of DFA systems by al-
lowing the matcher to be in multiple states concurrently.
In practice, this is achieved either through backtrack-
ing or by explicitly maintaining and updating multiple
states. In the ﬁrst case, algorithmic complexity attacks
are achieved by triggering excessive backtracking. In the
second, the attacker tries to force the NIDS to update
several states for each byte processed.

• Predicate-based systems such as Snort can be slowed
down if the attacker can cause more predicates to be eval-
uated than in the average case. We have presented an
attack that forces the repeated evaluation of a few predi-
cates many times. In contrast, attacks can be devised that
seek to evaluate many predicates a few times. For exam-
ple, Snort employs a multi-pattern string matcher [2] as
a pre-ﬁlter to pare down the rules to be matched for each
packet. Constructing payloads that trigger large numbers
of rules can lead to excessive predicate evaluations.

We have performed preliminary work that combines the
second and third observations above to yield packet pro-
cessing times in Snort that are up to 1000 times slower than
average. These results, combined with those of this pa-
per, suggest that left unaddressed, algorithmic complexity
attacks can pose signiﬁcant security risks to NIDS.

8. Conclusions and future work

Algorithmic complexity attacks are effective when they
trigger worst-case behavior that far exceeds average-case
behavior. We have described a new algorithmic complex-
ity attack, the backtracking attack, that exploits rule match-
ing algorithms of NIDS to achieve slowdowns of up to six
orders of magnitude. When faced with these attacks, a real-
time NIDS becomes unable to keep up with incoming traf-
ﬁc, and evasion ensues. We tested this attack on a live Snort
installation and showed that the protected network is vul-
nerable under this attack, along with the tens of thousands
of other networks protected by Snort.

To counter this attack, we have developed a semantics-
preserving defense based on the principle of memoization
that brings Snort performance on attack packets to within
an order of magnitude of benign packets. Our solution con-
tinues the trend of providing algorithmic solutions to algo-
rithmic complexity attacks.

In general, it is not clear how to ﬁnd and root out all
sources of algorithmic complexity attacks. To do so re-
quires knowledge of average- and worst-case processing
costs. Without a formal model of computation, such knowl-
edge is difﬁcult to obtain and is often acquired in an ad-
hoc manner. Mechanisms for formally characterizing and
identifying algorithms and data structures that are subject
to complexity attacks can serve as useful analysis tools for
developers of critical systems, such as NIDS. We are cur-
rently exploring these issues.

References

[1] Cert

advisory

ca-2001-26

nimda worm,

2001.

http://www.cert.org /advisories
/CA-2001-26.html.

[2] A. V. Aho and M. J. Corasick. Efﬁcient string matching: An
aid to bibliographic search. In Communications of the ACM,
June 1975.

[3] M. Attig and J. W. Lockwood. SIFT: Snort intrusion ﬁlter

for TCP. In Hot Interconnects, Aug. 2005.

[4] J. B. Cabrera, J. Gosar, W. Lee, and R. K. Mehra. On the
statistical distribution of processing times in network intru-
sion detection. In 43rd IEEE Conference on Decision and
Control, Dec. 2004.

Scalable pattern
[5] C. R. Clark and D. E. Schimmel.
In IEEE Sympo-
matching for high-speed networks.
sium on Field-Programmable Custom Computing Machines
(FCCM), pages 249–257, Napa, California, Apr. 2004.
[6] The Snort network intrusion detection system on the intel
ixp2400 network processor. Consystant White Paper, 2003.
[7] T. H. Cormen, C. E. Leiserson, and R. L. Rivest. Introduc-

tion to Algorithms. MIT Press/McGraw-Hill, 1990.

[8] S. Crosby. Denial of service through regular expressions. In

Usenix Security work in progress report, Aug. 2003.

[9] S. A. Crosby and D. S. Wallach. Denial of service via algo-
rithmic complexity attacks. In Usenix Security, Aug. 2003.
[10] M. Handley, V. Paxson, and C. Kreibich. Network intru-
sion detection: Evasion, trafﬁc normalization, and end-to-
end protocol semantics. In Usenix Security, Aug. 2001.
[11] C. Kruegel, F. Valeur, G. Vigna, and R. Kemmerer. Stateful
Intrusion Detection for High-Speed Networks. In Proceed-
ings of the IEEE Symposium on Security and Privacy, pages
285–293, Oakland, CA, May 2002. IEEE Press.

[12] W. Lee, J. B. D. Cabrera, A. Thomas, N. Balwalli, S. Saluja,
and Y. Zhang. Performance adaptation in real-time intrusion
detection systems. In RAID, Zurich, Switzerland, Oct. 2002.
[13] V. Paxson. Bro: a system for detecting network intruders in
real-time. In Computer Networks (Amsterdam, Netherlands:
1999), volume 31, pages 2435–2463, 1999.

[14] PCRE: The perl compatible regular expression library.

http://www.pcre.org.
[15] T. H. Ptacek and T. N. Newsham.

Insertion, evasion and
denial of service: Eluding network intrusion detection. In
Secure Networks, Inc., Jan. 1998.

[16] T. Reps.

“Maximal-munch” tokenization in linear time.
ACM Transactions on Programming Languages and Sys-
tems, 20(2):259–273, 1998.

[17] M. Roesch. Snort - lightweight intrusion detection for net-
works. In Proceedings of the 13th Systems Administration
Conference. USENIX, 1999.

[18] S. Rubin, S. Jha, and B. P. Miller. Automatic generation
and analysis of NIDS attacks. In ACSAC ’04, pages 28–38,
Washington, DC, USA, Dec. 2004. IEEE Computer Society.
[19] U. Shankar and V. Paxson. Active mapping: resisting NIDS
evasion without altering trafﬁc. In IEEE Symposium on Se-
curity and Privacy, pages 44–61, May 2003.

[20] R. Sidhu and V. Prasanna. Fast regular expression matching

using FPGAs, 2001.

[21] R. Smith, C. Estan, and S. Jha. Algorithmic complexity at-
tacks against Snort. University of Wisconsin Technical Re-
port 1561, Sept. 2006.

[22] R. Sommer and V. Paxson. Enhancing byte-level network
intrusion detection signatures with context. In ACM CCS,
Washington, DC, Oct. 2003.

[23] J. Sommers and P. Barford. Self-conﬁguring network trafﬁc
generation. In Internet Measurement Conference, pages 68–
81, 2004.

[24] I. Sourdis and D. Pnevmatikatos. Fast, large-scale string
match for a 10gbps FPGA-based network intrusion detec-
In International Conference on Field Pro-
tion system.
grammable Logic and Applications, Sept. 2003.

[25] L. Tan and T. Sherwood. A high throughput string matching
architecture for intrusion detection and prevention. In Inter-
national Symposium on Computer Architecture ISCA, June
2005.

[26] T. Vermeiren, E. Borghs, and B. Haagdorens. Evaluation of
software techniques for parallel packet processing on multi-
core processors. In IEEE Consumer Communications and
Networking Conference, Jan. 2004.



=== Content from www.acsac.org_628295c3_20250124_184322.html ===

# Advance Program

## Twenty-Second Annual Computer Security Applications Conference (ACSAC)

***Practical Solutions To Real World Security Problems***![acsac06-logo-djt.gif](acsac06-logo-djt.gif)**December 11-15, 2006**

 **Miami Beach Resort and Spa**

 **Miami Beach, FL, USA*****Presented by***
![acsalogonew-lite.gif](acsalogonew-lite.gif)

---

---

* [Conference and Registration Information](#Conference_and_Registration_Info)
  + [Invitation to ACSAC 22](#Invitation_to_ACSAC_22)
  + [Welcome Reception](#Welcome_Reception)
  + [Important Dates to Remember](#Important_Dates_to_Remember)
* [Conference Location](#Conference_Location)
  + [Hotel Information](#Hotel_Information)
  + [Transportation to Miami Beach](#Transportation_to_Miami_Beach)
  + [Meals and Special Diet Requests](#Meals_and_Special_Diet_Requests)
  + [Special Instructions for Foreign Visitors](#Special_Instructions_for_Foreign)
* [Conference At-A-Glance](#Conference_At_A_Glance)
* [Technical Program](#Technical_Program)
  + [Tuesday, December 12, 2006, 8:30-10:00](#Tuesday_December_12_2006_8_30_10)
  + [Tuesday, December 12, 2006, 10:30-12:00](#Tuesday_December_12_2006_10_30_1)
  + [Tuesday, December 12, 2006, 13:30-15:00](#Tuesday_December_12_2006_13_30_1)
  + [Tuesday, December 12, 2006, 15:30-17:00](#Tuesday_December_12_2006_15_30_1)
  + [Wednesday, December 13, 2006, 8:30-10:00](#Wednesday_December_13_2006_8_30)
  + [Wednesday, December 13, 2006, 10:30-12:00](#Wednesday_December_13_2006_10_30)
  + [Wednesday, December 13, 2006, 13:30-15:00](#Wednesday_December_13_2006_13_30)
  + [Wednesday, December 13, 2006, 15:30-17:00](#Wednesday_December_13_2006_15_30)
  + [Thursday, December 14, 2006, 8:30-10:00](#Thursday_December_14_2006_8_30_1)
  + [Thursday, December 14, 2006, 10:30-12:00](#Thursday_December_14_2006_10_30)
  + [Thursday, December 14, 2006, 13:30-15:00](#Thursday_December_14_2006_13_30)
  + [Thursday, December 14, 2006, 15:30-17:00](#Thursday_December_14_2006_15_30)
* [Workshop](#Workshop)
* [Tutorials](#Tutorials)
  + [Tutorial M1](#Tutorial_M1)
  + [Tutorial M2](#Tutorial_M2)
  + [Tutorial M3](#Tutorial_M3)
  + [Tutorial M4](#Tutorial_M4)
  + [Tutorial F5](#Tutorial_F5)
  + [Tutorial F6](#Tutorial_F6)
  + [Tutorial F7](#Tutorial_F7)
* [Awards and Opportunities](#Awards_and_Opportunities)
  + [Best Paper Award](#Best_Paper_Award)
  + [Best Student Paper Award](#Best_Student_Paper_Award)
  + [ACSA Conferenceship Program](#ACSA_Conferenceship_Program)
  + [Works in Progress (WiP) Session](#Works_in_Progress_WiP_Session)
* [Conference Committee](#Conference_Committee)
* [Program Committee](#Program_Committee)
* [ACSAC Steering Committee](#ACSAC_Steering_Committee)
* [ACSA](#ACSA)
* [About the Sponsor](#About_the_Sponsor)

---

---

# Conference and Registration Information

## Invitation to ACSAC 22

I would personally like to invite you to attend the 22nd Annual Computer Security Applications Conference (ACSAC). This year the conference travels south to Miami Beach, FL! Though the venue is new, we will continue the format established last year: the Technical Program consisting of refereed papers and panels will be presented Tuesday through Thursday, while a workshop will be held on Monday, and full and half-day tutorials will be offered both Monday and Friday. This advance program provides the details you will need to register, travel and attend, but please also visit <http://www.acsac.org>, where you can find many useful resources, including full archives of past ACSACs.We hope to see you in Miami Beach this December!Dan Thomsen, Conference Chair
## Welcome Reception

Please join us on Monday evening (Dec.11, 2006) at 6:00 pm for hors d'oeuvres and drinks and an opportunity to meet others in the security community. This is also a good opportunity for session chairs to meet their speakers!
## Important Dates to Remember

**November 13, 2006:** Last day to reserve a room at the conference hotel at the conference rate.**November 13, 2006:** Last day for early/reduced conference registration fee.**November 13, 2006:** Last day to cancel your conference registration and obtain a refund less a service charge of $25.00. Cancellations must be in writing. See the Registration Form for complete details.

---

---

# Conference Location

## Hotel Information

The conference will be held at the Miami Beach Resort and Spa (<http://www.miamibeachresortandspa.com>).
### Reservations

REGISTER EARLY! ACSAC has reserved a block of rooms at
Group Room Rates until November 13.
The Single/double price is $108.00/night plus tax.
(Prevailing Government per Diem).All reservations must be made directly with Miami Beach Resort and Spa.Online Instructions

1. Go to www.miamibeachresortandspa.com
2. Click on Reservations
3. At the bottom of the screen, click on the Group key
4. Enter Attendee Code CA7460121

Hotel reservations: +1-866-765-9090.Please be sure to use the Attendee Code to associate your reservation with the ACSAC conference.
This makes the special negotiated room rates available to you, gives the conference a credit which helps to lower the regisration fees.The room rate is available three days before and after the conference if you would like to stay over one or both weekends.**Cutoff Date:**
To qualify for the negotiated rates, hotel reservations by attendees must be received on or before
Monday, November 13, 2006.
### Directions to the Conference Hotel

* From Miami International Airport:
  + Exit Airport and follow the sign that says Lejune Road North
  + From Lejune Road North take the 112 East-Miami Beach (second lane from right)
  + 112 East Miami Beach after the toll will become I-195 - Miami Beach
  + Once you are on I-195 go straight (always on the left lane) and after you pass the long bridge you will be on Arthur Godfrey Rd (also 41st Street)
  + Keep going until you see Indian Creek Drive, there youll make a left
  + Indian Creek will merge with Collins Avenue  proceed to 4833 Collins Avenue

* From Ft. Lauderdale / Hollywood International Airport:
  + Exit Airport to I-95 South
  + Take I-95 to I-195 Miami Beach
  + Once you are on I-195 go straight (always on the left lane) and after you pass the long bridge you will be on Arthur Godfrey Rd (also 41st Street)
  + Keep going until you see Indian Creek Drive, there youll make a left
  + Indian Creek will merge with Collins Avenue  proceed to 4833 Collins Avenue

## Transportation to Miami Beach

Miami International Airport (www.miami-airport.com) is the closest airport (approximately
10 miles west of Miami Beach). For information
on taxis and shuttle services:
<http://www.miami-airport.com/html/taxi_and_shuttle_service.html>Fort Lauderdale airport www.broward.org/airport is also convenient. It's
approximately 30 minutes north of Miami Beach.
Information on public transportation:
<http://www.miamidade.gov/transit/>Official Miami Beach website:
<http://www.visitmiamibeach.us/>
## Meals and Special Diet Requests

The Conference Committee has selected lunch menus that we hope
everyone will enjoy. We realize that some individuals have
special dietary needs. We have made arrangements to offer a
vegetarian meal at lunch that will feature some combination of
pasta, vegetables, and/or fruits. Please indicate your dietary
request on the registration form and upon your arrival, please
check your registration packet to ensure that your lunch tickets
indicate your dietary request. If there are problems, please
contact the conference registration desk.
## Special Instructions for Foreign Visitors

If you are traveling from outside the United states, you may need to obtain a visa. Details on requesting a letter of invitation from ACSAC can be found at [visa request](http://www.acsac.org/visa.html).

---

---

# Conference At-A-Glance

| **Monday, December 11, 2006** | | | |
| --- | --- | --- | --- |
| 8:30-17:00 | [Workshop: Host Based Security Assessment](#WorkshopInfo) | | |
| 8:30-12:00 | [Tutorial M1](#TutorialM1) | [Tutorial M3](#TutorialM3) | [Tutorial M4](#TutorialM4) |
| 13:30-17:00 | [Tutorial M2](#TutorialM2) |
| **Tuesday, December 12, 2006** | | | |
| **Time** | **Track 1** | **Track 2** | **Track 3** |
| 8:30-10:00 | [Distinguished Practitioner: Dixie Baker, SAIC](#DistinguishedPractitionerPlenary) | | |
| 10:30-12:00 | [Applied Distributed Collaboration](#TechnicalSession01) | [Client Access in Untrusted Environments](#TechnicalSession02) | [Vulnerability Management](#CaseStudySession01) |
| 13:30-15:00 | [Network Intrusion Detection](#TechnicalSession03) | [Panel: Challenges for Web Services Security](#PanelSession01) | [Personal Identification Verification](#CaseStudySession02) |
| 15:30-17:00 | [Network Security](#TechnicalSession04) | [Security in Systems](#TechnicalSession05) | [Case Studies](#CaseStudySession03) |
| **Wednesday, December 13, 2006** | | | |
| **Time** | **Track 1** | **Track 2** | **Track 3** |
| 8:30-10:00 | [Invited Essayist: Brian Witten, Symantec Corporation](#InvitedEssayistPlenary) | | |
| 10:30-12:00 | [Applied Sandboxing](#TechnicalSession06) | [Malware](#TechnicalSession07) | [Case Studies](#CaseStudySession04) |
| 13:30-15:00 | [Applied Detection Techniques](#TechnicalSession08) | [Panel: Partnering with Industry and Academia - The DHS S&T Approach to Cyber Security Research, Development, Test, and Evaluation](#PanelSession02) | [Industrial Control System Security](#CaseStudySession05) |
| 15:30-17:30 | [Works in Progress](#WorksinProgressPlenary) | | [Case Studies](#CaseStudySession06) |
| **Thursday, December 14, 2006** | | | |
| **Time** | **Track 1** | **Track 2** | **Track 3** |
| 8:30-10:00 | [Classic Papers: Peter G. Neumann, SRI International  Jeremy Epstein, webMethods Inc.](#ClassicPapersPlenary) | | |
| 10:30-12:00 | [Applied Randomization](#TechnicalSession09) | [Intrusion Detection](#TechnicalSession10) | [Case Studies](#CaseStudySession07) |
| 13:30-15:00 | [Messaging Security](#TechnicalSession11) | [Countermeasures](#TechnicalSession12) | [Certification and Accreditation](#CaseStudySession08) |
| 15:30-17:30 | [Information Flow and Leakage](#TechnicalSession13) | [Panel: Highlights from the 2006 New Security Paradigms Workshop](#PanelSession03) | [Minimum Security Requirements](#CaseStudySession09) |
| **Friday, December 15, 2006** | | | |
| 8:30-12:00 | [Tutorial F5](#TutorialF5) | [Tutorial F6](#TutorialF6) | [Tutorial F7](#TutorialF7) |
| 13:30-17:00 |

---

---

# Technical Program

## Tuesday, December 12, 2006, 8:30-10:00

### Opening Plenary

***Introductory remarks:***

 Dan Thomsen, Cyber Defense Agency, LLC, Conference Chair

 Christoph Schuba, Linköpings University, Program Chair
***Introduction of the Distinguished Practitioner***

 Marshall Abrams, The MITRE Corporation***Distinguished Practitioner:***

 *Privacy and Security in Public Health: Maintaining the Delicate Balance between Personal Privacy and Population Safety.*

    Dr. Dixie Baker, SAICAmidst threats of pandemic influenza and bioterrorist attack, the
importance of public health surveillance and preparedness has never been
more important. Early detection of biological events, electronic reporting
of laboratory test results, efficient exchange of case reports across
jurisdictions, and timely alerting of health threats are critical components
of effective health protection. Equally important to public health
surveillance, preparedness, and response is the timely availability of
information relating to individuals' healthcare behaviors and clinical
conditions - posing a threat to personal privacy. Public health is
challenged to maintain an optimal balance between protecting the nation's
health and respecting the personal privacy of its citizens.![baker.gif](baker.gif)About the Speaker:Dr. Dixie B. Baker is a Technical Fellow and Vice President for Technology at Science Applications International Corporation (SAIC), where she serves as the Chief Technology Officer (CTO) for the Enterprise and Infrastructure Solutions Group (E&ISG). With a total staff of over 12,000 people, E&ISG leads SAICs business in homeland security, health and life sciences, energy, environment, and enterprise solutions. As CTO, Dr. Baker serves as the Groups principal visionary and spokesperson on science and technology issues, develops partnerships and strategic alliances with technology suppliers, oversees research and development investments, and represents SAIC in national and international forums. In addition, Dr. Baker serves as a senior consultant on projects of strategic importance to the Group and to SAIC.An internationally recognized thought leader in high-assurance architecture and information protection, Dr. Baker has applied her expertise primarily to the health and life sciences for the past ten years. She was the Principal Investigator for the Patient Centered Access to Secure Systems Online (PCASSO) project, a National Library of Medicine sponsored research project that is widely regarded as ground-breaking in providing patients safe and secure Web access to their complete medical records. Her research teams paper won the ACSACs 1997 Best Paper Award. She has provided testimony to the National Committee on Vital and Health Statistics (NCVHS) as input to the development of the Health Insurance Portability and Accountability Act (HIPAA) security standards, and more recently, as guidance toward technology solutions for protecting the confidentiality of health records released to third parties. For the Centers for Disease Control and Prevention (CDC), she defined, and now is helping implement, an architecture that will enable the production, management, distribution, and use of semantically interoperable data-collection instruments for disease surveillance across the U.S.Dr. Baker has published and lectured extensively in a number of technology and health-related areas, including information protection, high-assurance architecture, electronic medical records, and Internet safety. In 2001, she was awarded the John P. McGovern Lectureship in Information and Communications, presented by the Medical Library Association. In September 2004, at the invitation of the Ministry of Health of the Peoples Republic of China, she presented a keynote address and paper at the IDEAS04DH Workshop on Medical Information Systems, held in Beijing.Dr. Baker holds a Ph.D. in Education Research Methodologies and an M.S. in Computer Science from the University of Southern California, as well as M.S. and B.S. degrees from Florida State University and The Ohio State University respectively.

---

## Tuesday, December 12, 2006, 10:30-12:00

### Track 1: Technical Papers

**Title:** Applied Distributed Collaboration

 **Chair:** Christoph Schuba, Linköpings University*Shamon: A System for Distributed Mandatory Access Control*.

Revision: r1.9 - 27 Sep 2006


=== Content from bugs.gentoo.org_31583b68_20250126_025123.html ===

[![Gentoo Websites Logo](extensions/Gentoo/web/gentoo_org.png)](/ "Go to the Gentoo Bugzilla homepage")
Go to:
[Gentoo Home](https://www.gentoo.org/)
[Documentation](https://www.gentoo.org/support/documentation/)
[Forums](https://forums.gentoo.org/)
[Lists](https://www.gentoo.org/get-involved/mailing-lists/)
[Bugs](/)
[Planet](https://planet.gentoo.org/)
[Store](https://www.gentoo.org/inside-gentoo/stores/)
[Wiki](https://wiki.gentoo.org/)
**[Get Gentoo!](https://www.gentoo.org/downloads/)**

Gentoo's Bugzilla – Search by bug number

* [Home](./)
* | [New](enter_bug.cgi?format=guided)–[[Ex]](enter_bug.cgi)
* | [Browse](describecomponents.cgi)
* | [Search](query.cgi)
* | [Privacy Policy](https://wiki.gentoo.org/wiki/Foundation%3APrivacy_Policy)
* |

  [[?]](page.cgi?id=quicksearch.html "Quicksearch Help")
* | [Reports](report.cgi)
* |
  [Requests](request.cgi)
* |
  [New Account](createaccount.cgi)
* |
  [Log In](show_bug.cgi?GoAheadAndLogIn=1)

  [x]
* |
  [Forgot Password](show_bug.cgi?GoAheadAndLogIn=1#forgot)
  Login:

  [x]

You may find a single bug by entering its bug id here:

* + [Home](./)
  + | [New](enter_bug.cgi?format=guided)–[[Ex]](enter_bug.cgi)
  + | [Browse](describecomponents.cgi)
  + | [Search](query.cgi)
  + | [Privacy Policy](https://wiki.gentoo.org/wiki/Foundation%3APrivacy_Policy)
  + |

    [[?]](page.cgi?id=quicksearch.html "Quicksearch Help")
  + | [Reports](report.cgi)
  + |
    [Requests](request.cgi)
  + |
    [New Account](createaccount.cgi)
  + |
    [Log In](show_bug.cgi?GoAheadAndLogIn=1)

    [x]
  + |
    [Forgot Password](show_bug.cgi?GoAheadAndLogIn=1#forgot)
    Login:

    [x]



=== Content from www.acsac.org_ffd83814_20250124_184320.html ===
ACSAC (www.acsac.org): Backtracking Algorithmic Complexity Attacks Against a NIDS

---

***Backtracking Algorithmic Complexity Attacks Against a NIDS***

**Randy Smith
University of Wisconsin--Madison
USA

Cristian Estan
University of Wisconsin--Madison
USA

Somesh Jha
University of Wisconsin--Madison
USA**

Network Intrusion Detection Systems (NIDS) have become crucial to



=== Content from www.mandriva.com_5a0bb8fb_20250124_184335.html ===


* [Tuxedo.org](https://tuxedo.org/ "Go to: Tuxedo")
* Store
* Mandriva Linux
* Enterprise Solutions
* Contact Us
* Language ![](/sites/all/themes/mandriva_customer/images/arrow-grey-bottom.gif)
  + English (International)
  + Français
  + Ð ÑÑÑÐºÐ¸Ð¹

# [Mandriva](/mandriva/)

* Download
* Mandriva Linux
* Community
* Help

## One

### The Linux desktopthat's easy to try and easy to keep

Download
![Mandriva One](/sites/all/themes/mandriva_customer/images/front/packshot_Mandriva_One.png)

## Powerpack

### Mandriva Linux- and more

Download
![Mandriva Linux Powerpack](/sites/all/themes/mandriva_customer/images/front/en-49.png)

## Mandriva Flash

### The mobile and installable Linux desktopon a USB key

Buy on Store
![Mandriva Flash](/sites/all/themes/mandriva_customer/images/front/mandriva-flash2.png)

### [Community](community)

* User Forums
* Support - FAQ
* Free Software
* Become a member

### Enterprise Solutions

* Enterprise Linux Products
* Asset Management Solution
* Professional Services
* Support & Maintenance

* Go to site »

### Mandriva Partners

* Worldwide offices
* Partner Program
* Technology Partners
* OEM
* Classmate PC with Mandriva Linux

### News

* Mandriva presents the launch of its new mobile desktop: Mandriva Flash 2008 Spring
* Speaking about Mandriva Linux 2008 Spring in media
* Speaking about Mandriva Linux 2008 Spring in media
* Mandriva presents its latest distribution: Mandriva Linux 2008 Spring
* Mandriva and Novatice Technologies present Edutice, The ready to use solution dedicated to educationand multimedia spaces
* # [Top 50 Online Casinos UK in 2023](https://montycasinos.com/online-casinos/)

more

* Download
  + Mandriva Linux 2008
  + Writing an ISO image
* Mandriva Linux
  + One
  + Powerpack
  + Mandriva Flash
* Community
  + Start
  + Register
  + Stay informed
  + Participate
  + Free software
* Help
  + Getting help
  + Knowledge Base
  + Documentation
  + Forums
  + Support Requests
  + Customer Care
  + Enterprise Support
* Enterprise Solutions
  + Products
  + Support
  + Services
  + Partners
  + Go to site »

Language
English (International)
Français
Ð ÑÑÑÐºÐ¸Ð¹

* © 2007 Mandriva S.A.
* About Us
* Legal Information
* Privacy Policy
* Contact Us



=== Content from www.acsac.org_593905e5_20250124_184325.html ===
Backtracking Algorithmic Complexity Attacks Against a NIDS

Randy Smith Cristian Estan Somesh Jha
Computer Sciences Department
University of Wisconsin-Madison
{smithr,estan,jha}@cs.wisc.edu

Abstract

Network Intrusion Detection Systems (NIDS) have be-
come crucial to securing modern networks. To be effective,
a NIDS must be able to counter evasion attempts and oper-
ate at or near wire-speed. Failure to do so allows malicious
packets to slip through a NIDS undetected. In this paper, we
explore NIDS evasion through algorithmic complexity at-
tacks. We present a highly effective attack against the Snort
NIDS, and we provide a practical algorithmic solution that
successfully thwarts the attack. This attack exploits the be-
havior of rule matching, yielding inspection times that are
up to 1.5 million times slower than that of benign packets.
Our analysis shows that this attack is applicable to many
rules in Snort’s ruleset, rendering vulnerable the thousands
of networks protected by it. Our countermeasure conﬁnes
the inspection time to within one order of magnitude of be-
nign packets. Experimental results using a live system show
that an attacker needs only 4.0 kbps of bandwidth to perpet-
ually disable an unmodiﬁed NIDS, whereas all intrusions
are detected when our countermeasure is used.

1. Introduction

Network Intrusion Detection Systems (NIDS) and Intru-
sion Prevention Systems (IPS) have become crucial to se-
curing today’s networks. Typically, a NIDS residing on the
edge of a network performs deep packet inspection on every
packet that enters the protected domain. When a packet is
matched against a signature, an alert is raised, indicating an
attempted intrusion or other misuse.

To be effective in an online environment, packet inspec-
tion must be performed at or near wire speed. The con-
sequences of not doing so can be dire: an intrusion detec-
tion system that fails to perform packet inspection at the
required rate will allow packets to enter the network unde-
tected. Worse, an inline intrusion prevention system that
fails to keep up can cause excessive packet loss.

A NIDS must also guard against evasion attempts which

often succeed by exploiting ambiguities in a protocol def-
inition itself. For example, attack mechanisms have relied
on ambiguities in TCP to develop evasion techniques us-
ing overlapping IP fragments, TTL manipulation, and other
transformations [10, 15, 18].

In this paper, we explore NIDS evasion through the use
of algorithmic complexity attacks [9]. Given an algorithm
whose worst-case performance is signiﬁcantly worse than
its average case performance, an algorithmic complexity at-
tack occurs when an attacker is able to trigger worst-case
or near worst-case behavior. To mount evasion attempts in
NIDS, two attack vectors are required. The ﬁrst is the true
attack that targets a host inside the network. The second is
aimed squarely at the NIDS and serves as a cover by slow-
ing it down so that incoming packets (including the true at-
tack) are able to slip through undetected. Evasion is most
successful when the true attack enters the network, and nei-
ther it nor the second attack is detected by the NIDS.

We present an algorithmic complexity attack that ex-
ploits worst-case signature matching behavior in a NIDS.
By carefully constructing packet payloads, our attack forces
the signature matcher to repeatedly backtrack during in-
spection, yielding packet processing rates that are up to 1.5
million times slower than average. We term this type of
algorithmic complexity attack a backtracking attack. Our
experiments show that hundreds of intrusions can success-
fully enter the network undetected during the course of a
backtracking attack against a NIDS. Further, the backtrack-
ing attack itself requires very little bandwidth; i.e., a single
attack packet sent once every three seconds is enough to
perpetually disable a NIDS.

Our countermeasure to the backtracking attack is an al-
gorithmic, semantics-preserving enhancement to signature
matching based on the concept of memoization. The core
idea is straightforward: whereas the backtracking attack ex-
ploits the need of a signature matcher to evaluate signatures
at all successful string match offsets, a memoization table
can be used to store intermediate state that must otherwise
be recomputed. Our defense against the backtracking at-
tack relies on the use of better algorithms that reduce the

disparity between worst and average case without changing
functionality. Empirical results show that this solution con-
ﬁnes the processing times of attack packets to within one
order of magnitude of benign packets.

Our result applies directly to Snort [17], a popular open-
source package that provides both NIDS and IPS function-
ality and claims more than 150,000 active users. Snort uses
a signature-based architecture in which each signature is
composed of a sequence of operations, such as string or
regular expression matching, that together identify a dis-
tinct misuse. In our experiments, we use Snort over both
traces and live trafﬁc. In addition, we provide a practical
implementation of the defense by extending Snort’s signa-
ture matching functionality directly.

In summary, our contributions are two-fold. First, we
discuss NIDS evasion through algorithmic complexity at-
tacks. We present a highly effective real attack, the back-
tracking attack, that yields slowdowns of up to six orders
of magnitude and is feasible against the (estimated) tens
of thousands of networks monitored by Snort. Second, we
present an algorithmic defense, based on the principle of
memoization, that conﬁnes the slowdown to less than one
order of magnitude in general and to less than a factor of
two in most cases. We provide a practical implementation
of this solution and show its efﬁcacy in a live setup.1

We organize the remainder of this paper as follows: Sec-
tion 2 provides a summary of related work, and Section 3
describes the rule-matching architecture of Snort. In Sec-
tions 4 and 5 we present the backtracking attack and the
countermeasure, respectively. Section 6 details our experi-
mental results, and Section 7 considers other types of com-
plexity attacks. Section 8 concludes.

2. Related work

To our knowledge, Crosby and Wallach [8, 9] were the
ﬁrst to provide an algorithmic basis for denial of service
attacks. They exploit weaknesses in hash function imple-
mentations and backtracking behavior in common regular
expression libraries to produce worst-case behavior that is
signiﬁcantly more expensive than the average case. The re-
sult is denial of service in general and evasion in our con-
text. For their examples, the authors observe that algorith-
mic attacks against hash tables and regular expressions can
be thwarted by better algorithm and data structure selec-
tions. Our defense also relies on algorithmic improvements.
The backtracking attack we present falls within the gene-
ral family of algorithmic attacks, although to the best of our
knowledge our method of achieving evasion through back-
tracking is novel.

1We have presented our ﬁndings to the Snort developers, who have con-
ﬁrmed the efﬁcacy of the evasion attack and have integrated the solution
into their NIDS.

In a systems-oriented approach to addressing resource
consumption and other attacks, Lee et al. [12] dynamically
divide the workload among multiple modules, making ad-
justments as necessary to maintain performance. Load-
shedding is performed as necessary to distribute the load
to different modules, or to lower its priority. Alternatively,
Kruegel et al. [11] have proposed achieving high speed in-
trusion detection by distributing the load across several sen-
sors, using a scatterer to distribute the load and slicers and
reassemblers to provide stateful detection. Still other ap-
proaches seek to provide better performance by splitting up
(and possibly replicating) a sensor onto multiple cores or
processors [6, 26]. These approaches show that allocating
more hardware can better protect large networks with large
amounts of trafﬁc, but they are not a cost effective way of
dealing with algorithmic complexity attacks.

The use of custom hardware has also been proposed for
performing high-speed matching [3,5,20,24,25]. The back-
tracking attack is probably not applicable to these solutions.
As our focus is on software-based systems, we do not con-
sider hardware solutions further in this paper.

Both [12] and [13] propose the use of monitors to track
the resource usage and performance history of a NIDS.
In [12], if a monitor discovers abnormally long process-
ing times, the current operations are aborted and optionally
transferred to a lower priority process. For [13], on the other
hand, the monitor simply triggers a restart of the NIDS.
In the general case, such techniques may provide a useful
mechanism for ensuring guaranteed minimum performance
rates at the cost of decreased detection accuracy. However,
such mechanisms result in periodic lapses in detection ca-
pability. Our solution is semantics-preserving, in the sense
that it does not sacriﬁce detection to maintain performance.
Finally, NIDS evasion has been extensively studied in
the literature. The earliest work describing evasion was
presented by Paxson [13] and Ptacek and Newsham [15].
Handley et al. [10] show that normalization combined with
stateful analysis to remove protocol ambiguities can foil
evasion attempts, although it may affect stream semantics.
Shankar and Paxson [19] address semantics by providing
an online database of network attributes, such as the hop
count from the NIDS to a protected host, that provides the
same beneﬁts as normalization without the risk of chang-
ing stream semantics. These solutions are orthogonal to the
problem discussed in this paper.

3. Rule matching in Snort

Our work is performed in the context of the Snort NIDS.
Snort employs a signature-based approach to intrusion de-
tection, deﬁning distinct signatures, or rules, for each mis-
use to be searched for. Each signature is in turn composed
of a sequence of predicates, that describe the operations that

Description

Predicate
content:< str > Searches for occurrence of < str > in payload
pcre:/regex/
byte test
byte jump

Matches regular expression /regex/ against payload
Performs bitwise or logical tests on speciﬁed payload bytes
Jumps to an offset speciﬁed by given payload bytes

Type
multiple-match
multiple-match
single-match
single-match

Table 1. Subset of Snort predicates used for packet inspection. Multiple-match predicates may need
to be applied to a packet several times.

alert tcp $EXT NET any -> $HOME NET 99
(msg:"AudioPlayer jukebox exploit";

content:"fmt=";
pcre:"/ˆ(mp3|ogg)/",relative;
content:"player=";
pcre:"/.exe|.com/",relative;
content:"overflow",relative;
sid:5678)

//P1
//P2
//P3
//P4
//P5

Figure 1. Rule with simpliﬁed Snort syntax
describing a ﬁctional vulnerability.

the signature must perform. Section 3.1 gives an overview
of the language used to specify these rules. Section 3.2 de-
scribes the algorithm used to match rules against packets.

3.1. Expressing rules in Snort

Snort’s rules are composed of a header and a body. The
header speciﬁes the ports and IP addresses to which the rule
should apply and is used during the classiﬁcation stage. The
body has a sequence of predicates that express conditions
that need to succeed for the rule to match. A rule matches a
packet only if all predicates evaluated in sequence succeed.
Of the predicates that are part of Snort’s rule language, we
focus on those used to analyze the packet payloads. Table 1
summarizes the relevant rules.

Figure 1 depicts a signature using a simpliﬁed version of
Snort’s rule language. The header of the rule instructs Snort
to match this signature against all TCP trafﬁc from external
sources to servers in the home network running on port 99.
The body of the rule contains three content predicates,
two pcre [14] predicates, and two terms, msg and sid,
used for notiﬁcation and bookkeeping. The rule matches
packets that contain the string fmt= followed immediately
by mp3 or ogg, and also contain the string player=, fol-
lowed by .exe or .com, followed by overflow.

Predicates have one important side effect: during rule
matching a predicate records the position in the payload at
which it succeeded. Further, when a predicate contains a
relative modiﬁer, that predicate inspects the packet be-
ginning at the position at which the previous predicate suc-
ceeded, rather than the start of the payload. For example,
if predicate P3 from Figure 1 ﬁnds the string player=
at offset i in the payload, the subsequent pcre predicate
(P4) succeeds only if it matches the packet payload after
position i.

3.2. Matching signatures

When matching a rule against a packet, Snort evalu-
ates the predicates in the order they are presented in the
rule, and concludes that the packet does not match the
rule when it reaches a predicate that fails. To ensure cor-
rectness, Snort potentially needs to consider all payload
offsets at which content or pcre predicates can suc-
ceed. We term these multiple-match predicates.
In con-
trast, predicates byte test and byte jump are single-
match, meaning that any distinct predicate invocation eval-
uates the payload once.

In the presence of a multiple-match predicate P, Snort
must also retry all subsequent predicates that either directly
or indirectly depend on the match position of P. For ex-
ample, consider matching the rule in Figure 1 against the
payload in Figure 2. The caret (ˆ) in P2 indicates that P2
must ﬁnd a match in the payload immediately after the pre-
vious predicate’s match position.
If Snort considers only
P1’s ﬁrst match at offset 4, then P2 will fail since P2 is
looking for mp3 or ogg but ﬁnds aac instead. However,
if Snort also considers P1’s second match at offset 28, P2
will succeed and further predicates from the rule will be
evaluated. Snort explores possible matches by backtracking
until either it ﬁnds a set of matches for all predicates or it
determines that such a set does not exist.

Figure 3 presents a simpliﬁed version of the algorithm
used by Snort to match rules against packets.2 All predi-
cates support three operations. When a predicate is evalu-
ated, the algorithm calls getNewInstance to do the re-
quired initializations. The previous match’s offset is passed
to this function. The getNextMatch function checks
whether the predicate can be satisﬁed, and it sets the offset
of the match returned by calls to the getMatchOffset
predicate. Further invocations of getNextMatch return
true as long as more matches are found. For each of these
matches, all subsequent predicates are re-evaluated, be-
cause their outcome can depend on the offset of the match.
The rule matching stops when the last predicate succeeds,
or when all possible matches of the predicates have been ex-
plored. Figure 2 shows the stack at each stage of the algo-

2The Snort implementation uses tail calls and loops to link predicate
functions together and to perform the functionality described in Figure 3.
The algorithm presented here describes the behavior that is distributed
throughout these functions.

Payload
Offset

fmt=aac player=play 000 fmt=mp3 rate=14kbps player=cmd.exe?overflow
01234567890123456789012345678901234567890123456789012345678901234567

1

2

3

4

5

6

(P2, 4, f)
(P1, 0, 4) (P1, 0, 4)

(P2,28,31)
(P1, 0,28)
Figure 2. Packet payload matching the rule in Figure 1 and corresponding stack trace after each call
to getNextMatch on line 3 of Figure 3.

(P1, 0,28)

(P3,31,51)
(P2,28,31)
(P1, 0,28)

(P4,51,59)
(P3,31,51)
(P2,28,31)
(P1, 0,28)

(P5,59,67)
(P4,51,59)
(P3,31,51)
(P2,28,31)
(P1, 0,28)

MatchRule(P reds):
Stack ← (P reds[0].getNewInstance(0));
while Stack.size > 0 do
if Stack.top.getNextMatch() then
if Stack.size == P reds.size then return T rue;
ofst ← Stack .top.getMatchOffset();
Push(Stack, P reds[Stack.size].getNewInstance(ofst));
else Pop(Stack);
return F alse;

1
2
3
4
5
6

7

8

Figure 3. Rule matching in Snort. The algo-
rithm returns T rue only if all predicates suc-
ceed.

rithm. Each stack record contains three elements: the pred-
icate identiﬁer, the offset passed to getNewInstance
at record creation, and the offset of the match found by
getNextMatch (f if no match is found). In this exam-
ple, the algorithm concludes that the rule matches.

4. NIDS evasion via backtracking

The use of backtracking to cover all possible string or
regular expression matches exposes a matching algorithm
to severe denial of service attacks. By carefully crafting
packets sent to a host on a network that the NIDS is mon-
itoring, an attacker can trigger worst-case backtracking be-
havior that forces a NIDS to spend seconds trying to match
the targeted rule against the packet before eventually con-
cluding that the packet does not match. For the rule from
Figure 1, P2 will be evaluated for every occurrence of the
string fmt= in the packet payload. Furthermore, when-
ever this string is followed by mp3, P2 will succeed and
the matcher will evaluate P3, and if P3 succeeds it will
evaluate P4. If fmt=mp3 appears n1 times, P3 is evalu-
ated n1 times. If there are n2 occurrences of player=, P4
will be evaluated n2 times for each evaluation of P3, which
gives us a total of n1 · n2 evaluations for P4. Similarly, if
these occurrences are followed by n3 repetitions of .exe
or .com, P5 is evaluated n1 · n2 · n3 times. Figure 4 shows
a packet that has n1 = n2 = n3 = 3 repetitions. Figure 5
shows the evaluation tree representing the predicates eval-

uated by the algorithm as it explores all possible matches
when matching Figure 1 against the payloads in Figure 2
and in Figure 4. Our experiments show that with packets
constructed in this manner, it is possible to force the algo-
rithm to evaluate some predicates hundreds of millions of
times while matching a single rule against a single packet.

The amount of processing a backtracking attack can
cause depends strongly on the rule. Let n be the size of
a packet in bytes. If the rule has k unconstrained multiple-
match predicates that perform O(n) work in the worst case,
an attacker can force a rule-matching algorithm to perform
O(nk) work. Thus the following three factors determine the
power of a backtracking attack against a rule.

1. The number of backtracking-causing multiple-match
content and pcre predicates k. The rule from
Figure 1 has k = 4 because it has 4 backtracking-causing
multiple-match predicates (including P5 which does not
match the attack packet, but still needs to traverse the
packet before failing). Note that not all contents and
pcres can be used to trigger excessive backtracking.
Often, predicates that have constraints on the positions
they match cannot be used by an attacker to cause back-
tracking. An example of such a predicate is the ﬁrst
pcre from Figure 1, predicate P2, which has to match
immediately after the ﬁrst content.

2. The size of the attack packets n. We can use Snort’s re-
assembly module to amplify the effect of backtracking
attacks beyond that of a single maximum sized packet.
The rule from Figure 1 is open to attacks of complexity
O(n4). When Snort combines two attack packets into a
virtual packet and feeds it to the rule-matching engine, n
doubles, and the rule-matcher does 16 times more work
than for either packet alone.

3. The total length of the strings needed to match the k
predicates. If these strings are short, the attacker can re-
peat them many times in a single packet. This inﬂuences
the constants hidden by the O-notation. Let s1,. . . ,sk be
the lengths of the strings that can cause matches for the
k predicates. If we make their contribution to the pro-
cessing time explicit we can compute for each string the
exact number of repetitions. If we divide the packet into

Payload
Offset

fmt=mp3fmt=mp3fmt=mp3player=player=player=.exe.exe.exe
0123456789012345678901234567890123456789012345678901234

1

2

3

4

5

Figure 4. A packet payload that causes rule matching to backtrack excessively.

P1

4

P2

28

P2

31

P3

51

P4

59

46

4

P2

7

P3

P1

11

P2

14

P3

18

P2

21

P3

28

35

42

28

35

42

28

35

42

P4

50

54

46

P4

50

54

46

P4

50

54

46

P4

50

54

46

P4

50

54

46

P4

50

54

46

P4

50

54

46

P4

50

P4

54

46

54

50

P5

P5

P5

P5

P5

P5

P5

P5

P5

P5

P5

P5

P5

P5

P5

P5

P5

P5

P5

P5

P5

P5

P5

P5

P5

P5

P5

P5

67

Match found!

Figure 5. Predicate evaluation trees in Snort. The left tree represents the 6 predicate evaluations
performed on the payload in Figure 2, and the right tree shows the 43 evaluations performed for the
payload in Figure 4. Numbers on edges indicate payload offsets where a predicate matched.

(cid:1)k

(cid:1)k

i=1 ni) = O(nk/(kk

k equal-sized portions, each ﬁlled with repetitions of one
of these strings, we obtain ni = (cid:1)(cid:1)n/k(cid:2)/si(cid:2). The cost
of the attack is O(
i=1 si)).
Other factors such as the amount of overlap between
these strings, the length of the strings needed to match
predicates that do not cause backtracking, and the details
of the processing costs of the predicates also inﬂuence
the processing cost. These factors remain hidden by the
constants inside the O-notation.
Approximately 8% of the 3800+ rules in our ruleset were
susceptible to backtracking attacks to some degree. Our fo-
cus is on the most egregious attacks, which typically yielded
slowdowns ranging from three to ﬁve orders of magnitude.
We quantify the strength of these attacks experimentally in
Section 6.

MemoizedMatchRule(P reds):
Stack ← (P reds[0].getNewInstance(0));
M emoizationT able ← ∅;
while Stack.size > 0 do
if Stack.top.getNextMatch() then
if Stack.size == P reds.size then return T rue;
ofst ← Stack .top.getMatchOﬀset();
if (Stack.top, ofst) /∈ MemoizationTable then
M emoizationT able ←
M emoizationT able ∪ {(Stack.top, ofst)};
Push(Stack, P reds[Stack.size].getNewInstance(ofst));

1
2
3
4
5
6
7
8

9

10

11

else Pop(Stack);
return F alse;

Figure 6. The memoization-enhanced rule-
matching algorithm. Lines 2, 7, and 8 have
been added.

5. Memoization, a remedy for backtracking

As illustrated above, rule-matching engines are open to
backtracking attacks if they retain no memory of inter-
mediate results, which for Snort are predicate evaluations
that have already been determined to fail. Thus, match-
ing engines can be forced to unnecessarily evaluate the
same doomed-for-failure predicates over and over again, as
Figure 5 indicates.

Figure 6 shows our revised algorithm for rule matching
that uses memoization [7,16]. It is based on the observation
that the outcome of evaluating a sequence of predicates de-
pends only on the payload and the offset at which process-
ing starts. The memoization table holds (predicate, oﬀset )
pairs indicating for all predicates, except the ﬁrst, the offsets
at which they have been evaluated thus far. Before evaluat-

ing a predicate, the algorithm checks whether it has already
been evaluated at the given offset (line 7). If the predicate
has been evaluated before, it must have ultimately led to
failure, so it is not evaluated again unnecessarily. Other-
wise, the (predicate, oﬀset) pair is added to the memoiza-
tion table (line 8) and the predicate is evaluated (line 9).
Note that memoization ensures that no predicate is evalu-
ated more than n times. Thus, if a rule has k(cid:1) predicates per-
forming work at most linear in the packet size n, memoiza-
tion ensures that the amount of work performed by the rule
matching algorithm is at most O(k(cid:1) · n · n) = O(k(cid:1)n2). Fig-
ure 7 updates Figure 5 to reﬂect the effects of memoization.
The greyed out nodes in the large tree from Figure 7 corre-
spond to the predicates that would not be re-evaluated when
using memoization. For the most damaging backtracking

4

Monotonicity

P2

7

P3

P1

11

P2

14

P3

18

CPS

P2

21

P3

28

35

42

28

35

42

28

35

42

P4

50

46

54

46

P4

50

54

46

P4

50

54

46

P4

50

54

46

P4

50

54

46

P4

50

54

46

P4

50

54

46

P4

50

P4

54

46

54

50

P5

P5

P5

P5

P5

P5

P5

P5

P5

P5

P5

P5

P5

P5

P5

P5

P5

P5

P5

P5

P5

P5

P5

P5

P5

P5

P5

Figure 7. The memoization algorithm performs only 13 predicate evaluations instead of 43 as it
avoids the grayed-out nodes. The CPS optimization reduces the number of predicate evaluations
to 9, and the monotonicity optimization further reduces the evaluations to 5.

attacks against rules in Snort’s default rule set, memoization
can reduce the time spent matching a rule against the packet
by more than four orders of magnitude (with the optimiza-
tions from Section 5.1, more than ﬁve orders of magnitude).
To implement memoization, we used pre-allocated bit-
maps for the memoization table, with a separate bitmap for
each predicate except the ﬁrst. The size of the bitmaps (in
bits) is the same as the size v (in bytes) of the largest virtual
packet. Thus if the largest number of predicates in a rule is
m, the memory cost of memoization is v(m − 1)/8 bytes.
In our experiments, memoization increases the amount of
memory used in Snort by less than 0.1%.

A naive implementation of memoization would need to
initialize these bitmaps for every rule evaluated. We avoid
this cost by creating a small array that holds up to 5 offsets
and an index into the array. When a rule is to be evaluated,
only the index into the array needs to be initialized to 0. If
the number of offsets a predicate is evaluated at exceeds 5,
we switch to a bitmap (and pay the cost of initializing it). It
is extremely rare that packets not speciﬁcally constructed to
trigger backtracking incur the cost of initializing the bitmap.

5.1. Further optimizations

We present three optimizations to the basic memoiza-
tion algorithm: detecting constrained predicate sequences,
monotonicity-aware memoization, and avoiding unneces-
sary memoization after single-match predicates. The ﬁrst
two of these signiﬁcantly reduce worst case processing
time, and all optimizations we use reduce the memory re-
quired to perform memoization. Most importantly, all three
optimizations are sound when appropriately applied; none
of them changes the semantics of rule matching.

Constrained predicate sequences: We use the name
marker for predicates that ignore the value of the offset pa-
rameter. The outcome of a marker and of all predicates
subsequent to the marker are independent of where pred-
icates preceding the marker matched. As a result, mark-
ers break a rule into sequences of predicates that are inde-
pendent of each other. We use the name constrained pred-

icate sequence (CPS) for a sequence of predicates begin-
ning at one marker and ending just before the next marker.
For example, P3 in Figure 1 looks for the string player=
in the entire payload, not just after the offset where the
previous predicate matches because P3 does not have the
relative modiﬁer. Thus the rule can be broken into two
CPSes: P1-P2 and P3-P4-P5.

Instead of invoking the rule-matching algorithm on the
entire rule, we invoke it separately for individual CPSes and
fail whenever we ﬁnd a CPS that cannot be matched against
the packet. The algorithm does not need to backtrack across
CPS boundaries. Less backtracking is performed because
the ﬁrst predicate in each CPS is invoked at most once. For
the example in Figure 7, detecting CPSes causes the algo-
rithm not to revisit P1 and P2 once P2 has matched, thus
reducing the number of predicate invocations from 13 to 9.
Monotone predicates: Some expensive multiple-match
predicates used by Snort have the monotonicity property
which we deﬁne below. For these predicates we use the
more aggressive lowest-offset memoization.
In this opti-
mization, we skip calls to a monotone predicate if it has
previously been evaluated at an offset smaller than the offset
for the current instance. For example, say we ﬁrst evaluate
a monotone content predicate starting at offset 100 that
does not lead to a match of the entire rule. Later we evalu-
ate the same predicate starting at offset 200. The second in-
stance is guaranteed to ﬁnd only matches that have already
been explored by the ﬁrst instance. With basic memoiza-
tion, after each of these matches of the second instance we
check the memoization table and do not evaluate the next
predicate because we know it will lead to failure. But, the
content predicate itself is evaluated unnecessarily. With
monotonicity-aware memoization, we do not even evaluate
the content predicate at offset 200.

The monotonicity property generalizes to some regular
expressions too, and it can be deﬁned formally as follows:
let S1 be the set of matches obtained when predicate p is
evaluated at offset o1, and S2 the matches for starting offset
o2. If for all packets and ∀o1 ≤ o2 we have S2 ⊂ S1, then p
is monotone. In our example from Figure 1, all contents

and pcres are monotone with the exception of the ﬁrst
pcre, P2, because it matches at most once immediately
after the position where the previous predicate matched.

Lowest-offset memoization helps reduce worst case pro-
cessing because for some predicates the number of worst-
case invocations is reduced from O(n) to 1. For the exam-
ple in Figure 7, this optimization would have eliminated the
second and third evaluations for predicates P4, and P5 (and
for P3 also if CPSes are not detected). This further reduces
the number of predicate instances evaluated from 9 to 5.

Unnecessary memoization: Basic memoization guar-
antees that no predicate is evaluated more than n times.
For some rules with single-match predicates we can pro-
vide the same guarantee even if we omit memoizing some
predicates. If we employ memoization before evaluating a
single-match predicate, but not before evaluating its suc-
cessor, we can still guarantee that the successor will not
be evaluated more than n times (at most once for every
evaluation of our single-match predicate). Also, if we have
chains of single-match predicates it is enough to memoize
only before the ﬁrst one to ensure that none is evaluated
more than n times. Thus, our third optimization is not to
perform memoization after single-match predicates, such as
byte test and byte jump (see Table 1), except when
they are followed by a monotone predicate. For our rule set,
this optimization reduces by a factor of two the amount of
memory used for memoization.

6. Experimental results

We performed empirical evaluations with traces and in
a live setting.
In Section 6.1, we present measurements
comparing backtracking attack packets with traces of typ-
ical network trafﬁc. Our results show that three to six or-
ders of magnitude slowdowns achieved with the backtrack-
ing attack are reduced to less than one order of magnitude
slowdown under memoization. In Section 6.2, we show ac-
tual evasion using a non-memoized implementation, and the
resulting recovery with the memoized version.

For our experiments we used the Snort NIDS, version
2.4.3, conﬁgured to use the Aho-Corasick [2] string match-
ing algorithm. Snort is run on a 2.0 GHz Pentium 4 pro-
cessor and is loaded with a total of 3812 rules. We instru-
mented Snort using cycle-accurate Pentium performance
counters. When enabled, instrumentation introduced less
than 2% overhead to the observed quantities of interest. We
found that our measured observations were consistent with
the instrumentation results collected in [4].

6.1. Trace-based results

For benign trafﬁc, we obtained two groups of three traces
each captured on different days at distinct times. The ﬁrst

benign traffic, unmodified Snort
attack traffic, no memoization
attack traffic, w/ memoization+opt

1000000

100000

)
e
l
a
c
s

g
o
l
(

n
w
o
d
w
o
l
S

10000

1000

100

10

1

(IMAP)
1755

(IRC)
1382

(MS-SQL) (NetBIOS)

2003

2403
Targeted Rule ID

(Oracle)
2611

(SMTP)
3682

(SMTP)
2087

Figure 8. Relative processing times for be-
nign and attack trafﬁc, and attack trafﬁc
with memoization. Memoization conﬁnes the
slowdown to less than one order of magni-
tude.

group of traces were captured on the link between a univer-
sity campus and a departmental network with 1,200 desk-
top and laptop computers, a number of high-trafﬁc servers
(web, ftp, ntp), and scientiﬁc computing clusters generating
high volumes of trafﬁc. These traces are 7 minutes long and
range in size from 3.1 GB to just over 8 GB. The second
group of traces were captured in front of a few instructional
laboratories totaling 150 desktop clients. They are also 7
minutes long and range in size from 816 MB to 2.6 GB.

We created attack trafﬁc by generating ﬂows correspond-
ing to several protocols and supplying payloads that are
constructed in a similar manner to the payload construction
outlined in Section 4.

In the trace-based experiments, we fed the benign trafﬁc
and attack trafﬁc traces into Snort and observed the perfor-
mance. We performed these experiments with and without
memoization enabled. Figure 8 shows the slowdowns expe-
rienced due to backtracking attacks targeting several rules
and the corresponding defense rates. It summarizes the in-
formation in Table 2. In each group, the leftmost bar repre-
sents the cost of packet processing for the speciﬁed protocol
relative to 20.6 s/GB, the combined average packet process-
ing rate in all our traces. For Rule 1382 (IRC), the rate is
less than 1, reﬂecting the fact that the average trafﬁc pro-
cessing time for IRC trafﬁc is less than the baseline.

The central bar in each group shows the slowdown ob-
served by packets crafted to target the speciﬁc rules indi-
cated at the base of each group. The attacks result in pro-
cessing times that are typically several orders of magnitude
slower than the baseline, with the most egregious attack
coming in at a factor of 1.5 million times slower. Finally,
in the rightmost bar of each group we see the result of each
attack repeated with the memoization defense deployed. In

Protocol

Rule
ID

Trace
trafﬁc

IMAP
IRC
MS-SQL
NetBIOS
Oracle
SMTP

1755
1382
2003
2403
2611
3682

200.6
14.6
119.3
729.7
110.5
132.8
SMTP 3682, w/o reassembly
132.8

SMTP

2087

Processing time (seconds/gigabyte)
Backtracking attack

Slowdown
w.r.t. avg trafﬁc

Slowdown
w.r.t. same protocol

Original
89,181
1,956,858
18,206
357,777
6,220,768
30,933,874
1,986,624
175,657

Basic Memo. Memo+Opt.
91.9
87.6
140.4
122.0
174.0
126.4
103.1
164.5

1,802
1,170
715
57,173
3,666
2,192
903
5,123

Original

4,329×
94,993×
884×
17,368×
301,979×
1,501,644×
96,438×
8,527×

Original
Memo+Opt
4.46×
444×
4.25× 134,031×
152×
6.82×
490×
5.92×
8.45×
56,296×
6.14× 232,936×
14,960×
5.00×
1,323×
7.99×

Memo+Opt
0.46×
6.00×
1.17×
0.17×
1.57×
0.95×
0.78×
1.24×

Table 2. Strength of the backtracking attack and feasibility of the memoization defense. Columns
7-8 shows the overall slowdown under attack when memoization is not and is used. Columns 9-10
shows similar slowdowns with respect to the same protocol.

most cases, Snort performance when under attack is com-
parable if not better than when not under attack.

Table 2 details the attacks and the defenses quantitatively
for several different protocols. For each attack, Columns 1
and 2 give the protocol and the targeted Rule ID to which
the attack belongs, respectively. Column 3 shows the aver-
age processing time for each protocol. Columns 4 through
6 show the raw processing times for attack packets under an
unmodiﬁed Snort, Snort with basic memoization, and Snort
with fully optimized memoization. Columns 7-8 give over-
all slowdowns and Columns 9-10 supply the slowdowns
on a per-protocol basis. The backtracking attack achieves
slowdowns between 3 and 5 orders of magnitude for rules
from many protocols. When memoization is employed, the
overall slowdown is conﬁned to within one order of mag-
nitude. Per-protocol, memoization conﬁnes most attacks to
within a factor of two of their normal processing time.

Rows 7 and 8 highlight the impact that reassembly has on
the processing time. In this experiment, when reassembly
is performed the size of the virtual packet fed to the rule-
matching engine is only twice the size of a non-reassembled
packet, but the processing time is almost 16× longer.

The effects of the three memoization optimizations can
be seen by comparing Columns 5 and 6 in Table 2. The
strength of the optimizations varies by protocol, ranging
from just under a factor of 10 to just over a factor of 30, ex-
cluding the NetBIOS outlier. In the Snort rule set, NetBIOS
rules contain many predicates that can be decomposed into
constrained predicate sequences. These rules beneﬁt con-
siderably from the optimizations. The accompanying tech-
nical report [21] contains the individual contributions of
each optimization to the reduction in processing time.

Recall that the attacks applied are all low-bandwidth at-
tacks. Even though the overall slowdown rate using mem-
oization is up to an order of magnitude slower, these rates
apply only to the attack packets (which are few in number)
and not to the overall performance of Snort. Under mem-
oization, processing times for attack packets fall within the
normal variation exhibited by benign packets.

In the rightmost column, slowdowns less than 1.0 indi-
cate that with all the optimizations included, Snort was able
to process backtracking attack packets more quickly than it
could process legitimate trafﬁc. In other words, our opti-
mizations allowed Snort to reject these attack packets more
quickly than it otherwise was able since fewer overall pred-
icate evaluations are performed.

6.2. Evading a live Snort

In this section we demonstrate the efﬁcacy of the back-
tracking attack by applying it to a live Snort installation. We
ﬁrst show successful evasion by applying the attack under a
variety of conditions. We then show that with memoization,
all the formerly undetected attacks are observed.

Figure 9 shows the topology used for testing evasion for
this experiment. To induce denial of service in Snort, we use
an SMTP backtracking attack that connects to a Sendmail
SMTP server in the protected network. We are using this
attack to mask a Nimda [1] exploit normally recognized by
Snort. Both the Nimda exploit and its SMTP cover are sent
from the same attacking computer. Each Nimda exploit is
sent one byte at a time in packets spaced 1 second apart. To
simulate real world conditions, we used the Harpoon trafﬁc
generator [23] to continuously generate background trafﬁc
at 10 Mbps during the experiments.

We measure the effectiveness of the backtracking attack
by the number of malicious exploits that can slip by Snort
undetected over various time frames. We initiated a new
Nimda exploit attempt every second for 5 minutes, yield-
ing 300 overlapping intrusion attempts. Table 3 shows the
results. Test 1 is the control: when the backtracking ex-
ploit is not performed, Snort recognizes and reports all 300
exploits despite our fragmenting them. In Test 2, we sent
two backtracking attack packets every 60 seconds for the
duration of the experiment. Snort missed only one-third of
the attacks, detecting 222 out of 300 intrusion attempts. In
Test 3, we increased the frequency of the backtracking at-
tacks to 2 packets every 15 seconds, dropping the detection

External Network

Protected Network

Attacker

Background
Traffic
Generator

(cid:1)(cid:1)(cid:1)
(cid:0)(cid:0)(cid:0)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)

(cid:1)(cid:1)(cid:1)
(cid:0)(cid:0)(cid:0)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:1)(cid:1)(cid:1)
(cid:0)(cid:0)(cid:0)
(cid:0)
(cid:0)
(cid:1)
(cid:1)

Snort 2.4.3

2 GHz P4

Hub

Fast Eth Hub

Victim

HTTP Server
Sendmail Server

Background
Traffic
Generator

(cid:1)(cid:1)(cid:1)
(cid:0)(cid:0)(cid:0)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)

(cid:1)(cid:1)(cid:1)
(cid:0)(cid:0)(cid:0)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)

Figure 9. Live Snort evasion environment.
Snort monitors a network composed of web
and mail servers.

rate to just 2% of the transmitted exploits. Test 4 decreased
the detection rate even further, and in Tests 5 and 6 the at-
tacker successfully transmitted all 300 exploits without de-
tection. Aside from high CPU utilization during the attacks
and an occasional, sporadic port scan warning directed at
the SMTP attack, Snort gave no indication of any abnormal
activity or intrusion attempt.

These experiments show that

the transmission rate
needed to successfully penetrate a network undetected is
quite low, with both tests 5 and 6 requiring no more than
4.0 kbps of bandwidth. Test 5, in particular, suggests that
perpetual evasion can be achieved through regular, repeated
transmissions of backtracking attack packets.

Tests 7 and 8 demonstrate the effectiveness of memoiza-
tion. These tests repeat Tests 5 and 6 with memoization
enabled (including all optimizations). With memoization,
Snort successfully detected all intrusions in both tests.

In summary, these experiments validate the results of our
trace-based experiments and illustrate the real-world appli-
cability of the backtracking attack. Using carefully crafted
and timed packets, we can perpetually disable an IPS with-
out triggering any alarms, using at most 4 kilobits per sec-
ond of trafﬁc. Correspondingly, the memoization defense
can effectively be used to counter such attacks.

7. Discussion

Often, algorithmic complexity attacks and their solutions
seem obvious once they have been properly described. Nev-
ertheless, software is still written that is vulnerable to such
attacks, which begs the question–how can a NIDS or IPS
designer defend against complexity attacks that she has not
yet seen? A possible ﬁrst step is to explicitly consider
worst-case performance in critical algorithms and to look
at whether it is signiﬁcantly slower than average case and
can be exploited. For example, [9] has shown that in the
Bro NIDS, failure to consider worst-case time complexity
of hash functions leads to denial of service. With this mind-
set, we brieﬂy consider mechanisms employed by existing

Test

1
2
3
4
5
6
7

8

Description of backtrack
attack
Control; no attack
two packets every 60 sec.
two packets every 15 sec.
one packet every 5 sec.
one packet every 3 sec.
twenty packets initially
one packet every 3 sec.

(memoization enabled)

Exploits
detected
300/300
220/300
6/300
4/300
0/300
0/300
300/300

Required
rate (kbps)
N/A
0.4
1.6
2.4
4.0
0.8
N/A

twenty packets initially

300/300

N/A

(memoization enabled)

Table 3. Summary of
live Snort experi-
ments. Without memoization, 300 intru-
sions pass into the network undetected.

NIDS with an eye towards triggering the worst case.

• Deterministic ﬁnite automata (DFA) systems can experi-
ence exponential memory requirements when DFA’s cor-
responding to individual rules are combined.
In some
cases, automata are built incrementally [22] to reduce the
footprint of a DFA that cannot otherwise ﬁt in memory.
Because each byte of trafﬁc is examined exactly once in
a DFA, backtracking does not occur. However, it may be
possible for an adversary to construct packets that trig-
ger incremental state creation on each byte of payload,
resulting in consistently increased computation costs and
potentially leading to memory exhaustion.

• Nondeterministic ﬁnite atomata (NFA) systems reduce
the memory requirement costs of DFA systems by al-
lowing the matcher to be in multiple states concurrently.
In practice, this is achieved either through backtrack-
ing or by explicitly maintaining and updating multiple
states. In the ﬁrst case, algorithmic complexity attacks
are achieved by triggering excessive backtracking. In the
second, the attacker tries to force the NIDS to update
several states for each byte processed.

• Predicate-based systems such as Snort can be slowed
down if the attacker can cause more predicates to be eval-
uated than in the average case. We have presented an
attack that forces the repeated evaluation of a few predi-
cates many times. In contrast, attacks can be devised that
seek to evaluate many predicates a few times. For exam-
ple, Snort employs a multi-pattern string matcher [2] as
a pre-ﬁlter to pare down the rules to be matched for each
packet. Constructing payloads that trigger large numbers
of rules can lead to excessive predicate evaluations.

We have performed preliminary work that combines the
second and third observations above to yield packet pro-
cessing times in Snort that are up to 1000 times slower than
average. These results, combined with those of this pa-
per, suggest that left unaddressed, algorithmic complexity
attacks can pose signiﬁcant security risks to NIDS.

8. Conclusions and future work

Algorithmic complexity attacks are effective when they
trigger worst-case behavior that far exceeds average-case
behavior. We have described a new algorithmic complex-
ity attack, the backtracking attack, that exploits rule match-
ing algorithms of NIDS to achieve slowdowns of up to six
orders of magnitude. When faced with these attacks, a real-
time NIDS becomes unable to keep up with incoming traf-
ﬁc, and evasion ensues. We tested this attack on a live Snort
installation and showed that the protected network is vul-
nerable under this attack, along with the tens of thousands
of other networks protected by Snort.

To counter this attack, we have developed a semantics-
preserving defense based on the principle of memoization
that brings Snort performance on attack packets to within
an order of magnitude of benign packets. Our solution con-
tinues the trend of providing algorithmic solutions to algo-
rithmic complexity attacks.

In general, it is not clear how to ﬁnd and root out all
sources of algorithmic complexity attacks. To do so re-
quires knowledge of average- and worst-case processing
costs. Without a formal model of computation, such knowl-
edge is difﬁcult to obtain and is often acquired in an ad-
hoc manner. Mechanisms for formally characterizing and
identifying algorithms and data structures that are subject
to complexity attacks can serve as useful analysis tools for
developers of critical systems, such as NIDS. We are cur-
rently exploring these issues.

References

[1] Cert

advisory

ca-2001-26

nimda worm,

2001.

http://www.cert.org /advisories
/CA-2001-26.html.

[2] A. V. Aho and M. J. Corasick. Efﬁcient string matching: An
aid to bibliographic search. In Communications of the ACM,
June 1975.

[3] M. Attig and J. W. Lockwood. SIFT: Snort intrusion ﬁlter

for TCP. In Hot Interconnects, Aug. 2005.

[4] J. B. Cabrera, J. Gosar, W. Lee, and R. K. Mehra. On the
statistical distribution of processing times in network intru-
sion detection. In 43rd IEEE Conference on Decision and
Control, Dec. 2004.

Scalable pattern
[5] C. R. Clark and D. E. Schimmel.
In IEEE Sympo-
matching for high-speed networks.
sium on Field-Programmable Custom Computing Machines
(FCCM), pages 249–257, Napa, California, Apr. 2004.
[6] The Snort network intrusion detection system on the intel
ixp2400 network processor. Consystant White Paper, 2003.
[7] T. H. Cormen, C. E. Leiserson, and R. L. Rivest. Introduc-

tion to Algorithms. MIT Press/McGraw-Hill, 1990.

[8] S. Crosby. Denial of service through regular expressions. In

Usenix Security work in progress report, Aug. 2003.

[9] S. A. Crosby and D. S. Wallach. Denial of service via algo-
rithmic complexity attacks. In Usenix Security, Aug. 2003.
[10] M. Handley, V. Paxson, and C. Kreibich. Network intru-
sion detection: Evasion, trafﬁc normalization, and end-to-
end protocol semantics. In Usenix Security, Aug. 2001.
[11] C. Kruegel, F. Valeur, G. Vigna, and R. Kemmerer. Stateful
Intrusion Detection for High-Speed Networks. In Proceed-
ings of the IEEE Symposium on Security and Privacy, pages
285–293, Oakland, CA, May 2002. IEEE Press.

[12] W. Lee, J. B. D. Cabrera, A. Thomas, N. Balwalli, S. Saluja,
and Y. Zhang. Performance adaptation in real-time intrusion
detection systems. In RAID, Zurich, Switzerland, Oct. 2002.
[13] V. Paxson. Bro: a system for detecting network intruders in
real-time. In Computer Networks (Amsterdam, Netherlands:
1999), volume 31, pages 2435–2463, 1999.

[14] PCRE: The perl compatible regular expression library.

http://www.pcre.org.
[15] T. H. Ptacek and T. N. Newsham.

Insertion, evasion and
denial of service: Eluding network intrusion detection. In
Secure Networks, Inc., Jan. 1998.

[16] T. Reps.

“Maximal-munch” tokenization in linear time.
ACM Transactions on Programming Languages and Sys-
tems, 20(2):259–273, 1998.

[17] M. Roesch. Snort - lightweight intrusion detection for net-
works. In Proceedings of the 13th Systems Administration
Conference. USENIX, 1999.

[18] S. Rubin, S. Jha, and B. P. Miller. Automatic generation
and analysis of NIDS attacks. In ACSAC ’04, pages 28–38,
Washington, DC, USA, Dec. 2004. IEEE Computer Society.
[19] U. Shankar and V. Paxson. Active mapping: resisting NIDS
evasion without altering trafﬁc. In IEEE Symposium on Se-
curity and Privacy, pages 44–61, May 2003.

[20] R. Sidhu and V. Prasanna. Fast regular expression matching

using FPGAs, 2001.

[21] R. Smith, C. Estan, and S. Jha. Algorithmic complexity at-
tacks against Snort. University of Wisconsin Technical Re-
port 1561, Sept. 2006.

[22] R. Sommer and V. Paxson. Enhancing byte-level network
intrusion detection signatures with context. In ACM CCS,
Washington, DC, Oct. 2003.

[23] J. Sommers and P. Barford. Self-conﬁguring network trafﬁc
generation. In Internet Measurement Conference, pages 68–
81, 2004.

[24] I. Sourdis and D. Pnevmatikatos. Fast, large-scale string
match for a 10gbps FPGA-based network intrusion detec-
In International Conference on Field Pro-
tion system.
grammable Logic and Applications, Sept. 2003.

[25] L. Tan and T. Sherwood. A high throughput string matching
architecture for intrusion detection and prevention. In Inter-
national Symposium on Computer Architecture ISCA, June
2005.

[26] T. Vermeiren, E. Borghs, and B. Haagdorens. Evaluation of
software techniques for parallel packet processing on multi-
core processors. In IEEE Consumer Communications and
Networking Conference, Jan. 2004.



=== Content from secunia.com_6798c28d_20250124_184319.html ===


[Skip to main content](#main-content)
[![Home](/themes/custom/flexera/images/logo.svg)](https://www.flexera.com/)

[![Home](/themes/custom/flexera/images/logo.svg)](https://www.flexera.com/)

Search

## Main navigation

* Solutions
  + Column 1
    - Business challenge
      * [Software renewals and audits](https://www.flexera.com/solutions/software-renewals-audits)
      * [Software license management and optimization](https://www.flexera.com/solutions/software-usage-costs)
      * [SaaS spend management](https://www.flexera.com/solutions/saas-spend)
      * [Cloud cost management](https://www.flexera.com/solutions/cloud-cost)
      * [IT asset lifecycle management](https://www.flexera.com/solutions/it-asset-lifecycle)
      * [CMDB data quality](https://www.flexera.com/solutions/cmdb-data-quality)
      * [Accurate IT inventory](https://www.flexera.com/solutions/it-inventory)
      * [Security and regulatory risk management](https://www.flexera.com/solutions/it-security-regulatory-risk)
      * [Sustainable IT](https://www.flexera.com/solutions/sustainable-it)
      * [AI-powered transformation](https://www.flexera.com/solutions/ai-powered-transformation)
      * [Public sector](https://www.flexera.com/solutions/public-sector)
  + Column 2
    - Spend management by vendor
      * [IBM](https://www.flexera.com/solutions/vendor/ibm)
      * [Oracle](https://www.flexera.com/solutions/vendor/oracle)
      * [Microsoft](https://www.flexera.com/solutions/vendor/microsoft)
      * [SAP](https://www.flexera.com/solutions/vendor/sap)
      * [VMware](https://www.flexera.com/solutions/vendor/vmware)
      * [ServiceNow](https://www.flexera.com/solutions/vendor/servicenow)
      * [AWS](https://www.flexera.com/solutions/vendor/aws)
      * [Salesforce](https://www.flexera.com/solutions/vendor/salesforce)
      * [BMC](https://www.flexera.com/solutions/cmdb-data-quality/bmc)
      * [Adobe](https://www.flexera.com/solutions/vendor/adobe)

  ### Achieve more through a united FinOps and ITAM function

  The future is hybrid. Break down the walls between ITAM and FinOps to drive more revenue, more customer growth and more innovation.

  [Discover More](https://www.flexera.com/resources/hybrid-itam-finops)
* Products
  + Column 1
    - [Flexera One](https://www.flexera.com/products/flexera-one)
      * [IT Visibility](https://www.flexera.com/products/flexera-one/it-visibility)
      * [ITAM](https://www.flexera.com/products/flexera-one/it-asset-management)
      * [SaaS Management](https://www.flexera.com/products/flexera-one/saas-management)
      * [FinOps](https://www.flexera.com/products/flexera-one/finops)
      * [Technology Intelligence Platform](https://www.flexera.com/products/flexera-one/technology-intelligence-platform)
  + Column 2
    - [Snow Atlas](https://www.flexera.com/products/snow-atlas)
      * [Snow Spend Optimizer](https://www.flexera.com/products/snow-atlas/snow-spend-optimizer)
      * [Snow SaaS Management](https://www.flexera.com/products/snow-atlas/snow-saas-management)
  + Column 3
    - Hide group
      * [Security](https://www.flexera.com/products/security)
      * [Application Readiness](https://www.flexera.com/products/adminstudio)
      * [All products](https://www.flexera.com/products)
      * [All Snow products](https://www.flexera.com/products/snow)
      * [Integrations](https://www.flexera.com/products/integrations)

  ### Flexera 2024 State of the Cloud Report

  What do transformative initiatives such as GenAI, machine learning and sustainability mean for the cloud? Check out the 2024 State of the Cloud Report to find the answer as well as all the latest cloud computing trends.

  [View Report](https://info.flexera.com/CM-REPORT-State-of-the-Cloud)
* Success
  + Column 1
    - [Customer success](https://www.flexera.com/customer-success)
      * Support
        + [Flexera support portal](https://community.flexera.com/s/support-hub)
        + [Flexera product documentation](https://docs.flexera.com)
        + [Snow product documentation](https://docs.snowsoftware.io/)
      * Services and training
        + [Services](https://www.flexera.com/customer-success/services)
        + [Training](https://www.flexera.com/customer-success/training)
  + Column 2
    - Hide group
      * [Technology Intelligence Awards](https://www.flexera.com/customer-success/awards)
      * [Flexera community](https://community.flexera.com/s/)

  ### Insights from Gartner®

  Find a curated series of actionable and objective insights for IT executives and their teams. Get expert insights from valued analysts, courtesy of Flexera.

  [Discover More](https://www.flexera.com/resources/gartner-analyst-research)
* Resources
  + Column 1
    - [Resources](https://www.flexera.com/resources)
      * [Webinars](https://www.flexera.com/resources?type%5Bwebinar%5D=webinar)
      * [Videos](https://www.flexera.com/resources?type%5Bvideo%5D=video)
      * [Datasheets](https://www.flexera.com/resources?type%5Bdatasheet%5D=datasheet)
      * [White papers & reports](https://www.flexera.com/resources?type%5Bwhite-paper-industry-report%5D=white-paper-industry-report)
  + Column 2
    - Hide group
      * [Blog](/blog/)
      * [Case studies](https://www.flexera.com/resources/case-studies)
      * [Events](https://www.flexera.com/resources?type%5Bevent%5D=event)
      * [Analyst Research](https://www.flexera.com/resources/gartner-analyst-research)
      * [Glossary](https://www.flexera.com/resources/glossary)
      * [Demos & trials](https://www.flexera.com/resources?type%5Bdemo-trials%5D=demo-trials)
      * [Business value calculator](https://www.flexera.com/resources/business-value-calculator)

  ### Flexera 2025 IT Priorities Report

  Insights from Flexera’s 2025 IT Priorities Report highlight what’s top of mind for IT decision makers in the year ahead. Discover the challenges, priorities and opportunities that will shape the future IT landscape.

  [View Report](https://info.flexera.com/ITV-REPORT-IT-Priorities)
* About
  + Column 1
    - [Company](https://www.flexera.com/about-us)
      * [About](https://www.flexera.com/about-us)
      * [Careers](https://www.flexera.com/about-us/careers)
      * [Contact](https://www.flexera.com/about-us/contact-us)
      * [Leadership](https://www.flexera.com/about-us/leadership)
    - [Partners](https://www.flexera.com/about-us/partners)
      * [Partner program](https://www.flexera.com/about-us/partners/partner-program)
      * [Partner directory](https://www.flexera.com/about-us/partners/directory)
  + Column 2
    - [Press center](https://www.flexera.com/about-us/press-center)
      * [Press releases](https://www.flexera.com/about-us/all-press-releases)
      * [Awards](https://www.flexera.com/about-us/press-center#awards)
      * [Articles](https://www.flexera.com/about-us/all-articles)
    - Hide group
      * Social responsibility
        + [ESG](https://www.flexera.com/about-us/environmental-social-governance)
        + [Diversity](https://www.flexera.com/about-us/diversity)

  ### More value with technology intelligence

  The unparalleled synergy of Flexera and Snow provides the Technology Intelligence you need for more efficiency, insight and governance than ever before.

  [Discover More](https://www.flexera.com/more-value-with-technology-intelligence)

Search

en

* [English](https://www.flexera.com/products/security/software-vulnerability-research/secunia-research?referrer=secunia)
* [Deutsch](https://www.flexera.de/products/security/software-vulnerability-research/secunia-research?referrer=secunia)

## External Links

* External Links
  + [Community](https://community.flexera.com/)
  + [Product Access](https://app.flexera.com/login)
  + [Partner Portal](https://flexera.channeltivity.com/Login)

[Book a demo](/about-us/contact-us?C_Interest1=sales)

# Secunia Research

## The world’s best vulnerability intelligence

The Secunia Research team from Flexera provides the most accurate and reliable source of vulnerability intelligence.

[Contact Us](https://www.flexera.com/about-us/contact-us?C_Interest1=sales&C_SolutionInterest=SVM)
Watch video (0:29)

Related links

* [Anatomy of a security advisory](https://www.flexera.com/resources/infographics/anatomy-of-a-security-advisory)
* [Software Vulnerability Research](https://www.flexera.com/products/software-vulnerability-research)
* [Software Vulnerability Manager](/products/software-vulnerability-manager)
* [Security advisories from Secunia Research](https://www.flexera.com/products/security/software-vulnerability-advisories)
* [Report a vulnerability](https://www.flexera.com/about-us/contact-us/report-vulnerability)

 ![Secunia Research](/sites/default/files/2022-04/hero-secunia-research-bg.jpg)

Featured Details

## Multiple ways to consume Secunia Research

Secunia delivers software security research that provides reliable, curated and actionable vulnerability intelligence. Organizations can expect to receive standardized, validated and enriched vulnerability research on a specific version of a software product. Secunia Research supports four solutions:

![Software Vulnerability Research](/sites/default/files/2022-04/icon-secunia-research-svr.svg)

### [Software Vulnerability Research](https://www.flexera.com/products/software-vulnerability-research)

Software Vulnerability Research utilizes Secunia Research to drive awareness of vulnerabilities matching your specified criteria

[Learn More](https://www.flexera.com/products/software-vulnerability-research)

![Software Vulnerability Manager](/sites/default/files/2022-04/icon-secunia-research-svm.svg)

### [Software Vulnerability Manager](/products/software-vulnerability-manager)

Software Vulnerability Manager uses Secunia Research data to identify, prioritize and patch known vulnerable software detected in your environment

[Learn More](/products/software-vulnerability-manager)

![Data Platform](/sites/default/files/2022-04/icon-secunia-research-dp.svg)

### [Data Platform](https://www.flexera.com/products/data-platform)

Data Platform leverages Secunia Research to provide high-level insights based on major or minor versions of software in your normalized inventory

[Learn More](https://www.flexera.com/products/data-platform)

![Flexera One](/sites/default/files/2022-04/icon-secunia-research-flexera-one.svg)

### [Flexera One](/flexera-one)

Flexera One utilizes Secunia Research (alongside public NVD data) to provide more granular matching of build-level versions of software in your normalized inventory within its IT Asset Management and IT Visibility solutions

[Learn More](/flexera-one)

How it works

## Accurate, reliable vulnerability insights at your fingertips

The Secunia Research team from Flexera is comprised of several security specialists who conduct vulnerability research in various products in addition to testing, verifying and validating public vulnerability reports. Since its inception in 2002, the goal of the Secunia Research team is to provide the most accurate and reliable source of vulnerability intelligence.

Delivering the world’s best vulnerability intelligence requires skill and passion. Team members continually develop their skills exploring various high-profile closed and open-source software using a variety of approaches, focusing chiefly on thorough code audits and binary analysis. The team has received industry recognition, including naming members to [Microsoft’s Most Valuable Security Researchers](https://msrc-blog.microsoft.com/2019/08/07/announcing-2019-msrc-most-valuable-security-researchers/) list.

Secunia researchers discover hard-to-find vulnerabilities that aren’t normally identified with techniques such as fuzzing, and the results have been impressive. Members of the Secunia Research team have discovered critical vulnerabilities in products from vendors including Microsoft, Symantec, IBM, Adobe, RealNetworks, Trend Micro, HP, Blue Coat, Samba, CA, Mozilla and Apple.

The team produces invaluable security advisories based on research of the vulnerabilities affecting any given software update. Sometimes a single update can address multiple vulnerabilities of varying criticalities and threats; but these advisories aggregate and distill findings down to a single advisory perfect for the prioritization of patching efforts within [Software Vulnerability Manager](/products/software-vulnerability-manager). Criticality scores are consistently applied along with details around attack vector and other valuable details within [Software Vulnerability Research](/products/software-vulnerability-research/secunia-research). Illegitimate vulnerability reports are also investigated and rejected so you can focus only on what truly matters.

Informing IT, Transforming IT

## Industry insights to help keep you informed

[#### Webinar

### Stay Ahead of Cyber Threats: Flexera's Latest Vulnerability Insights

Join us for this session where we'll explore the latest findings from the Flexera Monthly Vulnerability Insights Report.](https://info.flexera.com/SVM-WBNR-Vulnerability-Insights-Roundtable)

[#### Webinar

### Dive deeper into the Flexera Annual Vulnerability Insights

We'll explore the key findings from the Flexera Annual Vulnerability Insights Report. Learn about the latest cybersecurity trends, the most targeted industries, the types of vulnerabilities, plus management and mitigation strategies.](https://info.flexera.com/SVM-WBNR-Flexera-Annual-Vulnerability-Insights?lead_source=Website%20Visitor&id=Flexera.com-Resources)

#### Video

### Close the Risk Window with Software Vulnerability Manager

Stop reacting. Gain control. Stay secure. Build a more effective risk mitigation process leveraging Secunia Research vulnerability intelligence and the largest repository of third-party patch data in the industry.

Remote video URL

[#### Trial

### Software Vulnerability Manager Assessment free trial

Get access to the complete set of modules of Software Vulnerability Manager: Research, Assessment and Patching](https://info.flexera.com/SVM-EVAL-Software-Vulnerability-Manager)

[#### Datasheet

### Protect your ServiceNow® investment with the highest quality data

IT Visibility offers certified ServiceNow integrations that accelerate platform expansion, improve ROI and increase efficiencies across ITIL processes by delivering clean software and hardware asset data directly.](/sites/default/files/datasheet-itv-maximize-servicenow-investment.pdf)

[#### Blog

### Avoid missing crucial vulnerability intelligence amid NVD backlog

Recent developments regarding the National Vulnerability Database (NVD) have some technology leaders on edge. Since February, the U.S. National Institute of Standards and Technology (NIST) has almost completely stopped enriching software vulnerabi...](https://www.flexera.com/blog/vulnerability-management/avoid-missing-crucial-vulnerability-intelligence-amid-nvd-backlog/)

[View all resources](https://www.flexera.com/resources?category%5Bsoftware-vulnerability-management%5D=software-vulnerability-management)

## Footer Menu

* Column
  + Business challenge
    - [Software renewals and audits](https://www.flexera.com/solutions/software-renewals-audits)
    - [Software license management and optimization](https://www.flexera.com/solutions/software-usage-costs)
    - [SaaS spend management](https://www.flexera.com/solutions/saas-spend)
    - [Cloud cost management](https://www.flexera.com/solutions/cloud-cost)
    - [IT asset lifecycle management](https://www.flexera.com/solutions/it-asset-lifecycle)
    - [CMDB data quality](https://www.flexera.com/solutions/cmdb-data-quality)
    - [Accurate IT inventory](https://www.flexera.com/solutions/it-inventory)
    - [Security and regulatory risk management](https://www.flexera.com/solutions/it-security-regulatory-risk)
    - [Sustainable IT](https://www.flexera.com/solutions/sustainable-it)
    - [AI-powered transformation](https://www.flexera.com/solutions/ai-powered-transformation)
    - [Public sector](https://www.flexera.com/solutions/public-sector)
* Column
  + Spend management by vendor
    - [IBM](https://www.flexera.com/solutions/vendor/ibm)
    - [Oracle](https://www.flexera.com/solutions/vendor/oracle)
    - [Microsoft](https://www.flexera.com/solutions/vendor/microsoft)
    - [SAP](https://www.flexera.com/solutions/vendor/sap)
    - [VMware](https://www.flexera.com/solutions/vendor/vmware)
    - [ServiceNow](https://www.flexera.com/solutions/vendor/servicenow)
    - [AWS](https://www.flexera.com/solutions/vendor/aws)
    - [Salesforce](https://www.flexera.com/solutions/vendor/salesforce)
    - [BMC](https://www.flexera.com/solutions/cmdb-data-quality/bmc)
    - [Adobe](https://www.flexera.com/solutions/vendor/adobe)
* Column
  + Products
    - [Flexera One](https://www.flexera.com/products/flexera-one)
    - [Snow Atlas](https://www.flexera.com/products/snow-atlas)
    - [Security](https://www.flexera.com/products/security)
    - [Application Readiness](https://www.flexera.com/products/adminstudio)
    - [All products](https://www.flexera.com/products)
    - [All Snow products](https://www.flexera.com/products/snow)
    - [Integrations](https://www.flexera.com/products/integrations)
* Column
  + Company
    - [About](https://www.flexera.com/about-us)
    - [Careers](https://www.flexera.com/about-us/careers)
    - [Leadership](https://www.flexera.com/about-us/leadership)
    - [Contact us](https://www.flexera.com/about-us/contact-us)
    - [Media / press center](https://www.flexera.com/about-us/press-center)
    - [Revenera.com](https://www.revenera.com)

 +1.800.374.4353

en

* [English](https://www.flexera.com/products/security/software-vulnerability-research/secunia-research?referrer=secunia)
* [Deutsch](https://www.flexera.de/products/security/software-vulnerability-research/secunia-research?referrer=secunia)

 [![Home](/themes/custom/flexera/images/logo.svg)](https://www.flexera.com/)

© 2025 Flexera. All Rights Reserved.

## Footer

* [Privacy Policy](https://www.flexera.com/legal/privacy-policy)
* [Terms and conditions](https://www.flexera.com/legal)
* [Contact Us](https://www.flexera.com/about-us/contact-us)
* [Impressum](https://www.flexera.com/about-us/impressum)
* [Site Map](https://www.flexera.com/sitemap)

#####

×

...



=== Content from security.gentoo.org_a5be36a3_20250124_184320.html ===

[![Gentoo](https://assets.gentoo.org/tyrian/v2/site-logo.png)](/ "Back to the homepage")
Security

[**Get Gentoo!**](https://get.gentoo.org/)
gentoo.org sites
[gentoo.org](https://www.gentoo.org/ "Main Gentoo website")
[Wiki](https://wiki.gentoo.org/ "Find and contribute documentation")
[Bugs](https://bugs.gentoo.org/ "Report issues and find common issues")
[Forums](https://forums.gentoo.org/ "Discuss with the community")
[Packages](https://packages.gentoo.org/ "Find software for your Gentoo")

[Planet](https://planet.gentoo.org/ "Find out what's going on in the developer community")
[Archives](https://archives.gentoo.org/ "Read up on past discussions")
[Sources](https://sources.gentoo.org/ "Browse our source code")

[Infra Status](https://infra-status.gentoo.org/ "Get updates on the services provided by Gentoo")

* [Home](/)
* [Stay informed](/subscribe)
* [Advisories](/glsa)

# Snort: Denial of service — GLSA **200702-03**

Snort contains a vulnerability in the rule matching algorithm that could
result in a Denial of Service.

### Affected packages

| Package | **net-analyzer/snort** on all architectures |
| --- | --- |
| Affected versions | < **2.6.1.2** |
| Unaffected versions | >= **2.6.1.2** |

### Background

Snort is a widely deployed intrusion detection program.

### Description

Randy Smith, Christian Estan and Somesh Jha discovered that the rule
matching algorithm of Snort can be exploited in a way known as a
"backtracking attack" to perform numerous time-consuming operations.

### Impact

A remote attacker could send specially crafted network packets, which
would result in the cessation of the detections and the consumption of
the CPU resources.

### Workaround

There is no known workaround at this time.

### Resolution

All Snort users should upgrade to the latest version:

```
 # emerge --sync
 # emerge --ask --oneshot --verbose ">=net-analyzer/snort-2.6.1.2"
```
### References

* [CVE-2006-6931](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2006-6931)

**Release date**

February 13, 2007

**Latest revision**

February 13, 2007: 01

**Severity**

normal

**Exploitable**

remote

**Bugzilla entries**

* [161632](https://bugs.gentoo.org/show_bug.cgi?id=161632)

### Questions or comments?

Please feel free to contact us.

**© 2001–2020 Gentoo Foundation, Inc.**



=== Content from secunia.com_f3763a85_20250124_184318.html ===


[Skip to main content](#main-content)
[![Home](/themes/custom/flexera/images/logo.svg)](https://www.flexera.com/)

[![Home](/themes/custom/flexera/images/logo.svg)](https://www.flexera.com/)

Search

## Main navigation

* Solutions
  + Column 1
    - Business challenge
      * [Software renewals and audits](https://www.flexera.com/solutions/software-renewals-audits)
      * [Software license management and optimization](https://www.flexera.com/solutions/software-usage-costs)
      * [SaaS spend management](https://www.flexera.com/solutions/saas-spend)
      * [Cloud cost management](https://www.flexera.com/solutions/cloud-cost)
      * [IT asset lifecycle management](https://www.flexera.com/solutions/it-asset-lifecycle)
      * [CMDB data quality](https://www.flexera.com/solutions/cmdb-data-quality)
      * [Accurate IT inventory](https://www.flexera.com/solutions/it-inventory)
      * [Security and regulatory risk management](https://www.flexera.com/solutions/it-security-regulatory-risk)
      * [Sustainable IT](https://www.flexera.com/solutions/sustainable-it)
      * [AI-powered transformation](https://www.flexera.com/solutions/ai-powered-transformation)
      * [Public sector](https://www.flexera.com/solutions/public-sector)
  + Column 2
    - Spend management by vendor
      * [IBM](https://www.flexera.com/solutions/vendor/ibm)
      * [Oracle](https://www.flexera.com/solutions/vendor/oracle)
      * [Microsoft](https://www.flexera.com/solutions/vendor/microsoft)
      * [SAP](https://www.flexera.com/solutions/vendor/sap)
      * [VMware](https://www.flexera.com/solutions/vendor/vmware)
      * [ServiceNow](https://www.flexera.com/solutions/vendor/servicenow)
      * [AWS](https://www.flexera.com/solutions/vendor/aws)
      * [Salesforce](https://www.flexera.com/solutions/vendor/salesforce)
      * [BMC](https://www.flexera.com/solutions/cmdb-data-quality/bmc)
      * [Adobe](https://www.flexera.com/solutions/vendor/adobe)

  ### Achieve more through a united FinOps and ITAM function

  The future is hybrid. Break down the walls between ITAM and FinOps to drive more revenue, more customer growth and more innovation.

  [Discover More](https://www.flexera.com/resources/hybrid-itam-finops)
* Products
  + Column 1
    - [Flexera One](https://www.flexera.com/products/flexera-one)
      * [IT Visibility](https://www.flexera.com/products/flexera-one/it-visibility)
      * [ITAM](https://www.flexera.com/products/flexera-one/it-asset-management)
      * [SaaS Management](https://www.flexera.com/products/flexera-one/saas-management)
      * [FinOps](https://www.flexera.com/products/flexera-one/finops)
      * [Technology Intelligence Platform](https://www.flexera.com/products/flexera-one/technology-intelligence-platform)
  + Column 2
    - [Snow Atlas](https://www.flexera.com/products/snow-atlas)
      * [Snow Spend Optimizer](https://www.flexera.com/products/snow-atlas/snow-spend-optimizer)
      * [Snow SaaS Management](https://www.flexera.com/products/snow-atlas/snow-saas-management)
  + Column 3
    - Hide group
      * [Security](https://www.flexera.com/products/security)
      * [Application Readiness](https://www.flexera.com/products/adminstudio)
      * [All products](https://www.flexera.com/products)
      * [All Snow products](https://www.flexera.com/products/snow)
      * [Integrations](https://www.flexera.com/products/integrations)

  ### Flexera 2024 State of the Cloud Report

  What do transformative initiatives such as GenAI, machine learning and sustainability mean for the cloud? Check out the 2024 State of the Cloud Report to find the answer as well as all the latest cloud computing trends.

  [View Report](https://info.flexera.com/CM-REPORT-State-of-the-Cloud)
* Success
  + Column 1
    - [Customer success](https://www.flexera.com/customer-success)
      * Support
        + [Flexera support portal](https://community.flexera.com/s/support-hub)
        + [Flexera product documentation](https://docs.flexera.com)
        + [Snow product documentation](https://docs.snowsoftware.io/)
      * Services and training
        + [Services](https://www.flexera.com/customer-success/services)
        + [Training](https://www.flexera.com/customer-success/training)
  + Column 2
    - Hide group
      * [Technology Intelligence Awards](https://www.flexera.com/customer-success/awards)
      * [Flexera community](https://community.flexera.com/s/)

  ### Insights from Gartner®

  Find a curated series of actionable and objective insights for IT executives and their teams. Get expert insights from valued analysts, courtesy of Flexera.

  [Discover More](https://www.flexera.com/resources/gartner-analyst-research)
* Resources
  + Column 1
    - [Resources](https://www.flexera.com/resources)
      * [Webinars](https://www.flexera.com/resources?type%5Bwebinar%5D=webinar)
      * [Videos](https://www.flexera.com/resources?type%5Bvideo%5D=video)
      * [Datasheets](https://www.flexera.com/resources?type%5Bdatasheet%5D=datasheet)
      * [White papers & reports](https://www.flexera.com/resources?type%5Bwhite-paper-industry-report%5D=white-paper-industry-report)
  + Column 2
    - Hide group
      * [Blog](/blog/)
      * [Case studies](https://www.flexera.com/resources/case-studies)
      * [Events](https://www.flexera.com/resources?type%5Bevent%5D=event)
      * [Analyst Research](https://www.flexera.com/resources/gartner-analyst-research)
      * [Glossary](https://www.flexera.com/resources/glossary)
      * [Demos & trials](https://www.flexera.com/resources?type%5Bdemo-trials%5D=demo-trials)
      * [Business value calculator](https://www.flexera.com/resources/business-value-calculator)

  ### Flexera 2025 IT Priorities Report

  Insights from Flexera’s 2025 IT Priorities Report highlight what’s top of mind for IT decision makers in the year ahead. Discover the challenges, priorities and opportunities that will shape the future IT landscape.

  [View Report](https://info.flexera.com/ITV-REPORT-IT-Priorities)
* About
  + Column 1
    - [Company](https://www.flexera.com/about-us)
      * [About](https://www.flexera.com/about-us)
      * [Careers](https://www.flexera.com/about-us/careers)
      * [Contact](https://www.flexera.com/about-us/contact-us)
      * [Leadership](https://www.flexera.com/about-us/leadership)
    - [Partners](https://www.flexera.com/about-us/partners)
      * [Partner program](https://www.flexera.com/about-us/partners/partner-program)
      * [Partner directory](https://www.flexera.com/about-us/partners/directory)
  + Column 2
    - [Press center](https://www.flexera.com/about-us/press-center)
      * [Press releases](https://www.flexera.com/about-us/all-press-releases)
      * [Awards](https://www.flexera.com/about-us/press-center#awards)
      * [Articles](https://www.flexera.com/about-us/all-articles)
    - Hide group
      * Social responsibility
        + [ESG](https://www.flexera.com/about-us/environmental-social-governance)
        + [Diversity](https://www.flexera.com/about-us/diversity)

  ### More value with technology intelligence

  The unparalleled synergy of Flexera and Snow provides the Technology Intelligence you need for more efficiency, insight and governance than ever before.

  [Discover More](https://www.flexera.com/more-value-with-technology-intelligence)

Search

en

* [English](https://www.flexera.com/products/security/software-vulnerability-research/secunia-research?referrer=secunia)
* [Deutsch](https://www.flexera.de/products/security/software-vulnerability-research/secunia-research?referrer=secunia)

## External Links

* External Links
  + [Community](https://community.flexera.com/)
  + [Product Access](https://app.flexera.com/login)
  + [Partner Portal](https://flexera.channeltivity.com/Login)

[Book a demo](/about-us/contact-us?C_Interest1=sales)

# Secunia Research

## The world’s best vulnerability intelligence

The Secunia Research team from Flexera provides the most accurate and reliable source of vulnerability intelligence.

[Contact Us](https://www.flexera.com/about-us/contact-us?C_Interest1=sales&C_SolutionInterest=SVM)
Watch video (0:29)

Related links

* [Anatomy of a security advisory](https://www.flexera.com/resources/infographics/anatomy-of-a-security-advisory)
* [Software Vulnerability Research](https://www.flexera.com/products/software-vulnerability-research)
* [Software Vulnerability Manager](/products/software-vulnerability-manager)
* [Security advisories from Secunia Research](https://www.flexera.com/products/security/software-vulnerability-advisories)
* [Report a vulnerability](https://www.flexera.com/about-us/contact-us/report-vulnerability)

 ![Secunia Research](/sites/default/files/2022-04/hero-secunia-research-bg.jpg)

Featured Details

## Multiple ways to consume Secunia Research

Secunia delivers software security research that provides reliable, curated and actionable vulnerability intelligence. Organizations can expect to receive standardized, validated and enriched vulnerability research on a specific version of a software product. Secunia Research supports four solutions:

![Software Vulnerability Research](/sites/default/files/2022-04/icon-secunia-research-svr.svg)

### [Software Vulnerability Research](https://www.flexera.com/products/software-vulnerability-research)

Software Vulnerability Research utilizes Secunia Research to drive awareness of vulnerabilities matching your specified criteria

[Learn More](https://www.flexera.com/products/software-vulnerability-research)

![Software Vulnerability Manager](/sites/default/files/2022-04/icon-secunia-research-svm.svg)

### [Software Vulnerability Manager](/products/software-vulnerability-manager)

Software Vulnerability Manager uses Secunia Research data to identify, prioritize and patch known vulnerable software detected in your environment

[Learn More](/products/software-vulnerability-manager)

![Data Platform](/sites/default/files/2022-04/icon-secunia-research-dp.svg)

### [Data Platform](https://www.flexera.com/products/data-platform)

Data Platform leverages Secunia Research to provide high-level insights based on major or minor versions of software in your normalized inventory

[Learn More](https://www.flexera.com/products/data-platform)

![Flexera One](/sites/default/files/2022-04/icon-secunia-research-flexera-one.svg)

### [Flexera One](/flexera-one)

Flexera One utilizes Secunia Research (alongside public NVD data) to provide more granular matching of build-level versions of software in your normalized inventory within its IT Asset Management and IT Visibility solutions

[Learn More](/flexera-one)

How it works

## Accurate, reliable vulnerability insights at your fingertips

The Secunia Research team from Flexera is comprised of several security specialists who conduct vulnerability research in various products in addition to testing, verifying and validating public vulnerability reports. Since its inception in 2002, the goal of the Secunia Research team is to provide the most accurate and reliable source of vulnerability intelligence.

Delivering the world’s best vulnerability intelligence requires skill and passion. Team members continually develop their skills exploring various high-profile closed and open-source software using a variety of approaches, focusing chiefly on thorough code audits and binary analysis. The team has received industry recognition, including naming members to [Microsoft’s Most Valuable Security Researchers](https://msrc-blog.microsoft.com/2019/08/07/announcing-2019-msrc-most-valuable-security-researchers/) list.

Secunia researchers discover hard-to-find vulnerabilities that aren’t normally identified with techniques such as fuzzing, and the results have been impressive. Members of the Secunia Research team have discovered critical vulnerabilities in products from vendors including Microsoft, Symantec, IBM, Adobe, RealNetworks, Trend Micro, HP, Blue Coat, Samba, CA, Mozilla and Apple.

The team produces invaluable security advisories based on research of the vulnerabilities affecting any given software update. Sometimes a single update can address multiple vulnerabilities of varying criticalities and threats; but these advisories aggregate and distill findings down to a single advisory perfect for the prioritization of patching efforts within [Software Vulnerability Manager](/products/software-vulnerability-manager). Criticality scores are consistently applied along with details around attack vector and other valuable details within [Software Vulnerability Research](/products/software-vulnerability-research/secunia-research). Illegitimate vulnerability reports are also investigated and rejected so you can focus only on what truly matters.

Informing IT, Transforming IT

## Industry insights to help keep you informed

[#### Webinar

### Stay Ahead of Cyber Threats: Flexera's Latest Vulnerability Insights

Join us for this session where we'll explore the latest findings from the Flexera Monthly Vulnerability Insights Report.](https://info.flexera.com/SVM-WBNR-Vulnerability-Insights-Roundtable)

[#### Webinar

### Dive deeper into the Flexera Annual Vulnerability Insights

We'll explore the key findings from the Flexera Annual Vulnerability Insights Report. Learn about the latest cybersecurity trends, the most targeted industries, the types of vulnerabilities, plus management and mitigation strategies.](https://info.flexera.com/SVM-WBNR-Flexera-Annual-Vulnerability-Insights?lead_source=Website%20Visitor&id=Flexera.com-Resources)

#### Video

### Close the Risk Window with Software Vulnerability Manager

Stop reacting. Gain control. Stay secure. Build a more effective risk mitigation process leveraging Secunia Research vulnerability intelligence and the largest repository of third-party patch data in the industry.

Remote video URL

[#### Trial

### Software Vulnerability Manager Assessment free trial

Get access to the complete set of modules of Software Vulnerability Manager: Research, Assessment and Patching](https://info.flexera.com/SVM-EVAL-Software-Vulnerability-Manager)

[#### Datasheet

### Protect your ServiceNow® investment with the highest quality data

IT Visibility offers certified ServiceNow integrations that accelerate platform expansion, improve ROI and increase efficiencies across ITIL processes by delivering clean software and hardware asset data directly.](/sites/default/files/datasheet-itv-maximize-servicenow-investment.pdf)

[#### Blog

### Avoid missing crucial vulnerability intelligence amid NVD backlog

Recent developments regarding the National Vulnerability Database (NVD) have some technology leaders on edge. Since February, the U.S. National Institute of Standards and Technology (NIST) has almost completely stopped enriching software vulnerabi...](https://www.flexera.com/blog/vulnerability-management/avoid-missing-crucial-vulnerability-intelligence-amid-nvd-backlog/)

[View all resources](https://www.flexera.com/resources?category%5Bsoftware-vulnerability-management%5D=software-vulnerability-management)

## Footer Menu

* Column
  + Business challenge
    - [Software renewals and audits](https://www.flexera.com/solutions/software-renewals-audits)
    - [Software license management and optimization](https://www.flexera.com/solutions/software-usage-costs)
    - [SaaS spend management](https://www.flexera.com/solutions/saas-spend)
    - [Cloud cost management](https://www.flexera.com/solutions/cloud-cost)
    - [IT asset lifecycle management](https://www.flexera.com/solutions/it-asset-lifecycle)
    - [CMDB data quality](https://www.flexera.com/solutions/cmdb-data-quality)
    - [Accurate IT inventory](https://www.flexera.com/solutions/it-inventory)
    - [Security and regulatory risk management](https://www.flexera.com/solutions/it-security-regulatory-risk)
    - [Sustainable IT](https://www.flexera.com/solutions/sustainable-it)
    - [AI-powered transformation](https://www.flexera.com/solutions/ai-powered-transformation)
    - [Public sector](https://www.flexera.com/solutions/public-sector)
* Column
  + Spend management by vendor
    - [IBM](https://www.flexera.com/solutions/vendor/ibm)
    - [Oracle](https://www.flexera.com/solutions/vendor/oracle)
    - [Microsoft](https://www.flexera.com/solutions/vendor/microsoft)
    - [SAP](https://www.flexera.com/solutions/vendor/sap)
    - [VMware](https://www.flexera.com/solutions/vendor/vmware)
    - [ServiceNow](https://www.flexera.com/solutions/vendor/servicenow)
    - [AWS](https://www.flexera.com/solutions/vendor/aws)
    - [Salesforce](https://www.flexera.com/solutions/vendor/salesforce)
    - [BMC](https://www.flexera.com/solutions/cmdb-data-quality/bmc)
    - [Adobe](https://www.flexera.com/solutions/vendor/adobe)
* Column
  + Products
    - [Flexera One](https://www.flexera.com/products/flexera-one)
    - [Snow Atlas](https://www.flexera.com/products/snow-atlas)
    - [Security](https://www.flexera.com/products/security)
    - [Application Readiness](https://www.flexera.com/products/adminstudio)
    - [All products](https://www.flexera.com/products)
    - [All Snow products](https://www.flexera.com/products/snow)
    - [Integrations](https://www.flexera.com/products/integrations)
* Column
  + Company
    - [About](https://www.flexera.com/about-us)
    - [Careers](https://www.flexera.com/about-us/careers)
    - [Leadership](https://www.flexera.com/about-us/leadership)
    - [Contact us](https://www.flexera.com/about-us/contact-us)
    - [Media / press center](https://www.flexera.com/about-us/press-center)
    - [Revenera.com](https://www.revenera.com)

 +1.800.374.4353

en

* [English](https://www.flexera.com/products/security/software-vulnerability-research/secunia-research?referrer=secunia)
* [Deutsch](https://www.flexera.de/products/security/software-vulnerability-research/secunia-research?referrer=secunia)

 [![Home](/themes/custom/flexera/images/logo.svg)](https://www.flexera.com/)

© 2025 Flexera. All Rights Reserved.

## Footer

* [Privacy Policy](https://www.flexera.com/legal/privacy-policy)
* [Terms and conditions](https://www.flexera.com/legal)
* [Contact Us](https://www.flexera.com/about-us/contact-us)
* [Impressum](https://www.flexera.com/about-us/impressum)
* [Site Map](https://www.flexera.com/sitemap)

#####

×

...


