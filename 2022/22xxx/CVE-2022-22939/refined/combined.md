=== Content from www.vmware.com_bec36726_20250114_223152.html ===


Menu

* [Products](https://www.broadcom.com/products/)
* [Solutions](https://www.broadcom.com/solutions)
* [Support and Services](https://www.broadcom.com/support)
* [Company](https://www.broadcom.com/company/about-us/)
* [How To Buy](https://www.broadcom.com/how-to-buy/#sales)

* Log in

  [Log In](/c/portal/login)
  [Register](https://profile.broadcom.com/web/registration)

[Register](https://profile.broadcom.com/web/registration)
[Login](/c/portal/login)

VMSA-2022-0003:VMware Cloud Foundation contains an information disclosure vulnerability due to the logging of plaintext credentials within some log files

Product/Component

VMware Cloud Foundation

0 more products

List of Products

1 Products

* VMware Cloud Foundation

Notification Id

23629

Last Updated

12 February 2022

Initial Publication Date

29 January 2022

Status

CLOSED

Severity

MEDIUM

CVSS Base Score

6.0

WorkAround

Affected CVE

CVE-2022-22939

             Advisory ID: VMSA-2022-0003.1   CVSSv3 Range: 6.0   Issue Date:2022-01-31   Updated On: 2022-02-14    CVE(s): CVE-2022-22939   Synopsis: VMware Cloud Foundation contains an information disclosure vulnerability due to the logging of plaintext credentials within some log files.

 [RSS Feed](https://www.vmware.com/security/advisories/VMSA-2022-0003.xml)

 Download PDF

 Download Text File

Share this page on social media:

##### **1. Impacted Products**

VMware Cloud Foundation (Cloud Foundation)

##### **2. Introduction**

An Information disclosure vulnerability in VMware Cloud Foundation SDDC Manager was discovered. Updates are available to remediate this vulnerability in VMware Cloud Foundation.

##### **3. Information disclosure vulnerability in VMware Cloud Foundation SDDC Manager (CVE-2022-22939)**

**Description**

VMware Cloud Foundation contains an information disclosure vulnerability due to logging of credentials in plain-text within multiple log files on the SDDC Manager.

**Known Attack Vectors**

A malicious actor with root access on VMware Cloud Foundation SDDC Manager may be able to view credentials in plaintext within one or more log files.

**Resolution**

To remediate CVE-2022-22939 apply the updates listed in the 'Fixed Version' column of the 'Response Matrix' below to affected deployments.

**Workarounds**

Workarounds for CVE-2022-22939 have been listed in the 'Workarounds' column of the 'Response Matrix' below.

**Additional Documentation**

None.

**Notes**

None.

**Acknowledgements**

None.

**Response Matrix**

| Product | Version | Running On | CVE Identifier | CVSSv3 | Severity | Fixed Version | Workarounds | Additional Documentation |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| VMware Cloud Foundation | 4.x | Any | CVE-2022-22939 | [6.0](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:L/AC:L/PR:H/UI:N/S:C/C:H/I:N/A:N) | moderate | [4.3.1.1](https://docs.vmware.com/en/VMware-Cloud-Foundation/4.3.1/rn/VMware-Cloud-Foundation-431-Release-Notes.html) | [KB87050](https://kb.vmware.com/s/article/87050) | None. |
| VMware Cloud Foundation | 3.x | Any | CVE-2022-22939 | [6.0](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:L/AC:L/PR:H/UI:N/S:C/C:H/I:N/A:N) | moderate | [3.11](https://docs.vmware.com/en/VMware-Cloud-Foundation/3.11/rn/VMware-Cloud-Foundation-311-Release-Notes.html) | N/A | None. |

##### **4. References**

**Fixed Version(s) and Release Notes:**

**VMware vCloud Foundation 4.x**

Downloads and Documentation:

<https://docs.vmware.com/en/VMware-Cloud-Foundation/4.3.1/rn/VMware-Cloud-Foundation-431-Release-Notes.html>

**VMware vCloud Foundation 3.x**
Downloads and Documentation:

<https://docs.vmware.com/en/VMware-Cloud-Foundation/3.11/rn/VMware-Cloud-Foundation-311-Release-Notes.html>

**Mitre CVE Dictionary Links:**
<https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-22939>

**FIRST CVSSv3 Calculator:**
CVE-2022-22939: <https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:L/AC:L/PR:H/UI:N/S:C/C:H/I:N/A:N>

##### **5. Change Log**

**2022-01-31 VMSA-2022-0003**
Initial security advisory.

**2022-02-14 VMSA-2022-0003.1**
Updated security advisory to add VMware Cloud Foundation 3.11 version in the response matrix of Section 3.

##### **6. Contact**

E-mail list for product security notifications and announcements:

<https://lists.vmware.com/cgi-bin/mailman/listinfo/security-announce>

This Security Advisory is posted to the following lists:

[[email protected]](/cdn-cgi/l/email-protection#96e5f3f5e3e4ffe2efbbf7f8f8f9e3f8f5f3d6faffe5e2e5b8e0fbe1f7e4f3b8f5f9fb)

[[email protected]](/cdn-cgi/l/email-protection#ef8d9a889b9d8e9eaf9c8a8c9a9d869b9689808c9a9cc18c8082)

[[email protected]](/cdn-cgi/l/email-protection#c9afbca5a5ada0baaaa5a6babcbbac89baacaaa5a0babdbae7a6bbae)

E-mail: [[email protected]](/cdn-cgi/l/email-protection#deadbbbdabacb7aaa79ea8b3a9bfacbbf0bdb1b3)

PGP key at:

<https://kb.vmware.com/kb/1055>

VMware Security Advisories

<https://www.vmware.com/security/advisories>

VMware Security Response Policy

<https://www.vmware.com/support/policies/security_response.html>

VMware Lifecycle Support Phases

<https://www.vmware.com/support/policies/lifecycle.html>

VMware Security & Compliance Blog

<https://blogs.vmware.com/security>

Twitter

<https://twitter.com/VMwareSRC>

Copyright 2022 VMware Inc. All rights reserved.

Hidden

#####

×

It appears your Broadcom Products and Services are

supported by one of our certified Support partners

Click below to be redirected to the appropriate Support

Partner Portal to request support

For non-product related issues (Support Portal / Licensing) Click HERE

Continue

#####

×

For **Technical Support** (issues with products or services)

1. Select **Technical** to be redirected to the My Entitlements page
2. Expand the product you require support on
3. Select the case icon from the case column
4. You will be redirected to the appropriate vendor portal where you can raise your technical request

For **Non-Technical Support** (issues with portal access, license keys, software downloads)

1. Select **Non-Technical** to be redirected to Broadcom's case management portal

Technical
Non-Technical

#####

×

# Access Denied

This feature has been disabled by your administrator.

#####

×

To prevent this message from showing again, please enable pop-up blockers for [support.broadcom.com](https://support.broadcom.com/)
or click Continue to proceed.

Continue

Top

* [Products](https://www.broadcom.com/products)
* [Solutions](https://www.broadcom.com/solutions)
* [Support and Services](https://www.broadcom.com/support)
* [Company](https://www.broadcom.com/)
* [How to Buy](https://www.broadcom.com/how-to-buy)

 Copyright © 2005-2024 Broadcom. All Rights Reserved. The term “Broadcom” refers to Broadcom Inc. and/or its subsidiaries.

* [Accessibility](https://www.broadcom.com/company/legal/accessibility)
* [Privacy](https://www.broadcom.com/company/legal/privacy)
* [Supplier Responsibility](https://www.broadcom.com/company/citizenship/supplier-responsibility)
* [Terms of Use](https://www.broadcom.com/company/legal/terms-of-use)
* [Site Map](https://www.broadcom.com/sitemap)



=== Content from docs.vmware.com_56dbc75b_20250115_124534.html ===


[![](/uicontent/images/VMware_by_Broadcom.png)Docs](/)

* ![](/uicontent/images/VMware_by_Broadcom_Gray-Black.png) Docs
  times
* [(current)](/)

* ![](/uicontent/images/nwMyLibrary.png)
  #####

  ![](/uicontent/images/icon-handshake.png)

  ![](/uicontent/images/icon-support.png)

  ![](/uicontent/images/icon-networking-bg.png)
  [VMware Communities](https://communities.vmware.com/)

  ![](/uicontent/images/icon-download-bg.png)

 This site will be decommissioned on January 30th 2025. After that date content will be available at [techdocs.broadcom.com](https://techdocs.broadcom.com/).

![](/uicontent/images/share-mylibrary.svg)

![](/uicontent/images/add-to-library-colorless.svg)

#

![](/uicontent/images/twitter.svg)
![](/uicontent/images/facebook.svg)
![](/uicontent/images/linkedin.svg)
![](/uicontent/images/weibo.svg)

![](/uicontent/images/pdf.svg)

![](/uicontent/images/feedback.svg)

![](/uicontent/images/edit.svg)

![](/uicontent/images/review.svg)

Twitter
Facebook
LinkedIn
微博

| VMware Cloud Foundation 4.3.1 | 21 SEP 2021 | Build 18624509  VMware Cloud Foundation 4.3.1.1 | 31 JAN 2022 | Build 19235535  Check for additions and updates to these release notes. |

## What's New

The VMware Cloud Foundation (VCF) 4.3.1 release includes the following:

* **Limited Support for in-place migration from VMware Cloud Foundation 3.10.1.2+ releases:** In addition to existing migration methods, customers can now engage the VMware Professional Service Organization (PSO) to perform an assessment for a potential in-place migration from VMware Cloud Foundation 3.10.1.2+ releases to VMware Cloud Foundation 4.3.1. Contact your sales and channel teams for guidance on choosing the best method for migrating your environment
* **Supportability and Serviceability (SoS) Utility APIs:** You can invoke VMware Cloud Foundation public APIs to run SoS Utility health checks and collect logs.
* **BOM updates**: Updated Bill of Materials with new product versions.

## VMware Cloud Foundation Bill of Materials (BOM)

The Cloud Foundation software product is comprised of the following software Bill-of-Materials (BOM). The components in the BOM are interoperable and compatible.

**VMware Response to Apache Log4j Remote Code Execution Vulnerability**: VMware Cloud Foundation is impacted by CVE-2021-44228, and CVE-2021-45046 as described in [VMSA-2021-0028](https://www.vmware.com/security/advisories/VMSA-2021-0028.html). To remediate these issues, see [Workaround instructions to address CVE-2021-44228 & CVE-2021-45046 in VMware Cloud Foundation (KB 87095)](https://kb.vmware.com/s/article/87095).

| Software Component | Version | Date | Build Number |
| --- | --- | --- | --- |
| Cloud Builder VM | 4.3.1 | 21 SEP 2021 | 18624509 |
| SDDC Manager | 4.3.1 | 21 SEP 2021 | 18624509 |
| VMware vCenter Server Appliance | 7.0 Update 2d | 21 SEP 2021 | 18455184 |
| VMware ESXi | 7.0 Update 2c | 24 AUG 2021 | 18426014 |
| VMware Virtual SAN Witness Appliance | 7.0 Update 2c | 24 AUG 2021 | 18426014 |
| VMware NSX-T Data Center | 3.1.3.1 | 26 AUG 2021 | 18504668 |
| VMware vRealize Suite Lifecycle Manager | 8.4.1 Patch 2 | 6 SEP 2021 | 18537943 |
| Workspace ONE Access | 3.3.5 | 20 MAY 2021 | 18049997 |
| vRealize Automation | 8.5 | 19 AUG 2021 | 18472703 |
| vRealize Log Insight | 8.4.1 | 15 JUN 2021 | 18136317 |
| vRealize Log Insight Content Pack for NSX-T | 4.0.2 | n/a | n/a |
| vRealize Log Insight Content Pack for vRealize Automation 8.3+ | 1.0 | n/a | n/a |
| vRealize Log Insight Content Pack for Linux | 2.1.0 | n/a | n/a |
| vRealize Log Insight Content Pack for Linux  - Systemd | 1.0.0 | n/a | n/a |
| vRealize Log Insight Content Pack for vRealize Suite Lifecycle Manager 8.0.1+ | 1.0.2 | n/a | n/a |
| vRealize Log Insight Content Pack for VMware Identity Manager | 2.0 | n/a | n/a |
| vRealize Operations Manager | 8.5 | 13 JUL 2021 | 18255622 |
| vRealize Operations Management Pack for VMware Identity Manager | 1.3 | n/a | n/a |

* VMware vSAN is included in the VMware ESXi bundle.
* You can use vRealize Suite Lifecycle Manager to deploy vRealize Automation, vRealize Operations Manager, vRealize Log Insight, and Workspace ONE Access.
* vRealize Log Insight content packs are installed when you deploy vRealize Log Insight.
* The vRealize Operations Manager management pack is installed when you deploy vRealize Operations Manager.
* VMware Solution Exchange and the vRealize Log Insight in-product marketplace store only the latest versions of the content packs for vRealize Log Insight. The Bill of Materials table contains the latest versions of the packs that were available at the time VMware Cloud Foundation is released. When you deploy the Cloud Foundation components, it is possible that the version of a content pack within the in-product marketplace for vRealize Log Insight is newer than the one used for this release.

## VMware Software Edition License Information

The SDDC Manager software is licensed under the Cloud Foundation license. As part of this product, the SDDC Manager software deploys specific VMware software products.

The following VMware software components deployed by SDDC Manager are licensed under the Cloud Foundation license:

* VMware ESXi
* VMware vSAN
* VMware NSX-T Data Center

The following VMware software components deployed by SDDC Manager are licensed separately:

* vCenter Server

**NOTE**: Only one vCenter Server license is required for all vCenter Servers deployed in a Cloud Foundation system.

For details about the specific VMware software editions that are licensed under the licenses you have purchased, see the Cloud Foundation Bill of Materials (BOM) section above.

For general information about the product, see [VMware Cloud Foundation](http://www.vmware.com/products/cloud-foundation.html).

## Supported Hardware

For details on supported configurations, see the [VMware Compatibility Guide (VCG)](https://www.vmware.com/resources/compatibility/search.php) and the Hardware Requirements section on the Prerequisite Checklist tab in the [Planning and Preparation Workbook](https://docs.vmware.com/en/VMware-Validated-Design/6.2/vmware-validated-design-62-vmware-cloud-foundation-42-sddc-planning-and-preparation-workbook.zip).

## Documentation

To access the Cloud Foundation documentation, go to the [VMware Cloud Foundation product documentation](https://docs.vmware.com/en/VMware-Cloud-Foundation/index.html).

To access the documentation for VMware software products that SDDC Manager can deploy, see the product documentation and use the drop-down menus on the page to choose the appropriate version:

* [VMware vSphere product documentation](https://docs.vmware.com/en/VMware-vSphere/index.html), also has documentation about ESXi and vCenter Server
* [VMware vSAN product documentation](https://docs.vmware.com/en/VMware-vSAN/index.html)
* [VMware NSX-T Data Center product documentation](https://docs.vmware.com/en/VMware-NSX-T/index.html)

## Browser Compatibility and Screen Resolutions

The Cloud Foundation web-based interface supports the latest two versions of the following web browsers except Internet Explorer:

* Google Chrome
* Mozilla Firefox
* Microsoft Edge
* Internet Explorer: Version 11

For the Web-based user interfaces, the supported standard resolution is 1024 by 768 pixels. For best results, use a screen resolution within these tested resolutions:

* 1024 by 768 pixels (standard)
* 1366 by 768 pixels
* 1280 by 1024 pixels
* 1680 by 1050 pixels

Resolutions below 1024 by 768, such as 640 by 960 or 480 by 800, are not supported.

## Installation and Upgrade Information

You can install VMware Cloud Foundation 4.3 as a new release or perform a sequential or skip-level upgrade to VMware Cloud Foundation 4.3.

**Installing as a New Release**

The new installation process has three phases:

**Phase One: Prepare the Environment**

The [Planning and Preparation Workbook](https://docs.vmware.com/en/VMware-Cloud-Foundation/4.3/vcf-planning-and-preparation-workbook.zip) provides detailed information about the software, tools, and external services that are required to implement a Software-Defined Data Center (SDDC) with VMware Cloud Foundation, using a standard architecture model.

**Phase Two: Image all servers with ESXi**

Image all servers with the ESXi version mentioned in the Cloud Foundation Bill of Materials (BOM) section. See the [VMware Cloud Foundation Deployment Guide](https://docs.vmware.com/en/VMware-Cloud-Foundation/4.3/vcf-deploy/GUID-F2DCF1B2-4EF6-444E-80BA-8F529A6D0725.html) for information on installing ESXi.

**Phase Three: Install Cloud Foundation 4.3**

See the [VMware Cloud Foundation Deployment Guide](https://docs.vmware.com/en/VMware-Cloud-Foundation/4.3/vcf-deploy/GUID-F2DCF1B2-4EF6-444E-80BA-8F529A6D0725.html) for information on deploying Cloud Foundation. Note that there is no ISO published for ESXi 7.0 Update 2c that is included in the VMware Cloud Foundation 4.3.1 BOM. For information on how to proceed, see[Create a Custom ISO Image for ESXi](https://docs.vmware.com/en/VMware-Cloud-Foundation/4.3/vcf-deploy/GUID-2674DA5A-8DF7-4212-A4A9-88CD798DC303.html).

**Upgrading to Cloud Foundation 4.3.1**

You can perform a sequential or skip-level upgrade to VMware Cloud Foundation 4.3.1 from VMware Cloud Foundation 4.3, 4.2.1, 4.2, 4.1.0.1, or 4.1. If your environment is at a version earlier than 4.1, you must upgrade the management domain and all VI workload domains to VMware Cloud Foundation 4.1 and then upgrade to VMware Cloud Foundation 4.3.1. For more information, see [VMware Cloud Foundation Lifecycle Management](https://docs.vmware.com/en/VMware-Cloud-Foundation/4.3/vcf-lifecycle/GUID-B384B08D-3652-45E2-8AA9-AF53066F5F70.html).

**IMPORTANT**: Before you upgrade a vCenter Server, take a file-based backup. See [Manually Back Up vCenter Server](https://docs.vmware.com/en/VMware-Cloud-Foundation/4.3/vcf-admin/GUID-E7D707F7-96C1-4228-8DDA-283345184AFC.html).

## VMware Cloud Foundation 4.3.1.1 Release Information

VMware Cloud Foundation 4.3.1.1 includes security fixes. You can perform a sequential or skip-level upgrade to VMware Cloud Foundation 4.3.1.1 from Cloud Foundation 4.3.1, 4.3, 4.2.1, 4.2, 4.1.0.1, or 4.1. If your environment is at a version earlier than 4.1, you must upgrade the management domain and all VI workload domains.

To upgrade the management domain, apply the following bundles, in order:

**NOTE**: Before triggering an upgrade to VCF 4.3.1.1, download all component upgrade/install bundles from VCF 4.3.1.0.

* VMware Cloud Foundation bundle.
* Configuration drift bundle.

**NOTE**: When you are upgrading from VMware Cloud Foundation from 4.3.1 to 4.3.1.1, no configuration drift bundle is required.

VMware Cloud Foundation 4.3.1.1 contains the following BOM updates:

| Software Component | Version | Date | Build Number |
| --- | --- | --- | --- |
| SDDC Manager | 4.3.1.1 | 31 JAN 2022 | 19235535 |

This release addresses the following issues in SDDC Manager:

* Apache Log4j Remote Code Execution Vulnerability: (CVE-2021-44228 and CVE-2021-45046) as described in [VMSA-2021-0028](https://www.vmware.com/security/advisories/VMSA-2021-0028.html).
* XML External Entity (XXE) Injection vulnerability as described in [CVE-2021-23463](https://nvd.nist.gov/vuln/detail/CVE-2021-23463)
* Credential logging vulnerability as described in [VMSA-2022-0003](https://www.vmware.com/security/advisories/VMSA-2022-0003.html). See [KB 87050](https://kb.vmware.com/s/article/87050) for more information.

## Resolved Issues

**The following issues are resolved in this release.**

* vRealize Operations Manager: VMware Security Advisory [VMSA-2021-0018](https://www.vmware.com/security/advisories/VMSA-2021-0018.html)
* Generate CSR task for a component hangs
* Supportability and Serviceability (SoS) Utility health checks fail with the error "Failed to get details"
* Update precheck fails with the error "Password has expired"
* vCenter Server: VMware Security Advisory [VMSA-2021-0020](https://www.vmware.com/security/advisories/VMSA-2021-0020.html) resolves resolves CVE-2021-22011 and CVE-2021-22018

## Known Issues

* [VMware Cloud Foundation Known Issues](#Known Issues- VMware Cloud Foundation Known Issues)
* [Upgrade Known Issues](#Known Issues- Upgrade Known Issues)
* [Bring-up Known Issues](#Known Issues- Bring-up Known Issues)
* [Workload Domain Known Issues](#Known Issues- Workload Domain Known Issues)
* [SDDC Manager Known Issues](#Known Issues- SDDC Manager Known Issues)
* [Multi-Instance Management Known Issues](#Known Issues-Multi-Instance Management Known Issues)
* [API Known Issues](#Known Issues-API Known Issues)
* [vRealize Suite Known Issues](#Known Issues- vRealize Suite Known Issues )

### VMware Cloud Foundation Known Issues

* **Stretched clusters and Workload Management**

  You cannot stretch a cluster on which Workload Management is deployed.

  Workaround: None.
* **NSX-T Guest Introspection (GI) and NSX-T Service Insertion (SI) are not supported on stretched clusters**

  There is no support for stretching clusters where NSX-T Guest Introspection (GI) or NSX-T Service Insertion (SI) are enabled. VMware Cloud Foundation detaches Transport Node Profiles from AZ2 hosts to allow AZ-specific network configurations. NSX-T GI and NSX-T SI require that the same Transport Node Profile be attached to all hosts in the cluster.

  Workaround: None

### Upgrade Known Issues

* **New -** **VxRail Async patch 7.0.410 bundle visible in the Lifecycle Manager (LCM) bundle management UI page and availability status as "future"**

  If you connect to the VMware Depot, the VxRail async patch bundle 7.0.410 might be visible in the Lifecycle Manager(LCM) UI. This is a known issue caused by the async patch bundle information being added to the existing partner bundle metadata (PBM) file. However, this issue has been resolved in the latest PBM, which has already been published. If you are still seeing this bundle in the LCM UI I and have not used the Async Patch Tool to enable this patch, you can follow the workaround to remove the patch. After completing the workaround, the bundle will no longer be displayed in the LCM UI.

  Workaround: Perform the bundle cleanup of VxRail async patch 7.0.410 by following the steps in **[KB 75050](https://kb.vmware.com/s/article/75050)**.
* **Network outage on NSX-T Data Center after upgrading VDS to 7.0.2**

  After upgrading NSX-T Data Center, if you upgrade the vSphere Distributed Switch (VDS) to version 7.0.2, you may experience network traffic disruption.

  Workaround: Do not upgrade vSphere Distributed Switches to version 7.0.2.
* **Async Patch Tool Known Issues**

  The Async Patch Tool is a utility that allows you to apply critical patches to certain VMware Cloud Foundation components (NSX-T Manager, vCenter Server, and ESXi) outside of VMware Cloud Foundation releases. The Async Patch Tool also allows you to enable upgrade of an async patched system to a new version of VMware Cloud Foundation.

  See the [Async Patch Tool Release Notes](https://docs.vmware.com/en/VMware-Cloud-Foundation/services/rn/async-patch-tool-release-notes/index.html) for known issues.
* **NSX-T upgrade causing host PSOD**

  ESXi host can PSOD during NSX-T upgrade when there is a mass migration of DFW filters, where flows are being revalidated while configuration cycle is occurring.

  See KB [87803](https://kb.vmware.com/s/article/87803) for more information. This issue is fixed in NSX-T 3.1.3.7.

  If a VCF upgrade is tried post application of this workaround, the LCM pre-check on DRS configuration will fail. This is expected behavior.
* **Cluster-level ESXi upgrade fails**

  Cluster-level selection during upgrade does not consider the health status of the clusters and may show a cluster's status as **Available**, even for a faulty cluster. If you select a faulty cluster, the upgrade fails.

  Workaround: Always perform an update precheck to validate the health status of the clusters. Resolve any issues before upgrading.
* **You are unable to update NSX-T Data Center in the management domain or in a workload domain with vSAN principal storage because of an error during the NSX-T transport node precheck stage.**

  In SDDC Manager, when you run the upgrade precheck before updating NSX-T Data Center, the NSX-T transport node validation results with the following error.

  `No coredump target has been configured. Host core dumps cannot be saved.:System logs on host sfo01-m01-esx04.sfo.rainpole.io are stored on non-persistent storage. Consult product documentation to configure a syslog server or a scratch partition.`

  Because the upgrade precheck results with an error, you cannot proceed with updating the NSX-T Data Center instance in the domain. VMware Validated Design supports vSAN as the principal storage in the management domain. However, vSAN datastores do no support scratch partitions. See KB article [2074026](https://kb.vmware.com/s/article/2074026).

  Workaround: Deactivate the update precheck validation for the subsequent NSX-T Data Center update.

  1. Log in to SDDC Manager as **vcf** using a Secure Shell (SSH) client.
  2. Open the `application-prod.properties` file for editing.

     `vi /opt/vmware/vcf/lcm/lcm-app/conf/application-prod.properties`
  3. Add the following property and save the file.

     `lcm.nsxt.suppress.prechecks=true`
  4. Restart the life cycle management service.

     `systemctl restart lcm`
  5. Log in to the SDDC Manager user interface and proceed with the update of NSX-T Data Center.
* **NSX-T upgrade may fail at the step NSX T TRANSPORT NODE POSTCHECK STAGE**

  NSX-T upgrade may not proceed beyond the `NSX T TRANSPORT NODE POSTCHECK STAGE`

  Workaround: Contact VMware support.
* **ESXi upgrade fails with the error "Incompatible patch or upgrade files. Please verify that the patch file is compatible with the host. Refer LCM and VUM log file."**

  This error occurs if any of the ESXi hosts that you are upgrading have detached storage devices.

  Workaround: Attach all storage devices to the ESXi hosts being upgraded, reboot the hosts, and retry the upgrade.
* **Skip level upgrades are not enabled for some product components after VMware Cloud Foundation is upgraded to 4.3**

  After performing skip level upgrade to VMware Cloud Foundation 4.3 from 4.1.x or 4.2.x, one or more of the following symptoms is observed:

  + vRealize bundles do not show up as available for upgrade
  + Bundles for previous versions of some product components (NSX-T Data Center, vCenter Server, ESXi) show up as available for upgrade

  See [KB 85505](http://kb.vmware.com/s/article/85505).
* **Domain prechecks for vRealize Suite products show incorrect health state**

  Domain pre-checks for vRealize Suite products may be wrongly marked as RED or GREEN. The issue only happens if pre-checks for some vRealize Suite products fail and then the precheck is retried at the resource level instead of the domain level.

  If the precheck fails, do not click the **Retry Precheck** option on the failed resources. Instead, run the precheck again on the entire workload domain. If the precheck is already retried on the failed resource, a restart of lcm service is required in order to clear up the cache.
* **vRealize Suite upgrade bundles have an incorrect description**

  vRealize Suite upgrade bundles are incorrectly described as install bundles. You can ignore the incorrect description and proceed with the upgrade.

  None.
* **vRealize Operations Manager upgrade fails on the step VREALIZE\_UPGRADE\_PREPARE\_BACKUP with the error: Waiting for vRealize Operations cluster to change state timed out**

  When upgrading vRealize Operations Manager, SDDC Manager takes the vRealize Operations Manager cluster offline and takes snapshots of the vRealize Operations Manager virtual machines. In some circumstances, taking the cluster offline takes a long time and the operation times out.

  Workaround: Take the vRealize Operations Manager cluster back online and retry the upgrade.

  1. Log in to the vRealize Operations Manager Administration UI (https://<vrops\_ip>/admin) using the admin credentials.
  2. If the cluster status is offline, in the Cluster Status section click Take Cluster Online. Wait for the cluster to initialize and be marked as green.
  3. In the SDDC Manager UI, the option to retry vRealize Operations Manager upgrade should be available. Retry the upgrade.

  If the upgrade continues to fail, take the snapshots manually and retry the upgrade. Since the snapshots already exist, SDDC Manager will skip that step and proceed with the upgrade.

  1. Log in to the vRealize Operations Manager Administration UI (https://<vrops\_ip>/admin) using the admin credentials. Ensure that that the vRealize Operations Manager Cluster Status is offline. If it is online, click Take Cluster Offline in the Cluster Status section. Wait for the cluster to be marked as offline.
  2. Log in to the management domain vCenter Server using the vSphere Client.
  3. Navigate to the vRealize Operations Manager virtual machines and create a snapshot for each virtual machine in the vRealize Operations Manager cluster. Use the following prefix "vROPS\_LCM\_UPGRADE\_MANUAL\_BACKUP" for the snapshots. Please note that the prefix should match the letter casing.
  4. After the snapshots are done, log in to the vRealize Operations Manager UI and take cluster online. Wait for the cluster initialization.
  5. In the SDDC Manager UI, the option to retry vRealize Operations Manager upgrade should be available. Retry the upgrade.

### Bring-up Known Issues

* **Bringup fails when creating NSX-T Data Center transport nodes**

  The bringup task "Create NSX-T Data Center Transport Nodes from Discovered Nodes" might fail if there's an ESXi host in the management cluster which is pending a reboot.

  Workaround: Reboot all ESXi hosts that are pending reboot and retry bringup.
* **The Cloud Foundation Builder VM remains locked after more than 15 minutes**

  The VMware Imaging Appliance (VIA) locks out the admin user after three unsuccessful login attempts. Normally, the lockout is reset after fifteen minutes but the underlying Cloud Foundation Builder VM does not automatically reset.

  Workaround: Log in to the VM console of the Cloud Foundation Builder VM as the **root** user. Unlock the account by resetting the password of the admin user with the following command.

  `pam_tally2 --user=<user> --reset`

### Workload Domain Known Issues

* **Cannot reuse a static IP pool that includes special characters in its name**

  If you chose Static IP Pool as the IP allocation method when creating a VI workload domain and you used special characters or spaces in the IP pool name, you are not able to reuse the IP pool when creating a new VI workload domain or adding a vSphere cluster to the workload domain.

  Workaround: Use only supported characters when naming a static IP pool. Supported characters:

  + a-z
  + A-Z
  + 0-9
  + - and \_
  + No spaces

  If you have an existing static IP pool that includes unsupported characters in its name, you can use the NSX Manager UI to rename it.
* **Unable to remove host from vSphere cluster in workload domain**

  The remove host workflow fails at the `Delete Transport Nodes` task. The NSX-T Data Center UI shows the deletion process stuck.

  1. Remove the host by force from the NSX-T Data Center UI.
  2. Restart the failed remove host workflow in SDDC Manager.
* **vCenter Server overwrites the NFS datastore name when adding a cluster to a VI workload domain**

  If you add an NFS datastore with the same NFS server IP address, but a different NFS datastore name, as an NFS datastore that already exists in the workload domain, then vCenter Server applies the existing datastore name to the new datastore.

  Workaround: If you want to add an NFS datastore with a different datastore name, then it must use a different NFS server IP address.
* **Removing a host from a cluster, deleting a cluster from a workload domain, or deleting a workload domain fails if Service VMs (SVMs) are present**

  If you deployed an endpoint protection service (such as guest introspection) to a cluster through NSX-T Data Center, then removing a host from the cluster, deleting the cluster, or deleting the workload domain containing the cluster will fail on the subtask `Enter Maintenance Mode on ESXi Hosts`.

  Workaround:

  + For host removal: Delete the Service VM from the host and retry the operation.
  + For cluster deletion: Delete the service deployment for the cluster and retry the operation.
  + For workload domain deleting: Delete the service deployment for all clusters in the workload domain and retry the operation.
* **Creation or expansion of a vSAN cluster with more than 32 hosts fails**

  By default, a vSAN cluster can grow up to 32 hosts. With large cluster support enabled, a vSAN cluster can grow up to a maximum of 64 hosts. However, even with large cluster support enabled, a creation or expansion task can fail on the sub-task `Enable vSAN on vSphere Cluster`.

  Workaround:

  1. Enable Large Cluster Support for the vSAN cluster in the vSphere Client. If it is already enabled skip to step 2.

     1. Select the vSAN cluster in the vSphere Client.
     2. Select **Configure > vSAN > Advanced Options**.
     3. Enable Large Cluster Support.
     4. Click **Apply**.
     5. Click **Yes**.
  2. Run a vSAN health check to see which hosts require rebooting.
  3. Put the hosts into Maintenance Mode and reboot the hosts.

  For more information about large cluster support, see [KB 2110081](https://kb.vmware.com/kb/2110081).
* **The vSAN Performance Service is not enabled for vSAN clusters when CEIP is not enabled**

  If you do not enable the VMware Customer Experience Improvement Program (CEIP) in SDDC Manager, when you create a workload domain or add a vSphere cluster to a workload domain, the vSAN Performance Service is not enabled for vSAN clusters. When CEIP is enabled, data from the vSAN Performance Service is provided to VMware and this data is used to aid VMware Support with troubleshooting and for products such as VMware Skyline, a proactive cloud monitoring service. See [Customer Experience Improvement Program](https://www.vmware.com/solutions/trustvmware/ceip.html) for more information on the data collected by CEIP.

  Workaround: Enable CEIP in SDDC Manager. See the [VMware Cloud Foundation Documentation](https://docs.vmware.com/en/VMware-Cloud-Foundation/index.html). After CEIP is enabled, a scheduled task that enables the vSAN Performance Service on existing clusters in workload domains runs every three hours. The service is also enabled for new workload domains and clusters. To enable the vSAN Performance Service immediately, see the [VMware vSphere Documentation](https://docs.vmware.com/en/VMware-vSphere/index.html).
* **Adding a vSphere cluster or adding a host to a workload domain fails**

  Under certain circumstances, adding a host or vSphere cluster to a workload domain fails at the `Configure NSX-T Transport Node` or `Create Transport Node Collection` subtask.

  Workaround:

  1. Enable SSH for the NSX Manager VMs.
  2. SSH into the NSX Manager VMs as `admin` and then log in as `root`.
  3. Run the following command on each NSX Manager VM:

     `sysctl -w net.ipv4.tcp_en=0`
  4. Login to NSX Manager UI for the workload domain.
  5. Navigate to **System > Fabric > Nodes > Host Transport Nodes**.
  6. Select the vCenter server for the workload domain from the **Managed by** drop-down menu.
  7. Expand the vSphere cluster and navigate to the transport nodes that are in a `partial success` state.
  8. Select the check box next to a `partial success`node, click **Configure NSX**.
  9. Click `Next` and then click `Apply`.
  10. Repeat steps 7-9 for each `partial success` node.

  When all host issues are resolved, transport node creation starts for the failed nodes. When all hosts are successfully created as transport nodes, retry the failed add vSphere cluster or add host task from the SDDC Manager UI.
* **vSAN partition and critical alerts are generated when the witness MTU is not set to 9000**

  If the MTU of the witness switch in the witness appliance is not set to 9000, the vSAN stretch cluster partition may occur.

  Workaround: Set the MTU of the witness switch in the witness appliance to 9000 MTU.
* **If the witness ESXi version does not match with the host ESXi version in the cluster, vSAN cluster partition may occur**

  vSAN stretch cluster workflow does not check the ESXi version of the witness host. If the witness ESXi version does not match the host version in the cluster, then vSAN cluster partition may happen.

  Workaround:

  1. Upgrade the witness host manually with the matching ESXi version using the vCenter VUM functionality.
  2. Replace or deploy the witness appliance matching with the ESXi version.
* **Deploying partner services on a workload domain displays an error**

  Deploying partner services, such as McAfee or Trend, on a workload domain enabled for vSphere Lifecycle Manager (vLCM) baselines, displays the “Configure NSX at cluster level to deploy Service VM” error.

  Workaround: Attach the transport node profile to the cluster and try deploying the partner service. After the service is deployed, keep the transport node profile attached to the cluster. If you want to delete the cluster later, you must first undeploy the partner service and detach the transport node profile from the cluster.
* **Adding host fails when host is on a different VLAN**

  A host add operation can sometimes fail if the host is on a different VLAN.

  Workaround:

  1. Before adding the host, add a new portgroup to the vSphere Distributed Switch for that cluster.
  2. Tag the new portgroup with the VLAN ID of the host to be added.
  3. Add the Host. This workflow fails at the "Migrate host vmknics to dvs" operation.
  4. Locate the failed host in vCenter, and migrate the vmk0 of the host to the new portgroup you created in step 1.

     For more information, see [Migrate VMkernel Adapters to a vSphere Distributed Switch](http://docs.vmware.com/en/VMware-vSphere/7.0/com.vmware.vsphere.networking.doc/GUID-0DE237B8-1938-4DE9-90EC-718E345B56A0.html) in the vSphere product documentation.
  5. Retry the Add Host operation.

  **NOTE**: If you later remove this host in the future, you must manually remove the portgroup as well if it is not being used by any other host.

### SDDC Manager Known Issues

* **Rotating or updating vSphere Single-Sign On (PSC) password can cause issues**

  If you have multiple VMware Cloud Foundation instances that share a single SSO domain, rotating or updating the vSphere SSO password for the first VCF instance causes the second VCF instance to become inaccessible.

  Workaround: See [KB 85485](https://kb.vmware.com/s/article/85485).
* **SDDC Manager UI does not load correctly**

  If you log in to the SDDC Manager UI using an Active Directory user name that includes a space, the UI does not load correctly.

  Workaround: None
* **SoS utility options for health check are missing information**

  Due to limitations of the ESXi service account, some information is unavailable in the following health check options:

  + `--hardware-compatibility-report`: No `Devices and Driver`information for ESXi hosts.
  + `--storage-health`: No `vSAN Health Status` or `Total no. of disks` information for ESXi hosts.

  Workaround: None.
* **Deactivating CEIP on SDDC Manager does not deactive CEIP on vRealize Automation and vRealize Suite Lifecycle Manager**

  When you deactive CEIP on the SDDC Manager Dashboard, data collection is not deactivated on vRealize Automation and vRealize Suite Lifecycle Manager. This is because of API deprecation in vRealize Suite 8.x.

  Workaround: Manually deactivate CEIP in vRealize Automation and vRealize Suite Lifecycle Manager. For more information, see VMware vRealize Automation Documentation and VMware vRealize Suite Lifecycle Manager Documentation.

### Multi-Instance Management Known Issues

* **Join operation fails**

  A join operation may fail if a controller SDDC Manager has a public certificate with a depth greater than one (that is, it has intermediate certificates).

  Workaround: Trust the intermediate certificate of the controller SDDC Manager. See [KB 80986](https://kb.vmware.com/s/article/80986).
* **Multi-Instance Management Dashboard operation fails**

  After a controller joins or leaves a federation, Kafka is restarted on all controllers in the federation. It can take up to 20 minutes for the federation to stabilize. Any operations performed on the dashboard during this time may fail.

  Workaround: Re-try the operation.
* **Federation creation information not displayed if you leave the Multi-Instance Management Dashboard**

  Federation creation progress is displayed on the Multi-Instance Management Dashboard. If you navigate to another screen and then return to the Multi-Instance Management Dashboard, progress messages are not displayed. Instead, an empty map with no Cloud Foundation instances are displayed until the federation is created.

  Workaround: Stay on the Multi-Instance Dashboard till the task is complete. If you have navigated away, wait for around 20 minutes and then return to the dashboard by which time the operation should have completed.

### API Known Issues

* **Updating DNS/NTP server does not apply the update to all NSX Managers**

  If you update the NTP or DNS server information for a VMware Cloud Foundation instance that includes more than one NSX Manager, only one of the NSX Managers gets updated with the new information.

  Workaround: Use the NSX Manager API or CLI to manually update the DNS/NTP server information for the remaining NSX Manager(s).
* **Unable to download SoS bundles from SDDC Manager API Explorer**

  In the SDDC Manager UI, you are not able to download the SoS support bundle or SoS health summary bundle using the commands under **Developer Center > API Explorer**.

  + `GET "/v1/system/support-bundles/{id}/data`
  + `GET "/v1/system/health-summary/{id}/data"`

  Use an alternative client such as curl to download the bundle. Example of downloading support bundle using curl**:**

  `$ curl --insecure -X GET https://10.0.0.4/v1/system/support-bundles/$ID/data -H "Content-Type: application/octet-stream" -H "Authorization: Bearer $TOKEN" -o sos-.tar % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 6961k 100 6961k 0 0 41.0M 0 --:--:-- --:--:-- --:--:-- 41.2M`

  Example of downloading health summary bundle using curl:

  `$ curl --insecure -X GET https://10.0.0.4/v1/system/health-summary/$ID/data -H "Content-Type: application/octet-stream" -H "Authorization: Bearer $TOKEN" -o healthcheck-.tar % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 981k 100 981k 0 0 7778k 0 --:--:-- --:--:-- --:--:-- 7789k`
* **The VMware Cloud Foundation API ignores NSX VDS uplink information for in-cluster expansion of an NSX Edge cluster**

  When you use the VMware Cloud Foundation API to expand an NSX Edge cluster and the new NSX Edge node is going to be hosted on the same vSphere cluster as the existing NSX Edge nodes (in-cluster), the `edgeClusterExpansionSpec` ignores any information you provide for `firstNsxVdsUplink` and `secondNsxVdsUplink`.

  Workaround: None. This is by design. For in-cluster expansions, new NSX Edge nodes use the same NSX VDS uplinks as the existing NSX Edge nodes in the NSX Edge cluster.
* **Stretch cluster operation fails**

  If the cluster that you are stretching does not include a powered-on VM with an operating system installed, the operation fails at the "Validate Cluster for Zero VMs" task.

  Workaround: Make sure the cluster has a powered-on VM with an operating system installed before stretching the cluster.

### vRealize Suite Known Issues

* **vRealize Operations Manager: VMware Security Advisory VMSA-2021-0018**

  [VMSA-2021-0018](http://www.vmware.com/security/advisories/VMSA-2021-0018.html) describes security vulnerabilities that affect VMware Cloud Foundation.

  Workaround: See [KB 85452](http://kb.vmware.com/s/article/85452) for information about applying vRealize Operations Security Patches that resolve the issues.
* **Updating the DNS or NTP server configuration does not apply the update to vRealize Automation**

  Using the Cloud Foundation API to update the DNS or NTP servers does not apply the update to vRealize Automation due to a bug in vRealize Suite Lifecycle Manager.

  Workaround: Manually update the DNS or NTP server(s) for vRealize Automation.

  **Update the DNS server(s) for vRealize Automation**

  1. SSH to the first vRealize Automation node using root credentials.
  2. Delete the current DNS server using the following command:

     `sed '/nameserver.*/d' -i /etc/resolv.conf`
  3. Add the new DNS server IP with following command:

     `echo nameserver [DNS server IP] >> /etc/resolv.conf`
  4. Repeat this command if there are multiple DNS servers.
  5. Validate the update with the following command:

     `cat /etc/resolv.conf`
  6. Repeat these steps for each vRealize Automation node.

  **Update the NTP server(s) for vRealize Automation**

  1. SSH to the first vRealize Automation node using root credentials.
  2. Run the following command to specify the new NTP server:

     `vracli ntp systemd --set [NTP server IP]`

     To add multiple NTP servers:

     `vracli ntp systemd --set [NTP server 1 IP,NTP server 2 IP]`
  3. Validate the update with the following command:

     `vracli ntp show-config`
  4. Apply the update to all vRealize Automation nodes with the following command:

     `vracli ntp apply`
  5. Validate the update by running the following command on each vRealize Automation node:

     `vracli ntp show-config`
* **Connecting vRealize Operations Manager to a workload domain fails at the "Create vCenter Server Adapter in vRealize Operations Manager for the Workload Domain" step**

  When you connect vRealize Operations Manager to a workload domain, it fails at the `Create vCenter Server Adapter in vRealize Operations Manager for the Workload Domain` step with a message similar to `Failed to configure vCenter <vcenter-hostname> in vROps <vrops-hostname>, because Failed to manage vROps adapter`. This issue can occur when the vRealize Operations cluster is offline.

  Workaround: Make sure that the vRealize Operations cluster is online.

  1. Log in to the vRealize Operations Manager administration interface.
  2. Click **Administration > Cluster Management** and check the cluster status.
  3. If the vRealize Operations cluster is offline, bring the cluster online.
  4. When the cluster status displays as online, retry connecting vRealize Operations Manager to a workload domain.
* **vRealize Operations Management Pack for VMware Identity Manager is not installed**

  If you install vRealize Operations Manager before you install Workspace ONE Access, then the vRealize Operations Management Pack for VMware Identity Manager is not installed.

  Workaround:

  1. Log in to the vRealize Suite Lifecycle Manager appliance.
  2. Click **VMware Marketplace**.
  3. Enter "Identity Manager" in the **Search** text box.
  4. Download and install the vRealize Operations Management Pack for VMware Identity Manager.
  5. Log in to vRealize Operations Manager.
  6. On the main navigation bar, click **Administration**.
  7. In the left pane, select **Solutions > Other accounts**.
  8. Click **Add account**.
  9. On the **Account types** page, click **VMware Identity Manager adapter**.
  10. Configure the settings, choosing the default collector group.
  11. In the **Connection information** section, click the **Add** icon.
  12. In the **Manage credential** dialog box, configure the Workspace ONE Access credentials and click **OK**.
  13. On the **New account** page, click **Validate** connection.
  14. In the **Info** dialog box, click **OK**.
  15. Click **Add**.
  16. On the **Other accounts** page, verify that the collection status of the adapter is **OK**.
* **Deploying a second vRealize Suite Lifecycle Manager fails**

  If you have multiple instances of VMware Cloud Foundation in the same SSO domain and you try to deploy vRealize Lifecycle Manager on both, the second deployment will fail with the message `Add vCenter Server and Data Center to vRealize Suite Lifecycle Manager Failed`.

  Workaround: Use a single vRealize Suite Lifecycle Manager to manage instances of VMware Cloud Foundation in the same SSO domain.
* **vRealize Suite Lifecycle Manager reports a "FAILED" inventory sync**

  After rotating a vCenter Server service account password in SDDC Manager, the inventory sync may fail for vRealize Suite environments managed by VMware Cloud Foundation.

  Workaround: Log in to vRealize Suite Lifecycle Manager to identify and troubleshoot the failed environment(s).

check-circle-line

exclamation-circle-line

close-line

![Scroll to top icon](/uicontent/images/scroll_top.svg)

![](/uicontent/images/feedback.svg)

             [![VMware](https://www.vmware.com/content/dam/vmwaredesigns/scrapercontent/resources/logos/vmware-logo-grey.svg "VMware")](https://www.vmware.com/)

Resources

* [Blogs](https://blogs.vmware.com/)
* [Careers](https://www.broadcom.com/jobs)
* [Communities](https://community.broadcom.com/)
* [Customer Stories](https://www.vmware.com/resources/customers)

* [News and Stories](https://news.broadcom.com/)
* [Topics](https://www.vmware.com/topics/)
* [Trust Center](https://www.vmware.com/info/trust-center/)

Support

* [Broadcom Support](https://support.broadcom.com/)
* [Documentation](https://docs.vmware.com)
* [Hands-On Labs](https://www.vmware.com/resources/hands-on-labs)
* [Licensing](https://www.broadcom.com/licensing)

* [Twitter](https://twitter.com/VMware)
* [YouTube](https://www.youtube.com/user/vmwaretv)
* [Facebook](https://www.facebook.com/vmware)
* [LinkedIn](https://www.linkedin.com/company/vmware/mycompany/)
* [Contact Sales](https://go-vmware.broadcom.com/contact-us)

---

  Copyright © 2005-2024 Broadcom. All Rights Reserved. The term "Broadcom" refers to Broadcom Inc. and/or its subsidiaries. [Accessibility](https://www.broadcom.com/company/legal/accessibility "Accessibility") [Privacy](https://www.broadcom.com/privacy "Privacy") [Supplier Responsibility](https://www.broadcom.com/company/citizenship/supplier-responsibility "Supplier Responsibility") [Terms Of Use](https://www.broadcom.com/company/legal/terms-of-use "Terms Of Use")

#####

×

Share on Social Media?

×

#####

×

exclamation-circle-line

check-circle-line

 :

#####

×

exclamation-circle-line

|

check-circle-line

exclamation-circle-line

x

#####

×

"
"?

×



=== Content from docs.vmware.com_c3358707_20250115_195932.html ===


[![](/uicontent/images/VMware_by_Broadcom.png)Docs](/)

* ![](/uicontent/images/VMware_by_Broadcom_Gray-Black.png) Docs
  times
* [(current)](/)

* ![](/uicontent/images/nwMyLibrary.png)
  #####

  ![](/uicontent/images/icon-handshake.png)

  ![](/uicontent/images/icon-support.png)

  ![](/uicontent/images/icon-networking-bg.png)
  [VMware Communities](https://communities.vmware.com/)

  ![](/uicontent/images/icon-download-bg.png)

 This site will be decommissioned on January 30th 2025. After that date content will be available at [techdocs.broadcom.com](https://techdocs.broadcom.com/).

![](/uicontent/images/share-mylibrary.svg)

![](/uicontent/images/add-to-library-colorless.svg)

#

![](/uicontent/images/twitter.svg)
![](/uicontent/images/facebook.svg)
![](/uicontent/images/linkedin.svg)
![](/uicontent/images/weibo.svg)

![](/uicontent/images/pdf.svg)

![](/uicontent/images/feedback.svg)

![](/uicontent/images/edit.svg)

![](/uicontent/images/review.svg)

Twitter
Facebook
LinkedIn
微博

| VMware Cloud Foundation 3.11 | 14 FEB 2021 | Build 19312783  VMware Cloud Foundation 3.11.0.1 | 07 APR 2022 | Build 19571759  Check for additions and updates to these release notes. |

## What's New

VMware Cloud Foundation 3.11 can either be upgraded from VMware Cloud Foundation 3.10.2.2 (sequential upgrade) or from VMware Cloud Foundation 3.5 or later (skip-level upgrade). It cannot be deployed as a new release. For more information, see **Upgrade Information** below.

The VMware Cloud Foundation (VCF) 3.11 release includes the following:

* **Security fixes for Apache Log4j Remote Code Execution Vulnerability**: This release fixes CVE-2021-44228 and CVE-2021-45046. See [VMSA-2021-0028](https://www.vmware.com/security/advisories/VMSA-2021-0028.html).
* **Security fixes for Apache HTTP Server**: This release fixes CVE-2021-40438. See [CVE-2021-40438](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-40438).
* **Improvements to upgrade prechecks**: Upgrade prechecks have been expanded to verify filesystem capacity, file permissions, and passwords. These improved prechecks help identify issues that you need to resolve to ensure a smooth upgrade.
* **Skip-level upgrade to VMware Cloud Foundation 3.11:** Upgrade directly to VMware Cloud Foundation 3.11 using the skip-level upgrade CLI tool, which has been updated with additional guardrails, prechecks, and usability improvements.
* **Scaling improvements**: VMware Cloud Foundation 3.11 supports up to 1000 ESXi hosts per SDDC Manager instance. See [VMware Configuration Maximums](https://configmax.vmware.com/home) for details on all supported maximums.
* **BOM Updates**: Updated Bill of Materials with new product versions.

## VMware Cloud Foundation Bill of Materials (BOM)

The Cloud Foundation software product is comprised of the following software Bill-of-Materials (BOM). The components in the BOM are interoperable and compatible.

| Software Component | Version | Date | Build Number |
| --- | --- | --- | --- |
| SDDC Manager | 3.11 | 14 FEB 2022 | 19312783 |
| VMware vCenter Server Appliance | 6.7 Update 3q | 08 FEB 2022 | 19300125 |
| VMware ESXi | ESXi670-202201001 | 25 JAN 2022 | 19195723 |
| VMware NSX Data Center for vSphere | 6.4.12 | 21 DEC 2021 | 19066632 |
| VMware NSX-T Data Center | 3.0.3.1 | 23 DEC 2021 | 19067109 |
| VMware vRealize Suite Lifecycle Manager | 2.1 Patch 3 | 12 JAN 2022 | 19201324 |
| VMware vRealize Log Insight | 4.8 | 11 APR 2019 | 13036238 |
| vRealize Log Insight Content Pack for NSX for vSphere | 3.9 | n/a | n/a |
| vRealize Log Insight Content Pack for Linux | 2.0.1 | n/a | n/a |
| vRealize Log Insight Content Pack for vRealize Automation 7.5+ | 1.0 | n/a | n/a |
| vRealize Log Insight Content Pack for vRealize Orchestrator 7.0.1+ | 2.1 | n/a | n/a |
| vRealize Log insight Content Pack for NSX-T | 3.8.2 | n/a | n/a |
| vSAN Content Pack for Log Insight | 2.2 | n/a | n/a |
| vRealize Operations Manager | 7.5 | 11 APR 2019 | 13165949 |
| vRealize Automation | 7.6 | 11 APR 2019 | 13027280 |
| VMware Horizon 7 | 7.10.3 | 17 DEC 2021 | 19069415 |

Note:

* vRealize Log Insight Content Packs are deployed during the workload domain creation.
* VMware Solution Exchange and the vRealize Log Insight in-product marketplace store only the latest versions of the content packs for vRealize Log Insight. The Bill of Materials table contains the latest versions of the packs that were available at the time VMware Cloud Foundation is released. When you deploy the Cloud Foundation components, it is possible that the version of a content pack within the in-product marketplace for vRealize Log Insight is newer than the one used for this release.
* To remediate [VMSA-2020-0007](https://www.vmware.com/security/advisories/VMSA-2020-0007.html) (CVE-2020-3953 and CVE-2020-3954) for vRealize Log Insight 4.8, you must apply the vRealize Log Insight 4.8 security patch. For information on the security patch, see [KB article 79168](https://kb.vmware.com/s/article/79168). ​

## VMware Software Edition License Information

The SDDC Manager software is licensed under the Cloud Foundation license. As part of this product, the SDDC Manager software deploys specific VMware software products.

The following VMware software components deployed by SDDC Manager are licensed under the Cloud Foundation license:

* VMware ESXi
* VMware vSAN
* VMware NSX Data Center for vSphere

The following VMware software components deployed by SDDC Manager are licensed separately:

* VMware vCenter Server

* **NOTE** Only one vCenter Server license is required for all vCenter Servers deployed in a Cloud Foundation system.

* VMware NSX-T
* VMware Horizon 7
* VMware vRealize Automation
* VMware vRealize Operations
* VMware vRealize Log Insight and content packs

* **NOTE** Cloud Foundation permits limited use of vRealize Log Insight for the management domain without the purchase of a vRealize Log Insight license.

For details about the specific VMware software editions that are licensed under the licenses you have purchased, see the **Cloud Foundation Bill of Materials (BOM)** section above.

For general information about the product, see [VMware Cloud Foundation](http://www.vmware.com/products/cloud-foundation.html).

## Supported Hardware

For details on supported configurations, see the [VMware Compatibility Guide (VCG)](https://www.vmware.com/resources/compatibility/search.php) and the Hardware Requirements section on the Prerequisite Checklist tab in the [Planning and Preparation Workbook](https://docs.vmware.com/en/VMware-Validated-Design/6.2/vmware-validated-design-62-vmware-cloud-foundation-42-sddc-planning-and-preparation-workbook.zip).

## Documentation

To access the Cloud Foundation documentation, go to the [VMware Cloud Foundation product documentation](https://docs.vmware.com/en/VMware-Cloud-Foundation/index.html).

To access the documentation for VMware software products that SDDC Manager can deploy, see the product documentation and use the drop-down menus on the page to choose the appropriate version:

* [VMware vSphere product documentation](https://docs.vmware.com/en/VMware-vSphere/index.html), also has documentation about ESXi and vCenter Server
* [VMware vSAN product documentation](https://docs.vmware.com/en/VMware-vSAN/index.html)
* [VMware NSX Data Center for vSphere product documentation](https://docs.vmware.com/en/VMware-NSX-for-vSphere/index.html)
* [VMware NSX-T Data Center product documentation](https://docs.vmware.com/en/VMware-NSX-T/index.html)
* [VMware vRealize Log Insight product documentation](https://docs.vmware.com/en/vRealize-Log-Insight/index.html)
* [VMware vRealize Automation product documentation](https://docs.vmware.com/en/vRealize-Automation/index.html)
* [VMware vRealize Operations Manager documentation](https://docs.vmware.com/en/vRealize-Operations-Manager/index.html)
* [VMware Horizon 7 documentation](https://docs.vmware.com/en/VMware-Horizon-7/index.html)

## Browser Compatibility and Screen Resolutions

The Cloud Foundation web-based interface supports the latest two versions of the following web browsers except Internet Explorer:

* Google Chrome
* Mozilla Firefox
* Microsoft Edge
* Internet Explorer: Version 11

For the Web-based user interfaces, the supported standard resolution is 1024 by 768 pixels. For best results, use a screen resolution within these tested resolutions:

* 1024 by 768 pixels (standard)
* 1366 by 768 pixels
* 1280 by 1024 pixels
* 1680 by 1050 pixels

Resolutions below 1024 by 768, such as 640 by 960 or 480 by 800, are not supported.

## Upgrade Information

You can upgrade to VMware Cloud Foundation 3.11 either from VMware Cloud Foundation 3.10.2.2 (sequential upgrade) or from VMware Cloud Foundation 3.7.1 or later (skip-level upgrade). VMware Cloud Foundation 3.11 cannot be deployed as a new release. For upgrade information, refer to the [*VMware Cloud Foundation Upgrade Guide*](https://docs.vmware.com/en/VMware-Cloud-Foundation/3.11/vcf-lifecycle/GUID-B384B08D-3652-45E2-8AA9-AF53066F5F70.html).

VMware Cloud Foundation 3.11 is supported as a source version for migration to VMware Cloud Foundation 4.x.

**vRealize Suite Lifecycle Manager Version 2.1.0 Patch 3**

There is no upgrade bundle for vRealize Suite Lifecycle Manager Version 2.1.0 Patch 3. To upgrade, follow the process described in the [VMware vRealize Suite Lifecycle Manager 2.1 Patch 3 Release Notes](https://docs.vmware.com/en/VMware-vRealize-Suite-Lifecycle-Manager/2.1/rn/VMware-vRealize-Suite-Lifecycle-Manager-21-Patch-3-Release-Notes.html).

**Design Considerations for Multiple Availability Zones**

NSX-T Data Center 3.x changes how the northbound traffic flow can be influenced. If you have the following architecture, you must change the Tier-0 gateway architecture **before** you upgrade to NSX-T Data Center 3.x:

* An NSX Edge cluster with edge nodes placed in both availability zones (typically two edge nodes pinned to Availability Zone 1 and two edge nodes pinned to Availability Zone 2)
* An Active/Active Tier-0 gateway architecture where the Tier-0 gateway spans edge nodes in both availability zones.
* Deployed in a data center infrastructure that cannot tolerate asymmetrical routing to or from each availability zone, for example, for physical data center firewalls, and other.

Change to a Tier-0 gateway architecture where the Tier-0 gateway is active only in a single availability zone at a time in one of the following ways:

* **Recommended**: Place an NSX Edge cluster with edge nodes in a single availability zone only (typically Availability Zone 1), that fail over using vSphere HA to Availability Zone 2 on failure. This change requires changes in the data center fabric including stretching of the Uplink and Edge TEP VLANs between the availability zones. See [KB 87426](https://kb.vmware.com/s/article/87426) for more information.
* Migrate to an Active/Standby Tier-0 gateway. Follow the [NSX-T Data Center 3.x product documentation](https://docs.vmware.com/en/VMware-NSX-T-Data-Center/index.html) for changing from an Active/Active to an Active/Standby architecture of the Tier-0 gateway.

**Changing from a Three N-VDS to Single N-VDS Edge Node Design**

Starting with NSX-T Data Center 2.5, a single N-VDS switch design is available in the NSX Edge node. Changing from three N-VDS instances to a single N-VDS provides network throughput and scalability improvements in NSX-T Data Center. It is recommended for all environments but highly recommended for environments deployed at scale.

The procedure involves the following high-level steps:

* Deploy a new NSX Edge cluster with new edge nodes based on the single N-VDS design.
* Deploy a new Tier-0 gateway and verify connectivity.
* Once tested, you can reconfigure your Tier-1 gateways to utilize the new Tier-0 gateway on the single N-VDS edge cluster.

See [KB 87426](https://kb.vmware.com/s/article/87426) for more information.

## VMware Cloud Foundation 3.11.0.1 Release Information

You can upgrade to VMware Cloud Foundation 3.11.0.1 either from VMware Cloud Foundation 3.11 (sequential upgrade) or from VMware Cloud Foundation 3.7.1 or later (skip-level upgrade). VMware Cloud Foundation 3.11.0.1 cannot be deployed as a new release. For upgrade information, refer to the VMware Cloud Foundation Upgrade Guide. It is strongly recommended that all customers on VCF 3.x upgrade to VCF 3.11.0.1.

VMware Cloud Foundation 3.11.0.1 contains the following BOM updates:

| Software Component | Version | Date | Build Number |
| --- | --- | --- | --- |
| SDDC Manager | 3.11.0.1 | 07 APR 2022 | 19571759 |
| VMware NSX Data Center for vSphere | 6.4.13 | 08 FEB 2022 | 19307994 |

SDDC Manager 3.11.0.1 fixes the issue:

* Deleting an NSX for vSphere (NSX-V) VI workload domain incorrectly deletes the NSX controllers for the management domain

VMware NSX Data Center for vSphere 6.4.13 addresses the security vulnerability described in [VMSA-2022-0005](https://www.vmware.com/security/advisories/VMSA-2022-0005.html)

## Resolved Issues

The following issues are resolved in VMware Cloud Foundation 3.11:

* VMware vCenter Server Appliance 6.7 Update 3p addresses security vulnerabilities CVE-2021-21980 and CVE-2021-22049 as described in VMware Security Advisory [VMSA-2021-0027](https://www.vmware.com/security/advisories/VMSA-2021-0027.html).
* Inapplicable ESXi upgrade bundles are displayed after upgrade has been scheduled
* Add host workflow fails
* When the user password in the `/opt/vmware/vcf/lcm/lcm-app/conf/application.properties` file contains a backslash (\), Lifecycle Manager does not start and displays the error `Password authentication failed for user lcm`.
* Credential logging vulnerability as described in [VMSA-2022-0003](https://www.vmware.com/security/advisories/VMSA-2022-0003.html). See [KB 87050](https://kb.vmware.com/s/article/87050) for more information.
* Deleting an NSX-T workload domain or cluster containing a dead host fails at transport node deletion step.

## Known Issues

* [Upgrade Known Issues](#Known Issues- Upgrade Known Issues)
* [vRealize Suite Known Issues](#Known Issues- vRealize Suite Known Issues )
* [Networking Known Issues](#Known Issues-Networking Known Issues)
* [SDDC Manager Known Issues](#Known Issues- SDDC Manager Known Issues)
* [Workload Domain Known Issues](#Known Issues- Workload Domain Known Issues)
* [Multi-Instance Management Known Issues](#Known Issues-Multi-Instance Management Known Issues)
* [Backup and Restore Known Issues](#Known Issues-Backup and Restore Known Issues)

### Upgrade Known Issues

* **File permissions precheck gets stuck or captures permission issues in wrong directories**

  There are two potential issues:

  + The LCM directories permission precheck does not finish for more than an hour.
  + The LCM directories permission precheck reports permission/ownership issues in `/var/log` directories.

  Workaround: See [KB 90205](https://kb.vmware.com/s/article/90205).
* **Upgrade precheck fails for PSC SSO**

  If the maximum lifetime password policy for vCenter Single Sign-On local accounts is set to a number greater than 9999, then the upgrade precheck fails.

  Workaround: Set the maximum lifetime password policy to any number less than or equal to 9999. See [KB 88163](https://kb.vmware.com/s/article/88163) for more information.
* **There is a cosmetic mismatch between the posted manifest date (02/11/2022) and the release notes software date (02/14/2022).**

  There are no technical side effects.
* **The vRealize Automation upgrade reports the "Precheck Execution Failure : Make sure the latest version of VMware Tools is installed" message**

  The vRealize Automation IaaS VMs must have the same version of VMware Tools as the ESXi hosts on which the VMs reside.

  Workaround: Upgrade VMware Tools on the vRealize Automation IaaS VMs.
* **Error upgrading vRealize Automation**

  Under certain circumstances, upgrading vRealize Automation may fail with a message similar to:

  ```
  An automated upgrade has failed. Manual intervention is required.
  vRealize Suite Lifecycle Manager Pre-upgrade checks for vRealize Automation have failed:
  vRealize Automation Validations : iaasms1.rainpole.local : RebootPending : Check if reboot is pending : Reboot the machine.
  vRealize Automation Validations : iaasms2.rainpole.local : RebootPending : Check if reboot is pending : Reboot the machine.
  Please retry the upgrade once the upgrade is available again.
  ```

  1. Log-in into the first VM listed in the error message using RDP or the VMware Remote Console.
  2. Reboot the VM.
  3. Wait 5 minutes after the login screen of the VM appears.
  4. Repeat steps 1-3 for the next VM listed in the error message.
  5. Once you have restarted all the VMs listed in the error message, retry the vRealize Automation upgrade.
* **When there is no associated workload domain to vRealize Automation, the VRA VM NODES CONSISTENCY CHECK upgrade precheck fails**

  This upgrade precheck compares the content in the logical inventory on the SDDC Manager and the content in the vRealize Lifecycle Manager environment. When there is no associated workload domain, the vRealize Lifecycle Manager environment does not contain information about the `iaasagent1.rainpole.local` and `iaasagent2.rainpole.local` nodes. Therefore the check fails.

  Workaround: None. You can safely ignore a failed `VRA VM NODES CONSISTENCY CHECK` during the upgrade precheck. The upgrade will succeed even with this error.
* **NSX Data Center for vSphere upgrade fails with the message "Host Prep remediation failed"**

  After addressing the issue, the NSX Data Center for vSphere bundle no longer appears as an available update.

  Workaround: To complete the upgrade, manually enable the anti-affinity rules.

  1. Log in to the management vCenter Server using the vSphere Client.
  2. Click **Menu > Hosts and Clusters** and select the cluster on which host prep remediation failed (for example SDDC-Cluster1).
  3. Click **Configure > Configuration > VM/Host Rules**.
  4. Select **NSX Controller Anti-Affinity Rule** and click **Edit**.
  5. Select **Enable rule** and click **OK**.

  This completes the NSX Data Center for vSphere upgrade.
* **Upgrade precheck may fail as you approach the maximum number of supported ESXi hosts**

  In a large environment with many ESXi hosts, vSAN prechecks may fail.

  Workaround: Turn off vSAN prechecks.

  1. SSH to the SDDC Manager appliance as the **vcf** user.
  2. Enter `su` to switch to the root user.
  3. Turn off the vSAN prechecks:

     + Enter `vi /opt/vmware/vcf/lcm/lcm-app/conf/application-prod.properties`
     + In the VSAN CONFIGURATION section, set the following properties to false:
     + `vsan.healthcheck.enabled=false`
     + `vsan.hcl.update.enabled=false`
     + `vsan.precheck.enabled=false`
     + Save the changes.
  4. Restart LCM.

     `systemctl restart lcm`
* **Task panel does not show correct upgrade tasks for NSX-T workload domain upgrades**

  When you upgrade NSX-T workload domains. the task panel does not show upgrade status correctly. This is a UI issue only and there is no impact on the upgrade workflow.

  Workaround: Monitor upgrade status by navigating to the Update/Patches tab of the relevant workload domain:

  1. On the SDDC Manager Dashboard, click *Inventory* -> *Workload Domains*.
  2. In the Domain column, click the appropriate workload domain name.
  3. Click the *Update/Patches* tab.
  4. Monitor upgrade status.
* **Exception displayed when a scheduled NSX-T upgrade begins during an idle SDDC Manager session**

  When a scheduled NSX-T upgrade begins during an idle SDDC Manager session, the following UI exception is displayed: Retrieving NSXT upgrade failed with unknown exception

  This is a UI issue only. There is no impact on the upgrade workflow.

  Workaround: Refresh the web browser.
* **After upgrading to VMware Cloud Foundation 3.11 the NSX Edge password precheck is turned off**

  Enable the NSX Edge password precheck so that future upgrades of NSX-T based VI workload domains do not fail due to expired passwords.

  Workaround:

  1. SSH to the SDDC Manager appliance as the **vcf** user.
  2. Type `su` to switch to the root user.
  3. Edit the /opt/vmware/vcf/lcm/lcm-app/conf/feature.properties file as shown below:

     + `#feature flag for NSXT Edge VM Password Expiry Precheck`
     + `feature.lcm.nsxt.edge.password.validation=true`
  4. Run the following command: `systemctl restart lcm`

### vRealize Suite Known Issues

* **vRealize Operations Manager: VMware Security Advisory VMSA-2021-0018**

  [VMSA-2021-0018](https://www.vmware.com/security/advisories/VMSA-2021-0018.html) describes security vulnerabilities that affect VMware Cloud Foundation.

  + The vRealize Operations Manager API contains an arbitrary file read vulnerability. A malicious actor with administrative access to vRealize Operations Manager API can read any arbitrary file on server leading to information disclosure.The Common Vulnerabilities and Exposures project (cve.mitre.org) has assigned identifier CVE-2021-22022 to this issue.
  + The vRealize Operations Manager API has insecure object reference vulnerability. A malicious actor with administrative access to vRealize Operations Manager API may be able to modify other users information leading to an account takeover.The Common Vulnerabilities and Exposures project (cve.mitre.org) has assigned identifier CVE-2021-22023 to this issue.
  + The vRealize Operations Manager API contains an arbitrary log-file read vulnerability. An unauthenticated malicious actor with network access to the vRealize Operations Manager API can read any log file resulting in sensitive information disclosure. The Common Vulnerabilities and Exposures project (cve.mitre.org) has assigned identifier CVE-2021-22024 to this issue.
  + The vRealize Operations Manager API contains a broken access control vulnerability leading to unauthenticated API access. An unauthenticated malicious actor with network access to the vRealize Operations Manager API can add new nodes to existing vROps cluster. The Common Vulnerabilities and Exposures project (cve.mitre.org) has assigned identifier CVE-2021-22025 to this issue.
  + The vRealize Operations Manager API contains a Server Side Request Forgery in multiple end points. An unauthenticated malicious actor with network access to the vRealize Operations Manager API can perform a Server Side Request Forgery attack leading to information disclosure. The Common Vulnerabilities and Exposures project (cve.mitre.org) has assigned identifiers CVE-2021-22026 and CVE-2021-22027 to this issue.

  Workaround: See [KB 85452](https://kb.vmware.com/s/article/85452) for information about applying vRealize Operations Security Patches that resolve the issues.
* **The password update for vRealize Automation and vRealize Operations Manager may run infinitely or may fail when the password contains special character "%"**

  Password management uses the vRealize Lifecycle Manager API to update the password of vRealize Automation and vRealize Operations Manager. When there is special character "%" in either of SSH or API or Administrator credential types of the vRealize Automation and vRealize Operations Manager users, then the vRealize Lifecycle Manager API hangs and doesn't respond to password management. There is a timeout of 5 mins and password management marks the operation as failed.

  Workaround:Retry the password update operation without the special character "%". Ensure that the passwords for all other vRealize Automation and vRealize Operations Manager accounts don't contain the "%" special character.

### Networking Known Issues

* **NSX Manager is not visible in the vSphere Web Client.**

  In addition to NSX Manager not being visible in the vSphere Web Client, the following error message displays in the NSX Home screen: "No NSX Managers available. Verify current user has role assigned on NSX Manager." This issue occurs when vCenter Server is not correctly configured for the account that is logged in.

  Workaround: To resolve this issue, follow the procedure detailed in Knowledge Base article 2080740 ["No NSX Managers available" error in the vSphere Web Client](https://kb.vmware.com/s/article/2080740).

### SDDC Manager Known Issues

* **Unable to delete VI workload domain enabled for vRealize Operations Manager from SDDC Manager.**

  Attempts to delete the vCenter adapter also fail, and return an SSL error.

  Workaround: Use the following procedure to resolve this issue.

  1. Create a vCenter adapter instance in vRealize Operations Manager, as described in [Configure a vCenter Adapter Instance in vRealize Operations Manager](https://docs.vmware.com/en/vRealize-Operations-Manager/7.5/com.vmware.vcom.config.doc/GUID-19DAD6AF-7262-4655-B69F-6C665E33B52F.html). This step is required because the existing adapter was deleted by the failed workload domain deletion.
  2. Follow the procedure described in [Knowledge Base article 56946](https://kb.vmware.com/s/article/56946).
  3. Restart the failed VI workload domain deletion workflow from the SDDC Manager interface.
* **APIs for managing SDDC cannot be executed from the SDDC Manager Dashboard**

  You cannot use the API Explorer in the SDDC Manager Dashboard to execute the APIs for managing SDDC (`/v1/sddc`).

  Workaround: None. These APIs can only be executed using the Cloud Builder as the host.

### Workload Domain Known Issues

* **Adding host fails when host is on a different VLAN**

  A host add operation can sometimes fail if the host is on a different VLAN.

  Workaround:

  1. Before adding the host, add a new portgroup to the VDS for that cluster.
  2. Tag the new portgroup with the VLAN ID of the host to be added.
  3. Add the Host. This workflow fails at the "Migrate host vmknics to dvs" operation.
  4. Locate the failed host in vCenter, and migrate the vmk0 of the host to the new portgroup you created in step 1. For more information, see [Migrate VMkernel Adapters to a vSphere Distributed Switch](https://docs.vmware.com/en/VMware-vSphere/6.7/com.vmware.vsphere.networking.doc/GUID-0DE237B8-1938-4DE9-90EC-718E345B56A0.html) in the vSphere product documentation.
  5. Retry the Add Host operation.

  **NOTE**: If you later remove this host in the future, you must manually remove the portgroup as well if it is not being used by any other host.
* **NSX Manager for VI workload domain is not displayed in vCenter**

  Although NFS-based VI workload domains are created successfully, the NSX Manager VM is not registered in vCenter Server and is not displayed in vCenter.

  Workaround: To resolve this issue, use the following procedure:

  1. Log in to NSX Manager (http://<nsxmanager IP>).
  2. Navigate to **Manage > NSX Management Service**.
  3. Un-register the lookup service and vCenter, then re-register.
  4. Close the browser and log in to vCenter.
* **A vCenter Server on which certificates have been rotated is not accessible from a Horizon workload domain**

  VMware Cloud Foundation does not support the certificate rotation on the Horizon workload domains.

  Workaround: See KB article [70956](https://kb.vmware.com/s/article/70956).
* **Deploying partner services on an NSX-T workload domain displays an error**

  Deploying partner services on an NSX-T workload domain such as McAfee or Trend displays the “Configure NSX at cluster level to deploy Service VM” error.

  Workaround: Attach the Transport node profile to the cluster and try deploying the partner service. After the service is deployed, detach the transport node profile from the cluster.
* **If the witness ESXi version does not match with the host ESXi version in the cluster, vSAN cluster partition may occur**

  vSAN stretch cluster workflow does not check the ESXi version of the witness host. If the witness ESXi version does not match the host version in the cluster, then vSAN cluster partition may happen.

  Workaround:

  1. Upgrade the witness host manually with the matching ESXi version using the vCenter VUM functionality.
  2. Replace or deploy the witness appliance matching with the ESXi version.
* **The certificate rotate operation on the second NSX-T domain fails**

  Certificate rotation works on the first NSX-T workload domain in your environment, but fails on all subsequent NSX-T workload domains.

  Workaround: None
* **Operations on NSX-T workload domains fails if their host FQDNs include uppercase letters**

  If the FQDNs of ESXi hosts in an NSX-T workload domain include uppercase letters, then the following operations may fail for the workload domain:

  + Add a host
  + Remove a host
  + Add a cluster
  + Remove a cluster
  + Delete the workload domain

  Workaround: See [KB 76553](http://kb.vmware.com/kb/76553).
* **VI workload domain creation or expansion operations fail**

  If there is a mismatch between the letter case (upper or lower) of an ESXi host's FQDN and the FQDN used when the host was commissioned, then workload domain creation and expansion may fail.

  Workaround: ESXi hosts should have lower case FQDNs and should be commissioned using lower case FQDNs.
* **Cluster is deleted even if VMs are up and running on the cluster**

  When you delete a cluster, it gets deleted even if there are VMs running on the cluster. This includes critical VMs such as Edge VMs, which may prevent you from accessing your environment after the cluster gets deleted.

  Workaround: Migrate the VMs to a different cluster before deleting the cluster.
* **Workload domain operations fail if cluster upgrade is in progress**

  Workload domain operations cannot be performed if one or more clusters are being upgraded. The UI does not block such oeprations during an upgrade.

  Workaround: Do not perform any operations on the workload domain when a cluster upgrade is in progress.
* **If you use the special character underscore (\_) in the vCenter Server host name for the workload domain create operation, the vCenter Server deployment fails**

  The vCenter deployment fails with the **"ERROR > Section 'new\_vcsa', subsection 'network', property 'system\_name' validation"** error message.

  Workaround: None. This is an issue in the vCenter Server product installer where the installer pre-validation fails. You should create the workload domain by providing valid vCenter Server host names.

### Multi-Instance Management Known Issues

* **Federation creation information not displayed if you leave the Multi-Instance Management Dashboard**

  Federation creation progress is displayed on the Multi-Instance Management Dashboard. If you navigate to another screen and then return to the Multi-Instance Management Dashboard, progress messages are not displayed. Instead, an empty map with no Cloud Foundation instances are displayed until the federation is created.

  Workaround: Stay on the Multi-Instance Dashboard till the task is complete. If you have navigated away, wait for around 20 minutes and then return to the dashboard by which time the operation should have completed.
* **The federation creation progress is not displayed**

  While federation creation is in progress, the SDDC manager UI displays the progress on the multi-site page. If you navigate into any other screen and come back to the multi-site screen, the progress messages are not displayed. An empty map with no VMware Cloud Foundation instances is displayed until the federation creation process completes.

  Workaround: None
* **Multi-Instance Management Dashboard operation fails**

  After a controller joins or leaves a federation, Kafka is restarted on all controllers in the federation. It can take up to 15 minutes for the federation to stabilize. Any operations performed on the dashboard during this time may fail.

  Workaround: Re-try the operation.

### Backup and Restore Known Issues

* **NSX Manager restore might not complete due to certificate rejection**

  A restore might not complete as a result of an installed certificate with no CRL Distribution Point and an incorrect setting (crl\_checking\_enable configuration to true).

  Workaround: None

check-circle-line

exclamation-circle-line

close-line

![Scroll to top icon](/uicontent/images/scroll_top.svg)

![](/uicontent/images/feedback.svg)

             [![VMware](https://www.vmware.com/content/dam/vmwaredesigns/scrapercontent/resources/logos/vmware-logo-grey.svg "VMware")](https://www.vmware.com/)

Resources

* [Blogs](https://blogs.vmware.com/)
* [Careers](https://www.broadcom.com/jobs)
* [Communities](https://community.broadcom.com/)
* [Customer Stories](https://www.vmware.com/resources/customers)

* [News and Stories](https://news.broadcom.com/)
* [Topics](https://www.vmware.com/topics/)
* [Trust Center](https://www.vmware.com/info/trust-center/)

Support

* [Broadcom Support](https://support.broadcom.com/)
* [Documentation](https://docs.vmware.com)
* [Hands-On Labs](https://www.vmware.com/resources/hands-on-labs)
* [Licensing](https://www.broadcom.com/licensing)

* [Twitter](https://twitter.com/VMware)
* [YouTube](https://www.youtube.com/user/vmwaretv)
* [Facebook](https://www.facebook.com/vmware)
* [LinkedIn](https://www.linkedin.com/company/vmware/mycompany/)
* [Contact Sales](https://go-vmware.broadcom.com/contact-us)

---

  Copyright © 2005-2024 Broadcom. All Rights Reserved. The term "Broadcom" refers to Broadcom Inc. and/or its subsidiaries. [Accessibility](https://www.broadcom.com/company/legal/accessibility "Accessibility") [Privacy](https://www.broadcom.com/privacy "Privacy") [Supplier Responsibility](https://www.broadcom.com/company/citizenship/supplier-responsibility "Supplier Responsibility") [Terms Of Use](https://www.broadcom.com/company/legal/terms-of-use "Terms Of Use")

#####

×

Share on Social Media?

×

#####

×

exclamation-circle-line

check-circle-line

 :

#####

×

exclamation-circle-line

|

check-circle-line

exclamation-circle-line

x

#####

×

"
"?

×


