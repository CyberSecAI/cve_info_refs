=== Content from docs.vmware.com_92c0cd72_20250115_114004.html ===


[![](/uicontent/images/VMware_by_Broadcom.png)Docs](/)

* ![](/uicontent/images/VMware_by_Broadcom_Gray-Black.png) Docs
  times
* [(current)](/)

* ![](/uicontent/images/nwMyLibrary.png)
  #####

  ![](/uicontent/images/icon-handshake.png)

  ![](/uicontent/images/icon-support.png)

  ![](/uicontent/images/icon-networking-bg.png)
  [VMware Communities](https://communities.vmware.com/)

  ![](/uicontent/images/icon-download-bg.png)

 This site will be decommissioned on January 30th 2025. After that date content will be available at [techdocs.broadcom.com](https://techdocs.broadcom.com/).

![](/uicontent/images/share-mylibrary.svg)

![](/uicontent/images/add-to-library-colorless.svg)

#

![](/uicontent/images/twitter.svg)
![](/uicontent/images/facebook.svg)
![](/uicontent/images/linkedin.svg)
![](/uicontent/images/weibo.svg)

![](/uicontent/images/pdf.svg)

![](/uicontent/images/feedback.svg)

![](/uicontent/images/edit.svg)

![](/uicontent/images/review.svg)

Twitter
Facebook
LinkedIn
微博

| VMware NSX Data Center for vSphere 6.4.13 | Released February 8, 2022 | Build 19307994  See the [Revision History](#revhistory) of this document. |
| --- |

## What's in the Release Notes

The release notes cover the following topics:

* [What's New](#whatsnew)
* [Versions, System Requirements, and Installation](#sysreqs)
* [Deprecated and Discontinued Functionality](#deprecs)
* [Upgrade Notes](#upgradenotes)
* [FIPS Compliance](#FIPS)
* [Revision History](#revhistory)
* [Resolved Issues](#resolvedissues)

## What's New in NSX Data Center for vSphere 6.4.13

VMware NSX for vSphere 6.4.13 addresses a number of specific customer bugs. See [Resolved Issues](#resolvedissues) for more information.

Versions, System Requirements and Installation

**Note:**

* The table below lists recommended versions of VMware software. These recommendations are general and should not replace or override environment-specific recommendations. This information is current as of the publication date of this document.
* For the **minimum supported** version of NSX and other VMware products, see the [VMware Product Interoperability Matrix](http://partnerweb.vmware.com/comp_guide2/sim/interop_matrix.php). VMware declares minimum supported versions based on internal testing.

| Product or Component | Version |

| NSX Data Center for vSphere | VMware recommends the latest NSX release for new deployments.  When upgrading existing deployments, please review the NSX Data Center for vSphere Release Notes or contact your VMware technical support representative for more information on specific issues before planning an upgrade. |
| vSphere | **For vSphere 6.5**: Recommended: 6.5 Update 3 **Note**: vSphere 6.5 Update 1 resolves the issue of EAM failing with OutOfMemory. See [VMware Knowledge Base Article 2135378](https://kb.vmware.com/kb/2135378) for more information. **Important:**   * If you are using multicast routing on vSphere 6.5, vSphere 6.5 Update 2 or higher is recommended. * If you are using NSX Guest Introspection on vSphere 6.5, vSphere 6.5 P03 or higher is recommended.   **For vSphere 6.7:** Recommended: 6.7 Update 2 **Important:**   * If you are using NSX Guest Introspection on vSphere 6.7, please refer to Knowledge Base Article [57248](https://kb.vmware.com/s/article/57248) prior to installing NSX 6.4.6, and consult VMware Customer Support for more information.   **Note:** vSphere 5.5 is not supported with NSX 6.4.  **Note**: vSphere 6.0 has reached End of General Support and is not supported with NSX 6.4.7 onwards. |

| Guest Introspection for Windows | It is recommended that you upgrade VMware Tools to 10.3.10 before upgrading NSX for vSphere. |
| Guest Introspection for Linux | Ensure that the guest virtual machine has a supported version of Linux installed. See [VMware NSX Administration Guide](https://docs.vmware.com/en/VMware-NSX-Data-Center-for-vSphere/6.4/com.vmware.nsx.admin.doc/GUID-636788A7-BB64-483A-A48D-4E62B3AFC0C8.html) for latest list of supported Linux versions. |

### System Requirements and Installation

For the complete list of NSX installation prerequisites, see the [System Requirements for NSX](https://docs.vmware.com/en/VMware-NSX-Data-Center-for-vSphere/6.4/com.vmware.nsx.install.doc/GUID-311BBB9F-32CC-4633-9F91-26A39296381A.html) section in the *NSX Installation Guide*.

For installation instructions, see the [*NSX Installation Guide*](https://docs.vmware.com/en/VMware-NSX-Data-Center-for-vSphere/6.4/com.vmware.nsx.install.doc/GUID-D8578F6E-A40C-493A-9B43-877C2B75ED52.html) or the [*NSX Cross-vCenter Installation Guide*](https://docs.vmware.com/en/VMware-NSX-Data-Center-for-vSphere/6.4/com.vmware.nsx.cross-vcenter-install.doc/GUID-803A5A77-74A0-483C-97C9-5B66A0B02240.html).

## Deprecated and Discontinued Functionality

### End of Life and End of Support Warnings

For information about NSX and other VMware products that must be upgraded soon, please consult the [*VMware Lifecycle Product Matrix*](https://www.vmware.com/files/pdf/support/Product-Lifecycle-Matrix.pdf).

* **NSX for vSphere 6.4.x** reached End of General Support (EOGS) on January 16, 2022. See knowledge base article [85706](https://kb.vmware.com/s/article/85706?lang=en_US&queryTerm=85706) for resources on migration planning and extended support.
* **NSX for vSphere 6.1.x** reached End of Availability (EOA) and End of General Support (EOGS) on January 15, 2017. (See also [*VMware knowledge base article 2144769*](http://kb.vmware.com/kb/2144769).)
* **vCNS Edges no longer supported**. You must upgrade to an NSX Edge first before upgrading to NSX 6.3 or later.
* **NSX for vSphere 6.2.x** has reached End of General Support (EOGS) as of August 20, 2018.
* **Based on security recommendations, 3DES as an encryption algorithm in NSX Edge IPsec VPN service is no longer supported**.
   It is recommended that you switch to one of the secure ciphers available in IPsec service. This change regarding encryption algorithm is applicable to IKE SA (phase1) as well as IPsec SA (phase2) negotiation for an IPsec site.

   If 3DES encryption algorithm is in use by NSX Edge IPsec service at the time of upgrade to the release in which its support is removed, it will be replaced by another recommended cipher and therefore the IPsec sites that were using 3DES will not come up unless the configuration on the remote peer is modified to match the encryption algorithm used in NSX Edge.

   If using 3DES encryption, modify the encryption algorithm in the IPsec site configuration to replace 3DES with one of the supported AES variants (AES / AES256 / AES-GCM). For example, for each IPsec site configuration with the encryption algorithm as 3DES, replace it with AES. Accordingly, update the IPsec configuration at the peer endpoint.

### General Behavior Changes

If you have more than one vSphere Distributed Switch, and if VXLAN is configured on one of them, you must connect any Distributed Logical Router interfaces to port groups on that vSphere Distributed Switch. Starting in NSX 6.4.1, this configuration is enforced in the UI and API. In earlier releases, you were not prevented from creating an invalid configuration.  If you upgrade to NSX 6.4.1 or later and have incorrectly connected DLR interfaces, you will need to take action to resolve this. See the [Upgrade Notes](#DLR-VDS-vaildation) for details.

### User Interface Removals and Changes

* In NSX 6.4.1, Service Composer Canvas is removed.
* In NSX 6.4.7, the following functionality is deprecated in vSphere Client 7.0:
  + NSX Edge: SSL VPN-Plus (see [KB 79929](https://kb.vmware.com/s/article/79929) for more information)
  + Tools: Endpoint Monitoring (all functionality)
  + Tools: Flow Monitoring (Flow Monitoring Dashboard, Details by Service, and Configuration)
  + System Events: NSX Ticket Logger

### Installation Behavior Changes

Starting with version 6.4.2, when you install NSX Data Center for vSphere on hosts that have physical NICs with ixgbe drivers, Receive Side Scaling (RSS) is not enabled on the ixgbe drivers by default. You must enable RSS manually on the hosts before installing NSX Data Center. Make sure that you enable RSS only on the hosts that have NICs with ixgbe drivers. For detailed steps about enabling RSS, see the VMware knowledge base article <https://kb.vmware.com/s/article/2034676>. This knowledge base article describes recommended RSS settings for improved VXLAN packet throughput.

This new behavior applies only when you are doing a fresh installation of kernel modules (VIB files) on the ESXi hosts. No changes are required when you are upgrading NSX-managed hosts to 6.4.2.

### API Removals and Behavior Changes

**Deprecations in NSX 6.4.2**

The following item is deprecated, and might be removed in a future release:

* `GET/POST/DELETE /api/2.0/vdn/controller/{controllerId}/syslog`. Use `GET/PUT /api/2.0/vdn/controller/cluster/syslog`instead.

**Behavior Changes in NSX 6.4.1**

When you create a new IP pool with `POST /api/2.0/services/ipam/pools/scope/globalroot-0`, or modify an existing IP pool with `PUT /api/2.0/services/ipam/pools/`, and the pool has multiple IP ranges defined, validation is done to ensure that the ranges do not overlap. This validation was not previously done.

**Deprecations in NSX 6.4.0**
 The following items are deprecated, and might be removed in a future release.

* The **systemStatus** parameter in `GET /api/4.0/edges/edgeID/status` is deprecated.
* `GET /api/2.0/services/policy/serviceprovider/firewall/` is deprecated. Use `GET /api/2.0/services/policy/serviceprovider/firewall/info` instead.
* Setting **tcpStrict** in the global configuration section of Distributed Firewall is deprecated. Starting in NSX 6.4.0, tcpStrict is defined at the section level. **Note**: If you upgrade to NSX 6.4.0 or later, the global configuration setting for tcpStrict is used to configure tcpStrict in each existing layer 3 section. tcpStrict is set to false in layer 2 sections and layer 3 redirect sections. See "Working with Distributed Firewall Configuration" in the *NSX API Guide* for more information.

**Behavior Changes in NSX 6.4.0**
 In NSX 6.4.0, the `<name>` parameter is required when you create a controller with `POST /api/2.0/vdn/controller`.

NSX 6.4.0 introduces these changes in error handling:

* Previously `POST /api/2.0/vdn/controller` responded with *201 Created* to indicate the controller creation job is created. However, the creation of the controller might still fail. Starting in NSX 6.4.0 the response is `202 Accepted`.
* Previously if you sent an API request which is not allowed in transit or standalone mode, the response status was *400 Bad Request*. Starting in 6.4.0 the response status is *403 Forbidden*.

### CLI Removals and Behavior Changes

**Do not use unsupported commands on NSX Controller nodes**
 There are undocumented commands to configure NTP and DNS on NSX Controller nodes. These commands are not supported, and should not be used on NSX Controller nodes. You should only use commands which are documented in the NSX CLI Guide.

## Upgrade Notes

* [General Upgrade Notes](#generalUpgrade)
* [Upgrade Notes for NSX Components](#upgradeNotesNSXcomponents)
* [Upgrade Notes for FIPS](#UpgradeNotesFIPS)

**Note:** For a list of known issues affecting installation and upgrades, see [Installation and Upgrade Known Issues](#installation-and-upgrade-known-issues-known).

### General Upgrade Notes

* After upgrading vCSA or ESXi to 7.0+, VMs on host experience East/West and North/South connectivity issues. See VMware knowledge base article [85070](https://kb.vmware.com/s/article/85070) for details.
* 3rd-party integrations that call NSX Controller APIs using weaker ciphers may stop working after upgrade to NSX Data Center for vSphere 6.4.11.
* To upgrade NSX, you must perform a full NSX upgrade including host cluster upgrade (which upgrades the host VIBs). For instructions, see the [*NSX Upgrade Guide*](https://docs.vmware.com/en/VMware-NSX-Data-Center-for-vSphere/6.4/com.vmware.nsx.upgrade.doc/GUID-4613AC10-BC73-4404-AF80-26E924EF5FE0.html) including the "Upgrade Host Clusters" section.
* Upgrading NSX VIBs on host clusters using VUM is not supported. Use Upgrade Coordinator, Host Preparation, or the associated REST APIs to upgrade NSX VIBs on host clusters.
* **System Requirements**: For information on system requirements while installing and upgrading NSX, see the [System Requirements for NSX](https://docs.vmware.com/en/VMware-NSX-Data-Center-for-vSphere/6.4/com.vmware.nsx.admin.doc/GUID-311BBB9F-32CC-4633-9F91-26A39296381A.html) section in NSX documentation.
* **Upgrade path for NSX:** The [VMware Product Interoperability Matrix](http://www.vmware.com/resources/compatibility/sim/interop_matrix.php#upgrade&solution=93) provides details about the upgrade paths from VMware NSX.
* **Cross-vCenter NSX upgrade** is covered in the [NSX Upgrade Guide](https://docs.vmware.com/en/VMware-NSX-Data-Center-for-vSphere/6.4/com.vmware.nsx.upgrade.doc/GUID-4613AC10-BC73-4404-AF80-26E924EF5FE0.html).
* **Downgrades are not supported:**
  + Always capture a backup of NSX Manager before proceeding with an upgrade.
  + Once NSX has been upgraded successfully, NSX cannot be downgraded.
* **To validate** that your upgrade to NSX 6.4.x was successful see [knowledge base article 2134525](http://kb.vmware.com/kb/2134525).
* There is no support for upgrades from vCloud Networking and Security to NSX 6.4.x. You must upgrade to a supported 6.2.x release first.
* **Interoperability:** Check the [VMware Product Interoperability Matrix](http://partnerweb.vmware.com/comp_guide2/sim/interop_matrix.php) for all relevant VMware products before upgrading.
  + **Upgrading to NSX Data Center for vSphere 6.4.7**: VIO is not compatible with NSX 6.4.7 due to multiple scale issues.
  + **Upgrading to NSX Data Center for vSphere 6.4**: NSX 6.4 is not compatible with vSphere 5.5.
  + **Upgrading to NSX Data Center for vSphere 6.4.5**: If NSX is deployed with VMware Integrated OpenStack (VIO), upgrade VIO to 4.1.2.2 or 5.1.0.1, as 6.4.5 is incompatible with previous releases due to spring package update to version 5.0.
  + **Upgrading to vSphere 6.5**: When upgrading to vSphere 6.5a or later 6.5 versions, you must first upgrade to NSX 6.3.0 or later. NSX 6.2.x is not compatible with vSphere 6.5. See "Upgrading vSphere in an NSX Environment" in the [*NSX Upgrade Guide*](http://docs.vmware.com/en/VMware-NSX-Data-Center-for-vSphere/6.4/com.vmware.nsx.upgrade.doc/GUID-4613AC10-BC73-4404-AF80-26E924EF5FE0.html).
  + **Upgrading to vSphere 6.7**: When upgrading to vSphere 6.7 you must first upgrade to NSX 6.4.1 or later. Earlier versions of NSX are not compatible with vSphere 6.7. See "Upgrading vSphere in an NSX Environment" in the [*NSX Upgrade Guide*](http://docs.vmware.com/en/VMware-NSX-Data-Center-for-vSphere/6.4/com.vmware.nsx.upgrade.doc/GUID-4613AC10-BC73-4404-AF80-26E924EF5FE0.html).
* **Partner services compatibility**: If your site uses VMware partner services for Guest Introspection or Network Introspection, you must review the  [VMware Compatibility Guide](http://www.vmware.com/resources/compatibility/search.php?deviceCategory=security) before you upgrade, to verify that your vendor's service is compatible with this release of NSX.
* **Networking and Security plug-in**: After upgrading NSX Manager, you must log out and log back in to the vSphere Web Client. If the NSX plug-in does not display correctly, clear your browser cache and history. If the Networking and Security plug-in does not appear in the vSphere Web Client, reset the vSphere Web Client server as explained in the [*NSX Upgrade Guide*](https://docs.vmware.com/en/VMware-NSX-Data-Center-for-vSphere/6.4/com.vmware.nsx.upgrade.doc/GUID-4613AC10-BC73-4404-AF80-26E924EF5FE0.html).
* **Stateless environments**: In NSX upgrades in a stateless host environment, the new VIBs are pre-added to the Host Image profile during the NSX upgrade process. As a result, NSX on stateless hosts upgrade process follows this sequence:

  Prior to NSX 6.2.0, there was a single URL on NSX Manager from which VIBs for a certain version of the ESX Host could be found. (Meaning the administrator only needed to know a single URL, regardless of NSX version.) In NSX 6.2.0 and later, the new NSX VIBs are available at different URLs. To find the correct VIBs, you must perform the following steps:

  1. Find the new VIB URL from https://<nsxmanager>/bin/vdn/nwfabric.properties.
  2. Fetch VIBs of required ESX host version from corresponding URL.
  3. Add them to host image profile.
* **Service Definitions functionality is not supported in NSX 6.4.7 UI with vSphere Client 7.0**:
   For example, if you have an old Trend Micro Service Definition registered with vSphere 6.5 or 6.7, follow any one of these two options:
  + Option #1: Before upgrading to vSphere 7.0, navigate to the **Service Definition** tab in the vSphere Web Client, edit the Service Definition to 7.0, and then upgrade to vSphere 7.0.
  + Option #2: After upgrading to vSphere 7.0, run the following NSX API to add or edit the Service Definition to 7.0.

POST  https://<nsmanager>/api/2.0/si/service/<service-id>/servicedeploymentspec/versioneddeploymentspec

### Upgrade Notes for NSX Components

**Support for VM Hardware version 11 for NSX components**

* For new installs of NSX Data Center for vSphere 6.4.2, the NSX components (Manager, Controller, Edge, Guest Introspection) are on VM Hardware version 11.
* For upgrades to NSX Data Center for vSphere 6.4.2, the NSX Edge and Guest Introspection components are automatically upgraded to VM Hardware version 11. The NSX Manager and NSX Controller components remain on VM Hardware version 8 following an upgrade. Users have the option to upgrade the VM Hardware to version 11. Consult KB (<https://kb.vmware.com/s/article/1010675>) for instructions on upgrading VM Hardware versions.
* For new installs of NSX 6.3.x, 6.4.0, 6.4.1, the NSX components (Manager, Controller, Edge, Guest Introspection) are on VM Hardware version 8.

**NSX Manager Upgrade**

* Important: If you are upgrading NSX 6.2.0, 6.2.1, or 6.2.2 to NSX 6.3.5 or later, you must complete a workaround before starting the upgrade. See [VMware Knowledge Base article 000051624](https://kb.vmware.com/s/article/000051624) for details.
* If you are upgrading from NSX 6.3.3 to NSX 6.3.4 or later you must first follow the workaround instructions in *[VMware Knowledge Base article 2151719](https://kb.vmware.com/kb/2151719).*
* If you use SFTP for NSX backups, change to hmac-sha2-256 after upgrading to 6.3.0 or later because there is no support for hmac-sha1. See *[VMware Knowledge Base article 2149282](https://kb.vmware.com/kb/2149282)*for a list of supported security algorithms.
* When you upgrade NSX Manager to NSX 6.4.1, a backup is automatically taken and saved locally as part of the upgrade process. See [Upgrade NSX Manager](https://docs.vmware.com/en/VMware-NSX-Data-Center-for-vSphere/6.4/com.vmware.nsx.upgrade.doc/GUID-E8BA0AB5-9948-40B3-8234-31B5105154FB.html) for more information.
* When you upgrade to NSX 6.4.0, the TLS settings are preserved. If you have only TLS 1.0 enabled, you will be able to view the NSX plug-in in the vSphere Web Client, but NSX Managers are not visible. There is no impact to datapath, but you cannot change any NSX Manager configuration. Log in to the NSX appliance management web UI at https://nsx-mgr-ip/ and enable TLS 1.1 and TLS 1.2. This reboots the NSX Manager appliance.

**Controller Upgrade**

* The NSX Controller cluster must contain three controller nodes. If it has fewer than three controllers, you must add controllers before starting the upgrade. See [Deploy NSX Controller Cluster](https://docs.vmware.com/en/VMware-NSX-Data-Center-for-vSphere/6.2/com.vmware.nsx.upgrade.doc/GUID-16603203-E131-45FA-AB10-01BFAD89EB97.html) for instructions.
* In NSX 6.3.3, the underlying operating system of the NSX Controller changes. This means that when you upgrade from NSX 6.3.2 or earlier to NSX 6.3.3 or later, instead of an in-place software upgrade, the existing controllers are deleted one at a time, and new Photon OS based controllers are deployed using the same IP addresses.

  When the controllers are deleted, this also deletes any associated DRS anti-affinity rules. You must create new anti-affinity rules in vCenter to prevent the new controller VMs from residing on the same host.

  See [Upgrade the NSX Controller Cluster](https://docs.vmware.com/en/VMware-NSX-Data-Center-for-vSphere/6.4/com.vmware.nsx.upgrade.doc/GUID-C3093104-56A5-4E29-BCD3-13A8A24D5C05.html) for more information on controller upgrades.

**Host Cluster Upgrade**

* **If you upgrade from NSX 6.3.2 or earlier to NSX 6.3.3 or later, the NSX VIB names change.**
   The esx-vxlan and esx-vsip VIBs are replaced with esx-nsxv if you have NSX 6.3.3 or later installed on ESXi 6.0 or later.
* **Rebootless upgrade and uninstall on hosts:** On vSphere 6.0 and later, once you have upgraded from NSX 6.2.x to NSX 6.3.x or later, any subsequent NSX VIB changes will not require a reboot. Instead hosts must enter maintenance mode to complete the VIB change. This affects both NSX host cluster upgrade, and ESXi upgrade. See the [NSX Upgrade Guide](https://docs.vmware.com/en/VMware-NSX-Data-Center-for-vSphere/6.4/com.vmware.nsx.upgrade.doc/GUID-4613AC10-BC73-4404-AF80-26E924EF5FE0.html) for more information.

​**NSX Edge Upgrade**

* **Validation added in NSX 6.4.1 to disallow an invalid distributed logical router configurations**: In environments where VXLAN is configured and more than one vSphere Distributed Switch is present, distributed logical router interfaces must be connected to the VXLAN-configured vSphere Distributed Switch only. Upgrading a DLR to NSX 6.4.1 or later will fail in those environments if the DLR has interfaces connected to the vSphere Distributed Switch that is not configured for VXLAN. Use the API to connect any incorrectly configured interfaces to port groups on the VXLAN-configured vSphere Distributed Switch. Once the configuration is valid, retry the upgrade. You can change the interface configuration using `PUT /api/4.0/edges/{edgeId}` or `PUT /api/4.0/edges/{edgeId}/interfaces/{index}`. See the *[NSX API Guide](https://docs.vmware.com/en/VMware-NSX-Data-Center-for-vSphere/6.4/nsx_64_api.pdf)* for more information.

* **Delete UDLR Control VM from vCenter Server that is associated with secondary NSX Manager before upgrading UDLR from 6.2.7 to 6.4.5:**
   In a multi-vCenter environment, when you upgrade NSX UDLRs from 6.2.7 to 6.4.5, the upgrade of the UDLR virtual appliance (UDLR Control VM) fails on the secondary NSX Manager, if HA is enabled on the UDLR Control VM. During the upgrade, the VM with ha index #0 in the HA pair is removed from the NSX database; but, this VM continues to exist on the vCenter Server. Therefore, when the UDLR Control VM is upgraded on the secondary NSX Manager, the upgrade fails because the name of the VM clashes with an existing VM on the vCenter Server. To resolve this issue, delete the Control VM from the vCenter Server that is associated with the UDLR on the secondary NSX Manager, and then upgrade the UDLR from 6.2.7 to 6.4.5.
* **Host clusters must be prepared for NSX before upgrading NSX Edge appliances**: Management-plane communication between NSX Manager and Edge via the VIX channel is no longer supported starting in 6.3.0. Only the message bus channel is supported. When you upgrade from NSX 6.2.x or earlier to NSX 6.3.0 or later, you must verify that host clusters where NSX Edge appliances are deployed are prepared for NSX, and that the messaging infrastructure status is GREEN. If host clusters are not prepared for NSX, upgrade of the NSX Edge appliance will fail. See "Upgrade NSX Edge" in the [*NSX Upgrade Guide*](https://docs.vmware.com/en/VMware-NSX-Data-Center-for-vSphere/6.4/com.vmware.nsx.upgrade.doc/GUID-4613AC10-BC73-4404-AF80-26E924EF5FE0.html) for details.
* **Upgrading Edge Services Gateway (ESG):**
   Starting in NSX 6.2.5, resource reservation is carried out at the time of NSX Edge upgrade. When vSphere HA is enabled on a cluster having insufficient resources, the upgrade operation may fail due to vSphere HA constraints being violated.

  To avoid such upgrade failures, perform the following steps before you upgrade an ESG:

  The following resource reservations are used by the NSX Manager if you have not explicitly set values at the time of install or upgrade.

  | NSX Edge Form Factor | CPU Reservation | Memory Reservation |
  | --- | --- | --- |
  | COMPACT | 1000MHz | 512 MB |
  | LARGE | 2000MHz | 1024 MB |
  | QUADLARGE | 4000MHz | 2048 MB |
  | X-LARGE | 6000MHz | 8192 MB |

  1. Always ensure that your installation follows the best practices laid out for vSphere HA. Refer to document [Knowledge Base article 1002080](https://kb.vmware.com/kb/1002080) .
  2. Use the NSX tuning configuration API:
      PUT https://<nsxmanager>/api/4.0/edgePublish/tuningConfiguration
      ensuring that values for edgeVCpuReservationPercentage and edgeMemoryReservationPercentage fit within available resources for the form factor (see table above for defaults).
* **Disable vSphere's Virtual Machine Startup option where vSphere HA is enabled and Edges are deployed.**  After you upgrade your 6.2.4 or earlier NSX Edges to 6.2.5 or later, you must turn off the vSphere Virtual Machine Startup option for each NSX Edge in a cluster where vSphere HA is enabled and Edges are deployed. To do this, open the vSphere Web Client, find the ESXi host where NSX Edge virtual machine resides, click Manage > Settings, and, under Virtual Machines, select VM Startup/Shutdown, click Edit, and make sure that the virtual machine is in Manual mode (that is, make sure it is not added to the Automatic Startup/Shutdown list).
* **Before upgrading to NSX 6.2.5 or later, make sure all load balancer cipher lists are colon separated.**  If your cipher list uses another separator such as a comma, make a PUT call to https://nsxmgr\_ip/api/4.0/edges/EdgeID/loadbalancer/config/applicationprofiles and replace each  *<ciphers> </ciphers>* list in *<clientssl> </clientssl>* and *<serverssl> </serverssl>* with a colon-separated list. For example, the relevant segment of the request body might look like the following. Repeat this procedure for all application profiles:

  ```

  <applicationProfile>
    <name>https-profile</name>
    <insertXForwardedFor>false</insertXForwardedFor>
    <sslPassthrough>false</sslPassthrough>
    <template>HTTPS</template>
    <serverSslEnabled>true</serverSslEnabled>
    <clientSsl>
      <ciphers>AES128-SHA:AES256-SHA:ECDHE-ECDSA-AES256-SHA</ciphers>
      <clientAuth>ignore</clientAuth>
      <serviceCertificate>certificate-4</serviceCertificate>
    </clientSsl>
    <serverSsl>
      <ciphers>AES128-SHA:AES256-SHA:ECDHE-ECDSA-AES256-SHA</ciphers>
      <serviceCertificate>certificate-4</serviceCertificate>
    </serverSsl>
    ...
  </applicationProfile>
  ```
* **Set Correct Cipher version for Load Balanced Clients on vROPs versions older than 6.2.0:**  vROPs pool members on vROPs versions older than 6.2.0 use TLS version 1.0 and therefore you must set a monitor extension value explicitly by setting "ssl-version=10" in the NSX Load Balancer configuration. See "Create a Service Monitor" in the [*NSX Administration Guide*](https://docs.vmware.com/en/VMware-NSX-Data-Center-for-vSphere/6.4/com.vmware.nsx.admin.doc/GUID-B5C70003-8194-4EC3-AB36-54C848508818.html) for instructions.
  ```

  {
      "expected" : null,
      "extension" : "ssl-version=10",
      "send" : null,
      "maxRetries" : 2,
      "name" : "sm_vrops",
      "url" : "/suite-api/api/deployment/node/status",
      "timeout" : 5,
      "type" : "https",
      "receive" : null,
      "interval" : 60,
      "method" : "GET"
  }
  ```
* **After upgrading to NSX 6.4.6, L2 bridges and interfaces on a DLR cannot connect to logical switches belonging to different transport zones:**  In NSX 6.4.5 or earlier, L2 bridge instances and interfaces on a Distributed Logical Router (DLR) supported use of logical switches that belonged to different transport zones. Starting in NSX 6.4.6, this configuration is not supported. The L2 bridge instances and interfaces on a DLR must connect to logical switches that are in a single transport zone. If logical switches from multiple transport zones are used, edge upgrade is blocked during pre-upgrade validation checks when you upgrade NSX to 6.4.6. To resolve this edge upgrade issue, ensure that the bridge instances and interfaces on a DLR are connected to logical switches in a single transport zone.
* **After upgrading to NSX 6.4.7, bridges and interfaces on a DLR cannot connect to dvPortGroups belonging to different VDS:** If such a configuration is present, NSX Manager upgrade to 6.4.7 is blocked in pre-upgrade validation checks. To resolve this, ensure that interfaces and L2 bridges of a DLR are connected to a single VDS.
* **After upgrading to NSX 6.4.7, DLR cannot be connected to VLAN-backed port groups if the transport zone of logical switch it is connected to spans more than one VDS:** This is to ensure correct alignment of DLR instances with logical switch dvPortGroups across hosts. If such configuration is present, NSX Manager upgrade to 6.4.7 is blocked in pre-upgrade validation checks. To resolve this issue, ensure that there are no logical interfaces connected to VLAN-backed port groups, if a logical interface exists with a logical switch belonging to a transport zone spanning multiple VDS.
* **After upgrading to NSX 6.4.7, different DLRs cannot have their interfaces and L2 bridges on a same network:** If such a configuration is present, NSX Manager upgrade to 6.4.7 is blocked in pre-upgrade validation checks. To resolve this issue, ensure that a network is used in only a single DLR.

### Upgrade Notes for FIPS

When you upgrade from a version of NSX earlier than NSX 6.3.0 to NSX 6.3.0 or later, you must not enable FIPS mode before the upgrade is completed. Enabling FIPS mode before the upgrade is complete will interrupt communication between upgraded and not-upgraded components. See "Understanding FIPS Mode and NSX Upgrade" in the [*NSX Upgrade Guide*](https://docs.vmware.com/en/VMware-NSX-Data-Center-for-vSphere/6.4/com.vmware.nsx.upgrade.doc/GUID-4613AC10-BC73-4404-AF80-26E924EF5FE0.html) for more information.

* Ciphers supported on OS X Yosemite and OS X El Capitan: If you are using SSL VPN client on OS X 10.11 (EL Capitan), you will be able to connect using AES128-GCM-SHA256, ECDHE-RSA-AES128-GCM-SHA256, ECDHE-RSA-AES256-GCM-SHA38, AES256-SHA and AES128-SHA ciphers and those using OS X 10.10 (Yosemite) will be able to connect using AES256-SHA and AES128-SHA ciphers only.
* Do not enable FIPS before the upgrade to NSX 6.3.x is complete. See "Understanding FIPS Mode and NSX Upgrade" in the [*NSX Upgrade Guide*](https://docs.vmware.com/en/VMware-NSX-Data-Center-for-vSphere/6.4/com.vmware.nsx.upgrade.doc/GUID-4613AC10-BC73-4404-AF80-26E924EF5FE0.html) for more information.
* Before you enable FIPS, verify any partner solutions are FIPS mode certified. See the [VMware Compatibility Guide](http://www.vmware.com/resources/compatibility/search.php?deviceCategory=security) and the relevant partner documentation.

## FIPS Compliance

NSX 6.4 uses FIPS 140-2 validated cryptographic modules for all security-related cryptography when correctly configured.

**Note:**

* **Controller and Clustering VPN:** The NSX Controller uses IPsec VPN to connect Controller clusters. The IPsec VPN uses the VMware Linux Kernel Cryptographic Module (VMware Photon OS 1.0 environment), which is in the process of being CMVP validated.
* **Edge IPsec VPN:** The NSX Edge IPsec VPN uses the VMware Linux Kernel Cryptographic Module (VMware NSX OS 4.4 environment), which is in the process of being CMVP validated.

## Document Revision History

February 8, 2022: First edition.
 May 31, 2022: Second edition. Added known issue 2968016.
 October 19, 2022. Added known issue 3046940.

## Resolved Issues

* **Fixed Issue 2726343: Reconfiguration of Load Balancer rejected with error message.**

  Reconfiguration of Load Balancer may fail with the following error message: "error code 10014 : edge appliance memory is not enough to apply the configuration, pls try it later". You will not be able to reconfigure the load balancer.
* **Fixed Issue 2737885:  On an NSX for vSphere 6.4.9 or 6.4.10 setup, log message, "Unexpected exception causing shutdown while sock still open java.io.EOFException" is observed on all three controller zookeeper's log.**

  May potentially cause issues on high scale.
* **Fixed Issue 2863457: Kernel crash during conntrack revalidation.**

  Edge kernel crash and HA failover.
* **Fixed Issue 2868854: Memory leak when using trunk interfaces on Edge.**

  Edge VM OOM situation and reboot, periodically. The amount of time needed to reach OOM depends on Edge VM size, configuration and network traffic.
* **Fixed Issue 2875095: IPAddress translation for security group containing Security Tag as members may return NPE if there exists at least one vNIC in the database with no IpAddress associated to it.**

  Unable to publish the DFW rule, security impacted. This issue occurs only when SpoofGuard is enabled.
* **Fixed Issue 2903445: The latency module heap size is not enough for bigger pcpu server.**

  Unable to install NSX for vSphere 6.4.12 on specific hardware.
* **Fixed Issue 2810821: Guest Introspection service status goes into Warning state for short amounts of time and comes back up.**

  There is no functional impact to the user, only a warning shown in the UI. The alarms generated are quickly resolved automatically.
* **Fixed Issue 2822265: More than 8000+ firewall rules were deleted after user filtered the rules. The clear filter API failed when trying to clear the filter.**

  Firewall rules might be deleted if you ignore the error displayed on the UI after API call to clear filter fails to complete within two minutes.
* **Fixed Issue 2886595: ALG-based services are not working in NSX Data Center for vSphere 6.4.11 and 6.4.12.**

  Edge Firewall drops packets for ALG-based services FTP/SFTP/SNMP when using NSX Data Center for vSphere 6.4.11 and 6.4.12.
* **Issue 3046940: Internal server error while creating Universal security group with Universal Macset on UI.**

  You will not be able to create a Universal security group with Universal Macset through the UI.

  Workaround: Create Universal Security Group with Universal Mac Set using the API.

## Known Issues

* **Issue 2543977: Distributed Firewall IPFIX collector is unable to see flows from UDP ports 137 or 138.**

  When IPFIX is enabled on the DFW, NetBIOS flows with ports 137 or 138 were not sent by the host.

  Workaround:

  Use vSphere Client or NSX REST API to remove excluded ports 137 or 138 from flow exclusions.
* **Issue 2574333: Edge vNIC configuration takes more than two minutes when NAT is configured with 8000 rules.**

  vSphere Client shows NSX Manager is not available. The UI times out after two minutes. vNIC configuration completes successfully in 2-3 mins.

  Workaround: None
* **Issue 1993241: IPSec Tunnel redundancy using static routing is not supported**

  IPSec traffic will fail if primary tunnel goes down, causing traffic disruption. This issue is known since NSX 6.4.2.

  Workaround: Disable and enable service.
* **Issue 1263858: SSL VPN does not send upgrade notification to remote client.**

  SSL VPN gateway does not send an upgrade notification to users. The administrator has to manually communicate that the SSL VPN gateway (server) is updated to remote users and they must update their clients.

  Workaround: Users need to uninstall the older version of client and install the latest version manually.
* **Issue 2598824: vMotion of NSX-v Manager displays warning message.**

  The following message is shown on vMotion of NSX-v Manager: "Virtual ethernet card 'Network adapter 1' is not supported. This is not a limitation of the host in general, but of the virtual machine's configured guest OS on the selected host.”

  Workaround: Ignore the warning message to successfully complete the vMotion.
* **Issue 2112662: Support bundle GUI does not list Hosts if VXLAN is not configured.**

  Unable to download tech support bundle of ESX host if VXLAN is not enabled.

  Workaround: Enable VXLAN on the cluster to see the ESX hosts.
* **Issue 2576294: When the logical interfaces on a DLR are connected to multiple VDS, the logical interfaces get attached to incorrect VDS on the host when the host is part of two VXLAN VDS.**

  If the DLR is force synced or redeployed in a misconfigured state, all the DLR logical interfaces get attached to the incorrect VDS. In addition, logical interfaces for all the new DLRs get attached to incorrect VDS on the host. Data path traffic is disrupted.

  Workaround:

  + Remove the second VDS that is not used for VXLAN preparation of the cluster.
  + Depending on the current state of the host configuration, either redeploy, or force sync, or sync route service to correct the configuration on host.
* **Issue 2582197: When an NSX Edge interface is configured with /32 subnet mask, the connected route is not seen in the routing or forwarding table.**

  Traffic to that interface might still work, but a static route would be needed to reach any peers. Users cannot use interfaces with /32 subnet mask. There is no issue if any other subnet mask is used.

  Workaround: Except for /32 subnet mask, use any other subnet mask.
* **Issue 2238989: Post upgrade of the vSphere Distributed Switch on the ESXi host, the Software RSS feature of the VDS does not take effect.**

  In hosts that have Software Receive Side Scaling (SoftRSS) enabled, **com.vmware.net.vdr.softrss** VDS property does not get restored post VDS upgrade. This causes SoftRSS to get disabled. The /var/run/log/vmkernel.log file shows errors related to softrss property configuration.

  Workaround:

  Before upgrading the VDS, remove the softrss property and reconfigure it post VDS upgrade.
* **Issue 2107188: When the name of a VDS switch contains non-ASCII characters, NSX VIBs upgrade fails on ESXi hosts.**

  The /var/run/log/esxupdate.log file shows the upgrade status.

  Workaround:

  Change the VDS name to use ASCII characters and upgrade again.
* **Issue 2590902: After upgrading to NSX 6.4.7, when a static IPv6 address is assigned to workload VMs on an IPv6 network, the VMs are unable to ping the IPv6 gateway interface of the edge.**

  This issue occurs after upgrading the vSphere Distributed Switches from 6.x to 7.0.

  Workaround 1:

  Select the VDS where all the hosts are connected, go to the Edit setting, and under Multicast option switch to basic.

  Workaround 2:

  Add the following rules on the edge firewall:

  + Ping allow rule.
  + Multicast Listener Discover (MLD) allow rule, which are icmp6, type 130 (v1) and type 143 (v2).
* **Issue 2395158: Firewall section is grayed out when Enterprise Admin role is assigned to an Active Directory group.**

  Enterprise Admin role is assigned to an Active Directory group by navigating to **Users and Domains** > **Users** > **Identity User** > **Specify vCenter Group**. When users belonging to an Active Directory group log in and lock a firewall section, the locked section is grayed out for these users after the page is refreshed.

  Workaround:

  Instead of assigning the Enterprise Admin role to the Active Directory group, assign this role directly to the users by navigating to **Users and Domains** > **Users** > **Identity User** > **Specify vCenter User**.
* **Issue 2444677: Kernel panic error occurs on the NSX Edge when large files are transferred through L2 VPN over IPSec VPN tunnels.**

  This error occurs when the MTU is set to 1500 bytes.

  Workaround: Set the MTU to 1600 bytes.
* **Issue 2574260: Updating an existing Logical Switch configuration to enable the use of guest VLANs (Virtual Guest Tagging or 802.1q VLAN tagging) does not produce the expected result.**

  The VDS portgroup that is associated with the Logical Switch is not updated accordingly.

  Workaround: None
* **Issue 2661353: If keepalives are enabled for GRE tunnel, the GRE tunnel flaps occasionally making it unstable.**

  There can be traffic outage if GRE tunnel flaps. This happens only when GRE keepalives are enabled.

  Workaround: Disable the GRE keepalives on ESG.
* **Issue 2723675: DNS resolution for private networks is not working with MacOS Client on BigSur.**

  DNS will not work on MacOS BigSur.

  Workaround: None.
* **Issue 2382623: VXLAN configuration task timed out waiting for Host Preparation status to turn GREEN.**

  VXLAN status on the cluster gives an error, "VXLAN configuration task timed-out waiting for Host Preparation status to turn GREEN", but host preparation is GREEN. This error is cosmetic and should not have any impact on the data path.

  Workaround: Identify the list of ESXi hosts contributing to the cluster's error by reviewing the NSX Manager logs (vsm.log) looking for the message above. Once the ESXi hosts are identified, take the offending ESXi hosts out of the cluster and put them back in. This will re-install the VIBs and correct the error status.
* **Issue 2968016: After upgrading to NSX for vSphere version 6.4.4 or higher, LB monitor with HOST header configured is not working as expected.**

  The NSX upgrade involves a nagios plugin version upgrade. On this new version, this can result in the HOST not being included in the header and the LB monitor with HOST header configured is failed.

  Workaround: See knowledge base article 79469 for details.

check-circle-line

exclamation-circle-line

close-line

![Scroll to top icon](/uicontent/images/scroll_top.svg)

![](/uicontent/images/feedback.svg)

             [![VMware](https://www.vmware.com/content/dam/vmwaredesigns/scrapercontent/resources/logos/vmware-logo-grey.svg "VMware")](https://www.vmware.com/)

Resources

* [Blogs](https://blogs.vmware.com/)
* [Careers](https://www.broadcom.com/jobs)
* [Communities](https://community.broadcom.com/)
* [Customer Stories](https://www.vmware.com/resources/customers)

* [News and Stories](https://news.broadcom.com/)
* [Topics](https://www.vmware.com/topics/)
* [Trust Center](https://www.vmware.com/info/trust-center/)

Support

* [Broadcom Support](https://support.broadcom.com/)
* [Documentation](https://docs.vmware.com)
* [Hands-On Labs](https://www.vmware.com/resources/hands-on-labs)
* [Licensing](https://www.broadcom.com/licensing)

* [Twitter](https://twitter.com/VMware)
* [YouTube](https://www.youtube.com/user/vmwaretv)
* [Facebook](https://www.facebook.com/vmware)
* [LinkedIn](https://www.linkedin.com/company/vmware/mycompany/)
* [Contact Sales](https://go-vmware.broadcom.com/contact-us)

---

  Copyright © 2005-2024 Broadcom. All Rights Reserved. The term "Broadcom" refers to Broadcom Inc. and/or its subsidiaries. [Accessibility](https://www.broadcom.com/company/legal/accessibility "Accessibility") [Privacy](https://www.broadcom.com/privacy "Privacy") [Supplier Responsibility](https://www.broadcom.com/company/citizenship/supplier-responsibility "Supplier Responsibility") [Terms Of Use](https://www.broadcom.com/company/legal/terms-of-use "Terms Of Use")

#####

×

Share on Social Media?

×

#####

×

exclamation-circle-line

check-circle-line

 :

#####

×

exclamation-circle-line

|

check-circle-line

exclamation-circle-line

x

#####

×

"
"?

×



=== Content from docs.vmware.com_b8439bff_20250115_114000.html ===


[![](/uicontent/images/VMware_by_Broadcom.png)Docs](/)

* ![](/uicontent/images/VMware_by_Broadcom_Gray-Black.png) Docs
  times
* [(current)](/)

* ![](/uicontent/images/nwMyLibrary.png)
  #####

  ![](/uicontent/images/icon-handshake.png)

  ![](/uicontent/images/icon-support.png)

  ![](/uicontent/images/icon-networking-bg.png)
  [VMware Communities](https://communities.vmware.com/)

  ![](/uicontent/images/icon-download-bg.png)

 This site will be decommissioned on January 30th 2025. After that date content will be available at [techdocs.broadcom.com](https://techdocs.broadcom.com/).

![](/uicontent/images/share-mylibrary.svg)

![](/uicontent/images/add-to-library-colorless.svg)

#

![](/uicontent/images/twitter.svg)
![](/uicontent/images/facebook.svg)
![](/uicontent/images/linkedin.svg)
![](/uicontent/images/weibo.svg)

![](/uicontent/images/pdf.svg)

![](/uicontent/images/feedback.svg)

![](/uicontent/images/edit.svg)

![](/uicontent/images/review.svg)

Twitter
Facebook
LinkedIn
微博

| VMware Cloud Foundation 3.11 | 14 FEB 2021 | Build 19312783  VMware Cloud Foundation 3.11.0.1 | 07 APR 2022 | Build 19571759  Check for additions and updates to these release notes. |

## What's New

VMware Cloud Foundation 3.11 can either be upgraded from VMware Cloud Foundation 3.10.2.2 (sequential upgrade) or from VMware Cloud Foundation 3.5 or later (skip-level upgrade). It cannot be deployed as a new release. For more information, see **Upgrade Information** below.

The VMware Cloud Foundation (VCF) 3.11 release includes the following:

* **Security fixes for Apache Log4j Remote Code Execution Vulnerability**: This release fixes CVE-2021-44228 and CVE-2021-45046. See [VMSA-2021-0028](https://www.vmware.com/security/advisories/VMSA-2021-0028.html).
* **Security fixes for Apache HTTP Server**: This release fixes CVE-2021-40438. See [CVE-2021-40438](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-40438).
* **Improvements to upgrade prechecks**: Upgrade prechecks have been expanded to verify filesystem capacity, file permissions, and passwords. These improved prechecks help identify issues that you need to resolve to ensure a smooth upgrade.
* **Skip-level upgrade to VMware Cloud Foundation 3.11:** Upgrade directly to VMware Cloud Foundation 3.11 using the skip-level upgrade CLI tool, which has been updated with additional guardrails, prechecks, and usability improvements.
* **Scaling improvements**: VMware Cloud Foundation 3.11 supports up to 1000 ESXi hosts per SDDC Manager instance. See [VMware Configuration Maximums](https://configmax.vmware.com/home) for details on all supported maximums.
* **BOM Updates**: Updated Bill of Materials with new product versions.

## VMware Cloud Foundation Bill of Materials (BOM)

The Cloud Foundation software product is comprised of the following software Bill-of-Materials (BOM). The components in the BOM are interoperable and compatible.

| Software Component | Version | Date | Build Number |
| --- | --- | --- | --- |
| SDDC Manager | 3.11 | 14 FEB 2022 | 19312783 |
| VMware vCenter Server Appliance | 6.7 Update 3q | 08 FEB 2022 | 19300125 |
| VMware ESXi | ESXi670-202201001 | 25 JAN 2022 | 19195723 |
| VMware NSX Data Center for vSphere | 6.4.12 | 21 DEC 2021 | 19066632 |
| VMware NSX-T Data Center | 3.0.3.1 | 23 DEC 2021 | 19067109 |
| VMware vRealize Suite Lifecycle Manager | 2.1 Patch 3 | 12 JAN 2022 | 19201324 |
| VMware vRealize Log Insight | 4.8 | 11 APR 2019 | 13036238 |
| vRealize Log Insight Content Pack for NSX for vSphere | 3.9 | n/a | n/a |
| vRealize Log Insight Content Pack for Linux | 2.0.1 | n/a | n/a |
| vRealize Log Insight Content Pack for vRealize Automation 7.5+ | 1.0 | n/a | n/a |
| vRealize Log Insight Content Pack for vRealize Orchestrator 7.0.1+ | 2.1 | n/a | n/a |
| vRealize Log insight Content Pack for NSX-T | 3.8.2 | n/a | n/a |
| vSAN Content Pack for Log Insight | 2.2 | n/a | n/a |
| vRealize Operations Manager | 7.5 | 11 APR 2019 | 13165949 |
| vRealize Automation | 7.6 | 11 APR 2019 | 13027280 |
| VMware Horizon 7 | 7.10.3 | 17 DEC 2021 | 19069415 |

Note:

* vRealize Log Insight Content Packs are deployed during the workload domain creation.
* VMware Solution Exchange and the vRealize Log Insight in-product marketplace store only the latest versions of the content packs for vRealize Log Insight. The Bill of Materials table contains the latest versions of the packs that were available at the time VMware Cloud Foundation is released. When you deploy the Cloud Foundation components, it is possible that the version of a content pack within the in-product marketplace for vRealize Log Insight is newer than the one used for this release.
* To remediate [VMSA-2020-0007](https://www.vmware.com/security/advisories/VMSA-2020-0007.html) (CVE-2020-3953 and CVE-2020-3954) for vRealize Log Insight 4.8, you must apply the vRealize Log Insight 4.8 security patch. For information on the security patch, see [KB article 79168](https://kb.vmware.com/s/article/79168). ​

## VMware Software Edition License Information

The SDDC Manager software is licensed under the Cloud Foundation license. As part of this product, the SDDC Manager software deploys specific VMware software products.

The following VMware software components deployed by SDDC Manager are licensed under the Cloud Foundation license:

* VMware ESXi
* VMware vSAN
* VMware NSX Data Center for vSphere

The following VMware software components deployed by SDDC Manager are licensed separately:

* VMware vCenter Server

* **NOTE** Only one vCenter Server license is required for all vCenter Servers deployed in a Cloud Foundation system.

* VMware NSX-T
* VMware Horizon 7
* VMware vRealize Automation
* VMware vRealize Operations
* VMware vRealize Log Insight and content packs

* **NOTE** Cloud Foundation permits limited use of vRealize Log Insight for the management domain without the purchase of a vRealize Log Insight license.

For details about the specific VMware software editions that are licensed under the licenses you have purchased, see the **Cloud Foundation Bill of Materials (BOM)** section above.

For general information about the product, see [VMware Cloud Foundation](http://www.vmware.com/products/cloud-foundation.html).

## Supported Hardware

For details on supported configurations, see the [VMware Compatibility Guide (VCG)](https://www.vmware.com/resources/compatibility/search.php) and the Hardware Requirements section on the Prerequisite Checklist tab in the [Planning and Preparation Workbook](https://docs.vmware.com/en/VMware-Validated-Design/6.2/vmware-validated-design-62-vmware-cloud-foundation-42-sddc-planning-and-preparation-workbook.zip).

## Documentation

To access the Cloud Foundation documentation, go to the [VMware Cloud Foundation product documentation](https://docs.vmware.com/en/VMware-Cloud-Foundation/index.html).

To access the documentation for VMware software products that SDDC Manager can deploy, see the product documentation and use the drop-down menus on the page to choose the appropriate version:

* [VMware vSphere product documentation](https://docs.vmware.com/en/VMware-vSphere/index.html), also has documentation about ESXi and vCenter Server
* [VMware vSAN product documentation](https://docs.vmware.com/en/VMware-vSAN/index.html)
* [VMware NSX Data Center for vSphere product documentation](https://docs.vmware.com/en/VMware-NSX-for-vSphere/index.html)
* [VMware NSX-T Data Center product documentation](https://docs.vmware.com/en/VMware-NSX-T/index.html)
* [VMware vRealize Log Insight product documentation](https://docs.vmware.com/en/vRealize-Log-Insight/index.html)
* [VMware vRealize Automation product documentation](https://docs.vmware.com/en/vRealize-Automation/index.html)
* [VMware vRealize Operations Manager documentation](https://docs.vmware.com/en/vRealize-Operations-Manager/index.html)
* [VMware Horizon 7 documentation](https://docs.vmware.com/en/VMware-Horizon-7/index.html)

## Browser Compatibility and Screen Resolutions

The Cloud Foundation web-based interface supports the latest two versions of the following web browsers except Internet Explorer:

* Google Chrome
* Mozilla Firefox
* Microsoft Edge
* Internet Explorer: Version 11

For the Web-based user interfaces, the supported standard resolution is 1024 by 768 pixels. For best results, use a screen resolution within these tested resolutions:

* 1024 by 768 pixels (standard)
* 1366 by 768 pixels
* 1280 by 1024 pixels
* 1680 by 1050 pixels

Resolutions below 1024 by 768, such as 640 by 960 or 480 by 800, are not supported.

## Upgrade Information

You can upgrade to VMware Cloud Foundation 3.11 either from VMware Cloud Foundation 3.10.2.2 (sequential upgrade) or from VMware Cloud Foundation 3.7.1 or later (skip-level upgrade). VMware Cloud Foundation 3.11 cannot be deployed as a new release. For upgrade information, refer to the [*VMware Cloud Foundation Upgrade Guide*](https://docs.vmware.com/en/VMware-Cloud-Foundation/3.11/vcf-lifecycle/GUID-B384B08D-3652-45E2-8AA9-AF53066F5F70.html).

VMware Cloud Foundation 3.11 is supported as a source version for migration to VMware Cloud Foundation 4.x.

**vRealize Suite Lifecycle Manager Version 2.1.0 Patch 3**

There is no upgrade bundle for vRealize Suite Lifecycle Manager Version 2.1.0 Patch 3. To upgrade, follow the process described in the [VMware vRealize Suite Lifecycle Manager 2.1 Patch 3 Release Notes](https://docs.vmware.com/en/VMware-vRealize-Suite-Lifecycle-Manager/2.1/rn/VMware-vRealize-Suite-Lifecycle-Manager-21-Patch-3-Release-Notes.html).

**Design Considerations for Multiple Availability Zones**

NSX-T Data Center 3.x changes how the northbound traffic flow can be influenced. If you have the following architecture, you must change the Tier-0 gateway architecture **before** you upgrade to NSX-T Data Center 3.x:

* An NSX Edge cluster with edge nodes placed in both availability zones (typically two edge nodes pinned to Availability Zone 1 and two edge nodes pinned to Availability Zone 2)
* An Active/Active Tier-0 gateway architecture where the Tier-0 gateway spans edge nodes in both availability zones.
* Deployed in a data center infrastructure that cannot tolerate asymmetrical routing to or from each availability zone, for example, for physical data center firewalls, and other.

Change to a Tier-0 gateway architecture where the Tier-0 gateway is active only in a single availability zone at a time in one of the following ways:

* **Recommended**: Place an NSX Edge cluster with edge nodes in a single availability zone only (typically Availability Zone 1), that fail over using vSphere HA to Availability Zone 2 on failure. This change requires changes in the data center fabric including stretching of the Uplink and Edge TEP VLANs between the availability zones. See [KB 87426](https://kb.vmware.com/s/article/87426) for more information.
* Migrate to an Active/Standby Tier-0 gateway. Follow the [NSX-T Data Center 3.x product documentation](https://docs.vmware.com/en/VMware-NSX-T-Data-Center/index.html) for changing from an Active/Active to an Active/Standby architecture of the Tier-0 gateway.

**Changing from a Three N-VDS to Single N-VDS Edge Node Design**

Starting with NSX-T Data Center 2.5, a single N-VDS switch design is available in the NSX Edge node. Changing from three N-VDS instances to a single N-VDS provides network throughput and scalability improvements in NSX-T Data Center. It is recommended for all environments but highly recommended for environments deployed at scale.

The procedure involves the following high-level steps:

* Deploy a new NSX Edge cluster with new edge nodes based on the single N-VDS design.
* Deploy a new Tier-0 gateway and verify connectivity.
* Once tested, you can reconfigure your Tier-1 gateways to utilize the new Tier-0 gateway on the single N-VDS edge cluster.

See [KB 87426](https://kb.vmware.com/s/article/87426) for more information.

## VMware Cloud Foundation 3.11.0.1 Release Information

You can upgrade to VMware Cloud Foundation 3.11.0.1 either from VMware Cloud Foundation 3.11 (sequential upgrade) or from VMware Cloud Foundation 3.7.1 or later (skip-level upgrade). VMware Cloud Foundation 3.11.0.1 cannot be deployed as a new release. For upgrade information, refer to the VMware Cloud Foundation Upgrade Guide. It is strongly recommended that all customers on VCF 3.x upgrade to VCF 3.11.0.1.

VMware Cloud Foundation 3.11.0.1 contains the following BOM updates:

| Software Component | Version | Date | Build Number |
| --- | --- | --- | --- |
| SDDC Manager | 3.11.0.1 | 07 APR 2022 | 19571759 |
| VMware NSX Data Center for vSphere | 6.4.13 | 08 FEB 2022 | 19307994 |

SDDC Manager 3.11.0.1 fixes the issue:

* Deleting an NSX for vSphere (NSX-V) VI workload domain incorrectly deletes the NSX controllers for the management domain

VMware NSX Data Center for vSphere 6.4.13 addresses the security vulnerability described in [VMSA-2022-0005](https://www.vmware.com/security/advisories/VMSA-2022-0005.html)

## Resolved Issues

The following issues are resolved in VMware Cloud Foundation 3.11:

* VMware vCenter Server Appliance 6.7 Update 3p addresses security vulnerabilities CVE-2021-21980 and CVE-2021-22049 as described in VMware Security Advisory [VMSA-2021-0027](https://www.vmware.com/security/advisories/VMSA-2021-0027.html).
* Inapplicable ESXi upgrade bundles are displayed after upgrade has been scheduled
* Add host workflow fails
* When the user password in the `/opt/vmware/vcf/lcm/lcm-app/conf/application.properties` file contains a backslash (\), Lifecycle Manager does not start and displays the error `Password authentication failed for user lcm`.
* Credential logging vulnerability as described in [VMSA-2022-0003](https://www.vmware.com/security/advisories/VMSA-2022-0003.html). See [KB 87050](https://kb.vmware.com/s/article/87050) for more information.
* Deleting an NSX-T workload domain or cluster containing a dead host fails at transport node deletion step.

## Known Issues

* [Upgrade Known Issues](#Known Issues- Upgrade Known Issues)
* [vRealize Suite Known Issues](#Known Issues- vRealize Suite Known Issues )
* [Networking Known Issues](#Known Issues-Networking Known Issues)
* [SDDC Manager Known Issues](#Known Issues- SDDC Manager Known Issues)
* [Workload Domain Known Issues](#Known Issues- Workload Domain Known Issues)
* [Multi-Instance Management Known Issues](#Known Issues-Multi-Instance Management Known Issues)
* [Backup and Restore Known Issues](#Known Issues-Backup and Restore Known Issues)

### Upgrade Known Issues

* **File permissions precheck gets stuck or captures permission issues in wrong directories**

  There are two potential issues:

  + The LCM directories permission precheck does not finish for more than an hour.
  + The LCM directories permission precheck reports permission/ownership issues in `/var/log` directories.

  Workaround: See [KB 90205](https://kb.vmware.com/s/article/90205).
* **Upgrade precheck fails for PSC SSO**

  If the maximum lifetime password policy for vCenter Single Sign-On local accounts is set to a number greater than 9999, then the upgrade precheck fails.

  Workaround: Set the maximum lifetime password policy to any number less than or equal to 9999. See [KB 88163](https://kb.vmware.com/s/article/88163) for more information.
* **There is a cosmetic mismatch between the posted manifest date (02/11/2022) and the release notes software date (02/14/2022).**

  There are no technical side effects.
* **The vRealize Automation upgrade reports the "Precheck Execution Failure : Make sure the latest version of VMware Tools is installed" message**

  The vRealize Automation IaaS VMs must have the same version of VMware Tools as the ESXi hosts on which the VMs reside.

  Workaround: Upgrade VMware Tools on the vRealize Automation IaaS VMs.
* **Error upgrading vRealize Automation**

  Under certain circumstances, upgrading vRealize Automation may fail with a message similar to:

  ```
  An automated upgrade has failed. Manual intervention is required.
  vRealize Suite Lifecycle Manager Pre-upgrade checks for vRealize Automation have failed:
  vRealize Automation Validations : iaasms1.rainpole.local : RebootPending : Check if reboot is pending : Reboot the machine.
  vRealize Automation Validations : iaasms2.rainpole.local : RebootPending : Check if reboot is pending : Reboot the machine.
  Please retry the upgrade once the upgrade is available again.
  ```

  1. Log-in into the first VM listed in the error message using RDP or the VMware Remote Console.
  2. Reboot the VM.
  3. Wait 5 minutes after the login screen of the VM appears.
  4. Repeat steps 1-3 for the next VM listed in the error message.
  5. Once you have restarted all the VMs listed in the error message, retry the vRealize Automation upgrade.
* **When there is no associated workload domain to vRealize Automation, the VRA VM NODES CONSISTENCY CHECK upgrade precheck fails**

  This upgrade precheck compares the content in the logical inventory on the SDDC Manager and the content in the vRealize Lifecycle Manager environment. When there is no associated workload domain, the vRealize Lifecycle Manager environment does not contain information about the `iaasagent1.rainpole.local` and `iaasagent2.rainpole.local` nodes. Therefore the check fails.

  Workaround: None. You can safely ignore a failed `VRA VM NODES CONSISTENCY CHECK` during the upgrade precheck. The upgrade will succeed even with this error.
* **NSX Data Center for vSphere upgrade fails with the message "Host Prep remediation failed"**

  After addressing the issue, the NSX Data Center for vSphere bundle no longer appears as an available update.

  Workaround: To complete the upgrade, manually enable the anti-affinity rules.

  1. Log in to the management vCenter Server using the vSphere Client.
  2. Click **Menu > Hosts and Clusters** and select the cluster on which host prep remediation failed (for example SDDC-Cluster1).
  3. Click **Configure > Configuration > VM/Host Rules**.
  4. Select **NSX Controller Anti-Affinity Rule** and click **Edit**.
  5. Select **Enable rule** and click **OK**.

  This completes the NSX Data Center for vSphere upgrade.
* **Upgrade precheck may fail as you approach the maximum number of supported ESXi hosts**

  In a large environment with many ESXi hosts, vSAN prechecks may fail.

  Workaround: Turn off vSAN prechecks.

  1. SSH to the SDDC Manager appliance as the **vcf** user.
  2. Enter `su` to switch to the root user.
  3. Turn off the vSAN prechecks:

     + Enter `vi /opt/vmware/vcf/lcm/lcm-app/conf/application-prod.properties`
     + In the VSAN CONFIGURATION section, set the following properties to false:
     + `vsan.healthcheck.enabled=false`
     + `vsan.hcl.update.enabled=false`
     + `vsan.precheck.enabled=false`
     + Save the changes.
  4. Restart LCM.

     `systemctl restart lcm`
* **Task panel does not show correct upgrade tasks for NSX-T workload domain upgrades**

  When you upgrade NSX-T workload domains. the task panel does not show upgrade status correctly. This is a UI issue only and there is no impact on the upgrade workflow.

  Workaround: Monitor upgrade status by navigating to the Update/Patches tab of the relevant workload domain:

  1. On the SDDC Manager Dashboard, click *Inventory* -> *Workload Domains*.
  2. In the Domain column, click the appropriate workload domain name.
  3. Click the *Update/Patches* tab.
  4. Monitor upgrade status.
* **Exception displayed when a scheduled NSX-T upgrade begins during an idle SDDC Manager session**

  When a scheduled NSX-T upgrade begins during an idle SDDC Manager session, the following UI exception is displayed: Retrieving NSXT upgrade failed with unknown exception

  This is a UI issue only. There is no impact on the upgrade workflow.

  Workaround: Refresh the web browser.
* **After upgrading to VMware Cloud Foundation 3.11 the NSX Edge password precheck is turned off**

  Enable the NSX Edge password precheck so that future upgrades of NSX-T based VI workload domains do not fail due to expired passwords.

  Workaround:

  1. SSH to the SDDC Manager appliance as the **vcf** user.
  2. Type `su` to switch to the root user.
  3. Edit the /opt/vmware/vcf/lcm/lcm-app/conf/feature.properties file as shown below:

     + `#feature flag for NSXT Edge VM Password Expiry Precheck`
     + `feature.lcm.nsxt.edge.password.validation=true`
  4. Run the following command: `systemctl restart lcm`

### vRealize Suite Known Issues

* **vRealize Operations Manager: VMware Security Advisory VMSA-2021-0018**

  [VMSA-2021-0018](https://www.vmware.com/security/advisories/VMSA-2021-0018.html) describes security vulnerabilities that affect VMware Cloud Foundation.

  + The vRealize Operations Manager API contains an arbitrary file read vulnerability. A malicious actor with administrative access to vRealize Operations Manager API can read any arbitrary file on server leading to information disclosure.The Common Vulnerabilities and Exposures project (cve.mitre.org) has assigned identifier CVE-2021-22022 to this issue.
  + The vRealize Operations Manager API has insecure object reference vulnerability. A malicious actor with administrative access to vRealize Operations Manager API may be able to modify other users information leading to an account takeover.The Common Vulnerabilities and Exposures project (cve.mitre.org) has assigned identifier CVE-2021-22023 to this issue.
  + The vRealize Operations Manager API contains an arbitrary log-file read vulnerability. An unauthenticated malicious actor with network access to the vRealize Operations Manager API can read any log file resulting in sensitive information disclosure. The Common Vulnerabilities and Exposures project (cve.mitre.org) has assigned identifier CVE-2021-22024 to this issue.
  + The vRealize Operations Manager API contains a broken access control vulnerability leading to unauthenticated API access. An unauthenticated malicious actor with network access to the vRealize Operations Manager API can add new nodes to existing vROps cluster. The Common Vulnerabilities and Exposures project (cve.mitre.org) has assigned identifier CVE-2021-22025 to this issue.
  + The vRealize Operations Manager API contains a Server Side Request Forgery in multiple end points. An unauthenticated malicious actor with network access to the vRealize Operations Manager API can perform a Server Side Request Forgery attack leading to information disclosure. The Common Vulnerabilities and Exposures project (cve.mitre.org) has assigned identifiers CVE-2021-22026 and CVE-2021-22027 to this issue.

  Workaround: See [KB 85452](https://kb.vmware.com/s/article/85452) for information about applying vRealize Operations Security Patches that resolve the issues.
* **The password update for vRealize Automation and vRealize Operations Manager may run infinitely or may fail when the password contains special character "%"**

  Password management uses the vRealize Lifecycle Manager API to update the password of vRealize Automation and vRealize Operations Manager. When there is special character "%" in either of SSH or API or Administrator credential types of the vRealize Automation and vRealize Operations Manager users, then the vRealize Lifecycle Manager API hangs and doesn't respond to password management. There is a timeout of 5 mins and password management marks the operation as failed.

  Workaround:Retry the password update operation without the special character "%". Ensure that the passwords for all other vRealize Automation and vRealize Operations Manager accounts don't contain the "%" special character.

### Networking Known Issues

* **NSX Manager is not visible in the vSphere Web Client.**

  In addition to NSX Manager not being visible in the vSphere Web Client, the following error message displays in the NSX Home screen: "No NSX Managers available. Verify current user has role assigned on NSX Manager." This issue occurs when vCenter Server is not correctly configured for the account that is logged in.

  Workaround: To resolve this issue, follow the procedure detailed in Knowledge Base article 2080740 ["No NSX Managers available" error in the vSphere Web Client](https://kb.vmware.com/s/article/2080740).

### SDDC Manager Known Issues

* **Unable to delete VI workload domain enabled for vRealize Operations Manager from SDDC Manager.**

  Attempts to delete the vCenter adapter also fail, and return an SSL error.

  Workaround: Use the following procedure to resolve this issue.

  1. Create a vCenter adapter instance in vRealize Operations Manager, as described in [Configure a vCenter Adapter Instance in vRealize Operations Manager](https://docs.vmware.com/en/vRealize-Operations-Manager/7.5/com.vmware.vcom.config.doc/GUID-19DAD6AF-7262-4655-B69F-6C665E33B52F.html). This step is required because the existing adapter was deleted by the failed workload domain deletion.
  2. Follow the procedure described in [Knowledge Base article 56946](https://kb.vmware.com/s/article/56946).
  3. Restart the failed VI workload domain deletion workflow from the SDDC Manager interface.
* **APIs for managing SDDC cannot be executed from the SDDC Manager Dashboard**

  You cannot use the API Explorer in the SDDC Manager Dashboard to execute the APIs for managing SDDC (`/v1/sddc`).

  Workaround: None. These APIs can only be executed using the Cloud Builder as the host.

### Workload Domain Known Issues

* **Adding host fails when host is on a different VLAN**

  A host add operation can sometimes fail if the host is on a different VLAN.

  Workaround:

  1. Before adding the host, add a new portgroup to the VDS for that cluster.
  2. Tag the new portgroup with the VLAN ID of the host to be added.
  3. Add the Host. This workflow fails at the "Migrate host vmknics to dvs" operation.
  4. Locate the failed host in vCenter, and migrate the vmk0 of the host to the new portgroup you created in step 1. For more information, see [Migrate VMkernel Adapters to a vSphere Distributed Switch](https://docs.vmware.com/en/VMware-vSphere/6.7/com.vmware.vsphere.networking.doc/GUID-0DE237B8-1938-4DE9-90EC-718E345B56A0.html) in the vSphere product documentation.
  5. Retry the Add Host operation.

  **NOTE**: If you later remove this host in the future, you must manually remove the portgroup as well if it is not being used by any other host.
* **NSX Manager for VI workload domain is not displayed in vCenter**

  Although NFS-based VI workload domains are created successfully, the NSX Manager VM is not registered in vCenter Server and is not displayed in vCenter.

  Workaround: To resolve this issue, use the following procedure:

  1. Log in to NSX Manager (http://<nsxmanager IP>).
  2. Navigate to **Manage > NSX Management Service**.
  3. Un-register the lookup service and vCenter, then re-register.
  4. Close the browser and log in to vCenter.
* **A vCenter Server on which certificates have been rotated is not accessible from a Horizon workload domain**

  VMware Cloud Foundation does not support the certificate rotation on the Horizon workload domains.

  Workaround: See KB article [70956](https://kb.vmware.com/s/article/70956).
* **Deploying partner services on an NSX-T workload domain displays an error**

  Deploying partner services on an NSX-T workload domain such as McAfee or Trend displays the “Configure NSX at cluster level to deploy Service VM” error.

  Workaround: Attach the Transport node profile to the cluster and try deploying the partner service. After the service is deployed, detach the transport node profile from the cluster.
* **If the witness ESXi version does not match with the host ESXi version in the cluster, vSAN cluster partition may occur**

  vSAN stretch cluster workflow does not check the ESXi version of the witness host. If the witness ESXi version does not match the host version in the cluster, then vSAN cluster partition may happen.

  Workaround:

  1. Upgrade the witness host manually with the matching ESXi version using the vCenter VUM functionality.
  2. Replace or deploy the witness appliance matching with the ESXi version.
* **The certificate rotate operation on the second NSX-T domain fails**

  Certificate rotation works on the first NSX-T workload domain in your environment, but fails on all subsequent NSX-T workload domains.

  Workaround: None
* **Operations on NSX-T workload domains fails if their host FQDNs include uppercase letters**

  If the FQDNs of ESXi hosts in an NSX-T workload domain include uppercase letters, then the following operations may fail for the workload domain:

  + Add a host
  + Remove a host
  + Add a cluster
  + Remove a cluster
  + Delete the workload domain

  Workaround: See [KB 76553](http://kb.vmware.com/kb/76553).
* **VI workload domain creation or expansion operations fail**

  If there is a mismatch between the letter case (upper or lower) of an ESXi host's FQDN and the FQDN used when the host was commissioned, then workload domain creation and expansion may fail.

  Workaround: ESXi hosts should have lower case FQDNs and should be commissioned using lower case FQDNs.
* **Cluster is deleted even if VMs are up and running on the cluster**

  When you delete a cluster, it gets deleted even if there are VMs running on the cluster. This includes critical VMs such as Edge VMs, which may prevent you from accessing your environment after the cluster gets deleted.

  Workaround: Migrate the VMs to a different cluster before deleting the cluster.
* **Workload domain operations fail if cluster upgrade is in progress**

  Workload domain operations cannot be performed if one or more clusters are being upgraded. The UI does not block such oeprations during an upgrade.

  Workaround: Do not perform any operations on the workload domain when a cluster upgrade is in progress.
* **If you use the special character underscore (\_) in the vCenter Server host name for the workload domain create operation, the vCenter Server deployment fails**

  The vCenter deployment fails with the **"ERROR > Section 'new\_vcsa', subsection 'network', property 'system\_name' validation"** error message.

  Workaround: None. This is an issue in the vCenter Server product installer where the installer pre-validation fails. You should create the workload domain by providing valid vCenter Server host names.

### Multi-Instance Management Known Issues

* **Federation creation information not displayed if you leave the Multi-Instance Management Dashboard**

  Federation creation progress is displayed on the Multi-Instance Management Dashboard. If you navigate to another screen and then return to the Multi-Instance Management Dashboard, progress messages are not displayed. Instead, an empty map with no Cloud Foundation instances are displayed until the federation is created.

  Workaround: Stay on the Multi-Instance Dashboard till the task is complete. If you have navigated away, wait for around 20 minutes and then return to the dashboard by which time the operation should have completed.
* **The federation creation progress is not displayed**

  While federation creation is in progress, the SDDC manager UI displays the progress on the multi-site page. If you navigate into any other screen and come back to the multi-site screen, the progress messages are not displayed. An empty map with no VMware Cloud Foundation instances is displayed until the federation creation process completes.

  Workaround: None
* **Multi-Instance Management Dashboard operation fails**

  After a controller joins or leaves a federation, Kafka is restarted on all controllers in the federation. It can take up to 15 minutes for the federation to stabilize. Any operations performed on the dashboard during this time may fail.

  Workaround: Re-try the operation.

### Backup and Restore Known Issues

* **NSX Manager restore might not complete due to certificate rejection**

  A restore might not complete as a result of an installed certificate with no CRL Distribution Point and an incorrect setting (crl\_checking\_enable configuration to true).

  Workaround: None

check-circle-line

exclamation-circle-line

close-line

![Scroll to top icon](/uicontent/images/scroll_top.svg)

![](/uicontent/images/feedback.svg)

             [![VMware](https://www.vmware.com/content/dam/vmwaredesigns/scrapercontent/resources/logos/vmware-logo-grey.svg "VMware")](https://www.vmware.com/)

Resources

* [Blogs](https://blogs.vmware.com/)
* [Careers](https://www.broadcom.com/jobs)
* [Communities](https://community.broadcom.com/)
* [Customer Stories](https://www.vmware.com/resources/customers)

* [News and Stories](https://news.broadcom.com/)
* [Topics](https://www.vmware.com/topics/)
* [Trust Center](https://www.vmware.com/info/trust-center/)

Support

* [Broadcom Support](https://support.broadcom.com/)
* [Documentation](https://docs.vmware.com)
* [Hands-On Labs](https://www.vmware.com/resources/hands-on-labs)
* [Licensing](https://www.broadcom.com/licensing)

* [Twitter](https://twitter.com/VMware)
* [YouTube](https://www.youtube.com/user/vmwaretv)
* [Facebook](https://www.facebook.com/vmware)
* [LinkedIn](https://www.linkedin.com/company/vmware/mycompany/)
* [Contact Sales](https://go-vmware.broadcom.com/contact-us)

---

  Copyright © 2005-2024 Broadcom. All Rights Reserved. The term "Broadcom" refers to Broadcom Inc. and/or its subsidiaries. [Accessibility](https://www.broadcom.com/company/legal/accessibility "Accessibility") [Privacy](https://www.broadcom.com/privacy "Privacy") [Supplier Responsibility](https://www.broadcom.com/company/citizenship/supplier-responsibility "Supplier Responsibility") [Terms Of Use](https://www.broadcom.com/company/legal/terms-of-use "Terms Of Use")

#####

×

Share on Social Media?

×

#####

×

exclamation-circle-line

check-circle-line

 :

#####

×

exclamation-circle-line

|

check-circle-line

exclamation-circle-line

x

#####

×

"
"?

×



=== Content from www.vmware.com_3b4e56ba_20250114_191113.html ===


Menu

* [Products](https://www.broadcom.com/products/)
* [Solutions](https://www.broadcom.com/solutions)
* [Support and Services](https://www.broadcom.com/support)
* [Company](https://www.broadcom.com/company/about-us/)
* [How To Buy](https://www.broadcom.com/how-to-buy/#sales)

* Log in

  [Log In](/c/portal/login)
  [Register](https://profile.broadcom.com/web/registration)

[Register](https://profile.broadcom.com/web/registration)
[Login](/c/portal/login)

VMSA-2022-0005:VMware NSX Data Center for vSphere update addresses CLI shell injection vulnerability

Product/Component

VMware Cloud Foundation

1 more products

List of Products

2 Products

* VMware Cloud Foundation
* VMware NSX Networking

Notification Id

23620

Last Updated

05 April 2022

Initial Publication Date

13 February 2022

Status

CLOSED

Severity

HIGH

CVSS Base Score

8.8

WorkAround

Affected CVE

CVE-2022-22945

             Advisory ID:  VMSA-2022-0005.2   CVSSv3 Range: 8.8   Issue Date:2022-02-15   Updated On: 2022-04-07    CVE(s): CVE-2022-22945   Synopsis: VMware NSX Data Center for vSphere update addresses CLI shell injection vulnerability (CVE-2022-22945)

 [RSS Feed](https://www.vmware.com/security/advisories/VMSA-2022-0005.xml)

 Download PDF

 Download Text File

Share this page on social media:

##### **1. Impacted Products**

* VMware NSX Data Center for vSphere (NSX-V)
* VMware Cloud Foundation (Cloud Foundation)

##### **2. Introduction**

A CLI shell injection vulnerability affecting VMware NSX Data Center for vSphere was privately reported to VMware. Updates are available to address this vulnerability in affected VMware products.

##### **3. VMware NSX Data Center for vSphere update addresses CLI shell injection vulnerability (CVE-2022-22945)**

**Description**

VMware NSX Data Center for vSphere contains a CLI shell injection vulnerability in the NSX Edge appliance component. VMware has evaluated the severity of this issue to be in the [Important severity range](https://www.vmware.com/support/policies/security_response.html) with a maximum CVSSv3 base score of [8.8](https://www.first.org/cvss/calculator/3.0#CVSS:3.0/AV:L/AC:L/PR:L/UI:N/S:C/C:H/I:H/A:H).

**Known Attack Vectors**

A malicious actor with SSH access to an NSX-Edge appliance (NSX-V) can execute arbitrary commands on the operating system as root.

**Resolution**

To remediate CVE-2022-22945 apply the patches listed in the 'Fixed Version' column of the 'Response Matrix' below.

**Workarounds**

Workarounds for CVE-2022-22945 have been listed in the 'Workarounds' column of the 'Response Matrix' below.

**Additional Documentation**

Additional documentation for CVE-2022-22945 have been listed in the 'Additional Documentation' column of the 'Response Matrix' for Cloud Foundation (NSX-V) below.

**Notes**

None

**Acknowledgements**

VMware would like to thank Dimitri Di Cristofaro (@d\_glenx) and Przemek Reszke (@kolokokop) from SECFORCE LTD for reporting this issue to us.

**Response Matrix**

| Product | Version | Running On | CVE Identifier | CVSSv3 | Severity | Fixed Version | Workarounds | Additional Documentation |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| NSX Data Center for vSphere | Any | Any | CVE-2022-22945 | [8.8](https://www.first.org/cvss/calculator/3.0#CVSS:3.0/AV:L/AC:L/PR:L/UI:N/S:C/C:H/I:H/A:H) | important | [6.4.13](https://docs.vmware.com/en/VMware-NSX-Data-Center-for-vSphere/6.4/rn/VMware-NSX-Data-Center-for-vSphere-6413-Release-Notes.html) | [KB87893](https://kb.vmware.com/s/article/87893) | None |

**Impacted Product Suites that Deploy Response Matrix Components:**

| Product | Version | Running On | CVE Identifier | CVSSv3 | Severity | Fixed Version | Workarounds | Additional Documentation |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| Cloud Foundation (NSX-V) | 3.x | Any | CVE-2022-22945 | [8.8](https://www.first.org/cvss/calculator/3.0#CVSS:3.0/AV:L/AC:L/PR:L/UI:N/S:C/C:H/I:H/A:H) | important | [3.11.0.1](https://docs.vmware.com/en/VMware-Cloud-Foundation/3.11/rn/VMware-Cloud-Foundation-311-Release-Notes.html#31101) | [KB87893](https://kb.vmware.com/s/article/87893) | [KB87863](https://kb.vmware.com/s/article/87863) |

##### **4. References**

Fixed Version(s) and Release Notes:

NSX Data Center for vSphere 6.4.13
Downloads and Documentation:
<https://customerconnect.vmware.com/en/downloads/details?downloadGroup=NSXV_6413&productId=417&rPId=84646>

<https://docs.vmware.com/en/VMware-NSX-Data-Center-for-vSphere/6.4/rn/VMware-NSX-Data-Center-for-vSphere-6413-Release-Notes.html>

VMware vCloud Foundation 3.x

Downloads and Documentation:

<https://docs.vmware.com/en/VMware-Cloud-Foundation/3.11/rn/VMware-Cloud-Foundation-311-Release-Notes.html#31101>

Mitre CVE Dictionary Links:
<https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-22945>

FIRST CVSSv3 Calculator:

CVE-2022-22945 - <https://www.first.org/cvss/calculator/3.0#CVSS:3.0/AV:L/AC:L/PR:L/UI:N/S:C/C:H/I:H/A:H>

##### **5. Change Log**

2022-02-15: VMSA-2022-0005
Initial security advisory.

2022-03-14: VMSA-2022-0005.1
Updated security advisory Response Matrix to include workarounds and additional documentation.

2022-04-07: VMSA-2022-0005.2
Updated advisory with updates to VMware Cloud Foundation 3.x.

##### **6. Contact**

E-mail list for product security notifications and announcements:

[https://lists.vmware.com/cgi-bin/mailman/listinfo/security-announce](https://lists.vmware.com/cgi-bin/mailman/listinfo/security-announce "https://lists.vmware.com/cgi-bin/mailman/listinfo/security-announce")

This Security Advisory is posted to the following lists:

[[email protected]](/cdn-cgi/l/email-protection#b0c3d5d3c5c2d9c4c99dd1dededfc5ded3d5f0dcd9c3c4c39ec6ddc7d1c2d59ed3dfdd)

E-mail: [[email protected]](/cdn-cgi/l/email-protection#f08395938582998489b0869d87918295de939f9d)

PGP key at:

<https://kb.vmware.com/kb/1055>

VMware Security Advisories

<https://www.vmware.com/security/advisories>

VMware Security Response Policy

<https://www.vmware.com/support/policies/security_response.html>

VMware Lifecycle Support Phases

<https://www.vmware.com/support/policies/lifecycle.html>

VMware Security & Compliance Blog

<https://blogs.vmware.com/security>

Twitter

<https://twitter.com/VMwareSRC>

Copyright 2022 VMware Inc. All rights reserved.

Hidden

#####

×

It appears your Broadcom Products and Services are

supported by one of our certified Support partners

Click below to be redirected to the appropriate Support

Partner Portal to request support

For non-product related issues (Support Portal / Licensing) Click HERE

Continue

#####

×

For **Technical Support** (issues with products or services)

1. Select **Technical** to be redirected to the My Entitlements page
2. Expand the product you require support on
3. Select the case icon from the case column
4. You will be redirected to the appropriate vendor portal where you can raise your technical request

For **Non-Technical Support** (issues with portal access, license keys, software downloads)

1. Select **Non-Technical** to be redirected to Broadcom's case management portal

Technical
Non-Technical

#####

×

# Access Denied

This feature has been disabled by your administrator.

#####

×

To prevent this message from showing again, please enable pop-up blockers for [support.broadcom.com](https://support.broadcom.com/)
or click Continue to proceed.

Continue

Top

* [Products](https://www.broadcom.com/products)
* [Solutions](https://www.broadcom.com/solutions)
* [Support and Services](https://www.broadcom.com/support)
* [Company](https://www.broadcom.com/)
* [How to Buy](https://www.broadcom.com/how-to-buy)

 Copyright © 2005-2024 Broadcom. All Rights Reserved. The term “Broadcom” refers to Broadcom Inc. and/or its subsidiaries.

* [Accessibility](https://www.broadcom.com/company/legal/accessibility)
* [Privacy](https://www.broadcom.com/company/legal/privacy)
* [Supplier Responsibility](https://www.broadcom.com/company/citizenship/supplier-responsibility)
* [Terms of Use](https://www.broadcom.com/company/legal/terms-of-use)
* [Site Map](https://www.broadcom.com/sitemap)



=== Content from kb.vmware.com_ed989c5c_20250115_113958.html ===


search

cancel

Search

### NSX-V CVE-2022-22945 workaround

book
#### Article ID: 319136

calendar\_today
#### Updated On:

#### Products

VMware NSX Data Center for vSphere

Show More
Show Less

#### Issue/Introduction

Symptoms:
This KB contains a workaround for CVE-2022-22945, which is covered in VMSA-2022-0005 here:
 <https://www.vmware.com/security/advisories/VMSA-2022-0005.html>

#### Environment

VMware NSX Data Center for vSphere 6.4.x

#### Resolution

- Upgrade to NSX-V 6.4.13.
- If using VMware Cloud Foundation, upgrade to a VCF version with NSX-T or NSX-V 6.4.13, once available.

Workaround:
By default, SSH access to the following virtual machines listed below is disabled.  However, if a customer has enabled SSH capabilities to these virtual machines, it is recommend to disable SSH access for the following VMs until upgrading to a Fixed Version to mitigate this vulnerability.  If these virtual machines are not present in the environment, there is no action necessary:

**NSX Edge Services Gateway
NSX Distributed Logical Router Control VM**

Documentation on how to disable SSH for Edge Services Gateway VM - <https://docs.vmware.com/en/VMware-NSX-Data-Center-for-vSphere/6.4/com.vmware.nsx.admin.doc/GUID-1EA25D37-F1C7-45C8-AEBA-A555ACC972BC.html>
Documentation on how to disable SSH for Distributed Logical Router Control VM - <https://docs.vmware.com/en/VMware-NSX-Data-Center-for-vSphere/6.4/com.vmware.nsx.admin.doc/GUID-A20103B0-ABA1-4884-8EC3-287874E23181.html>

VMware recommends only enabling SSH access to these virtual machines in the case of troubleshooting purposes only.

#### Feedback

thumb\_up
Yes

thumb\_down
No

Powered by
[![Wolken Software](https://cdn.wolkenservicedesk.com/wolken-footer-image.png)](https://www.wolkensoftware.com/)


