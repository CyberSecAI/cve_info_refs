=== Content from github.com_21399c50_20250114_200903.html ===

[Skip to content](#start-of-content)

## Navigation Menu

Toggle navigation

[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fcommit%2F6104f0d4091c260ce9352f9155f7e9b725eab012)

* Product

  + [GitHub Copilot
    Write better code with AI](https://github.com/features/copilot)
  + [Security
    Find and fix vulnerabilities](https://github.com/features/security)
  + [Actions
    Automate any workflow](https://github.com/features/actions)
  + [Codespaces
    Instant dev environments](https://github.com/features/codespaces)
  + [Issues
    Plan and track work](https://github.com/features/issues)
  + [Code Review
    Manage code changes](https://github.com/features/code-review)
  + [Discussions
    Collaborate outside of code](https://github.com/features/discussions)
  + [Code Search
    Find more, search less](https://github.com/features/code-search)

  Explore
  + [All features](https://github.com/features)
  + [Documentation](https://docs.github.com)
  + [GitHub Skills](https://skills.github.com)
  + [Blog](https://github.blog)
* Solutions

  By company size
  + [Enterprises](https://github.com/enterprise)
  + [Small and medium teams](https://github.com/team)
  + [Startups](https://github.com/enterprise/startups)
  By use case
  + [DevSecOps](/solutions/use-case/devsecops)
  + [DevOps](/solutions/use-case/devops)
  + [CI/CD](/solutions/use-case/ci-cd)
  + [View all use cases](/solutions/use-case)

  By industry
  + [Healthcare](/solutions/industry/healthcare)
  + [Financial services](/solutions/industry/financial-services)
  + [Manufacturing](/solutions/industry/manufacturing)
  + [Government](/solutions/industry/government)
  + [View all industries](/solutions/industry)

  [View all solutions](/solutions)
* Resources

  Topics
  + [AI](/resources/articles/ai)
  + [DevOps](/resources/articles/devops)
  + [Security](/resources/articles/security)
  + [Software Development](/resources/articles/software-development)
  + [View all](/resources/articles)

  Explore
  + [Learning Pathways](https://resources.github.com/learn/pathways)
  + [White papers, Ebooks, Webinars](https://resources.github.com)
  + [Customer Stories](https://github.com/customer-stories)
  + [Partners](https://partner.github.com)
  + [Executive Insights](https://github.com/solutions/executive-insights)
* Open Source

  + [GitHub Sponsors
    Fund open source developers](/sponsors)
  + [The ReadME Project
    GitHub community articles](https://github.com/readme)
  Repositories
  + [Topics](https://github.com/topics)
  + [Trending](https://github.com/trending)
  + [Collections](https://github.com/collections)
* Enterprise

  + [Enterprise platform
    AI-powered developer platform](/enterprise)
  Available add-ons
  + [Advanced Security
    Enterprise-grade security features](https://github.com/enterprise/advanced-security)
  + [GitHub Copilot
    Enterprise-grade AI features](/features/copilot#enterprise)
  + [Premium Support
    Enterprise-grade 24/7 support](/premium-support)
* [Pricing](https://github.com/pricing)

Search or jump to...

# Search code, repositories, users, issues, pull requests...

Search

Clear

[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)

# Provide feedback

We read every piece of feedback, and take your input very seriously.

Include my email address so I can be contacted

  Cancel

 Submit feedback

# Saved searches

## Use saved searches to filter your results more quickly

Name

Query

To see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).

  Cancel

 Create saved search

[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fcommit%2F6104f0d4091c260ce9352f9155f7e9b725eab012)

[Sign up](/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E%2Fvoltron%2Fcommit_fragments%2Frepo_layout&source=header-repo&source_repo=tensorflow%2Ftensorflow)
Reseting focus

You signed in with another tab or window. Reload to refresh your session.
You signed out in another tab or window. Reload to refresh your session.
You switched accounts on another tab or window. Reload to refresh your session.

Dismiss alert

{{ message }}

[tensorflow](/tensorflow)
/
**[tensorflow](/tensorflow/tensorflow)**
Public

* [Notifications](/login?return_to=%2Ftensorflow%2Ftensorflow) You must be signed in to change notification settings
* [Fork
  74.4k](/login?return_to=%2Ftensorflow%2Ftensorflow)
* [Star
   187k](/login?return_to=%2Ftensorflow%2Ftensorflow)

* [Code](/tensorflow/tensorflow)
* [Issues
  827](/tensorflow/tensorflow/issues)
* [Pull requests
  5k+](/tensorflow/tensorflow/pulls)
* [Actions](/tensorflow/tensorflow/actions)
* [Projects
  2](/tensorflow/tensorflow/projects)
* [Security](/tensorflow/tensorflow/security)
* [Insights](/tensorflow/tensorflow/pulse)

Additional navigation options

* [Code](/tensorflow/tensorflow)
* [Issues](/tensorflow/tensorflow/issues)
* [Pull requests](/tensorflow/tensorflow/pulls)
* [Actions](/tensorflow/tensorflow/actions)
* [Projects](/tensorflow/tensorflow/projects)
* [Security](/tensorflow/tensorflow/security)
* [Insights](/tensorflow/tensorflow/pulse)

## Commit

[Permalink](/tensorflow/tensorflow/commit/6104f0d4091c260ce9352f9155f7e9b725eab012)

This commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.

Strengthen input verification for SpecializeType by replacing DCHECK …

[Browse files](/tensorflow/tensorflow/tree/6104f0d4091c260ce9352f9155f7e9b725eab012)
Browse the repository at this point in the history

```
…with explicit test/status return.

PiperOrigin-RevId: 453436708
```

* Loading branch information

[![@tensorflower-gardener](https://avatars.githubusercontent.com/u/17151892?s=40&v=4)](/tensorflower-gardener)

Dan Moldovan
authored and
[tensorflower-gardener](/tensorflow/tensorflow/commits?author=tensorflower-gardener "View all commits by tensorflower-gardener")
committed
Jun 7, 2022

1 parent
[c6c1755](/tensorflow/tensorflow/commit/c6c17551b2767ea507e9a2a05f3b4e64714abd74)

commit 6104f0d

 Show file tree

 Hide file tree

Showing
**2 changed files**
with
**18 additions**
and
**1 deletion**.

* Whitespace
* Ignore whitespace

* Split
* Unified

* tensorflow/core/framework

  + tensorflow/core/framework/full\_type\_util.cc
    [full\_type\_util.cc](#diff-96ec3142e38ef06c083b62389ebdab600587acf66e29c8b29aadcb86b2d6d61d)
  + tensorflow/core/framework/full\_type\_util\_test.cc
    [full\_type\_util\_test.cc](#diff-1e7579036332be3b80efeb6d48e4278ca3077a69f530eb672216d0087a3f1ec9)

## There are no files selected for viewing

6 changes: 5 additions & 1 deletion

6
[tensorflow/core/framework/full\_type\_util.cc](#diff-96ec3142e38ef06c083b62389ebdab600587acf66e29c8b29aadcb86b2d6d61d "tensorflow/core/framework/full_type_util.cc")

Show comments

[View file](/tensorflow/tensorflow/blob/6104f0d4091c260ce9352f9155f7e9b725eab012/tensorflow/core/framework/full_type_util.cc)
Edit file

Delete file

This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters.
[Learn more about bidirectional Unicode characters](https://github.co/hiddenchars)

  [Show hidden characters](%7B%7B%20revealButtonHref%20%7D%7D)

| Original file line number | Diff line number | Diff line change |
| --- | --- | --- |
| Expand Up | | @@ -175,7 +175,11 @@ Status SubstituteVar(AttrMap& attrs, FullTypeDef& t) { |
|  |  | } |
|  |  |  |
|  |  | Status SubstituteForEach(AttrMap& attrs, FullTypeDef& t) { |
|  |  | DCHECK\_EQ(t.args\_size(), 3); |
|  |  | if (t.args\_size() != 3) { |
|  |  | return Status(error::INVALID\_ARGUMENT, |
|  |  | absl::StrCat("illegal FOR\_EACH type, expected 3 args, got ", |
|  |  | t.args\_size())); |
|  |  | } |
|  |  |  |
|  |  | const auto& cont = t.args(0); |
|  |  | const auto& tmpl = t.args(1); |
| Expand Down | |  |

13 changes: 13 additions & 0 deletions

13
[tensorflow/core/framework/full\_type\_util\_test.cc](#diff-1e7579036332be3b80efeb6d48e4278ca3077a69f530eb672216d0087a3f1ec9 "tensorflow/core/framework/full_type_util_test.cc")

Show comments

[View file](/tensorflow/tensorflow/blob/6104f0d4091c260ce9352f9155f7e9b725eab012/tensorflow/core/framework/full_type_util_test.cc)
Edit file

Delete file

This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters.
[Learn more about bidirectional Unicode characters](https://github.co/hiddenchars)

  [Show hidden characters](%7B%7B%20revealButtonHref%20%7D%7D)

| Original file line number | Diff line number | Diff line change |
| --- | --- | --- |
| Expand Up | | @@ -510,6 +510,19 @@ TEST(SpecializeType, ForEachOverridesTargetOfNestedForEach) { |
|  |  | EXPECT\_EQ(t\_actual.args(1).args(0).args(0).args\_size(), 0); |
|  |  | } |
|  |  |  |
|  |  | TEST(SpecializeType, ForEachRejectsMalformedInput) { |
|  |  | OpDef op; |
|  |  | FullTypeDef\* t = op.add\_output\_arg()->mutable\_experimental\_full\_type(); |
|  |  | t->set\_type\_id(TFT\_FOR\_EACH); |
|  |  | t->add\_args()->set\_type\_id(TFT\_PRODUCT); |
|  |  |  |
|  |  | NodeDef ndef; |
|  |  | AttrSlice attrs(ndef); |
|  |  |  |
|  |  | FullTypeDef ft; |
|  |  | EXPECT\_FALSE(SpecializeType(attrs, op, ft).ok()); |
|  |  | } |
|  |  |  |
|  |  | TEST(SpecializeType, RemovesLegacyVariant) { |
|  |  | OpDef op; |
|  |  | FullTypeDef\* t = op.add\_output\_arg()->mutable\_experimental\_full\_type(); |
| Expand Down | |  |

Toggle all file notes
Toggle all file annotations

### 0 comments on commit `6104f0d`

Please
[sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fcommit%2F6104f0d4091c260ce9352f9155f7e9b725eab012) to comment.

## Footer

© 2025 GitHub, Inc.

### Footer navigation

* [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)
* [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)
* [Security](https://github.com/security)
* [Status](https://www.githubstatus.com/)
* [Docs](https://docs.github.com/)
* [Contact](https://support.github.com?tags=dotcom-footer)
* Manage cookies
* Do not share my personal information

You can’t perform that action at this time.



=== Content from github.com_3392b5c7_20250114_200904.html ===

[Skip to content](#start-of-content)

## Navigation Menu

Toggle navigation

[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fsecurity%2Fadvisories%2FGHSA-g468-qj8g-vcjc)

* Product

  + [GitHub Copilot
    Write better code with AI](https://github.com/features/copilot)
  + [Security
    Find and fix vulnerabilities](https://github.com/features/security)
  + [Actions
    Automate any workflow](https://github.com/features/actions)
  + [Codespaces
    Instant dev environments](https://github.com/features/codespaces)
  + [Issues
    Plan and track work](https://github.com/features/issues)
  + [Code Review
    Manage code changes](https://github.com/features/code-review)
  + [Discussions
    Collaborate outside of code](https://github.com/features/discussions)
  + [Code Search
    Find more, search less](https://github.com/features/code-search)

  Explore
  + [All features](https://github.com/features)
  + [Documentation](https://docs.github.com)
  + [GitHub Skills](https://skills.github.com)
  + [Blog](https://github.blog)
* Solutions

  By company size
  + [Enterprises](https://github.com/enterprise)
  + [Small and medium teams](https://github.com/team)
  + [Startups](https://github.com/enterprise/startups)
  By use case
  + [DevSecOps](/solutions/use-case/devsecops)
  + [DevOps](/solutions/use-case/devops)
  + [CI/CD](/solutions/use-case/ci-cd)
  + [View all use cases](/solutions/use-case)

  By industry
  + [Healthcare](/solutions/industry/healthcare)
  + [Financial services](/solutions/industry/financial-services)
  + [Manufacturing](/solutions/industry/manufacturing)
  + [Government](/solutions/industry/government)
  + [View all industries](/solutions/industry)

  [View all solutions](/solutions)
* Resources

  Topics
  + [AI](/resources/articles/ai)
  + [DevOps](/resources/articles/devops)
  + [Security](/resources/articles/security)
  + [Software Development](/resources/articles/software-development)
  + [View all](/resources/articles)

  Explore
  + [Learning Pathways](https://resources.github.com/learn/pathways)
  + [White papers, Ebooks, Webinars](https://resources.github.com)
  + [Customer Stories](https://github.com/customer-stories)
  + [Partners](https://partner.github.com)
  + [Executive Insights](https://github.com/solutions/executive-insights)
* Open Source

  + [GitHub Sponsors
    Fund open source developers](/sponsors)
  + [The ReadME Project
    GitHub community articles](https://github.com/readme)
  Repositories
  + [Topics](https://github.com/topics)
  + [Trending](https://github.com/trending)
  + [Collections](https://github.com/collections)
* Enterprise

  + [Enterprise platform
    AI-powered developer platform](/enterprise)
  Available add-ons
  + [Advanced Security
    Enterprise-grade security features](https://github.com/enterprise/advanced-security)
  + [GitHub Copilot
    Enterprise-grade AI features](/features/copilot#enterprise)
  + [Premium Support
    Enterprise-grade 24/7 support](/premium-support)
* [Pricing](https://github.com/pricing)

Search or jump to...

# Search code, repositories, users, issues, pull requests...

Search

Clear

[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)

# Provide feedback

We read every piece of feedback, and take your input very seriously.

Include my email address so I can be contacted

  Cancel

 Submit feedback

# Saved searches

## Use saved searches to filter your results more quickly

Name

Query

To see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).

  Cancel

 Create saved search

[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fsecurity%2Fadvisories%2FGHSA-g468-qj8g-vcjc)

[Sign up](/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E%2Frepos%2Fadvisories%2Fshow&source=header-repo&source_repo=tensorflow%2Ftensorflow)
Reseting focus

You signed in with another tab or window. Reload to refresh your session.
You signed out in another tab or window. Reload to refresh your session.
You switched accounts on another tab or window. Reload to refresh your session.

Dismiss alert

{{ message }}

[tensorflow](/tensorflow)
/
**[tensorflow](/tensorflow/tensorflow)**
Public

* [Notifications](/login?return_to=%2Ftensorflow%2Ftensorflow) You must be signed in to change notification settings
* [Fork
  74.4k](/login?return_to=%2Ftensorflow%2Ftensorflow)
* [Star
   187k](/login?return_to=%2Ftensorflow%2Ftensorflow)

* [Code](/tensorflow/tensorflow)
* [Issues
  827](/tensorflow/tensorflow/issues)
* [Pull requests
  5k+](/tensorflow/tensorflow/pulls)
* [Actions](/tensorflow/tensorflow/actions)
* [Projects
  2](/tensorflow/tensorflow/projects)
* [Security](/tensorflow/tensorflow/security)
* [Insights](/tensorflow/tensorflow/pulse)

Additional navigation options

* [Code](/tensorflow/tensorflow)
* [Issues](/tensorflow/tensorflow/issues)
* [Pull requests](/tensorflow/tensorflow/pulls)
* [Actions](/tensorflow/tensorflow/actions)
* [Projects](/tensorflow/tensorflow/projects)
* [Security](/tensorflow/tensorflow/security)
* [Insights](/tensorflow/tensorflow/pulse)

# `CHECK`-fail in `tensorflow::full\_type::SubstituteFromAttrs`

Low

[pak-laura](/pak-laura)
published
GHSA-g468-qj8g-vcjc
Sep 15, 2022

## Package

pip

tensorflow, tensorflow-cpu, tensorflow-gpu
([pip](/advisories?query=ecosystem%3Apip))

## Affected versions

< 2.10.0

## Patched versions

2.7.4, 2.8.3, 2.9.2, 2.10.0

## Description

### Impact

When [`tensorflow::full_type::SubstituteFromAttrs`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/math_ops.cc) receives a `FullTypeDef& t` that is not exactly three args, it triggers a `CHECK`-fail instead of returning a status.

```
Status SubstituteForEach(AttrMap& attrs, FullTypeDef& t) {
  DCHECK_EQ(t.args_size(), 3);

  const auto& cont = t.args(0);
  const auto& tmpl = t.args(1);
  const auto& t_var = t.args(2);
```
### Patches

We have patched the issue in GitHub commit [6104f0d4091c260ce9352f9155f7e9b725eab012](https://github.com/tensorflow/tensorflow/commit/6104f0d4091c260ce9352f9155f7e9b725eab012).

The fix will be included in TensorFlow 2.10.0. We will also cherrypick this commit on TensorFlow 2.9.1, TensorFlow 2.8.1, and TensorFlow 2.7.2, as these are also affected and still in supported range.

### For more information

Please consult [our security guide](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.

### Severity

Low

### CVE ID

CVE-2022-36016

### Weaknesses

No CWEs

## Footer

© 2025 GitHub, Inc.

### Footer navigation

* [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)
* [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)
* [Security](https://github.com/security)
* [Status](https://www.githubstatus.com/)
* [Docs](https://docs.github.com/)
* [Contact](https://support.github.com?tags=dotcom-footer)
* Manage cookies
* Do not share my personal information

You can’t perform that action at this time.



=== Content from github.com_660a9bae_20250114_200902.html ===

[Skip to content](#start-of-content)

## Navigation Menu

Toggle navigation

[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fblob%2Fmaster%2Ftensorflow%2Fcore%2Fops%2Fmath_ops.cc)

* Product

  + [GitHub Copilot
    Write better code with AI](https://github.com/features/copilot)
  + [Security
    Find and fix vulnerabilities](https://github.com/features/security)
  + [Actions
    Automate any workflow](https://github.com/features/actions)
  + [Codespaces
    Instant dev environments](https://github.com/features/codespaces)
  + [Issues
    Plan and track work](https://github.com/features/issues)
  + [Code Review
    Manage code changes](https://github.com/features/code-review)
  + [Discussions
    Collaborate outside of code](https://github.com/features/discussions)
  + [Code Search
    Find more, search less](https://github.com/features/code-search)

  Explore
  + [All features](https://github.com/features)
  + [Documentation](https://docs.github.com)
  + [GitHub Skills](https://skills.github.com)
  + [Blog](https://github.blog)
* Solutions

  By company size
  + [Enterprises](https://github.com/enterprise)
  + [Small and medium teams](https://github.com/team)
  + [Startups](https://github.com/enterprise/startups)
  By use case
  + [DevSecOps](/solutions/use-case/devsecops)
  + [DevOps](/solutions/use-case/devops)
  + [CI/CD](/solutions/use-case/ci-cd)
  + [View all use cases](/solutions/use-case)

  By industry
  + [Healthcare](/solutions/industry/healthcare)
  + [Financial services](/solutions/industry/financial-services)
  + [Manufacturing](/solutions/industry/manufacturing)
  + [Government](/solutions/industry/government)
  + [View all industries](/solutions/industry)

  [View all solutions](/solutions)
* Resources

  Topics
  + [AI](/resources/articles/ai)
  + [DevOps](/resources/articles/devops)
  + [Security](/resources/articles/security)
  + [Software Development](/resources/articles/software-development)
  + [View all](/resources/articles)

  Explore
  + [Learning Pathways](https://resources.github.com/learn/pathways)
  + [White papers, Ebooks, Webinars](https://resources.github.com)
  + [Customer Stories](https://github.com/customer-stories)
  + [Partners](https://partner.github.com)
  + [Executive Insights](https://github.com/solutions/executive-insights)
* Open Source

  + [GitHub Sponsors
    Fund open source developers](/sponsors)
  + [The ReadME Project
    GitHub community articles](https://github.com/readme)
  Repositories
  + [Topics](https://github.com/topics)
  + [Trending](https://github.com/trending)
  + [Collections](https://github.com/collections)
* Enterprise

  + [Enterprise platform
    AI-powered developer platform](/enterprise)
  Available add-ons
  + [Advanced Security
    Enterprise-grade security features](https://github.com/enterprise/advanced-security)
  + [GitHub Copilot
    Enterprise-grade AI features](/features/copilot#enterprise)
  + [Premium Support
    Enterprise-grade 24/7 support](/premium-support)
* [Pricing](https://github.com/pricing)

Search or jump to...

# Search code, repositories, users, issues, pull requests...

Search

Clear

[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)

# Provide feedback

We read every piece of feedback, and take your input very seriously.

Include my email address so I can be contacted

  Cancel

 Submit feedback

# Saved searches

## Use saved searches to filter your results more quickly

Name

Query

To see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).

  Cancel

 Create saved search

[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fblob%2Fmaster%2Ftensorflow%2Fcore%2Fops%2Fmath_ops.cc)

[Sign up](/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E%2Fblob%2Fshow&source=header-repo&source_repo=tensorflow%2Ftensorflow)
Reseting focus

You signed in with another tab or window. Reload to refresh your session.
You signed out in another tab or window. Reload to refresh your session.
You switched accounts on another tab or window. Reload to refresh your session.

Dismiss alert

{{ message }}

[tensorflow](/tensorflow)
/
**[tensorflow](/tensorflow/tensorflow)**
Public

* [Notifications](/login?return_to=%2Ftensorflow%2Ftensorflow) You must be signed in to change notification settings
* [Fork
  74.4k](/login?return_to=%2Ftensorflow%2Ftensorflow)
* [Star
   187k](/login?return_to=%2Ftensorflow%2Ftensorflow)

* [Code](/tensorflow/tensorflow)
* [Issues
  827](/tensorflow/tensorflow/issues)
* [Pull requests
  5k+](/tensorflow/tensorflow/pulls)
* [Actions](/tensorflow/tensorflow/actions)
* [Projects
  2](/tensorflow/tensorflow/projects)
* [Security](/tensorflow/tensorflow/security)
* [Insights](/tensorflow/tensorflow/pulse)

Additional navigation options

* [Code](/tensorflow/tensorflow)
* [Issues](/tensorflow/tensorflow/issues)
* [Pull requests](/tensorflow/tensorflow/pulls)
* [Actions](/tensorflow/tensorflow/actions)
* [Projects](/tensorflow/tensorflow/projects)
* [Security](/tensorflow/tensorflow/security)
* [Insights](/tensorflow/tensorflow/pulse)

## Files

 master
## Breadcrumbs

1. [tensorflow](/tensorflow/tensorflow/tree/master)
2. /[tensorflow](/tensorflow/tensorflow/tree/master/tensorflow)
3. /[core](/tensorflow/tensorflow/tree/master/tensorflow/core)
4. /[ops](/tensorflow/tensorflow/tree/master/tensorflow/core/ops)
/
# math\_ops.cc

Copy path Blame  Blame
## Latest commit

## History

[History](/tensorflow/tensorflow/commits/master/tensorflow/core/ops/math_ops.cc)2146 lines (1866 loc) · 72 KB master
## Breadcrumbs

1. [tensorflow](/tensorflow/tensorflow/tree/master)
2. /[tensorflow](/tensorflow/tensorflow/tree/master/tensorflow)
3. /[core](/tensorflow/tensorflow/tree/master/tensorflow/core)
4. /[ops](/tensorflow/tensorflow/tree/master/tensorflow/core/ops)
/
# math\_ops.cc

Top
## File metadata and controls

* Code
* Blame

2146 lines (1866 loc) · 72 KB[Raw](https://github.com/tensorflow/tensorflow/raw/refs/heads/master/tensorflow/core/ops/math_ops.cc)1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798991001011021031041051061071081091101111121131141151161171181191201211221231241251261271281291301311321331341351361371381391401411421431441451461471481491501511521531541551561571581591601611621631641651661671681691701711721731741751761771781791801811821831841851861871881891901911921931941951961971981992002012022032042052062072082092102112122132142152162172182192202212222232242252262272282292302312322332342352362372382392402412422432442452462472482492502512522532542552562572582592602612622632642652662672682692702712722732742752762772782792802812822832842852862872882892902912922932942952962972982993003013023033043053063073083093103113123133143153163173183193203213223233243253263273283293303313323333343353363373383393403413423433443453463473483493503513523533543553563573583593603613623633643653663673683693703713723733743753763773783793803813823833843853863873883893903913923933943953963973983994004014024034044054064074084094104114124134144154164174184194204214224234244254264274284294304314324334344354364374384394404414424434444454464474484494504514524534544554564574584594604614624634644654664674684694704714724734744754764774784794804814824834844854864874884894904914924934944954964974984995005015025035045055065075085095105115125135145155165175185195205215225235245255265275285295305315325335345355365375385395405415425435445455465475485495505515525535545555565575585595605615625635645655665675685695705715725735745755765775785795805815825835845855865875885895905915925935945955965975985996006016026036046056066076086096106116126136146156166176186196206216226236246256266276286296306316326336346356366376386396406416426436446456466476486496506516526536546556566576586596606616626636646656666676686696706716726736746756766776786796806816826836846856866876886896906916926936946956966976986997007017027037047057067077087097107117127137147157167177187197207217227237247257267277287297307317327337347357367377387397407417427437447457467477487497507517527537547557567577587597607617627637647657667677687697707717727737747757767777787797807817827837847857867877887897907917927937947957967977987998008018028038048058068078088098108118128138148158168178188198208218228238248258268278288298308318328338348358368378388398408418428438448458468478488498508518528538548558568578588598608618628638648658668678688698708718728738748758768778788798808818828838848858868878888898908918928938948958968978988999009019029039049059069079089099109119129139149159169179189199209219229239249259269279289299309319329339349359369379389399409419429439449459469479489499509519529539549559569579589599609619629639649659669679689699709719729739749759769779789799809819829839849859869879889899909919929939949959969979989991000/\* Copyright 2015 The TensorFlow Authors. All Rights Reserved.
Licensed under the Apache License, Version 2.0 (the "License");you may not use this file except in compliance with the License.You may obtain a copy of the License at
 http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an "AS IS" BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.==============================================================================\*/#include <limits>#include <vector>
#include "tensorflow/core/framework/common\_shape\_fns.h"#include "tensorflow/core/framework/numeric\_op.h"#include "tensorflow/core/framework/op.h"#include "tensorflow/core/framework/shape\_inference.h"
// TODO(intel-tf): Move all MKL ops in this file to a separate file,// mkl\_math\_ops.cc.
namespace tensorflow {
using shape\_inference::DimensionHandle;using shape\_inference::InferenceContext;using shape\_inference::ShapeHandle;
REGISTER\_OP("AddN") .Input("inputs: N \* T") .Output("sum: T") .Attr("N: int >= 1") .Attr("T: {numbertype, variant}") .SetIsCommutative() .SetIsAggregate() .SetShapeFn([](InferenceContext\* c) { ShapeHandle cur = c->input(c->num\_inputs() - 1); for (int i = c->num\_inputs() - 2; i >= 0; --i) { TF\_RETURN\_WITH\_CONTEXT\_IF\_ERROR(c->Merge(c->input(i), cur, &cur), "From merging shape ", i, " with other shapes."); } c->set\_output(0, cur);
 DataType dtype; TF\_RETURN\_IF\_ERROR(c->GetAttr("T", &dtype));
 if (dtype != DT\_VARIANT) { // Exit early if not DT\_VARIANT. return absl::OkStatus(); } else { // DT\_VARIANT shape handle shape inference. All sizes and dtypes must // be the same; all shapes must be compatible via Merge. std::vector<shape\_inference::ShapeAndType> cur\_shapes\_and\_types; auto\* shapes\_and\_types = c->input\_handle\_shapes\_and\_types(c->num\_inputs() - 1); if (shapes\_and\_types) { cur\_shapes\_and\_types = \*shapes\_and\_types; }
 for (int i = c->num\_inputs() - 2; i >= 0; --i) { auto shapes\_and\_types\_i = c->input\_handle\_shapes\_and\_types(i); if (!shapes\_and\_types && shapes\_and\_types\_i) { // TODO(ebrevdo): Find cases where this happens and fix their shape // inference. If we are calling AddN on variant types, they should // all have consistent shape\_and\_type info. shapes\_and\_types = shapes\_and\_types\_i; } else if (shapes\_and\_types && shapes\_and\_types\_i) { if (shapes\_and\_types\_i->size() != shapes\_and\_types->size()) { return errors::InvalidArgument( "shapes\_and\_types[", i, "].size() == ", shapes\_and\_types\_i->size(), " != shapes\_and\_types[0].size() == ", shapes\_and\_types->size()); } for (int j = 0; j < shapes\_and\_types->size(); ++j) { if (shapes\_and\_types->at(j).dtype != shapes\_and\_types\_i->at(j).dtype) { return errors::InvalidArgument( "shapes\_and\_types[", i, "][", j, "].dtype() == ", DataTypeString(shapes\_and\_types\_i->at(j).dtype), " != shapes\_and\_types[0][", j, "].dtype == ", DataTypeString(shapes\_and\_types->at(j).dtype)); } TF\_RETURN\_WITH\_CONTEXT\_IF\_ERROR( c->Merge(shapes\_and\_types\_i->at(j).shape, cur\_shapes\_and\_types.at(j).shape, &cur\_shapes\_and\_types.at(j).shape), "From merging shapes\_and\_types[", i, "][", j, "].shape with ", "shapes\_and\_types[0][", j, "].shape"); } } } if (shapes\_and\_types) { c->set\_output\_handle\_shapes\_and\_types(0, cur\_shapes\_and\_types); } return absl::OkStatus(); } });
// --------------------------------------------------------------------------
// Note that the following operator is just a placeholder and has no// associated kernel. The code in accumulate\_n\_optimizer.cc replaces// this placeholder with a graph of operators that do have kernels.// The Python code that generates instances of this op is currently in// contrib/framework/python/ops/accumulate\_n\_v2.pyREGISTER\_OP("AccumulateNV2") .Input("inputs: N \* T") .Output("sum: T") .Attr("N: int >= 1") .Attr("T: numbertype") .Attr("shape: shape") .SetIsCommutative() .SetIsAggregate() .SetShapeFn(shape\_inference::ExplicitShape);
// --------------------------------------------------------------------------
REGISTER\_OP("BatchMatMul") .Input("x: T") .Input("y: T") .Output("output: T") .Attr( "T: {bfloat16, half, float, double, int32, int64, complex64, " "complex128}") .Attr("adj\_x: bool = false") .Attr("adj\_y: bool = false") .Attr("grad\_x: bool = false") .Attr("grad\_y: bool = false") .SetShapeFn(shape\_inference::BatchMatMulShape);
REGISTER\_OP("BatchMatMulV2") .Input("x: T") .Input("y: T") .Output("output: T") .Attr( "T: {bfloat16, half, float, double, int16, int32, int64, uint8, " "uint16, uint32, uint64, complex64, complex128}") .Attr("adj\_x: bool = false") .Attr("adj\_y: bool = false") .Attr("grad\_x: bool = false") .Attr("grad\_y: bool = false") .SetShapeFn(shape\_inference::BatchMatMulV2Shape);
REGISTER\_OP("BatchMatMulV3") .Input("x: Ta") .Input("y: Tb") .Output("output: Tout") .Attr( "Ta: {bfloat16, half, float, double, uint8, int8, int16, int32, int64, " "complex64, complex128}") .Attr( "Tb: {bfloat16, half, float, double, uint8, int8, int16, int32, int64, " "complex64, complex128}") .Attr( "Tout: {bfloat16, half, float, double, int16, int32, int64, complex64, " "complex128}") .Attr("adj\_x: bool = false") .Attr("adj\_y: bool = false") .Attr("grad\_x: bool = false") .Attr("grad\_y: bool = false") .SetShapeFn(shape\_inference::BatchMatMulV2Shape);
#ifdef INTEL\_MKLREGISTER\_OP("\_MklBatchMatMul") .Input("x: T") .Input("y: T") .Output("output: T") .Attr("T: {bfloat16, float, half}") .Attr("adj\_x: bool = false") .Attr("adj\_y: bool = false") .Attr("grad\_x: bool = false") .Attr("grad\_y: bool = false") .SetShapeFn(shape\_inference::BatchMatMulShape);
REGISTER\_OP("\_MklBatchMatMulV2") .Input("x: T") .Input("y: T") .Output("output: T") .Attr("T: {bfloat16, float, half}") .Attr("adj\_x: bool = false") .Attr("adj\_y: bool = false") .Attr("grad\_x: bool = false") .Attr("grad\_y: bool = false") .SetShapeFn(shape\_inference::BatchMatMulV2Shape);#endif // INTEL\_MKL
// --------------------------------------------------------------------------// Casting Ops//// NOTE: Only a smaller number of types are supported by// Cast. The exact casting rule is TBD. The current// implementation uses C++ static cast rules for numeric// types, which may be changed in the future.REGISTER\_OP("Cast") .Input("x: SrcT") .Output("y: DstT") .Attr("SrcT: type") .Attr("DstT: type") .Attr("Truncate: bool = false") .SetTypeConstructor(full\_type::NoOp()) .SetForwardTypeFn(full\_type::KeepExisting()) .SetShapeFn(shape\_inference::UnchangedShape);
REGISTER\_OP("\_HostCast") .Input("x: SrcT") .Output("y: DstT") .Attr("SrcT: type") .Attr("DstT: type") .Attr("Truncate: bool = false") .SetTypeConstructor(full\_type::NoOp()) .SetForwardTypeFn(full\_type::KeepExisting()) .SetShapeFn(shape\_inference::UnchangedShape) .Doc(R"doc(Cast x of type SrcT to y of DstT.
\_HostCast requires its input and produces its output in host memory.)doc");
// --------------------------------------------------------------------------
REGISTER\_OP("Abs") .Input("x: T") .Output("y: T") .Attr("T: {bfloat16, half, float, double, int8, int16, int32, int64}") .SetShapeFn(shape\_inference::UnchangedShape);
REGISTER\_OP("ComplexAbs") .Input("x: T") .Output("y: Tout") .Attr("T: {complex64, complex128} = DT\_COMPLEX64") .Attr("Tout: {float, double} = DT\_FLOAT") .SetShapeFn(shape\_inference::UnchangedShape);
// Declares cwise unary operations signature: 't -> 't#define UNARY() \ Input("x: T") \ .Output("y: T") \ .Attr( \ "T: {bfloat16, half, float, double, int8, int16, int32, int64, " \ "complex64, complex128}") \ .SetShapeFn(shape\_inference::UnchangedShape)
#define UNARY\_UNSIGNED() \ Input("x: T") \ .Output("y: T") \ .Attr( \ "T: {bfloat16, half, float, double, int8, int16, int32, int64, " \ "uint8, uint16, uint32, uint64, complex64, complex128}") \ .SetShapeFn(shape\_inference::UnchangedShape)
#define UNARY\_REAL() \ Input("x: T") \ .Output("y: T") \ .Attr("T: {bfloat16, half, float, double}") \ .SetShapeFn(shape\_inference::UnchangedShape)
#define UNARY\_COMPLEX() \ Input("x: T") \ .Output("y: T") \ .Attr("T: {bfloat16, half, float, double, complex64, complex128}") \ .SetShapeFn(shape\_inference::UnchangedShape)
#define UNARY\_GRADIENT\_COMPLEX() \ Input("y: T") \ .Input("dy: T") \ .Output("z: T") \ .Attr("T: {bfloat16, half, float, double, complex64, complex128}") \ .SetShapeFn(shape\_inference::UnchangedShape)
REGISTER\_OP("Neg").UNARY();
REGISTER\_OP("Inv").UNARY();
REGISTER\_OP("InvGrad").UNARY\_GRADIENT\_COMPLEX();
REGISTER\_OP("Reciprocal").UNARY();
REGISTER\_OP("ReciprocalGrad").UNARY\_GRADIENT\_COMPLEX();
REGISTER\_OP("Square").UNARY\_UNSIGNED();
REGISTER\_OP("Sqrt").UNARY\_COMPLEX();
REGISTER\_OP("SqrtGrad").UNARY\_GRADIENT\_COMPLEX();
REGISTER\_OP("Rsqrt").UNARY\_COMPLEX();
REGISTER\_OP("Round").UNARY();
REGISTER\_OP("RsqrtGrad").UNARY\_GRADIENT\_COMPLEX();
REGISTER\_OP("Exp").UNARY\_COMPLEX();
REGISTER\_OP("Expm1").UNARY\_COMPLEX();
REGISTER\_OP("Log").UNARY\_COMPLEX();
REGISTER\_OP("Log1p").UNARY\_COMPLEX();
REGISTER\_OP("Sinh").UNARY\_COMPLEX();
REGISTER\_OP("Cosh").UNARY\_COMPLEX();
REGISTER\_OP("Tanh").UNARY\_COMPLEX();
REGISTER\_OP("Asinh").UNARY\_COMPLEX();
REGISTER\_OP("Acosh").UNARY\_COMPLEX();
REGISTER\_OP("Atanh").UNARY\_COMPLEX();
REGISTER\_OP("TanhGrad").UNARY\_GRADIENT\_COMPLEX();
REGISTER\_OP("Lgamma").UNARY\_REAL();
REGISTER\_OP("Digamma").UNARY\_REAL();
REGISTER\_OP("Erf").UNARY\_REAL();REGISTER\_OP("Erfinv").UNARY\_REAL();REGISTER\_OP("Ndtri").UNARY\_REAL();REGISTER\_OP("Erfc").UNARY\_REAL();
REGISTER\_OP("Sigmoid").UNARY\_COMPLEX();
REGISTER\_OP("SigmoidGrad").UNARY\_GRADIENT\_COMPLEX();
REGISTER\_OP("Sin").UNARY\_COMPLEX();
REGISTER\_OP("Cos").UNARY\_COMPLEX();
REGISTER\_OP("Tan").UNARY\_COMPLEX();
REGISTER\_OP("Asin").UNARY\_COMPLEX();
REGISTER\_OP("Acos").UNARY\_COMPLEX();
REGISTER\_OP("Atan").UNARY\_COMPLEX();
REGISTER\_OP("\_UnaryOpsComposition") .Input("x: T") .Output("y: T") .Attr("T: {float, half, double}") .Attr("op\_names: list(string)") .SetShapeFn(shape\_inference::UnchangedShape) .Doc(R"doc(\*NOTE\*: Do not invoke this operator directly in Python. Graph rewrite pass isexpected to create these operators.)doc");
#undef UNARY#undef UNARY\_REAL#undef UNARY\_COMPLEX
REGISTER\_OP("IsNan") .Input("x: T") .Output("y: bool") .Attr("T: {bfloat16, half, float, double}") .SetShapeFn(shape\_inference::UnchangedShape);
REGISTER\_OP("IsInf") .Input("x: T") .Output("y: bool") .Attr("T: {bfloat16, half, float, double}") .SetShapeFn(shape\_inference::UnchangedShape);
REGISTER\_OP("IsFinite") .Input("x: T") .Output("y: bool") .Attr("T: {bfloat16, half, float, double}") .SetShapeFn(shape\_inference::UnchangedShape);
REGISTER\_OP("Sign") .Input("x: T") .Output("y: T") .Attr( "T: {bfloat16, half, float, double, int8, int16, int32, int64, " "complex64, complex128}") .SetShapeFn(shape\_inference::UnchangedShape);
REGISTER\_OP("Floor") .Input("x: T") .Output("y: T") .Attr("T: {bfloat16, half, float, double}") .SetShapeFn(shape\_inference::UnchangedShape);
REGISTER\_OP("Ceil") .Input("x: T") .Output("y: T") .Attr("T: {bfloat16, half, float, double}") .SetShapeFn(shape\_inference::UnchangedShape);
REGISTER\_OP("Rint") .Input("x: T") .Output("y: T") .Attr("T: {bfloat16, half, float, double}") .SetShapeFn(shape\_inference::UnchangedShape);
// Declares cwise binary operations signature: 't, 't -> 't.
#define BINARY\_MORE() \ Input("x: T").Input("y: T").Output("z: T").Attr( \ "T: {bfloat16, half, float, double, uint8, int8, uint16, int16, int32, " \ "uint32, uint64, int64, complex64, complex128}")
#define BINARY\_FEWER() \ Input("x: T").Input("y: T").Output("z: T").Attr( \ "T: {bfloat16, half, float, double, int32, int64, complex64, " \ "complex128}")
REGISTER\_OP("Add") .Input("x: T") .Input("y: T") .Output("z: T") .Attr( "T: {bfloat16, half, float, double, uint8, int8, int16, int32, int64, " "complex64, complex128, string}") .SetShapeFn(shape\_inference::BroadcastBinaryOpShapeFn);
REGISTER\_OP("AddV2") .Input("x: T") .Input("y: T") .Output("z: T") .Attr( "T: {bfloat16, half, float, double, uint8, uint16, uint32, uint64, " "int8, int16, int32, int64, complex64, complex128}") .SetShapeFn(shape\_inference::BroadcastBinaryOpShapeFn) .SetIsAggregate() .SetIsCommutative();
REGISTER\_OP("Sub") .Input("x: T") .Input("y: T") .Output("z: T") .Attr( "T: {bfloat16, half, float, double, uint8, int8, uint16, int16, int32, " "int64, complex64, complex128, uint32, uint64}") .SetShapeFn(shape\_inference::BroadcastBinaryOpShapeFn);
REGISTER\_OP("Mul").BINARY\_MORE().SetIsCommutative().SetShapeFn( shape\_inference::BroadcastBinaryOpShapeFn);
REGISTER\_OP("MulNoNan") .Input("x: T") .Input("y: T") .Output("z: T") .Attr("T: {bfloat16, half, float, double, complex64, complex128}") .SetShapeFn(shape\_inference::BroadcastBinaryOpShapeFn);
REGISTER\_OP("Div").BINARY\_MORE().SetShapeFn( shape\_inference::BroadcastBinaryOpShapeFn);
REGISTER\_OP("DivNoNan") .Input("x: T") .Input("y: T") .Output("z: T") .Attr("T: {half, float, bfloat16, double, complex64, complex128}") .SetShapeFn(shape\_inference::BroadcastBinaryOpShapeFn);
REGISTER\_OP("FloorDiv") .BINARY\_MORE() .SetShapeFn(shape\_inference::BroadcastBinaryOpShapeFn);
REGISTER\_OP("TruncateDiv") .BINARY\_MORE() .SetShapeFn(shape\_inference::BroadcastBinaryOpShapeFn);
REGISTER\_OP("RealDiv").BINARY\_MORE().SetShapeFn( shape\_inference::BroadcastBinaryOpShapeFn);
// Note SquaredDifference implements conj(x - y)\*(x - y).REGISTER\_OP("SquaredDifference") .BINARY\_FEWER() .SetIsCommutative() .SetShapeFn(shape\_inference::BroadcastBinaryOpShapeFn);
REGISTER\_OP("Xlogy") .Input("x: T") .Input("y: T") .Output("z: T") .Attr("T: {half, bfloat16, float, double, complex64, complex128}") .SetShapeFn(shape\_inference::BroadcastBinaryOpShapeFn);
REGISTER\_OP("Xlog1py") .Input("x: T") .Input("y: T") .Output("z: T") .Attr("T: {half, bfloat16, float, double, complex64, complex128}") .SetShapeFn(shape\_inference::BroadcastBinaryOpShapeFn);
REGISTER\_OP("Xdivy") .Input("x: T") .Input("y: T") .Output("z: T") .Attr("T: {half, bfloat16, float, double, complex64, complex128}") .SetShapeFn(shape\_inference::BroadcastBinaryOpShapeFn);
#undef BINARY\_FEWER#undef BINARY\_MORE
REGISTER\_OP("Maximum") .Input("x: T") .Input("y: T") .Output("z: T") .Attr( "T: {bfloat16, half, float, double, int8, uint8, int16, uint16, " "int32, uint32, int64, uint64}") .SetShapeFn(shape\_inference::BroadcastBinaryOpShapeFn);
REGISTER\_OP("Minimum") .Input("x: T") .Input("y: T") .Output("z: T") .Attr( "T: {bfloat16, half, float, double, int8, uint8, int16, uint16, " "int32, uint32, int64, uint64}") .SetShapeFn(shape\_inference::BroadcastBinaryOpShapeFn);
REGISTER\_OP("Mod") .Input("x: T") .Input("y: T") .Output("z: T") .Attr("T: {int32, int64, float16, half, bfloat16, float, double}") .SetShapeFn(shape\_inference::BroadcastBinaryOpShapeFn);
REGISTER\_OP("FloorMod") .Input("x: T") .Input("y: T") .Output("z: T") .Attr( "T: {int8, int16, int32, int64, uint8, uint16, uint32, uint64, " "bfloat16, half, float, double}") .SetShapeFn(shape\_inference::BroadcastBinaryOpShapeFn);
REGISTER\_OP("TruncateMod") .Input("x: T") .Input("y: T") .Output("z: T") .Attr("T: {int32, int64, bfloat16, half, float, double}") .SetShapeFn(shape\_inference::BroadcastBinaryOpShapeFn);
REGISTER\_OP("Pow") .Input("x: T") .Input("y: T") .Output("z: T") .Attr( "T: {bfloat16, float, half, double, int8, int16, int32, int64, " "complex64, complex128}") .SetShapeFn(shape\_inference::BroadcastBinaryOpShapeFn);
REGISTER\_OP("Igammac") .Input("a: T") .Input("x: T") .Output("z: T") .Attr("T: {bfloat16, half, float, double}") .SetShapeFn(shape\_inference::BroadcastBinaryOpShapeFn);
REGISTER\_OP("Igamma") .Input("a: T") .Input("x: T") .Output("z: T") .Attr("T: {bfloat16, half, float, double}") .SetShapeFn(shape\_inference::BroadcastBinaryOpShapeFn);
REGISTER\_OP("IgammaGradA") .Input("a: T") .Input("x: T") .Output("z: T") .Attr("T: {float, double}") .SetShapeFn(shape\_inference::BroadcastBinaryOpShapeFn);
REGISTER\_OP("Zeta") .Input("x: T") .Input("q: T") .Output("z: T") .Attr("T: {float, double}") .SetShapeFn(shape\_inference::BroadcastBinaryOpShapeFn);
REGISTER\_OP("Polygamma") .Input("a: T") .Input("x: T") .Output("z: T") .Attr("T: {float, double}") .SetShapeFn(shape\_inference::BroadcastBinaryOpShapeFn);
REGISTER\_OP("Atan2") .Input("y: T") .Input("x: T") .Output("z: T") .Attr("T: {bfloat16, half, float, double}") .SetShapeFn(shape\_inference::BroadcastBinaryOpShapeFn);
REGISTER\_OP("Betainc") .Input("a: T") .Input("b: T") .Input("x: T") .Output("z: T") .Attr("T: {float, double}") .SetShapeFn([](InferenceContext\* c) { const int num\_inputs = 3; ShapeHandle output = c->UnknownShape(); int num\_scalars = 0; ShapeHandle some\_non\_scalar; for (int i = 0; i < num\_inputs; ++i) { ShapeHandle in = c->input(i); if (!c->RankKnown(in)) { some\_non\_scalar = in; // An input with unknown rank could be either a scalar (to be // broadcast) or some other shape. } else if (c->Rank(in) == 0) { // Input is a scalar, it will be broadcast to the output shape. ++num\_scalars; } else { TF\_RETURN\_IF\_ERROR(c->Merge(output, in, &output)); some\_non\_scalar = output; } }
 if (num\_scalars == num\_inputs - 1) { // If all but one input is known to be a scalar, then output is the // remaining input. output = some\_non\_scalar; } else if (num\_scalars == num\_inputs) { // If all are scalars, output is scalar; pick the first one arbitrarily. output = c->input(0); }
 c->set\_output(0, output); return absl::OkStatus(); });
// --------------------------------------------------------------------------
// Declares cwise binary comparison operations signature: 't, 't -> bool,// where 't has a natural total order.#define COMPARISON() \ Input("x: T") \ .Input("y: T") \ .Output("z: bool") \ .Attr("T: realnumbertype") \ .SetShapeFn(shape\_inference::BroadcastBinaryOpShapeFn)
REGISTER\_OP("Less").COMPARISON();
REGISTER\_OP("LessEqual").COMPARISON();
REGISTER\_OP("Greater").COMPARISON();
REGISTER\_OP("GreaterEqual").COMPARISON();
#undef COMPARISON
// --------------------------------------------------------------------------
#define EQUALITY\_COMPARISON() \ Input("x: T") \ .Input("y: T") \ .Output("z: bool") \ .SetIsCommutative() \ .Attr("T: type") \ .Attr("incompatible\_shape\_error: bool = true") \ .SetShapeFn([](InferenceContext\* c) { \ ShapeHandle x = c->input(0); \ ShapeHandle y = c->input(1); \ ShapeHandle output; \ bool incompatible\_shape\_error; \ TF\_RETURN\_IF\_ERROR(c->GetAttr("incompatible\_shape\_error", \ &incompatible\_shape\_error)); \ TF\_RETURN\_IF\_ERROR(BroadcastBinaryOpOutputShapeFnHelper( \ c, x, y, incompatible\_shape\_error, &output)); \ c->set\_output(0, output); \ return OkStatus(); \ })
REGISTER\_OP("Equal").EQUALITY\_COMPARISON();
REGISTER\_OP("NotEqual").EQUALITY\_COMPARISON();
#undef EQUALITY\_COMPARISON
REGISTER\_OP("ApproximateEqual") .Input("x: T") .Input("y: T") .Output("z: bool") .SetIsCommutative() .Attr("T: numbertype") .Attr("tolerance: float = 0.00001") .SetShapeFn([](InferenceContext\* c) { // The inputs 'x' and 'y' must have the same shape. ShapeHandle data\_x = c->input(0); ShapeHandle data\_y = c->input(1); TF\_RETURN\_IF\_ERROR(c->Merge(data\_x, data\_y, &data\_x)); return shape\_inference::UnchangedShape(c); });
// --------------------------------------------------------------------------
REGISTER\_OP("LogicalNot") .Input("x: bool") .Output("y: bool") .SetShapeFn(shape\_inference::UnchangedShape);
#define BINARY\_LOGICAL() \ Input("x: bool") \ .Input("y: bool") \ .Output("z: bool") \ .SetIsCommutative() \ .SetShapeFn(shape\_inference::BroadcastBinaryOpShapeFn)
REGISTER\_OP("LogicalAnd").BINARY\_LOGICAL();
REGISTER\_OP("LogicalOr").BINARY\_LOGICAL();
#undef BINARY\_LOGICAL
// --------------------------------------------------------------------------
REGISTER\_OP("Select") .Input("condition: bool") .Input("t: T") .Input("e: T") .Output("output: T") .Attr("T: type") .SetShapeFn([](InferenceContext\* c) { auto\* handle\_data\_1 = c->input\_handle\_shapes\_and\_types(1); auto\* handle\_data\_2 = c->input\_handle\_shapes\_and\_types(2); // Merge handle shape and dtype if applicable. if (handle\_data\_1 != nullptr && handle\_data\_2 != nullptr) { const auto size = handle\_data\_1->size(); std::vector<shape\_inference::ShapeAndType> merged\_handle\_data(size); if (size != handle\_data\_2->size()) { return errors::InvalidArgument( "Trying to merge handles pointing to different numbers of " "tensors."); }
 for (int i = 0; i < size; ++i) { const shape\_inference::ShapeAndType& s1 = (\*handle\_data\_1)[i]; const shape\_inference::ShapeAndType& s2 = (\*handle\_data\_2)[i]; if (s1.dtype != s2.dtype) { // TODO(apassos) resolve this in the manner of b/32476923 return errors::InvalidArgument( "Trying to merge handles pointing to different dtypes."); } merged\_handle\_data[i].dtype = s1.dtype; TF\_RETURN\_IF\_ERROR( c->Merge(s1.shape, s2.shape, &merged\_handle\_data[i].shape)); }
 c->set\_output\_handle\_shapes\_and\_types(0, merged\_handle\_data); }
 // The inputs 'then' and 'else' must have the same shape. ShapeHandle data = c->input(1); ShapeHandle other = c->input(2); TF\_RETURN\_IF\_ERROR(c->Merge(data, other, &data));
 // The input 'cond' must either have the same shape as 'then' and // 'else', or be a vector if 'then' and 'else' are at least vectors. ShapeHandle cond = c->input(0);
 if (!c->RankKnown(cond) || !c->RankKnown(data)) { c->set\_output(0, data); return absl::OkStatus(); }
 // rank of shape and data is known.
 const int32\_t cond\_rank = c->Rank(cond); const int32\_t data\_rank = c->Rank(data);
 if (cond\_rank == 0) { // The rank of 'cond' is a scalar. // t and e can have any shape. c->set\_output(0, data); return absl::OkStatus(); }
 if (cond\_rank != 1) { // If 'cond' is not a vector, and not a scalar, // then shape must match 'then' and 'else' TF\_RETURN\_IF\_ERROR(c->Merge(data, cond, &data)); c->set\_output(0, data); return absl::OkStatus(); }
 if (data\_rank == 0) { // if 'then' and 'else' are scalar also the cond must be TF\_RETURN\_IF\_ERROR(c->Merge(data, cond, &data)); c->set\_output(0, data); return absl::OkStatus(); }
 if (cond\_rank == 1) { // if the cond is a vector and the 'then' is not a scalar, // the first dimension of 'then' and 'else' TF\_RETURN\_IF\_ERROR(c->Merge(cond, c->Vector(c->Dim(data, 0)), &cond)); c->set\_output(0, data); return absl::OkStatus(); }
 c->set\_output(0, data);
 return absl::OkStatus(); });
REGISTER\_OP("SelectV2") .Input("condition: bool") .Input("t: T") .Input("e: T") .Output("output: T") .Attr("T: type") .SetShapeFn([](InferenceContext\* c) { auto\* handle\_data\_1 = c->input\_handle\_shapes\_and\_types(1); auto\* handle\_data\_2 = c->input\_handle\_shapes\_and\_types(2); // Merge handle shape and dtype if applicable. if (handle\_data\_1 != nullptr && handle\_data\_2 != nullptr) { const auto size = handle\_data\_1->size(); std::vector<shape\_inference::ShapeAndType> merged\_handle\_data(size); if (size != handle\_data\_2->size()) { return errors::InvalidArgument( "Trying to merge handles pointing to different numbers of " "tensors."); }
 for (int i = 0; i < size; ++i) { const shape\_inference::ShapeAndType& s1 = (\*handle\_data\_1)[i]; const shape\_inference::ShapeAndType& s2 = (\*handle\_data\_2)[i]; if (s1.dtype != s2.dtype) { // TODO(apassos) resolve this in the manner of b/32476923 return errors::InvalidArgument( "Trying to merge handles pointing to different dtypes."); } merged\_handle\_data[i].dtype = s1.dtype; TF\_RETURN\_IF\_ERROR( c->Merge(s1.shape, s2.shape, &merged\_handle\_data[i].shape)); }
 c->set\_output\_handle\_shapes\_and\_types(0, merged\_handle\_data); }
 // The inputs 'cond', 'then', and 'else' must be broadcastable. // TODO (yongtang): Consolidate 3-ary broadcast instead of // multiple 2-ary broadcast. ShapeHandle cond = c->input(0); ShapeHandle then = c->input(1); ShapeHandle else\_ = c->input(2); ShapeHandle other; TF\_RETURN\_IF\_ERROR( BroadcastBinaryOpOutputShapeFnHelper(c, then, else\_, true, &other)); ShapeHandle output; TF\_RETURN\_IF\_ERROR( BroadcastBinaryOpOutputShapeFnHelper(c, cond, other, true, &output)); c->set\_output(0, output); return absl::OkStatus(); });
// --------------------------------------------------------------------------
REGISTER\_OP("MatMul") .Input("a: T") .Input("b: T") .Output("product: T") .Attr("transpose\_a: bool = false") .Attr("transpose\_b: bool = false") .Attr( "T: {bfloat16, half, float, double, int32, int64, uint8, " "uint16, uint32, uint64, complex64, complex128}") .Attr("grad\_a: bool = false") .Attr("grad\_b: bool = false") .SetShapeFn(shape\_inference::MatMulShape);
#ifdef INTEL\_MKLREGISTER\_OP("\_MklMatMul") .Input("a: T") .Input("b: T") .Output("product: T") .Attr("transpose\_a: bool = false") .Attr("transpose\_b: bool = false") .Attr("T: {bfloat16, float, half}") .Attr("grad\_a: bool = false") .Attr("grad\_b: bool = false") .SetShapeFn(shape\_inference::MatMulShape);#endif // INTEL\_MKL
REGISTER\_OP("SparseMatMul") .Input("a: Ta") .Input("b: Tb") .Output("product: float") .Attr("transpose\_a: bool = false") .Attr("transpose\_b: bool = false") .Attr("a\_is\_sparse: bool = false") .Attr("b\_is\_sparse: bool = false") .Attr("Ta: {float, bfloat16} = DT\_FLOAT") .Attr("Tb: {float, bfloat16} = DT\_FLOAT") .SetShapeFn(shape\_inference::MatMulShape);
REGISTER\_OP("\_FusedMatMul") .Input("a: T") .Input("b: T") .Input("args: num\_args \* T") .Output("product: T") .Attr("transpose\_a: bool = false") .Attr("transpose\_b: bool = false") .Attr("T: {bfloat16, half, float}") .Attr("num\_args: int >= 0") .Attr("fused\_ops: list(string) = []") // Attributes for the FusedBatchNorm ----------- // .Attr("epsilon: float = 0.0001") // Attributes for the LeakyRelu ---------------- // .Attr("leakyrelu\_alpha: float = 0.2") // --------------------------------------------- // .SetShapeFn(shape\_inference::MatMulShape) .Doc(R"doc(Performs a MatMul followed by a specified series of operations.
The inputs to the MatMul are specified by `a` and `b`. The series of operationsthat follows is specified by the `fused\_ops` attribute, which is a list of TF opnames specified as strings (e.g. "Relu"). They are performed in order, where the(first) input to each op is the output of the preceding op. The first input andthe output of each fused\_op must be of type T.
Currently supported fused\_op combinations are: ["BiasAdd"] and ["BiasAdd",A],where A is one of {"Elu","Relu","Relu6"}.
\* The first input to BiasAdd is the MatMul result, and the additional BiasAddinput is specified by `args`.\* If there is an op A specified, the output of the BiasAdd is the input to op A,and op A produces the \_FusedConv2D output. Otherwise, the BiasAdd produces the\_FusedConv2D output.
\*NOTE\*: Do not invoke this operator directly in Python. Grappler isexpected to create these operators.)doc");
// --------------------------------------------------------------------------
// For operations where the output is a reduction function along some// dimensions of the input.REGISTER\_OP("Sum") .Input("input: T") .Input("reduction\_indices: Tidx") .Output("output: T") .Attr("keep\_dims: bool = false") .Attr("T: numbertype") .Attr("Tidx: {int32, int64} = DT\_INT32") .SetShapeFn(shape\_inference::ReductionShape);
REGISTER\_OP("EuclideanNorm") .Input("input: T") .Input("reduction\_indices: Tidx") .Output("output: T") .Attr("keep\_dims: bool = false") .Attr("T: numbertype") .Attr("Tidx: {int32, int64} = DT\_INT32") .SetShapeFn(shape\_inference::ReductionShape);
REGISTER\_OP("Mean") .Input("input: T") .Input("reduction\_indices: Tidx") .Output("output: T") .Attr("keep\_dims: bool = false") .Attr("T: numbertype") .Attr("Tidx: {int32, int64} = DT\_INT32") .SetShapeFn(shape\_inference::ReductionShape);
REGISTER\_OP("Prod") .Input("input: T") .Input("reduction\_indices: Tidx") .Output("output: T") .Attr("keep\_dims: bool = false") .Attr("T: numbertype") .Attr("Tidx: {int32, int64} = DT\_INT32") .SetShapeFn(shape\_inference::ReductionShape);
REGISTER\_OP("Min") .Input("input: T") .Input("reduction\_indices: Tidx") .Output("output: T") .Attr("keep\_dims: bool = false") .Attr("T: {realnumbertype, quantizedtype}") .Attr("Tidx: {int32, int64} = DT\_INT32") .SetShapeFn(shape\_inference::ReductionShape);
REGISTER\_OP("Max") .Input("input: T") .Input("reduction\_indices: Tidx") .Output("output: T") .Attr("keep\_dims: bool = false") .Attr("T: {realnumbertype, quantizedtype}") .Attr("Tidx: {int32, int64} = DT\_INT32") .SetShapeFn(shape\_inference::ReductionShape);
namespace {
absl::Status ArgOpShape(shape\_inference::InferenceContext\* c) {[View remainder of file in raw view](https://github.com/tensorflow/tensorflow/raw/refs/heads/master/tensorflow/core/ops/math_ops.cc)

## Footer

© 2025 GitHub, Inc.

### Footer navigation

* [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)
* [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)
* [Security](https://github.com/security)
* [Status](https://www.githubstatus.com/)
* [Docs](https://docs.github.com/)
* [Contact](https://support.github.com?tags=dotcom-footer)
* Manage cookies
* Do not share my personal information

You can’t perform that action at this time.


