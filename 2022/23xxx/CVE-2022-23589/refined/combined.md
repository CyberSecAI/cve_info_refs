=== Content from github.com_5c226872_20250114_210433.html ===

[Skip to content](#start-of-content)

## Navigation Menu

Toggle navigation

[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fblob%2Fa1320ec1eac186da1d03f033109191f715b2b130%2Ftensorflow%2Fcore%2Fgrappler%2Fmutable_graph_view.cc)

* Product

  + [GitHub Copilot
    Write better code with AI](https://github.com/features/copilot)
  + [Security
    Find and fix vulnerabilities](https://github.com/features/security)
  + [Actions
    Automate any workflow](https://github.com/features/actions)
  + [Codespaces
    Instant dev environments](https://github.com/features/codespaces)
  + [Issues
    Plan and track work](https://github.com/features/issues)
  + [Code Review
    Manage code changes](https://github.com/features/code-review)
  + [Discussions
    Collaborate outside of code](https://github.com/features/discussions)
  + [Code Search
    Find more, search less](https://github.com/features/code-search)

  Explore
  + [All features](https://github.com/features)
  + [Documentation](https://docs.github.com)
  + [GitHub Skills](https://skills.github.com)
  + [Blog](https://github.blog)
* Solutions

  By company size
  + [Enterprises](https://github.com/enterprise)
  + [Small and medium teams](https://github.com/team)
  + [Startups](https://github.com/enterprise/startups)
  By use case
  + [DevSecOps](/solutions/use-case/devsecops)
  + [DevOps](/solutions/use-case/devops)
  + [CI/CD](/solutions/use-case/ci-cd)
  + [View all use cases](/solutions/use-case)

  By industry
  + [Healthcare](/solutions/industry/healthcare)
  + [Financial services](/solutions/industry/financial-services)
  + [Manufacturing](/solutions/industry/manufacturing)
  + [Government](/solutions/industry/government)
  + [View all industries](/solutions/industry)

  [View all solutions](/solutions)
* Resources

  Topics
  + [AI](/resources/articles/ai)
  + [DevOps](/resources/articles/devops)
  + [Security](/resources/articles/security)
  + [Software Development](/resources/articles/software-development)
  + [View all](/resources/articles)

  Explore
  + [Learning Pathways](https://resources.github.com/learn/pathways)
  + [White papers, Ebooks, Webinars](https://resources.github.com)
  + [Customer Stories](https://github.com/customer-stories)
  + [Partners](https://partner.github.com)
  + [Executive Insights](https://github.com/solutions/executive-insights)
* Open Source

  + [GitHub Sponsors
    Fund open source developers](/sponsors)
  + [The ReadME Project
    GitHub community articles](https://github.com/readme)
  Repositories
  + [Topics](https://github.com/topics)
  + [Trending](https://github.com/trending)
  + [Collections](https://github.com/collections)
* Enterprise

  + [Enterprise platform
    AI-powered developer platform](/enterprise)
  Available add-ons
  + [Advanced Security
    Enterprise-grade security features](https://github.com/enterprise/advanced-security)
  + [GitHub Copilot
    Enterprise-grade AI features](/features/copilot#enterprise)
  + [Premium Support
    Enterprise-grade 24/7 support](/premium-support)
* [Pricing](https://github.com/pricing)

Search or jump to...

# Search code, repositories, users, issues, pull requests...

Search

Clear

[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)

# Provide feedback

We read every piece of feedback, and take your input very seriously.

Include my email address so I can be contacted

  Cancel

 Submit feedback

# Saved searches

## Use saved searches to filter your results more quickly

Name

Query

To see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).

  Cancel

 Create saved search

[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fblob%2Fa1320ec1eac186da1d03f033109191f715b2b130%2Ftensorflow%2Fcore%2Fgrappler%2Fmutable_graph_view.cc)

[Sign up](/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E%2Fblob%2Fshow&source=header-repo&source_repo=tensorflow%2Ftensorflow)
Reseting focus

You signed in with another tab or window. Reload to refresh your session.
You signed out in another tab or window. Reload to refresh your session.
You switched accounts on another tab or window. Reload to refresh your session.

Dismiss alert

{{ message }}

[tensorflow](/tensorflow)
/
**[tensorflow](/tensorflow/tensorflow)**
Public

* [Notifications](/login?return_to=%2Ftensorflow%2Ftensorflow) You must be signed in to change notification settings
* [Fork
  74.4k](/login?return_to=%2Ftensorflow%2Ftensorflow)
* [Star
   187k](/login?return_to=%2Ftensorflow%2Ftensorflow)

* [Code](/tensorflow/tensorflow)
* [Issues
  827](/tensorflow/tensorflow/issues)
* [Pull requests
  5k+](/tensorflow/tensorflow/pulls)
* [Actions](/tensorflow/tensorflow/actions)
* [Projects
  2](/tensorflow/tensorflow/projects)
* [Security](/tensorflow/tensorflow/security)
* [Insights](/tensorflow/tensorflow/pulse)

Additional navigation options

* [Code](/tensorflow/tensorflow)
* [Issues](/tensorflow/tensorflow/issues)
* [Pull requests](/tensorflow/tensorflow/pulls)
* [Actions](/tensorflow/tensorflow/actions)
* [Projects](/tensorflow/tensorflow/projects)
* [Security](/tensorflow/tensorflow/security)
* [Insights](/tensorflow/tensorflow/pulse)

## Files

 a1320ec
## Breadcrumbs

1. [tensorflow](/tensorflow/tensorflow/tree/a1320ec1eac186da1d03f033109191f715b2b130)
2. /[tensorflow](/tensorflow/tensorflow/tree/a1320ec1eac186da1d03f033109191f715b2b130/tensorflow)
3. /[core](/tensorflow/tensorflow/tree/a1320ec1eac186da1d03f033109191f715b2b130/tensorflow/core)
4. /[grappler](/tensorflow/tensorflow/tree/a1320ec1eac186da1d03f033109191f715b2b130/tensorflow/core/grappler)
/
# mutable\_graph\_view.cc

Copy path Blame  Blame
## Latest commit

## History

[History](/tensorflow/tensorflow/commits/a1320ec1eac186da1d03f033109191f715b2b130/tensorflow/core/grappler/mutable_graph_view.cc)1619 lines (1416 loc) · 60.3 KB a1320ec
## Breadcrumbs

1. [tensorflow](/tensorflow/tensorflow/tree/a1320ec1eac186da1d03f033109191f715b2b130)
2. /[tensorflow](/tensorflow/tensorflow/tree/a1320ec1eac186da1d03f033109191f715b2b130/tensorflow)
3. /[core](/tensorflow/tensorflow/tree/a1320ec1eac186da1d03f033109191f715b2b130/tensorflow/core)
4. /[grappler](/tensorflow/tensorflow/tree/a1320ec1eac186da1d03f033109191f715b2b130/tensorflow/core/grappler)
/
# mutable\_graph\_view.cc

Top
## File metadata and controls

* Code
* Blame

1619 lines (1416 loc) · 60.3 KB[Raw](https://github.com/tensorflow/tensorflow/raw/a1320ec1eac186da1d03f033109191f715b2b130/tensorflow/core/grappler/mutable_graph_view.cc)1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798991001011021031041051061071081091101111121131141151161171181191201211221231241251261271281291301311321331341351361371381391401411421431441451461471481491501511521531541551561571581591601611621631641651661671681691701711721731741751761771781791801811821831841851861871881891901911921931941951961971981992002012022032042052062072082092102112122132142152162172182192202212222232242252262272282292302312322332342352362372382392402412422432442452462472482492502512522532542552562572582592602612622632642652662672682692702712722732742752762772782792802812822832842852862872882892902912922932942952962972982993003013023033043053063073083093103113123133143153163173183193203213223233243253263273283293303313323333343353363373383393403413423433443453463473483493503513523533543553563573583593603613623633643653663673683693703713723733743753763773783793803813823833843853863873883893903913923933943953963973983994004014024034044054064074084094104114124134144154164174184194204214224234244254264274284294304314324334344354364374384394404414424434444454464474484494504514524534544554564574584594604614624634644654664674684694704714724734744754764774784794804814824834844854864874884894904914924934944954964974984995005015025035045055065075085095105115125135145155165175185195205215225235245255265275285295305315325335345355365375385395405415425435445455465475485495505515525535545555565575585595605615625635645655665675685695705715725735745755765775785795805815825835845855865875885895905915925935945955965975985996006016026036046056066076086096106116126136146156166176186196206216226236246256266276286296306316326336346356366376386396406416426436446456466476486496506516526536546556566576586596606616626636646656666676686696706716726736746756766776786796806816826836846856866876886896906916926936946956966976986997007017027037047057067077087097107117127137147157167177187197207217227237247257267277287297307317327337347357367377387397407417427437447457467477487497507517527537547557567577587597607617627637647657667677687697707717727737747757767777787797807817827837847857867877887897907917927937947957967977987998008018028038048058068078088098108118128138148158168178188198208218228238248258268278288298308318328338348358368378388398408418428438448458468478488498508518528538548558568578588598608618628638648658668678688698708718728738748758768778788798808818828838848858868878888898908918928938948958968978988999009019029039049059069079089099109119129139149159169179189199209219229239249259269279289299309319329339349359369379389399409419429439449459469479489499509519529539549559569579589599609619629639649659669679689699709719729739749759769779789799809819829839849859869879889899909919929939949959969979989991000/\* Copyright 2018 The TensorFlow Authors. All Rights Reserved.
Licensed under the Apache License, Version 2.0 (the "License");you may not use this file except in compliance with the License.You may obtain a copy of the License at
 http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an "AS IS" BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.==============================================================================\*/
#include "tensorflow/core/grappler/mutable\_graph\_view.h"
#include <algorithm>#include <utility>
#include "absl/container/flat\_hash\_map.h"#include "absl/strings/str\_cat.h"#include "absl/strings/str\_join.h"#include "absl/strings/string\_view.h"#include "absl/strings/substitute.h"#include "tensorflow/core/framework/function.h"#include "tensorflow/core/framework/graph.pb.h"#include "tensorflow/core/framework/node\_def.pb.h"#include "tensorflow/core/graph/graph.h"#include "tensorflow/core/graph/tensor\_id.h"#include "tensorflow/core/grappler/op\_types.h"#include "tensorflow/core/grappler/utils.h"#include "tensorflow/core/lib/core/errors.h"#include "tensorflow/core/lib/core/stringpiece.h"#include "tensorflow/core/lib/gtl/map\_util.h"#include "tensorflow/core/platform/types.h"
namespace tensorflow {namespace grappler {
namespace {
bool IsTensorIdPortValid(const TensorId& tensor\_id) { return tensor\_id.index() >= Graph::kControlSlot;}
bool IsTensorIdRegular(const TensorId& tensor\_id) { return tensor\_id.index() > Graph::kControlSlot;}
bool IsTensorIdControlling(const TensorId& tensor\_id) { return tensor\_id.index() == Graph::kControlSlot;}
bool IsOutputPortControlling(const MutableGraphView::OutputPort& port) { return port.port\_id == Graph::kControlSlot;}
// Determines if node is an Identity where it's first regular input is a Switch// node.bool IsIdentityConsumingSwitch(const MutableGraphView& graph, const NodeDef& node) { if ((IsIdentity(node) || IsIdentityNSingleInput(node)) && node.input\_size() > 0) { TensorId tensor\_id = ParseTensorName(node.input(0)); if (IsTensorIdControlling(tensor\_id)) { return false; }
 NodeDef\* input\_node = graph.GetNode(tensor\_id.node()); return IsSwitch(\*input\_node); } return false;}
// Determines if node input can be deduped by regular inputs when used as a// control dependency. Specifically, if a node is an Identity that leads to a// Switch node, when used as a control dependency, that control dependency// should not be deduped even though the same node is used as a regular input.bool CanDedupControlWithRegularInput(const MutableGraphView& graph, const NodeDef& control\_node) { return !IsIdentityConsumingSwitch(graph, control\_node);}
// Determines if node input can be deduped by regular inputs when used as a// control dependency. Specifically, if a node is an Identity that leads to a// Switch node, when used as a control dependency, that control dependency// should not be deduped even though the same node is used as a regular input.bool CanDedupControlWithRegularInput(const MutableGraphView& graph, absl::string\_view control\_node\_name) { NodeDef\* control\_node = graph.GetNode(control\_node\_name); if (control\_node == nullptr) { return false; } return CanDedupControlWithRegularInput(graph, \*control\_node);}
bool HasRegularFaninNode(const MutableGraphView& graph, const NodeDef& node, absl::string\_view fanin\_node\_name) { const int num\_regular\_fanins = graph.NumFanins(node, /\*include\_controlling\_nodes=\*/false); for (int i = 0; i < num\_regular\_fanins; ++i) { if (ParseTensorName(node.input(i)).node() == fanin\_node\_name) { return true; } } return false;}
using FanoutsMap = absl::flat\_hash\_map<MutableGraphView::OutputPort, absl::flat\_hash\_set<MutableGraphView::InputPort>>;
void SwapControlledFanoutInputs(const MutableGraphView& graph, const FanoutsMap::iterator& control\_fanouts, absl::string\_view to\_node\_name) { absl::string\_view from\_node\_name(control\_fanouts->first.node->name()); string control = TensorIdToString({to\_node\_name, Graph::kControlSlot}); for (const auto& control\_fanout : control\_fanouts->second) { const int start = graph.NumFanins(\*control\_fanout.node, /\*include\_controlling\_nodes=\*/false); for (int i = start; i < control\_fanout.node->input\_size(); ++i) { TensorId tensor\_id = ParseTensorName(control\_fanout.node->input(i)); if (tensor\_id.node() == from\_node\_name) { control\_fanout.node->set\_input(i, control); break; } } }}
void SwapRegularFanoutInputs(FanoutsMap\* fanouts, NodeDef\* from\_node, absl::string\_view to\_node\_name, int max\_port) { MutableGraphView::OutputPort port; port.node = from\_node; for (int i = 0; i <= max\_port; ++i) { port.port\_id = i; auto it = fanouts->find(port); if (it == fanouts->end()) { continue; } string input = TensorIdToString({to\_node\_name, i}); for (const auto& fanout : it->second) { fanout.node->set\_input(fanout.port\_id, input); } }}
using MaxOutputPortsMap = absl::flat\_hash\_map<const NodeDef\*, int>;
void SwapFanoutInputs(const MutableGraphView& graph, FanoutsMap\* fanouts, MaxOutputPortsMap\* max\_output\_ports, NodeDef\* from\_node, NodeDef\* to\_node) { auto from\_control\_fanouts = fanouts->find({from\_node, Graph::kControlSlot}); if (from\_control\_fanouts != fanouts->end()) { SwapControlledFanoutInputs(graph, from\_control\_fanouts, to\_node->name()); } auto to\_control\_fanouts = fanouts->find({to\_node, Graph::kControlSlot}); if (to\_control\_fanouts != fanouts->end()) { SwapControlledFanoutInputs(graph, to\_control\_fanouts, from\_node->name()); } auto from\_max\_port = max\_output\_ports->find(from\_node); if (from\_max\_port != max\_output\_ports->end()) { SwapRegularFanoutInputs(fanouts, from\_node, to\_node->name(), from\_max\_port->second); } auto to\_max\_port = max\_output\_ports->find(to\_node); if (to\_max\_port != max\_output\_ports->end()) { SwapRegularFanoutInputs(fanouts, to\_node, from\_node->name(), to\_max\_port->second); }}
void SwapFanoutsMapValues(FanoutsMap\* fanouts, const MutableGraphView::OutputPort& from\_port, const FanoutsMap::iterator& from\_fanouts, const MutableGraphView::OutputPort& to\_port, const FanoutsMap::iterator& to\_fanouts) { const bool from\_exists = from\_fanouts != fanouts->end(); const bool to\_exists = to\_fanouts != fanouts->end();
 if (from\_exists && to\_exists) { std::swap(from\_fanouts->second, to\_fanouts->second); } else if (from\_exists) { fanouts->emplace(to\_port, std::move(from\_fanouts->second)); fanouts->erase(from\_port); } else if (to\_exists) { fanouts->emplace(from\_port, std::move(to\_fanouts->second)); fanouts->erase(to\_port); }}
void SwapRegularFanoutsAndMaxPortValues(FanoutsMap\* fanouts, MaxOutputPortsMap\* max\_output\_ports, NodeDef\* from\_node, NodeDef\* to\_node) { auto from\_max\_port = max\_output\_ports->find(from\_node); auto to\_max\_port = max\_output\_ports->find(to\_node); bool from\_exists = from\_max\_port != max\_output\_ports->end(); bool to\_exists = to\_max\_port != max\_output\_ports->end();
 auto forward\_fanouts = [fanouts](NodeDef\* from, NodeDef\* to, int start, int end) { for (int i = start; i <= end; ++i) { MutableGraphView::OutputPort from\_port(from, i); auto from\_fanouts = fanouts->find(from\_port); if (from\_fanouts != fanouts->end()) { MutableGraphView::OutputPort to\_port(to, i); fanouts->emplace(to\_port, std::move(from\_fanouts->second)); fanouts->erase(from\_port); } } };
 if (from\_exists && to\_exists) { const int from = from\_max\_port->second; const int to = to\_max\_port->second; const int shared = std::min(from, to); for (int i = 0; i <= shared; ++i) { MutableGraphView::OutputPort from\_port(from\_node, i); auto from\_fanouts = fanouts->find(from\_port); MutableGraphView::OutputPort to\_port(to\_node, i); auto to\_fanouts = fanouts->find(to\_port); SwapFanoutsMapValues(fanouts, from\_port, from\_fanouts, to\_port, to\_fanouts); } if (to > from) { forward\_fanouts(to\_node, from\_node, shared + 1, to); } else if (from > to) { forward\_fanouts(from\_node, to\_node, shared + 1, from); }
 std::swap(from\_max\_port->second, to\_max\_port->second); } else if (from\_exists) { forward\_fanouts(from\_node, to\_node, 0, from\_max\_port->second);
 max\_output\_ports->emplace(to\_node, from\_max\_port->second); max\_output\_ports->erase(from\_node); } else if (to\_exists) { forward\_fanouts(to\_node, from\_node, 0, to\_max\_port->second);
 max\_output\_ports->emplace(from\_node, to\_max\_port->second); max\_output\_ports->erase(to\_node); }}
bool HasFanoutValue(const FanoutsMap& fanouts, const FanoutsMap::iterator& it) { return it != fanouts.end() && !it->second.empty();}
Status MutationError(absl::string\_view function\_name, absl::string\_view params, absl::string\_view msg) { return errors::InvalidArgument(absl::Substitute( "MutableGraphView::$0($1) error: $2.", function\_name, params, msg));}
using ErrorHandler = std::function<Status(absl::string\_view)>;
ErrorHandler UpdateFanoutsError(absl::string\_view from\_node\_name, absl::string\_view to\_node\_name) { return [from\_node\_name, to\_node\_name](absl::string\_view msg) { string params = absl::Substitute("from\_node\_name='$0', to\_node\_name='$1'", from\_node\_name, to\_node\_name); return MutationError("UpdateFanouts", params, msg); };}
Status CheckFaninIsRegular(const TensorId& fanin, ErrorHandler handler) { if (!IsTensorIdRegular(fanin)) { return handler(absl::Substitute("fanin '$0' must be a regular tensor id", fanin.ToString())); } return Status::OK();}
Status CheckFaninIsValid(const TensorId& fanin, ErrorHandler handler) { if (!IsTensorIdPortValid(fanin)) { return handler(absl::Substitute("fanin '$0' must be a valid tensor id", fanin.ToString())); } return Status::OK();}
Status CheckAddingFaninToSelf(absl::string\_view node\_name, const TensorId& fanin, ErrorHandler handler) { if (node\_name == fanin.node()) { return handler( absl::Substitute("can't add fanin '$0' to self", fanin.ToString())); } return Status::OK();}
Status CheckRemovingFaninFromSelf(absl::string\_view node\_name, const TensorId& fanin, ErrorHandler handler) { if (node\_name == fanin.node()) { return handler(absl::Substitute("can't remove fanin '$0' from self", fanin.ToString())); } return Status::OK();}
string NodeMissingErrorMsg(absl::string\_view node\_name) { return absl::Substitute("node '$0' was not found", node\_name);}
Status CheckNodeExists(absl::string\_view node\_name, NodeDef\* node, ErrorHandler handler) { if (node == nullptr) { return handler(NodeMissingErrorMsg(node\_name)); } return Status::OK();}
Status CheckPortRange(int port, int min, int max, ErrorHandler handler) { if (port < min || port > max) { if (max < min) { return handler("no available ports as node has no regular fanins"); } return handler( absl::Substitute("port must be in range [$0, $1]", min, max)); } return Status::OK();}
string SwapNodeNamesSwitchControlErrorMsg(absl::string\_view node\_name) { return absl::Substitute( "can't swap node name '$0' as it will become a Switch control dependency", node\_name);}
string GeneratedNameForIdentityConsumingSwitch( const MutableGraphView::OutputPort& fanin) { return AddPrefixToNodeName( absl::StrCat(fanin.node->name(), "\_", fanin.port\_id), kMutableGraphViewCtrl);}
} // namespace
void MutableGraphView::AddAndDedupFanouts(NodeDef\* node) { // TODO(lyandy): Checks for self loops, Switch control dependencies, fanins // exist, and all regular fanins come before controlling fanins. absl::flat\_hash\_set<absl::string\_view> fanins; absl::flat\_hash\_set<absl::string\_view> controlling\_fanins; int max\_input\_port = -1; int pos = 0; const int last\_idx = node->input\_size() - 1; int last\_pos = last\_idx; while (pos <= last\_pos) { TensorId tensor\_id = ParseTensorName(node->input(pos)); absl::string\_view input\_node\_name = tensor\_id.node(); bool is\_control\_input = IsTensorIdControlling(tensor\_id); bool can\_dedup\_control\_with\_regular\_input = CanDedupControlWithRegularInput(\*this, input\_node\_name); bool can\_dedup\_control = is\_control\_input && (can\_dedup\_control\_with\_regular\_input || controlling\_fanins.contains(input\_node\_name)); if (!gtl::InsertIfNotPresent(&fanins, input\_node\_name) && can\_dedup\_control) { node->mutable\_input()->SwapElements(pos, last\_pos); --last\_pos; } else { OutputPort output(nodes()[input\_node\_name], tensor\_id.index());
 if (is\_control\_input) { fanouts()[output].emplace(node, Graph::kControlSlot); } else { max\_input\_port = pos; max\_regular\_output\_port()[output.node] = std::max(max\_regular\_output\_port()[output.node], output.port\_id); fanouts()[output].emplace(node, pos); } ++pos; } if (is\_control\_input) { controlling\_fanins.insert(input\_node\_name); } }
 if (last\_pos < last\_idx) { node->mutable\_input()->DeleteSubrange(last\_pos + 1, last\_idx - last\_pos); }
 if (max\_input\_port > -1) { max\_regular\_input\_port()[node] = max\_input\_port; }}
void MutableGraphView::UpdateMaxRegularOutputPortForRemovedFanin( const OutputPort& fanin, const absl::flat\_hash\_set<InputPort>& fanin\_fanouts) { int max\_port = max\_regular\_output\_port()[fanin.node]; if (!fanin\_fanouts.empty() || max\_port != fanin.port\_id) { return; } bool updated\_max\_port = false; for (int i = fanin.port\_id - 1; i >= 0; --i) { OutputPort fanin\_port(fanin.node, i); if (!fanouts()[fanin\_port].empty()) { max\_regular\_output\_port()[fanin.node] = i; updated\_max\_port = true; break; } } if (!updated\_max\_port) { max\_regular\_output\_port().erase(fanin.node); }}
void MutableGraphView::UpdateMaxRegularOutputPortForAddedFanin( const OutputPort& fanin) { if (max\_regular\_output\_port()[fanin.node] < fanin.port\_id) { max\_regular\_output\_port()[fanin.node] = fanin.port\_id; }}
const absl::flat\_hash\_set<MutableGraphView::InputPort>&MutableGraphView::GetFanout(const GraphView::OutputPort& port) const { return GetFanout(MutableGraphView::OutputPort(const\_cast<NodeDef\*>(port.node), port.port\_id));}
absl::flat\_hash\_set<MutableGraphView::OutputPort> MutableGraphView::GetFanin( const GraphView::InputPort& port) const { return GetFanin(MutableGraphView::InputPort(const\_cast<NodeDef\*>(port.node), port.port\_id));}
const MutableGraphView::OutputPort MutableGraphView::GetRegularFanin( const GraphView::InputPort& port) const { return GetRegularFanin(MutableGraphView::InputPort( const\_cast<NodeDef\*>(port.node), port.port\_id));}
NodeDef\* MutableGraphView::AddNode(NodeDef&& node) { auto\* node\_in\_graph = graph()->add\_node(); \*node\_in\_graph = std::move(node);
 AddUniqueNodeOrDie(node\_in\_graph);
 AddAndDedupFanouts(node\_in\_graph); return node\_in\_graph;}
Status MutableGraphView::AddSubgraph(GraphDef&& subgraph) { // 1. Add all new functions and check that functions with the same name // have identical definition. const int function\_size = subgraph.library().function\_size(); if (function\_size > 0) { absl::flat\_hash\_map<absl::string\_view, const FunctionDef\*> graph\_fdefs; for (const FunctionDef& fdef : graph()->library().function()) { graph\_fdefs.emplace(fdef.signature().name(), &fdef); }
 for (FunctionDef& fdef : \*subgraph.mutable\_library()->mutable\_function()) { const auto graph\_fdef = graph\_fdefs.find(fdef.signature().name());
 if (graph\_fdef == graph\_fdefs.end()) { VLOG(3) << "Add new function definition: " << fdef.signature().name(); graph()->mutable\_library()->add\_function()->Swap(&fdef); } else { if (!FunctionDefsEqual(fdef, \*graph\_fdef->second)) { return MutationError( "AddSubgraph", absl::Substitute("function\_size=$0", function\_size), absl::StrCat( "Found different function definition with the same name: ", fdef.signature().name())); } } } }
 // 2. Add all nodes to the underlying graph. int node\_size\_before = graph()->node\_size();
 for (NodeDef& node : \*subgraph.mutable\_node()) { auto\* node\_in\_graph = graph()->add\_node(); node\_in\_graph->Swap(&node); TF\_RETURN\_IF\_ERROR(AddUniqueNode(node\_in\_graph)); }
 // TODO(ezhulenev, lyandy): Right now AddAndDedupFanouts do not check that // fanins actually exists in the graph, and there is already TODO for that.
 for (int i = node\_size\_before; i < graph()->node\_size(); ++i) { NodeDef\* node = graph()->mutable\_node(i); AddAndDedupFanouts(node); }
 return Status::OK();}
Status MutableGraphView::UpdateNode( absl::string\_view node\_name, absl::string\_view op, absl::string\_view device, absl::Span<const std::pair<string, AttrValue>> attrs) { auto error\_status = [node\_name, op, device, attrs](absl::string\_view msg) { std::vector<string> attr\_strs; attr\_strs.reserve(attrs.size()); for (const auto& attr : attrs) { string attr\_str = absl::Substitute("('$0', $1)", attr.first, attr.second.ShortDebugString()); attr\_strs.push\_back(attr\_str); } string params = absl::Substitute("node\_name='$0', op='$1', device='$2', attrs={$3}", node\_name, op, device, absl::StrJoin(attr\_strs, ", ")); return MutationError("UpdateNodeOp", params, msg); };
 NodeDef\* node = GetNode(node\_name); TF\_RETURN\_IF\_ERROR(CheckNodeExists(node\_name, node, error\_status));
 MutableGraphView::OutputPort control\_port(node, Graph::kControlSlot); auto control\_fanouts = GetFanout(control\_port); if (op == "Switch" && !control\_fanouts.empty()) { return error\_status( "can't change node op to Switch when node drives a control dependency " "(alternatively, we could add the identity node needed, but it seems " "like an unlikely event and probably a mistake)"); }
 if (node->device() != device) { node->set\_device(string(device)); } node->mutable\_attr()->clear(); for (const auto& attr : attrs) { (\*node->mutable\_attr())[attr.first] = attr.second; }
 if (node->op() == op) { return Status::OK(); }
 node->set\_op(string(op));
 if (CanDedupControlWithRegularInput(\*this, \*node)) { for (const auto& control\_fanout : control\_fanouts) { if (HasRegularFaninNode(\*this, \*control\_fanout.node, node->name())) { RemoveControllingFaninInternal(control\_fanout.node, node); } } }
 return Status::OK();}
Status MutableGraphView::UpdateNodeName(absl::string\_view from\_node\_name, absl::string\_view to\_node\_name, bool update\_fanouts) { auto error\_status = [from\_node\_name, to\_node\_name, update\_fanouts](absl::string\_view msg) { string params = absl::Substitute( "from\_node\_name='$0', to\_node\_name='$1', update\_fanouts=$2", from\_node\_name, to\_node\_name, update\_fanouts); return MutationError("UpdateNodeName", params, msg); };
 NodeDef\* node = GetNode(from\_node\_name); TF\_RETURN\_IF\_ERROR(CheckNodeExists(from\_node\_name, node, error\_status));
 if (node->name() == to\_node\_name) { return Status::OK(); } if (HasNode(to\_node\_name)) { return error\_status( "can't update node name because new node name is in use"); } auto max\_output\_port = max\_regular\_output\_port().find(node); const bool has\_max\_output\_port = max\_output\_port != max\_regular\_output\_port().end(); auto control\_fanouts = fanouts().find({node, Graph::kControlSlot});
 if (update\_fanouts) { SwapControlledFanoutInputs(\*this, control\_fanouts, to\_node\_name); if (has\_max\_output\_port) { SwapRegularFanoutInputs(&fanouts(), node, to\_node\_name, max\_output\_port->second); } } else if (has\_max\_output\_port || HasFanoutValue(fanouts(), control\_fanouts)) { return error\_status("can't update node name because node has fanouts"); }
 nodes().erase(node->name()); node->set\_name(string(to\_node\_name)); nodes().emplace(node->name(), node); return Status::OK();}
Status MutableGraphView::SwapNodeNames(absl::string\_view from\_node\_name, absl::string\_view to\_node\_name, bool update\_fanouts) { auto error\_status = [from\_node\_name, to\_node\_name, update\_fanouts](absl::string\_view msg) { string params = absl::Substitute( "from\_node\_name='$0', to\_node\_name='$1', update\_fanouts=$2", from\_node\_name, to\_node\_name, update\_fanouts); return MutationError("SwapNodeNames", params, msg); };
 NodeDef\* from\_node = GetNode(from\_node\_name); TF\_RETURN\_IF\_ERROR(CheckNodeExists(from\_node\_name, from\_node, error\_status)); if (from\_node\_name == to\_node\_name) { return Status::OK(); } NodeDef\* to\_node = GetNode(to\_node\_name); TF\_RETURN\_IF\_ERROR(CheckNodeExists(to\_node\_name, to\_node, error\_status));
 auto swap\_names = [this, from\_node, to\_node]() { nodes().erase(from\_node->name()); nodes().erase(to\_node->name()); std::swap(\*from\_node->mutable\_name(), \*to\_node->mutable\_name()); nodes().emplace(from\_node->name(), from\_node); nodes().emplace(to\_node->name(), to\_node); };
 if (update\_fanouts) { SwapFanoutInputs(\*this, &fanouts(), &max\_regular\_output\_port(), from\_node, to\_node); swap\_names(); return Status::OK(); }
 bool from\_is\_switch = IsSwitch(\*from\_node); MutableGraphView::OutputPort to\_control(to\_node, Graph::kControlSlot); auto to\_control\_fanouts = fanouts().find(to\_control); if (from\_is\_switch && HasFanoutValue(fanouts(), to\_control\_fanouts)) { return error\_status(SwapNodeNamesSwitchControlErrorMsg(from\_node\_name)); }
 bool to\_is\_switch = IsSwitch(\*to\_node); MutableGraphView::OutputPort from\_control(from\_node, Graph::kControlSlot); auto from\_control\_fanouts = fanouts().find(from\_control); if (to\_is\_switch && HasFanoutValue(fanouts(), from\_control\_fanouts)) { return error\_status(SwapNodeNamesSwitchControlErrorMsg(to\_node\_name)); }
 // Swap node names. swap\_names();
 // Swap controlling fanouts. // // Note: To and from control fanout iterators are still valid as no mutations // has been performed on fanouts(). SwapFanoutsMapValues(&fanouts(), from\_control, from\_control\_fanouts, to\_control, to\_control\_fanouts);
 // Swap regular fanouts. SwapRegularFanoutsAndMaxPortValues(&fanouts(), &max\_regular\_output\_port(), from\_node, to\_node);
 // Update fanins to remove self loops. auto update\_fanins = [this](NodeDef\* node, absl::string\_view old\_node\_name) { for (int i = 0; i < node->input\_size(); ++i) { TensorId tensor\_id = ParseTensorName(node->input(i)); if (tensor\_id.node() == node->name()) { const int idx = tensor\_id.index(); const int node\_idx = IsTensorIdControlling(tensor\_id) ? Graph::kControlSlot : i;
 MutableGraphView::OutputPort from\_fanin(node, idx); absl::flat\_hash\_set<InputPort>\* from\_fanouts = &fanouts()[from\_fanin]; from\_fanouts->erase({node, node\_idx}); UpdateMaxRegularOutputPortForRemovedFanin(from\_fanin, \*from\_fanouts);
 MutableGraphView::OutputPort to\_fanin(nodes().at(old\_node\_name), idx); fanouts()[to\_fanin].insert({node, node\_idx}); UpdateMaxRegularOutputPortForAddedFanin(to\_fanin); node->set\_input(i, TensorIdToString({old\_node\_name, idx})); } } }; update\_fanins(from\_node, to\_node->name()); update\_fanins(to\_node, from\_node->name());
 // Dedup control dependencies. auto dedup\_control\_fanouts = [this](NodeDef\* node, const FanoutsMap::iterator& control\_fanouts) { if (CanDedupControlWithRegularInput(\*this, \*node) && control\_fanouts != fanouts().end()) { for (auto it = control\_fanouts->second.begin(); it != control\_fanouts->second.end();) { // Advance `it` before invalidation from removal. const auto& control\_fanout = \*it++; if (HasRegularFaninNode(\*this, \*control\_fanout.node, node->name())) { RemoveControllingFaninInternal(control\_fanout.node, node); } } } }; auto dedup\_switch\_control = [this, dedup\_control\_fanouts](NodeDef\* node) { OutputPort port; port.node = node; const int max\_port = gtl::FindWithDefault(max\_regular\_output\_port(), node, -1); for (int i = 0; i <= max\_port; ++i) { port.port\_id = i; auto it = fanouts().find(port); if (it == fanouts().end()) { continue; } for (const auto& fanout : it->second) { auto fanout\_controls = fanouts().find({fanout.node, Graph::kControlSlot}); dedup\_control\_fanouts(fanout.node, fanout\_controls); } } };
 if (!from\_is\_switch) { if (to\_is\_switch) { dedup\_switch\_control(from\_node); } else { // Fetch iterator again as the original iterator might have been // invalidated by container rehash triggered due to mutations. auto from\_control\_fanouts = fanouts().find(from\_control); dedup\_control\_fanouts(from\_node, from\_control\_fanouts); } } if (!to\_is\_switch) { if (from\_is\_switch) { dedup\_switch\_control(to\_node); } else { // Fetch iterator again as the original iterator might have been // invalidated by container rehash triggered due to mutations. auto to\_control\_fanouts = fanouts().find(to\_control); dedup\_control\_fanouts(to\_node, to\_control\_fanouts); } }
 return Status::OK();}
Status MutableGraphView::UpdateFanouts(absl::string\_view from\_node\_name, absl::string\_view to\_node\_name) { NodeDef\* from\_node = GetNode(from\_node\_name); TF\_RETURN\_IF\_ERROR( CheckNodeExists(from\_node\_name, from\_node, UpdateFanoutsError(from\_node\_name, to\_node\_name))); NodeDef\* to\_node = GetNode(to\_node\_name); TF\_RETURN\_IF\_ERROR(CheckNodeExists( to\_node\_name, to\_node, UpdateFanoutsError(from\_node\_name, to\_node\_name)));
 return UpdateFanoutsInternal(from\_node, to\_node);}
Status MutableGraphView::UpdateFanoutsInternal(NodeDef\* from\_node, NodeDef\* to\_node) { VLOG(2) << absl::Substitute("Update fanouts from '$0' to '$1'.", from\_node->name(), to\_node->name()); if (from\_node == to\_node) { return Status::OK(); }
 // Update internal state with the new output\_port->input\_port edge. const auto add\_edge = [this](const OutputPort& output\_port, const InputPort& input\_port) { fanouts()[output\_port].insert(input\_port); };
 // Remove invalidated edge from the internal state. const auto remove\_edge = [this](const OutputPort& output\_port, const InputPort& input\_port) { fanouts()[output\_port].erase(input\_port); };
 // For the control fanouts we do not know the input index in a NodeDef, // so we have to traverse all control inputs.
 auto control\_fanouts = GetFanout(GraphView::OutputPort(from\_node, Graph::kControlSlot));
 bool to\_node\_is\_switch = IsSwitch(\*to\_node); for (const InputPort& control\_port : control\_fanouts) { // Node can't be control dependency of itself. if (control\_port.node == to\_node) continue;
 // Can't add Switch node as a control dependency. if (to\_node\_is\_switch) { // Trying to add a Switch as a control dependency, which if allowed will // make the graph invalid. return UpdateFanoutsError(from\_node->name(), to\_node->name())( absl::Substitute("can't update fanouts to node '$0' as it will " "become a Switch control dependency", to\_node->name())); }
 NodeDef\* node = control\_port.node; RemoveControllingFaninInternal(node, from\_node); AddFaninInternal(node, {to\_node, Graph::kControlSlot}); }
 // First we update regular fanouts. For the regular fanouts // `input\_port:port\_id` is the input index in NodeDef.
 auto regular\_edges = GetFanoutEdges(\*from\_node, /\*include\_controlled\_edges=\*/false);
 // Maximum index of the `from\_node` output tensor that is still used as an // input to some other node. int keep\_max\_regular\_output\_port = -1;
 for (const Edge& edge : regular\_edges) { const OutputPort output\_port = edge.src; const InputPort input\_port = edge.dst;
 // If the `to\_node` reads from the `from\_node`, skip this edge (see // AddAndUpdateFanoutsWithoutSelfLoops test for an example). if (input\_port.node == to\_node) { keep\_max\_regular\_output\_port = std::max(keep\_max\_regular\_output\_port, output\_port.port\_id); continue; }
 // Update input at destination node. input\_port.node->set\_input( input\_port.port\_id, TensorIdToString({to\_node->name(), output\_port.port\_id}));
 // Remove old edge between the `from\_node` and the fanout node. remove\_edge(output\_port, input\_port); // Add an edge between the `to\_node` and new fanout node. add\_edge(OutputPort(to\_node, output\_port.port\_id), input\_port); // Dedup control dependency. if (CanDedupControlWithRegularInput(\*this, \*to\_node)) { RemoveControllingFaninInternal(input\_port.node, to\_node); } }
 // Because we update all regular fanouts of `from\_node`, we can just copy // the value `num\_regular\_outputs`. max\_regular\_output\_port()[to\_node] = max\_regular\_output\_port()[from\_node];
 // Check if all fanouts were updated to read from the `to\_node`. if (keep\_max\_regular\_output\_port >= 0) { max\_regular\_output\_port()[from\_node] = keep\_max\_regular\_output\_port; } else { max\_regular\_output\_port().erase(from\_node); }
 return Status::OK();}
bool MutableGraphView::AddFaninInternal(NodeDef\* node, const OutputPort& fanin) { int num\_regular\_fanins = NumFanins(\*node, /\*include\_controlling\_nodes=\*/false); bool input\_is\_control = IsOutputPortControlling(fanin); bool can\_dedup\_control\_with\_regular\_input = CanDedupControlWithRegularInput(\*this, \*fanin.node); // Don't add duplicate control dependencies. if (input\_is\_control) { const int start = can\_dedup\_control\_with\_regular\_input ? 0 : num\_regular\_fanins; for (int i = start; i < node->input\_size(); ++i) { if (ParseTensorName(node->input(i)).node() == fanin.node->name()) { return false; } } }
 InputPort input; input.node = node; input.port\_id = input\_is\_control ? Graph::kControlSlot : num\_regular\_fanins;
 node->add\_input(TensorIdToString({fanin.node->name(), fanin.port\_id})); if (!input\_is\_control) { const int last\_node\_input = node->input\_size() - 1; // If there are control dependencies in node, move newly inserted fanin to // be before such control dependencies. if (num\_regular\_fanins < last\_node\_input) { node->mutable\_input()->SwapElements(last\_node\_input, num\_regular\_fanins); } }
 fanouts()[fanin].insert(input); if (max\_regular\_output\_port()[fanin.node] < fanin.port\_id) { max\_regular\_output\_port()[fanin.node] = fanin.port\_id; }
 // Update max input port and dedup control dependencies. if (!input\_is\_control) { max\_regular\_input\_port()[node] = num\_regular\_fanins; if (can\_dedup\_control\_with\_regular\_input) { RemoveControllingFaninInternal(node, fanin.node); } }
 return true;}
Status MutableGraphView::AddRegularFanin(absl::string\_view node\_name, const TensorId& fanin) { auto error\_status = [node\_name, fanin](absl::string\_view msg) { string params = absl::Substitute("node\_name='$0', fanin='$1'", node\_name, fanin.ToString()); return MutationError("AddRegularFanin", params, msg); };
 TF\_RETURN\_IF\_ERROR(CheckFaninIsRegular(fanin, error\_status)); TF\_RETURN\_IF\_ERROR(CheckAddingFaninToSelf(node\_name, fanin, error\_status)); NodeDef\* node = GetNode(node\_name); TF\_RETURN\_IF\_ERROR(CheckNodeExists(node\_name, node, error\_status)); NodeDef\* fanin\_node = GetNode(fanin.node()); TF\_RETURN\_IF\_ERROR(CheckNodeExists(fanin.node(), fanin\_node, error\_status));
 AddFaninInternal(node, {fanin\_node, fanin.index()}); return Status::OK();}
Status MutableGraphView::AddRegularFaninByPort(absl::string\_view node\_name, int port, const TensorId& fanin) { auto error\_status = [node\_name, port, fanin](absl::string\_view msg) { string params = absl::Substitute("node\_name='$0', port=$1, fanin='$2'", node\_name, port, fanin.ToString()); return MutationError("AddRegularFaninByPort", params, msg); };
 TF\_RETURN\_IF\_ERROR(CheckFaninIsRegular(fanin, error\_status)); TF\_RETURN\_IF\_ERROR(CheckAddingFaninToSelf(node\_name, fanin, error\_status)); NodeDef\* node = GetNode(node\_name); TF\_RETURN\_IF\_ERROR(CheckNodeExists(node\_name, node, error\_status)); const int num\_regular\_fanins = NumFanins(\*node, /\*include\_controlling\_nodes=\*/false); TF\_RETURN\_IF\_ERROR( CheckPortRange(port, /\*min=\*/0, num\_regular\_fanins, error\_status)); NodeDef\* fanin\_node = GetNode(fanin.node()); TF\_RETURN\_IF\_ERROR(CheckNodeExists(fanin.node(), fanin\_node, error\_status));
 const int last\_node\_input = node->input\_size(); node->add\_input(TensorIdToString(fanin)); node->mutable\_input()->SwapElements(num\_regular\_fanins, last\_node\_input); for (int i = num\_regular\_fanins - 1; i >= port; --i) { TensorId tensor\_id = ParseTensorName(node->input(i)); OutputPort fanin\_port(nodes()[tensor\_id.node()], tensor\_id.index()); absl::flat\_hash\_set<InputPort>\* fanouts\_set = &fanouts()[fanin\_port]; fanouts\_set->erase({node, i}); fanouts\_set->insert({node, i + 1}); node->mutable\_input()->SwapElements(i, i + 1); }
 OutputPort fanin\_port(fanin\_node, fanin.index()); fanouts()[fanin\_port].insert({node, port}); UpdateMaxRegularOutputPortForAddedFanin(fanin\_port);
 max\_regular\_input\_port()[node] = num\_regular\_fanins; if (CanDedupControlWithRegularInput(\*this, \*fanin\_node)) { RemoveControllingFaninInternal(node, fanin\_node); }
 return Status::OK();}
NodeDef\* MutableGraphView::GetControllingFaninToAdd(absl::string\_view node\_name, const OutputPort& fanin, string\* error\_msg) { if (!IsSwitch(\*fanin.node)) { return fanin.node; } else { if (IsOutputPortControlling(fanin)) { // Can't add a Switch node control dependency. TensorId tensor\_id(fanin.node->name(), fanin.port\_id); \*error\_msg = absl::Substitute( "can't add fanin '$0' as it will become a Switch control dependency", tensor\_id.ToString()); return nullptr; } // We can't anchor control dependencies directly on the switch node: unlike // other nodes only one of the outputs of the switch node will be generated // when the switch node is executed, and we need to make sure the control // dependency is only triggered when the corresponding output is triggered. // We start by looking for an identity node connected to the output of the // switch node, and use it to anchor the control dependency. for (const auto& fanout : GetFanout(fanin)) { if (IsIdentity(\*fanout.node) || IsIdentityNSingleInput(\*fanout.node)) { if (fanout.node->name() == node\_name) { \*error\_msg = absl::Substitute("can't add found fanin '$0' to self", AsControlDependency(fanout.node->name())); return nullptr; } return fanout.node; } }
 // No node found, check if node to be created is itself. if (GeneratedNameForIdentityConsumingSwitch(fanin) == node\_name) { \*error\_msg = absl::Substitute("can't add generated fanin '$0' to self", AsControlDependency(string(node\_name))); } } return nullptr;}
NodeDef\* MutableGraphView::GetOrCreateIdentityConsumingSwitch( const OutputPort& fanin) { // We haven't found an existing node where we can anchor the control // dependency: add a new identity node.[View remainder of file in raw view](https://github.com/tensorflow/tensorflow/raw/a1320ec1eac186da1d03f033109191f715b2b130/tensorflow/core/grappler/mutable_graph_view.cc)

## Footer

© 2025 GitHub, Inc.

### Footer navigation

* [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)
* [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)
* [Security](https://github.com/security)
* [Status](https://www.githubstatus.com/)
* [Docs](https://docs.github.com/)
* [Contact](https://support.github.com?tags=dotcom-footer)
* Manage cookies
* Do not share my personal information

You can’t perform that action at this time.



=== Content from github.com_159f7126_20250114_210436.html ===

[Skip to content](#start-of-content)

## Navigation Menu

Toggle navigation

[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fblob%2Fa1320ec1eac186da1d03f033109191f715b2b130%2Ftensorflow%2Fcore%2Fgrappler%2Foptimizers%2Fconstant_folding.cc)

* Product

  + [GitHub Copilot
    Write better code with AI](https://github.com/features/copilot)
  + [Security
    Find and fix vulnerabilities](https://github.com/features/security)
  + [Actions
    Automate any workflow](https://github.com/features/actions)
  + [Codespaces
    Instant dev environments](https://github.com/features/codespaces)
  + [Issues
    Plan and track work](https://github.com/features/issues)
  + [Code Review
    Manage code changes](https://github.com/features/code-review)
  + [Discussions
    Collaborate outside of code](https://github.com/features/discussions)
  + [Code Search
    Find more, search less](https://github.com/features/code-search)

  Explore
  + [All features](https://github.com/features)
  + [Documentation](https://docs.github.com)
  + [GitHub Skills](https://skills.github.com)
  + [Blog](https://github.blog)
* Solutions

  By company size
  + [Enterprises](https://github.com/enterprise)
  + [Small and medium teams](https://github.com/team)
  + [Startups](https://github.com/enterprise/startups)
  By use case
  + [DevSecOps](/solutions/use-case/devsecops)
  + [DevOps](/solutions/use-case/devops)
  + [CI/CD](/solutions/use-case/ci-cd)
  + [View all use cases](/solutions/use-case)

  By industry
  + [Healthcare](/solutions/industry/healthcare)
  + [Financial services](/solutions/industry/financial-services)
  + [Manufacturing](/solutions/industry/manufacturing)
  + [Government](/solutions/industry/government)
  + [View all industries](/solutions/industry)

  [View all solutions](/solutions)
* Resources

  Topics
  + [AI](/resources/articles/ai)
  + [DevOps](/resources/articles/devops)
  + [Security](/resources/articles/security)
  + [Software Development](/resources/articles/software-development)
  + [View all](/resources/articles)

  Explore
  + [Learning Pathways](https://resources.github.com/learn/pathways)
  + [White papers, Ebooks, Webinars](https://resources.github.com)
  + [Customer Stories](https://github.com/customer-stories)
  + [Partners](https://partner.github.com)
  + [Executive Insights](https://github.com/solutions/executive-insights)
* Open Source

  + [GitHub Sponsors
    Fund open source developers](/sponsors)
  + [The ReadME Project
    GitHub community articles](https://github.com/readme)
  Repositories
  + [Topics](https://github.com/topics)
  + [Trending](https://github.com/trending)
  + [Collections](https://github.com/collections)
* Enterprise

  + [Enterprise platform
    AI-powered developer platform](/enterprise)
  Available add-ons
  + [Advanced Security
    Enterprise-grade security features](https://github.com/enterprise/advanced-security)
  + [GitHub Copilot
    Enterprise-grade AI features](/features/copilot#enterprise)
  + [Premium Support
    Enterprise-grade 24/7 support](/premium-support)
* [Pricing](https://github.com/pricing)

Search or jump to...

# Search code, repositories, users, issues, pull requests...

Search

Clear

[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)

# Provide feedback

We read every piece of feedback, and take your input very seriously.

Include my email address so I can be contacted

  Cancel

 Submit feedback

# Saved searches

## Use saved searches to filter your results more quickly

Name

Query

To see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).

  Cancel

 Create saved search

[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fblob%2Fa1320ec1eac186da1d03f033109191f715b2b130%2Ftensorflow%2Fcore%2Fgrappler%2Foptimizers%2Fconstant_folding.cc)

[Sign up](/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E%2Fblob%2Fshow&source=header-repo&source_repo=tensorflow%2Ftensorflow)
Reseting focus

You signed in with another tab or window. Reload to refresh your session.
You signed out in another tab or window. Reload to refresh your session.
You switched accounts on another tab or window. Reload to refresh your session.

Dismiss alert

{{ message }}

[tensorflow](/tensorflow)
/
**[tensorflow](/tensorflow/tensorflow)**
Public

* [Notifications](/login?return_to=%2Ftensorflow%2Ftensorflow) You must be signed in to change notification settings
* [Fork
  74.4k](/login?return_to=%2Ftensorflow%2Ftensorflow)
* [Star
   187k](/login?return_to=%2Ftensorflow%2Ftensorflow)

* [Code](/tensorflow/tensorflow)
* [Issues
  827](/tensorflow/tensorflow/issues)
* [Pull requests
  5k+](/tensorflow/tensorflow/pulls)
* [Actions](/tensorflow/tensorflow/actions)
* [Projects
  2](/tensorflow/tensorflow/projects)
* [Security](/tensorflow/tensorflow/security)
* [Insights](/tensorflow/tensorflow/pulse)

Additional navigation options

* [Code](/tensorflow/tensorflow)
* [Issues](/tensorflow/tensorflow/issues)
* [Pull requests](/tensorflow/tensorflow/pulls)
* [Actions](/tensorflow/tensorflow/actions)
* [Projects](/tensorflow/tensorflow/projects)
* [Security](/tensorflow/tensorflow/security)
* [Insights](/tensorflow/tensorflow/pulse)

## Files

 a1320ec
## Breadcrumbs

1. [tensorflow](/tensorflow/tensorflow/tree/a1320ec1eac186da1d03f033109191f715b2b130)
2. /[tensorflow](/tensorflow/tensorflow/tree/a1320ec1eac186da1d03f033109191f715b2b130/tensorflow)
3. /[core](/tensorflow/tensorflow/tree/a1320ec1eac186da1d03f033109191f715b2b130/tensorflow/core)
4. /[grappler](/tensorflow/tensorflow/tree/a1320ec1eac186da1d03f033109191f715b2b130/tensorflow/core/grappler)
5. /[optimizers](/tensorflow/tensorflow/tree/a1320ec1eac186da1d03f033109191f715b2b130/tensorflow/core/grappler/optimizers)
/
# constant\_folding.cc

Copy path Blame  Blame
## Latest commit

## History

[History](/tensorflow/tensorflow/commits/a1320ec1eac186da1d03f033109191f715b2b130/tensorflow/core/grappler/optimizers/constant_folding.cc)4050 lines (3771 loc) · 154 KB a1320ec
## Breadcrumbs

1. [tensorflow](/tensorflow/tensorflow/tree/a1320ec1eac186da1d03f033109191f715b2b130)
2. /[tensorflow](/tensorflow/tensorflow/tree/a1320ec1eac186da1d03f033109191f715b2b130/tensorflow)
3. /[core](/tensorflow/tensorflow/tree/a1320ec1eac186da1d03f033109191f715b2b130/tensorflow/core)
4. /[grappler](/tensorflow/tensorflow/tree/a1320ec1eac186da1d03f033109191f715b2b130/tensorflow/core/grappler)
5. /[optimizers](/tensorflow/tensorflow/tree/a1320ec1eac186da1d03f033109191f715b2b130/tensorflow/core/grappler/optimizers)
/
# constant\_folding.cc

Top
## File metadata and controls

* Code
* Blame

4050 lines (3771 loc) · 154 KB[Raw](https://github.com/tensorflow/tensorflow/raw/a1320ec1eac186da1d03f033109191f715b2b130/tensorflow/core/grappler/optimizers/constant_folding.cc)1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798991001011021031041051061071081091101111121131141151161171181191201211221231241251261271281291301311321331341351361371381391401411421431441451461471481491501511521531541551561571581591601611621631641651661671681691701711721731741751761771781791801811821831841851861871881891901911921931941951961971981992002012022032042052062072082092102112122132142152162172182192202212222232242252262272282292302312322332342352362372382392402412422432442452462472482492502512522532542552562572582592602612622632642652662672682692702712722732742752762772782792802812822832842852862872882892902912922932942952962972982993003013023033043053063073083093103113123133143153163173183193203213223233243253263273283293303313323333343353363373383393403413423433443453463473483493503513523533543553563573583593603613623633643653663673683693703713723733743753763773783793803813823833843853863873883893903913923933943953963973983994004014024034044054064074084094104114124134144154164174184194204214224234244254264274284294304314324334344354364374384394404414424434444454464474484494504514524534544554564574584594604614624634644654664674684694704714724734744754764774784794804814824834844854864874884894904914924934944954964974984995005015025035045055065075085095105115125135145155165175185195205215225235245255265275285295305315325335345355365375385395405415425435445455465475485495505515525535545555565575585595605615625635645655665675685695705715725735745755765775785795805815825835845855865875885895905915925935945955965975985996006016026036046056066076086096106116126136146156166176186196206216226236246256266276286296306316326336346356366376386396406416426436446456466476486496506516526536546556566576586596606616626636646656666676686696706716726736746756766776786796806816826836846856866876886896906916926936946956966976986997007017027037047057067077087097107117127137147157167177187197207217227237247257267277287297307317327337347357367377387397407417427437447457467477487497507517527537547557567577587597607617627637647657667677687697707717727737747757767777787797807817827837847857867877887897907917927937947957967977987998008018028038048058068078088098108118128138148158168178188198208218228238248258268278288298308318328338348358368378388398408418428438448458468478488498508518528538548558568578588598608618628638648658668678688698708718728738748758768778788798808818828838848858868878888898908918928938948958968978988999009019029039049059069079089099109119129139149159169179189199209219229239249259269279289299309319329339349359369379389399409419429439449459469479489499509519529539549559569579589599609619629639649659669679689699709719729739749759769779789799809819829839849859869879889899909919929939949959969979989991000/\* Copyright 2017 The TensorFlow Authors. All Rights Reserved.
Licensed under the Apache License, Version 2.0 (the "License");you may not use this file except in compliance with the License.You may obtain a copy of the License at
 http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an "AS IS" BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.==============================================================================\*/
#define EIGEN\_USE\_THREADS
#include "tensorflow/core/grappler/optimizers/constant\_folding.h"
#include <cmath>
#include "absl/strings/string\_view.h"#include "absl/strings/substitute.h"#include "tensorflow/core/framework/allocator.h"#include "tensorflow/core/framework/attr\_value.pb.h"#include "tensorflow/core/framework/function.pb.h"#include "tensorflow/core/framework/node\_def.pb.h"#include "tensorflow/core/framework/op.h"#include "tensorflow/core/framework/op\_def.pb.h"#include "tensorflow/core/framework/tensor.pb.h"#include "tensorflow/core/framework/tensor\_shape.pb.h"#include "tensorflow/core/framework/tensor\_util.h"#include "tensorflow/core/framework/types.h"#include "tensorflow/core/framework/types.pb.h"#include "tensorflow/core/framework/versions.pb.h"#include "tensorflow/core/grappler/clusters/cluster.h"#include "tensorflow/core/grappler/costs/graph\_properties.h"#include "tensorflow/core/grappler/grappler\_item.h"#include "tensorflow/core/grappler/op\_types.h"#include "tensorflow/core/grappler/optimizers/evaluation\_utils.h"#include "tensorflow/core/grappler/utils.h"#include "tensorflow/core/grappler/utils/symbolic\_shapes.h"#include "tensorflow/core/lib/core/errors.h"#include "tensorflow/core/lib/core/stringpiece.h"#include "tensorflow/core/lib/gtl/cleanup.h"#include "tensorflow/core/lib/gtl/inlined\_vector.h"#include "tensorflow/core/lib/strings/numbers.h"#include "tensorflow/core/lib/strings/strcat.h"#include "tensorflow/core/platform/cpu\_info.h"#include "tensorflow/core/platform/denormal.h"#include "tensorflow/core/platform/env.h"#include "tensorflow/core/platform/setround.h"#include "tensorflow/core/platform/tensor\_coding.h"#include "tensorflow/core/public/version.h"#include "tensorflow/core/util/bcast.h"#include "tensorflow/core/util/saved\_tensor\_slice\_util.h"
namespace tensorflow {namespace grappler {using TensorVector = gtl::InlinedVector<TensorValue, 4>;
// We only fold/materialize constants smaller than 100kB.const int64\_t kMaxConstantSize = 100 \* 1024;
namespace {template <typename T>bool AllValuesAre(const TensorProto& proto, const T& value) { Tensor tensor; if (!tensor.FromProto(proto)) { return false; } auto values = tensor.flat<T>(); for (int i = 0; i < tensor.NumElements(); ++i) { if (values(i) != value) { return false; } } return true;}
// Add new\_input as a control input to node if it does not already depend on it.// TODO(rmlarsen): Move the following two utility functions to utils.{h,cc} and// clean up code that should be using them.bool MaybeAddControlInput(const string& ctrl\_input, NodeDef\* node, GraphDef\* graph, NodeMap\* node\_map) { bool already\_exists = false; for (const string& input : node->input()) { if (input == ctrl\_input || AsControlDependency(input) == ctrl\_input) { already\_exists = true; break; } } if (!already\_exists) { const string ctrl\_dep = ConstantFolding::AddControlDependency(ctrl\_input, graph, node\_map); node->add\_input(ctrl\_dep); node\_map->AddOutput(NodeName(ctrl\_input), node->name()); } return !already\_exists;}
// Remove old\_input as a control input to node.bool MaybeRemoveControlInput(const string& old\_input, NodeDef\* node, GraphDef\* graph, NodeMap\* node\_map) { bool removed\_input = false; bool update\_node\_map = true; const string old\_input\_ctrl\_dep = AsControlDependency(NodeName(old\_input)); for (int i = 0; i < node->input\_size(); ++i) { const string& input = node->input(i); if (old\_input\_ctrl\_dep == input) { if (IsControlInput(input)) { node->mutable\_input()->SwapElements(i, node->input\_size() - 1); node->mutable\_input()->RemoveLast(); removed\_input = true; } else { // There is a non-control input from the same node. // Don't remove the output from the NodeMap. update\_node\_map = false; } } } if (update\_node\_map) { node\_map->RemoveOutput(NodeName(old\_input), node->name()); } return removed\_input;}
bool HasTPUAttributes(const NodeDef& node) { AttrSlice attrs(node); for (const auto& attr : attrs) { if (attr.first.find("\_tpu\_") != attr.first.npos) { return true; } } return false;}
template <typename T>bool PackedValuesNotEqual(T a, T b) { return a != b;}
template <>bool PackedValuesNotEqual(float a, float b) { return reinterpret\_cast<int32\_t&>(a) != reinterpret\_cast<int32\_t&>(b);}
template <>bool PackedValuesNotEqual(double a, double b) { return reinterpret\_cast<int64\_t&>(a) != reinterpret\_cast<int64\_t&>(b);}
float QuantizedTypeMinAsFloat(DataType data\_type) { switch (data\_type) { case DT\_QINT8: return Eigen::NumTraits<qint8>::lowest(); case DT\_QUINT8: return Eigen::NumTraits<quint8>::lowest(); case DT\_QINT16: return Eigen::NumTraits<qint16>::lowest(); case DT\_QUINT16: return Eigen::NumTraits<quint16>::lowest(); case DT\_QINT32: return Eigen::NumTraits<qint32>::lowest(); default: return 0.0f; }}
float QuantizedTypeMaxAsFloat(DataType data\_type) { switch (data\_type) { case DT\_QINT8: return Eigen::NumTraits<qint8>::highest(); case DT\_QUINT8: return Eigen::NumTraits<quint8>::highest(); case DT\_QINT16: return Eigen::NumTraits<qint16>::highest(); case DT\_QUINT16: return Eigen::NumTraits<quint16>::highest(); case DT\_QINT32: return Eigen::NumTraits<qint32>::highest(); default: return 0.0f; }}
} // namespace
ConstantFolding::ConstantFolding(RewriterConfig::Toggle opt\_level, DeviceBase\* cpu\_device, bool disable\_compressed\_tensor\_optimization, bool fold\_quantization\_emulation) : opt\_level\_(opt\_level), cpu\_device\_(cpu\_device), disable\_compressed\_tensor\_optimization\_( disable\_compressed\_tensor\_optimization), fold\_quantization\_emulation\_(fold\_quantization\_emulation) { resource\_mgr\_.reset(new ResourceMgr());}
ConstantFolding::ConstantFolding(DeviceBase\* cpu\_device, bool disable\_compressed\_tensor\_optimization, bool fold\_quantization\_ops) : ConstantFolding(RewriterConfig::ON, cpu\_device, disable\_compressed\_tensor\_optimization, fold\_quantization\_ops) {}
// staticstring ConstantFolding::AddControlDependency(const string& input\_name, GraphDef\* graph, NodeMap\* node\_map) { if (IsControlInput(input\_name)) { return input\_name; } const NodeDef\* node = node\_map->GetNode(input\_name); // Sanity check for missing node. if (!node) { return input\_name; } if (!IsSwitch(\*node)) { return AsControlDependency(\*node); } else { // We can't anchor control dependencies directly on the switch node: unlike // other nodes only one of the outputs of the switch node will be generated // when the switch node is executed, and we need to make sure the control // dependency is only triggered when the corresponding output is triggered. // We start by looking for an identity node connected to the output of the // switch node, and use it to anchor the control dependency. for (const NodeDef\* output : node\_map->GetOutputs(node->name())) { if (IsIdentity(\*output) || IsIdentityNSingleInput(\*output)) { if (IsSameInput(node->input(0), input\_name)) { return AsControlDependency(\*output); } } } // We haven't found an existing node where we can anchor the control // dependency: add a new identity node. int port = 0; string ctrl\_dep\_name = ParseNodeName(input\_name, &port); strings::StrAppend(&ctrl\_dep\_name, "\_", port); ctrl\_dep\_name = AddPrefixToNodeName(ctrl\_dep\_name, kConstantFoldingCtrl); const DataType output\_type = node->attr().at("T").type();
 NodeDef\* added\_node = node\_map->GetNode(ctrl\_dep\_name); if (added\_node == nullptr) { added\_node = graph->add\_node(); added\_node->set\_name(ctrl\_dep\_name); added\_node->set\_op("Identity"); added\_node->set\_device(node->device());
 (\*added\_node->mutable\_attr())["T"].set\_type(output\_type); \*added\_node->add\_input() = input\_name; node\_map->AddNode(added\_node->name(), added\_node); node\_map->AddOutput(node->name(), added\_node->name()); } return AsControlDependency(\*added\_node); }}
// Forward inputs at the given indices to outputs and add a control dependency// on node.bool ConstantFolding::ForwardInputs(NodeDef\* node, absl::Span<const int> inputs\_to\_forward) { for (int input\_idx : inputs\_to\_forward) { if (input\_idx < 0 || input\_idx >= node->input\_size()) { return false; } }
 const auto& tmp = node\_map\_->GetOutputs(node->name()); const std::vector<NodeDef\*> consumers(tmp.begin(), tmp.end()); bool updated\_graph = false; for (int input\_idx : inputs\_to\_forward) { const string& input = node->input(input\_idx); if (IsControlInput(input) && consumers.size() > 1) { continue; } const NodeDef\* input\_node = node\_map\_->GetNode(NodeName(input)); if (input\_node == nullptr) { LOG(ERROR) << "Bad input: " << input; break; } // Update each consumer. for (NodeDef\* consumer : consumers) { bool add\_dep = false; for (int consumer\_input\_idx = 0; consumer\_input\_idx < consumer->input\_size(); ++consumer\_input\_idx) { const string& consumer\_input = consumer->input(consumer\_input\_idx); if (IsControlInput(consumer\_input)) { break; } // It is illegal to add control dependencies to \_Retval nodes, so we // can't bypass value producing `node` and forward inputs to `consumer`. if (IsRetval(\*consumer)) { break; } int output\_idx; const string input\_node\_name = ParseNodeName(consumer\_input, &output\_idx); if (input\_node\_name == node->name() && output\_idx == input\_idx) { consumer->set\_input(consumer\_input\_idx, input); // We will keep the input from the node through a control // dependency, so we only need to add the consumer as an output // for the input node. node\_map\_->AddOutput(NodeName(input), consumer->name()); add\_dep = true; } } if (add\_dep) { consumer->add\_input(AsControlDependency(node->name())); updated\_graph = true; } } }
 if (updated\_graph) { for (NodeDef\* consumer : consumers) { DedupControlInputs(consumer); } } return updated\_graph;}
// Puts the given value into the tensor at the given "flat" index.static Status PutValueIntoTensor(const int64\_t value, const DataType& type, const int index, Tensor\* tensor) { if (type == DT\_INT32) { if (value >= INT\_MAX) { return Status(error::INVALID\_ARGUMENT, "int32 overflow"); } tensor->flat<int32>()(index) = static\_cast<int32>(value); } else { tensor->flat<int64\_t>()(index) = value; } return Status::OK();}
// Writes the given tensor shape into the given tensor.// Op is assumed to be Shape, ShapeN, Size or Rank.static Status ConvertShapeToConstant(const string& op, const DataType& type, const PartialTensorShape& shp, Tensor\* tensor) { if (op == "Shape" || op == "ShapeN") { \*tensor = Tensor(type, TensorShape({shp.dims()})); for (int i = 0; i < shp.dims(); ++i) { TF\_RETURN\_IF\_ERROR(PutValueIntoTensor(shp.dim\_size(i), type, i, tensor)); } } else if (op == "Size") { int64\_t size = 1; for (int i = 0; i < shp.dims(); ++i) { size \*= shp.dim\_size(i); } \*tensor = Tensor(type, TensorShape({})); TF\_RETURN\_IF\_ERROR(PutValueIntoTensor(size, type, 0, tensor)); } else { CHECK\_EQ(op, "Rank"); \*tensor = Tensor(type, TensorShape({})); TF\_RETURN\_IF\_ERROR(PutValueIntoTensor(shp.dims(), type, 0, tensor)); } return Status::OK();}
// TODO(rmlarsen): Perhaps we should move this to the GraphOptimizer base class.bool ConstantFolding::OptimizedNodeExists(const NodeDef& node, StringPiece suffix) const { return node\_map\_->NodeExists(OptimizedNodeName(node, suffix));}
string ConstantFolding::OptimizedNodeName(const NodeDef& node, StringPiece suffix) const { return AddPrefixToNodeName(strings::StrCat(node.name(), suffix), kConstantFoldingConst);}
bool ConstantFolding::IsReallyConstant(const NodeDef& node) const { if (!IsConstant(node)) { return false; } // If the node is fed it's not constant anymore. return feed\_nodes\_.find(node.name()) == feed\_nodes\_.end();}
// TODO(rmlarsen): Refactor to shared util.bool ConstantFolding::GetTensorFromConstNode(const string& node\_name\_or\_input, Tensor\* tensor) { const NodeDef\* node = node\_map\_->GetNode(node\_name\_or\_input); return node != nullptr && IsReallyConstant(\*node) && CheckAttrExists(\*node, "value").ok() && tensor->FromProto(node->attr().at("value").tensor());}
// Materialize the shapes using constants whenever possible.Status ConstantFolding::MaterializeShapes(const GraphProperties& properties) { // We may add some nodes to the graph to encode control dependencies and hold // the materialized shapes: there is no need to process these added nodes, so // only iterate over the nodes of the input graph. const int node\_count = graph\_->node\_size(); for (int node\_idx = 0; node\_idx < node\_count; ++node\_idx) { NodeDef\* node = graph\_->mutable\_node(node\_idx); const string op = node->op(); if (op != "Shape" && op != "Size" && op != "Rank" && op != "ShapeN" && op != "TensorArraySizeV3") { continue; } const std::vector<OpInfo::TensorProperties>& output = properties.GetOutputProperties(node->name()); const std::vector<OpInfo::TensorProperties>& input = properties.GetInputProperties(node->name()); if (input.empty() || output.empty()) { continue; }
 if (op == "Shape" || op == "Size" || op == "Rank") { CHECK\_EQ(1, output.size()); CHECK\_EQ(1, input.size());
 const DataType type = output[0].dtype(); CHECK(type == DT\_INT32 || type == DT\_INT64); const PartialTensorShape shape(input[0].shape());
 if ((op != "Rank" && !shape.IsFullyDefined()) || (op == "Rank" && shape.unknown\_rank())) { continue; }
 Tensor constant\_value(type); if (!ConvertShapeToConstant(op, type, shape, &constant\_value).ok()) { continue; }
 // TODO(rmlarsen): Remove this workaround for b/150861569 // The bug involves an expression of the form Shape(ExpandDims(x) // with an incorrectly inferred zero-size first dimension. if (op == "Shape") { if (shape.dims() > 0 && shape.dim\_size(0) == 0) continue; }
 // Repurpose the existing node to be the constant. // Device placement is preserved. graph\_modified\_ = true; node->set\_op("Const"); EraseRegularNodeAttributes(node); (\*node->mutable\_attr())["dtype"].set\_type(type); constant\_value.AsProtoTensorContent( (\*node->mutable\_attr())["value"].mutable\_tensor());
 // Turn the data input into a control dependency: this is needed to // ensure that the constant value will only be run in the // cases where the shape/rank/size would have been run in // the original graph. string ctrl\_dep = AddControlDependency(node->input(0), graph\_, node\_map\_.get()); node\_map\_->UpdateInput(node->name(), node->input(0), ctrl\_dep); node->set\_input(0, ctrl\_dep); // Done with the Shape/Size/Rank node, move to the next node. continue; }
 if (op == "TensorArraySizeV3") { const NodeDef\* array = CHECK\_NOTNULL(node\_map\_->GetNode(node->input(0))); if (array->input\_size() == 0 || (array->attr().count("dynamic\_size") != 0 && array->attr().at("dynamic\_size").b())) { continue; } const NodeDef\* array\_size = CHECK\_NOTNULL(node\_map\_->GetNode(array->input(0))); if (IsReallyConstant(\*array\_size)) { // Don't materialize 0 sizes to avoid triggering incorrect static // checks. A 0 sized array that can't grow isn't useful anyway. if (array\_size->attr().count("value") == 0) { continue; } const TensorProto& raw\_val = array\_size->attr().at("value").tensor(); if (raw\_val.dtype() != DT\_INT32) { continue; } Tensor value(raw\_val.dtype(), raw\_val.tensor\_shape()); if (!value.FromProto(raw\_val)) { continue; } if (value.flat<int32>()(0) == 0) { continue; }
 graph\_modified\_ = true; node->set\_op("Const"); \*node->mutable\_attr() = array\_size->attr(); node->set\_input(0, AsControlDependency(NodeName(node->input(0)))); node->set\_input(1, AddControlDependency(NodeName(node->input(1)), graph\_, node\_map\_.get())); } continue; }
 // Handle ShapeN materialization case. // It's possible that not all input tensors have known shapes. CHECK\_EQ(op, "ShapeN"); CHECK\_EQ(input.size(), output.size()); const NodeDef\* const shape\_n\_node = node; for (int port\_idx = 0, idx\_limit = output.size(); port\_idx < idx\_limit; ++port\_idx) { const DataType type = output[port\_idx].dtype(); CHECK(type == DT\_INT32 || type == DT\_INT64); const PartialTensorShape shape(input[port\_idx].shape()); if (!shape.IsFullyDefined()) { continue; } Tensor constant\_value(type); auto status = ConvertShapeToConstant(op, type, shape, &constant\_value); if (!status.ok()) { continue; }
 // We make a copy because we mutate the nodes. auto fanouts = node\_map\_->GetOutputs(shape\_n\_node->name()); // Find all nodes consuming this shape and connect them through the new // constant node instead. for (NodeDef\* output : fanouts) { // Track whether there are any direct edges left between shape\_n\_node // and this output node after the transformation. bool direct\_edges\_exist = false; for (int k = 0; k < output->input\_size(); ++k) { int port; const string node\_name = ParseNodeName(output->input(k), &port); if (node\_name == shape\_n\_node->name() && port == port\_idx) { // Create a const node as ShapeN's output if not already. const string const\_name = OptimizedNodeName( \*shape\_n\_node, strings::StrCat("-matshapes-", port\_idx)); if (node\_map\_->GetNode(const\_name) == nullptr) { NodeDef\* added\_node = graph\_->add\_node(); added\_node->set\_name(const\_name); added\_node->set\_op("Const"); added\_node->set\_device(shape\_n\_node->device()); node\_map\_->AddNode(added\_node->name(), added\_node); (\*added\_node->mutable\_attr())["dtype"].set\_type(type); constant\_value.AsProtoTensorContent( (\*added\_node->mutable\_attr())["value"].mutable\_tensor()); // We add a control dependency to the original ShapeN node, // so that the node will only be run if all inputs of the // original ShapeN node are run. string ctrl\_dep = AddControlDependency(shape\_n\_node->name(), graph\_, node\_map\_.get()); \*added\_node->add\_input() = ctrl\_dep; node\_map\_->AddOutput(NodeName(ctrl\_dep), added\_node->name()); } \*output->mutable\_input(k) = const\_name; node\_map\_->AddOutput(const\_name, output->name()); graph\_modified\_ = true; } if (node\_name == shape\_n\_node->name() && port != port\_idx) { direct\_edges\_exist = true; } } if (!direct\_edges\_exist) { node\_map\_->RemoveOutput(node->name(), output->name()); } } } }
 return Status::OK();}
namespace {bool ExtractShape(const NodeDef& shape\_node, const GraphProperties& properties, BCast::Vec\* shape, int64\_t\* min\_id) { if (shape\_node.op() == "Shape") { const std::vector<OpInfo::TensorProperties>& prop1 = properties.GetInputProperties(shape\_node.name()); if (prop1.size() != 1) { return false; } const TensorShapeProto& shp = prop1[0].shape(); if (shp.unknown\_rank()) { return false; } for (const auto& dim : shp.dim()) { shape->push\_back(dim.size()); \*min\_id = std::min<int64\_t>(\*min\_id, dim.size()); } } else { if (shape\_node.attr().count("value") == 0) { return false; } const TensorProto& raw\_val = shape\_node.attr().at("value").tensor(); if (raw\_val.dtype() != DT\_INT64 && raw\_val.dtype() != DT\_INT32) { return false; } Tensor value(raw\_val.dtype(), raw\_val.tensor\_shape()); if (!value.FromProto(raw\_val)) { return false; } for (int j = 0; j < value.NumElements(); ++j) { if (raw\_val.dtype() == DT\_INT64) { shape->push\_back(value.vec<int64\_t>()(j)); } else { shape->push\_back(value.vec<int>()(j)); } } } return true;}} // namespace
Status ConstantFolding::MaterializeBroadcastGradientArgs( const NodeDef& node, const GraphProperties& properties) { const NodeDef\* shape\_node1 = node\_map\_->GetNode(node.input(0)); const NodeDef\* shape\_node2 = node\_map\_->GetNode(node.input(1)); if (shape\_node1 == nullptr || (shape\_node1->op() != "Shape" && !IsReallyConstant(\*shape\_node1)) || shape\_node2 == nullptr || (shape\_node2->op() != "Shape" && !IsReallyConstant(\*shape\_node2))) { return Status::OK(); }
 // Don't optimize this again if it was already optimized and folded. if (OptimizedNodeExists(node, "-folded-1") || OptimizedNodeExists(node, "-folded-2")) { return Status::OK(); } int64\_t min\_id = 0; BCast::Vec shape1; if (!ExtractShape(\*shape\_node1, properties, &shape1, &min\_id)) { return Status::OK(); } BCast::Vec shape2; if (!ExtractShape(\*shape\_node2, properties, &shape2, &min\_id)) { return Status::OK(); } // A value of -1 means we don't known anything about the dimension. Replace // the -1 values with unique dimension ids since we don't want two '-1' // dimensions to be considered equal. for (auto& id : shape1) { if (id == -1) { id = --min\_id; } } for (auto& id : shape2) { if (id == -1) { id = --min\_id; } }
 // Beware: the reduction dimensions computed by the BCast class are valid iff // we assume that two distinct symbolic dimensions can't be equal and a // symbolic dimension can't be equal to 1. This is often but not always true, // so to make this optimization safe we filter out these cases. const int common\_dims = std::min(shape1.size(), shape2.size()); for (int i = 0; i < common\_dims; ++i) { if (shape1[i] >= 0 && shape2[i] >= 0) { continue; } if (shape1[i] != shape2[i]) { // We're either dealing with 2 different symbolic dimensions or a symbolic // and a know dimensions. We can't be sure whether both are equal or not, // so we can't be sure whether we'll be broadcasting or not. return Status::OK(); } } // These extra dims could be equal to 1, in which case there is no // broadcasting. It could also be greater than 1, in which case there would // be broadcasting. Since we don't know, we'll just punt. for (int i = common\_dims, end = shape1.size(); i < end; ++i) { if (shape1[i] < 0) { return Status::OK(); } } for (int i = common\_dims, end = shape2.size(); i < end; ++i) { if (shape2[i] < 0) { return Status::OK(); } }
 BCast bcast(shape1, shape2); if (!bcast.IsValid()) { return Status::OK(); }
 BCast::Vec reduce\_dims[2]; reduce\_dims[0] = bcast.grad\_x\_reduce\_idx(); reduce\_dims[1] = bcast.grad\_y\_reduce\_idx();
 TF\_RETURN\_IF\_ERROR(CheckAttrExists(node, "T")); const DataType type = node.attr().at("T").type(); NodeDef\* out[2]; for (int j = 0; j < 2; ++j) { int reduction\_indices = reduce\_dims[j].size(); Tensor value(type, TensorShape({reduction\_indices})); for (int i = 0; i < reduction\_indices; ++i) { if (type == DT\_INT32) { value.vec<int32>()(i) = reduce\_dims[j][i]; } else { value.vec<int64\_t>()(i) = reduce\_dims[j][i]; } } string const\_name = OptimizedNodeName(node, strings::StrCat("-bcastargs-", j)); out[j] = node\_map\_->GetNode(const\_name); if (out[j] == nullptr) { out[j] = graph\_->add\_node(); TF\_RETURN\_IF\_ERROR( CreateNodeDef(const\_name, TensorValue(&value), out[j])); out[j]->set\_device(node.device()); node\_map\_->AddNode(const\_name, out[j]); string ctrl\_dep = AddControlDependency(node.name(), graph\_, node\_map\_.get()); \*out[j]->add\_input() = ctrl\_dep; node\_map\_->AddOutput(NodeName(ctrl\_dep), const\_name); } }
 // We make a copy here since we might mutate the set. const auto outputs = node\_map\_->GetOutputs(node.name()); for (NodeDef\* output : outputs) { for (int k = 0; k < output->input\_size(); ++k) { int port; string node\_name = ParseNodeName(output->input(k), &port); if (node\_name == node.name() && port >= 0 && port < 2 && out[port]) { \*output->mutable\_input(k) = out[port]->name(); node\_map\_->UpdateInput(output->name(), node\_name, out[port]->name()); } } }
 return Status::OK();}
Status ConstantFolding::MaterializeReductionIndices( NodeDef\* node, const GraphProperties& properties) { if (node->input\_size() < 2) { return Status::OK(); } const NodeDef\* indices = node\_map\_->GetNode(node->input(1)); if (!indices || IsReallyConstant(\*indices)) { // The reduction indices are already constant, there's nothing to do. return Status::OK(); }
 const std::vector<OpInfo::TensorProperties>& input\_props = properties.GetInputProperties(node->name()); if (input\_props.size() != 2) { return Status::OK(); } const OpInfo::TensorProperties& input\_prop = input\_props[0]; if (input\_prop.shape().unknown\_rank()) { // We can't do anything if we don't know the rank of the input. return Status::OK(); } const int input\_rank = input\_prop.shape().dim\_size(); if (input\_rank < 1) { // Unexpected graph, don't try to change it. return Status::OK(); } const OpInfo::TensorProperties& reduction\_indices\_prop = input\_props[1]; DataType dtype = reduction\_indices\_prop.dtype(); if (dtype != DT\_INT32 && dtype != DT\_INT64) { return Status::OK(); } PartialTensorShape reduction\_indices\_shape(reduction\_indices\_prop.shape()); const int num\_reduction\_indices = reduction\_indices\_shape.num\_elements();
 const std::vector<OpInfo::TensorProperties>& output\_props = properties.GetOutputProperties(node->name()); if (output\_props.size() != 1) { return Status::OK(); } const OpInfo::TensorProperties& output\_prop = output\_props[0]; const int output\_rank = output\_prop.shape().unknown\_rank() ? -1 : output\_prop.shape().dim\_size();
 bool full\_reduction = output\_rank == 0 || num\_reduction\_indices == input\_rank; if (!full\_reduction) { // A full reduction will generate a tensor of one of the shapes // [], [1], [1, 1], [1, 1, ...]. Even if we do not know the number of // elements in the output of the reduction, we may deduce it from reshape // nodes following it. for (const NodeDef\* fanout : node\_map\_->GetOutputs(node->name())) { full\_reduction = false; if (!IsReshape(\*fanout)) { return Status::OK(); } const std::vector<OpInfo::TensorProperties>& reshape\_props = properties.GetOutputProperties(fanout->name()); if (reshape\_props.size() != 1) { return Status::OK(); } const OpInfo::TensorProperties& reshape\_prop = reshape\_props[0]; PartialTensorShape shape(reshape\_prop.shape()); if (shape.num\_elements() != 1) { return Status::OK(); } else { full\_reduction = true; } } if (!full\_reduction) { return Status::OK(); } }
 // We know it's a full reduction. We can generate the full set of indices to // reduce as a constant node. string const\_name = OptimizedNodeName(\*node, "-reduction\_indices"); if (node\_map\_->GetNode(const\_name)) { return Status::OK(); } NodeDef\* reduction\_indices = graph\_->add\_node(); Tensor value(dtype, TensorShape({input\_rank})); for (int i = 0; i < input\_rank; ++i) { if (dtype == DT\_INT32) { value.vec<int32>()(i) = i; } else { value.vec<int64\_t>()(i) = i; } } TF\_RETURN\_IF\_ERROR( CreateNodeDef(const\_name, TensorValue(&value), reduction\_indices));
 reduction\_indices->set\_device(node->device()); string ctrl\_dep = AddControlDependency(node->input(1), graph\_, node\_map\_.get()); \*reduction\_indices->add\_input() = ctrl\_dep; node\_map\_->AddNode(const\_name, reduction\_indices); node\_map\_->AddOutput(NodeName(ctrl\_dep), const\_name);
 node->set\_input(1, reduction\_indices->name()); node\_map\_->UpdateInput(node->name(), indices->name(), reduction\_indices->name());
 return Status::OK();}
Status ConstantFolding::MaterializeConstantValuedNode( NodeDef\* node, const GraphProperties& properties) { if (disable\_compressed\_tensor\_optimization\_) { return Status::OK(); } // Nodes that generate constant-valued outputs can be represented compactly in // compressed format, regardless of their shape. const std::vector<OpInfo::TensorProperties>& output\_props = properties.GetOutputProperties(node->name()); if (output\_props.size() != 1) return Status::OK(); const auto& output\_shape = output\_props[0].shape(); if (!PartialTensorShape(output\_shape).IsFullyDefined()) { return Status::OK(); } if (IsFill(\*node)) { const auto output\_dtype = output\_props[0].dtype(); NodeDef\* input\_node = nullptr; for (int i = 0; i < 2; ++i) { input\_node = node\_map\_->GetNode(NodeName(node->input(i))); if (input\_node == nullptr || !IsReallyConstant(\*input\_node)) { return Status::OK(); } } TF\_RETURN\_IF\_ERROR(CheckAttrExists(\*input\_node, "value"));
 // Copy the input tensor to the fill node, set the output shape and data // type, and change the node type to Const. TensorProto\* tensor = (\*node->mutable\_attr())["value"].mutable\_tensor(); const TensorProto& input\_tensor = input\_node->attr().at("value").tensor(); if (!input\_tensor.tensor\_content().empty()) { // Convert the value to repeated field format, so we can use the // decompression mechanism to store only a single value in the constant // node, even if the shape specified in the original Fill is large. Tensor t; if (!t.FromProto(input\_tensor)) { return errors::InvalidArgument( "Could not construct Tensor form TensorProto in node: ", input\_node->name()); } tensor->clear\_tensor\_content(); t.AsProtoField(tensor); } else { \*tensor = input\_tensor; } \*(tensor->mutable\_tensor\_shape()) = output\_shape; (\*node->mutable\_attr())["dtype"].set\_type(output\_dtype); node->mutable\_attr()->erase("T"); node->mutable\_attr()->erase("index\_type"); node->set\_op("Const"); for (int i = 0; i < 2; i++) { // Change inputs to a control inputs. const string ctrl\_dep = AsControlDependency(node->input(i)); node\_map\_->UpdateInput(node->name(), node->input(i), ctrl\_dep); node->set\_input(i, ctrl\_dep); } graph\_modified\_ = true; } else { double value = (IsZerosLike(\*node) ? 0.0 : (IsOnesLike(\*node) ? 1.0 : -1.0)); if (value >= 0) { TF\_RETURN\_IF\_ERROR(ReplaceOperationWithConstant( value, properties, output\_shape, node, graph\_)); } } return Status::OK();}
// Materialize output values inferred by the shape inference.Status ConstantFolding::MaterializeOutputValues( NodeDef\* node, const GraphProperties& properties) { const std::vector<OpInfo::TensorProperties>& output = properties.GetOutputProperties(node->name()); if (output.size() != 1 || !output[0].has\_value() || !IsFoldable(\*node, &properties)) { return Status::OK(); }
 // If this is a trivial Identity node with a constant input, just route the // input around it. if (IsIdentity(\*node)) { NodeDef\* input = node\_map\_->GetNode(node->input(0)); if (IsReallyConstant(\*input)) { std::vector<int> inputs\_to\_forward; std::iota(inputs\_to\_forward.begin(), inputs\_to\_forward.end(), 0); graph\_modified\_ = ForwardInputs(node, inputs\_to\_forward); return Status::OK(); } } // Repurpose the existing node to be the constant. // Device placement is preserved. TensorProto value\_copy = output[0].value(); return ReplaceOperationWithConstantTensor(output[0].dtype(), &value\_copy, node, graph\_);}
Status ConstantFolding::MaterializeConstants( const GraphProperties& properties) { const int node\_count = graph\_->node\_size(); for (int i = 0; i < node\_count; ++i) { NodeDef& node = \*graph\_->mutable\_node(i); const string& op = node.op(); if (op == "BroadcastGradientArgs") { TF\_RETURN\_IF\_ERROR(MaterializeBroadcastGradientArgs(node, properties)); } else if (IsReduction(node)) { TF\_RETURN\_IF\_ERROR(MaterializeReductionIndices(&node, properties)); } else if (IsFill(node) || IsZerosLike(node) || IsOnesLike(node)) { TF\_RETURN\_IF\_ERROR(MaterializeConstantValuedNode(&node, properties)); } else { TF\_RETURN\_IF\_ERROR(MaterializeOutputValues(&node, properties)); } } return Status::OK();}
bool ConstantFolding::IsFoldable(const NodeDef& node, const GraphProperties\* properties) { string key = strings::StrCat(node.name(), "/", node.op()); auto it = maybe\_foldable\_nodes\_.find(key); if (it == maybe\_foldable\_nodes\_.end()) { it = maybe\_foldable\_nodes\_ .emplace(std::move(key), MaybeFoldable(node, properties)) .first; } if (!it->second) { return false; } else { return IsFoldableUncached(node, properties); }}
bool ConstantFolding::IsFoldableUncached( const NodeDef& node, const GraphProperties\* properties) const { // Folding not applicable to ops with no inputs. if (node.input().empty()) { return false; } // We can only fold nodes if all their inputs are known statically, except in // the case of a merge node that propagate the first inputs that becomes // available, and therefore only requires a single constant input to be // foldable. bool merge\_has\_constant\_input = false; const bool is\_merge = IsMerge(node); for (const auto& input : node.input()) { if (IsControlInput(input)) { continue; } const NodeDef\* input\_node = node\_map\_->GetNode(input); if (!input\_node) { return false; } bool is\_const = IsReallyConstant(\*input\_node); if (is\_const) { // Don't fold strings constants for now since this causes problems with // checkpointing. if (input\_node->attr().count("dtype") == 0 || input\_node->attr().at("dtype").type() == DT\_STRING) { return false; } // Special case: If a Merge node has at least one constant input that // does not depend on a control input, we can fold it. merge\_has\_constant\_input |= !HasControlInputs(\*input\_node); } else if (!is\_merge) { return false; } } if (is\_merge && !merge\_has\_constant\_input) return false; if (disable\_compressed\_tensor\_optimization\_ && (IsFill(node) || IsZerosLike(node) || IsOnesLike(node)))[View remainder of file in raw view](https://github.com/tensorflow/tensorflow/raw/a1320ec1eac186da1d03f033109191f715b2b130/tensorflow/core/grappler/optimizers/constant_folding.cc)

## Footer

© 2025 GitHub, Inc.

### Footer navigation

* [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)
* [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)
* [Security](https://github.com/security)
* [Status](https://www.githubstatus.com/)
* [Docs](https://docs.github.com/)
* [Contact](https://support.github.com?tags=dotcom-footer)
* Manage cookies
* Do not share my personal information

You can’t perform that action at this time.



=== Content from github.com_d351b168_20250114_210437.html ===

[Skip to content](#start-of-content)

## Navigation Menu

Toggle navigation

[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fcommit%2F045deec1cbdebb27d817008ad5df94d96a08b1bf)

* Product

  + [GitHub Copilot
    Write better code with AI](https://github.com/features/copilot)
  + [Security
    Find and fix vulnerabilities](https://github.com/features/security)
  + [Actions
    Automate any workflow](https://github.com/features/actions)
  + [Codespaces
    Instant dev environments](https://github.com/features/codespaces)
  + [Issues
    Plan and track work](https://github.com/features/issues)
  + [Code Review
    Manage code changes](https://github.com/features/code-review)
  + [Discussions
    Collaborate outside of code](https://github.com/features/discussions)
  + [Code Search
    Find more, search less](https://github.com/features/code-search)

  Explore
  + [All features](https://github.com/features)
  + [Documentation](https://docs.github.com)
  + [GitHub Skills](https://skills.github.com)
  + [Blog](https://github.blog)
* Solutions

  By company size
  + [Enterprises](https://github.com/enterprise)
  + [Small and medium teams](https://github.com/team)
  + [Startups](https://github.com/enterprise/startups)
  By use case
  + [DevSecOps](/solutions/use-case/devsecops)
  + [DevOps](/solutions/use-case/devops)
  + [CI/CD](/solutions/use-case/ci-cd)
  + [View all use cases](/solutions/use-case)

  By industry
  + [Healthcare](/solutions/industry/healthcare)
  + [Financial services](/solutions/industry/financial-services)
  + [Manufacturing](/solutions/industry/manufacturing)
  + [Government](/solutions/industry/government)
  + [View all industries](/solutions/industry)

  [View all solutions](/solutions)
* Resources

  Topics
  + [AI](/resources/articles/ai)
  + [DevOps](/resources/articles/devops)
  + [Security](/resources/articles/security)
  + [Software Development](/resources/articles/software-development)
  + [View all](/resources/articles)

  Explore
  + [Learning Pathways](https://resources.github.com/learn/pathways)
  + [White papers, Ebooks, Webinars](https://resources.github.com)
  + [Customer Stories](https://github.com/customer-stories)
  + [Partners](https://partner.github.com)
  + [Executive Insights](https://github.com/solutions/executive-insights)
* Open Source

  + [GitHub Sponsors
    Fund open source developers](/sponsors)
  + [The ReadME Project
    GitHub community articles](https://github.com/readme)
  Repositories
  + [Topics](https://github.com/topics)
  + [Trending](https://github.com/trending)
  + [Collections](https://github.com/collections)
* Enterprise

  + [Enterprise platform
    AI-powered developer platform](/enterprise)
  Available add-ons
  + [Advanced Security
    Enterprise-grade security features](https://github.com/enterprise/advanced-security)
  + [GitHub Copilot
    Enterprise-grade AI features](/features/copilot#enterprise)
  + [Premium Support
    Enterprise-grade 24/7 support](/premium-support)
* [Pricing](https://github.com/pricing)

Search or jump to...

# Search code, repositories, users, issues, pull requests...

Search

Clear

[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)

# Provide feedback

We read every piece of feedback, and take your input very seriously.

Include my email address so I can be contacted

  Cancel

 Submit feedback

# Saved searches

## Use saved searches to filter your results more quickly

Name

Query

To see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).

  Cancel

 Create saved search

[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fcommit%2F045deec1cbdebb27d817008ad5df94d96a08b1bf)

[Sign up](/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E%2Fvoltron%2Fcommit_fragments%2Frepo_layout&source=header-repo&source_repo=tensorflow%2Ftensorflow)
Reseting focus

You signed in with another tab or window. Reload to refresh your session.
You signed out in another tab or window. Reload to refresh your session.
You switched accounts on another tab or window. Reload to refresh your session.

Dismiss alert

{{ message }}

[tensorflow](/tensorflow)
/
**[tensorflow](/tensorflow/tensorflow)**
Public

* [Notifications](/login?return_to=%2Ftensorflow%2Ftensorflow) You must be signed in to change notification settings
* [Fork
  74.4k](/login?return_to=%2Ftensorflow%2Ftensorflow)
* [Star
   187k](/login?return_to=%2Ftensorflow%2Ftensorflow)

* [Code](/tensorflow/tensorflow)
* [Issues
  827](/tensorflow/tensorflow/issues)
* [Pull requests
  5k+](/tensorflow/tensorflow/pulls)
* [Actions](/tensorflow/tensorflow/actions)
* [Projects
  2](/tensorflow/tensorflow/projects)
* [Security](/tensorflow/tensorflow/security)
* [Insights](/tensorflow/tensorflow/pulse)

Additional navigation options

* [Code](/tensorflow/tensorflow)
* [Issues](/tensorflow/tensorflow/issues)
* [Pull requests](/tensorflow/tensorflow/pulls)
* [Actions](/tensorflow/tensorflow/actions)
* [Projects](/tensorflow/tensorflow/projects)
* [Security](/tensorflow/tensorflow/security)
* [Insights](/tensorflow/tensorflow/pulse)

## Commit

[Permalink](/tensorflow/tensorflow/commit/045deec1cbdebb27d817008ad5df94d96a08b1bf)

This commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.

Prevent null pointer dereference in `mutable_graph_view`

[Browse files](/tensorflow/tensorflow/tree/045deec1cbdebb27d817008ad5df94d96a08b1bf)
Browse the repository at this point in the history

```
PiperOrigin-RevId: 409684472
Change-Id: I577eb9d9ac470fcec0501423171e739a4ec0cb5c
```

* Loading branch information

[![@mihaimaruseac](https://avatars.githubusercontent.com/u/323199?s=40&v=4)](/mihaimaruseac) [![@tensorflower-gardener](https://avatars.githubusercontent.com/u/17151892?s=40&v=4)](/tensorflower-gardener)

[mihaimaruseac](/tensorflow/tensorflow/commits?author=mihaimaruseac "View all commits by mihaimaruseac")
authored and
[tensorflower-gardener](/tensorflow/tensorflow/commits?author=tensorflower-gardener "View all commits by tensorflower-gardener")
committed
Nov 13, 2021

1 parent
[0a365c0](/tensorflow/tensorflow/commit/0a365c029e437be0349c31f8d4c9926b69fa3fa1)

commit 045deec

Showing
**1 changed file**
with
**3 additions**
and
**0 deletions**.

* Whitespace
* Ignore whitespace

* Split
* Unified

## There are no files selected for viewing

3 changes: 3 additions & 0 deletions

3
[tensorflow/core/grappler/mutable\_graph\_view.cc](#diff-5ffe9391827bf1274f27baed3d62ac9b8418d435f64dff2437cd74c5c3e11a29 "tensorflow/core/grappler/mutable_graph_view.cc")

Show comments

[View file](/tensorflow/tensorflow/blob/045deec1cbdebb27d817008ad5df94d96a08b1bf/tensorflow/core/grappler/mutable_graph_view.cc)
Edit file

Delete file

This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters.
[Learn more about bidirectional Unicode characters](https://github.co/hiddenchars)

  [Show hidden characters](%7B%7B%20revealButtonHref%20%7D%7D)

| Original file line number | Diff line number | Diff line change |
| --- | --- | --- |
| Expand Up | | @@ -68,6 +68,9 @@ bool IsIdentityConsumingSwitch(const MutableGraphView& graph, |
|  |  | } |
|  |  |  |
|  |  | NodeDef\* input\_node = graph.GetNode(tensor\_id.node()); |
|  |  | if (input\_node == nullptr) { |
|  |  | return false; |
|  |  | } |
|  |  | return IsSwitch(\*input\_node); |
|  |  | } |
|  |  | return false; |
| Expand Down | |  |

Toggle all file notes
Toggle all file annotations

### 0 comments on commit `045deec`

Please
[sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fcommit%2F045deec1cbdebb27d817008ad5df94d96a08b1bf) to comment.

## Footer

© 2025 GitHub, Inc.

### Footer navigation

* [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)
* [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)
* [Security](https://github.com/security)
* [Status](https://www.githubstatus.com/)
* [Docs](https://docs.github.com/)
* [Contact](https://support.github.com?tags=dotcom-footer)
* Manage cookies
* Do not share my personal information

You can’t perform that action at this time.



=== Content from github.com_63b18099_20250114_210441.html ===

[Skip to content](#start-of-content)

## Navigation Menu

Toggle navigation

[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fsecurity%2Fadvisories%2FGHSA-9px9-73fg-3fqp)

* Product

  + [GitHub Copilot
    Write better code with AI](https://github.com/features/copilot)
  + [Security
    Find and fix vulnerabilities](https://github.com/features/security)
  + [Actions
    Automate any workflow](https://github.com/features/actions)
  + [Codespaces
    Instant dev environments](https://github.com/features/codespaces)
  + [Issues
    Plan and track work](https://github.com/features/issues)
  + [Code Review
    Manage code changes](https://github.com/features/code-review)
  + [Discussions
    Collaborate outside of code](https://github.com/features/discussions)
  + [Code Search
    Find more, search less](https://github.com/features/code-search)

  Explore
  + [All features](https://github.com/features)
  + [Documentation](https://docs.github.com)
  + [GitHub Skills](https://skills.github.com)
  + [Blog](https://github.blog)
* Solutions

  By company size
  + [Enterprises](https://github.com/enterprise)
  + [Small and medium teams](https://github.com/team)
  + [Startups](https://github.com/enterprise/startups)
  By use case
  + [DevSecOps](/solutions/use-case/devsecops)
  + [DevOps](/solutions/use-case/devops)
  + [CI/CD](/solutions/use-case/ci-cd)
  + [View all use cases](/solutions/use-case)

  By industry
  + [Healthcare](/solutions/industry/healthcare)
  + [Financial services](/solutions/industry/financial-services)
  + [Manufacturing](/solutions/industry/manufacturing)
  + [Government](/solutions/industry/government)
  + [View all industries](/solutions/industry)

  [View all solutions](/solutions)
* Resources

  Topics
  + [AI](/resources/articles/ai)
  + [DevOps](/resources/articles/devops)
  + [Security](/resources/articles/security)
  + [Software Development](/resources/articles/software-development)
  + [View all](/resources/articles)

  Explore
  + [Learning Pathways](https://resources.github.com/learn/pathways)
  + [White papers, Ebooks, Webinars](https://resources.github.com)
  + [Customer Stories](https://github.com/customer-stories)
  + [Partners](https://partner.github.com)
  + [Executive Insights](https://github.com/solutions/executive-insights)
* Open Source

  + [GitHub Sponsors
    Fund open source developers](/sponsors)
  + [The ReadME Project
    GitHub community articles](https://github.com/readme)
  Repositories
  + [Topics](https://github.com/topics)
  + [Trending](https://github.com/trending)
  + [Collections](https://github.com/collections)
* Enterprise

  + [Enterprise platform
    AI-powered developer platform](/enterprise)
  Available add-ons
  + [Advanced Security
    Enterprise-grade security features](https://github.com/enterprise/advanced-security)
  + [GitHub Copilot
    Enterprise-grade AI features](/features/copilot#enterprise)
  + [Premium Support
    Enterprise-grade 24/7 support](/premium-support)
* [Pricing](https://github.com/pricing)

Search or jump to...

# Search code, repositories, users, issues, pull requests...

Search

Clear

[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)

# Provide feedback

We read every piece of feedback, and take your input very seriously.

Include my email address so I can be contacted

  Cancel

 Submit feedback

# Saved searches

## Use saved searches to filter your results more quickly

Name

Query

To see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).

  Cancel

 Create saved search

[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fsecurity%2Fadvisories%2FGHSA-9px9-73fg-3fqp)

[Sign up](/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E%2Frepos%2Fadvisories%2Fshow&source=header-repo&source_repo=tensorflow%2Ftensorflow)
Reseting focus

You signed in with another tab or window. Reload to refresh your session.
You signed out in another tab or window. Reload to refresh your session.
You switched accounts on another tab or window. Reload to refresh your session.

Dismiss alert

{{ message }}

[tensorflow](/tensorflow)
/
**[tensorflow](/tensorflow/tensorflow)**
Public

* [Notifications](/login?return_to=%2Ftensorflow%2Ftensorflow) You must be signed in to change notification settings
* [Fork
  74.4k](/login?return_to=%2Ftensorflow%2Ftensorflow)
* [Star
   187k](/login?return_to=%2Ftensorflow%2Ftensorflow)

* [Code](/tensorflow/tensorflow)
* [Issues
  827](/tensorflow/tensorflow/issues)
* [Pull requests
  5k+](/tensorflow/tensorflow/pulls)
* [Actions](/tensorflow/tensorflow/actions)
* [Projects
  2](/tensorflow/tensorflow/projects)
* [Security](/tensorflow/tensorflow/security)
* [Insights](/tensorflow/tensorflow/pulse)

Additional navigation options

* [Code](/tensorflow/tensorflow)
* [Issues](/tensorflow/tensorflow/issues)
* [Pull requests](/tensorflow/tensorflow/pulls)
* [Actions](/tensorflow/tensorflow/actions)
* [Projects](/tensorflow/tensorflow/projects)
* [Security](/tensorflow/tensorflow/security)
* [Insights](/tensorflow/tensorflow/pulse)

# Null pointer dereference in Grappler's `IsConstant`

Moderate

[mihaimaruseac](/mihaimaruseac)
published
GHSA-9px9-73fg-3fqp
Feb 2, 2022

## Package

pip

tensorflow, tensorflow-cpu, tensorflow-gpu
([pip](/advisories?query=ecosystem%3Apip))

## Affected versions

< 2.8.0

## Patched versions

2.5.3, 2.6.3, 2.7.1

## Description

### Impact

Under certain scenarios, Grappler component of TensorFlow can trigger a null pointer dereference. There are 2 places where this can occur, for the same malicious alteration of a `SavedModel` file (fixing the first one would trigger the same dereference in the second place):

First, during [constant folding](https://github.com/tensorflow/tensorflow/blob/a1320ec1eac186da1d03f033109191f715b2b130/tensorflow/core/grappler/optimizers/constant_folding.cc#L3466-L3497), the `GraphDef` might not have the required nodes for the binary operation:

```
  NodeDef* mul_left_child = node_map_->GetNode(node->input(0));
  NodeDef* mul_right_child = node_map_->GetNode(node->input(1));
  // One child must be constant, and the second must be Conv op.
  const bool left_child_is_constant = IsReallyConstant(*mul_left_child);
  const bool right_child_is_constant = IsReallyConstant(*mul_right_child);
```

If a node is missing, the correposning `mul_*child` would be null, and the dereference in the subsequent line would be incorrect.

We have a similar issue during [`IsIdentityConsumingSwitch`](https://github.com/tensorflow/tensorflow/blob/a1320ec1eac186da1d03f033109191f715b2b130/tensorflow/core/grappler/mutable_graph_view.cc#L59-L74):

```
  NodeDef* input_node = graph.GetNode(tensor_id.node());
  return IsSwitch(*input_node);
```
### Patches

We have patched the issue in GitHub commits [0a365c029e437be0349c31f8d4c9926b69fa3fa1](https://github.com/tensorflow/tensorflow/commit/0a365c029e437be0349c31f8d4c9926b69fa3fa1) and [045deec1cbdebb27d817008ad5df94d96a08b1bf](https://github.com/tensorflow/tensorflow/commit/045deec1cbdebb27d817008ad5df94d96a08b1bf).

The fix will be included in TensorFlow 2.8.0. We will also cherrypick this commit on TensorFlow 2.7.1, TensorFlow 2.6.3, and TensorFlow 2.5.3, as these are also affected and still in supported range.

### For more information

Please consult [our security guide](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.

### Severity

Moderate

### CVE ID

CVE-2022-23589

### Weaknesses

No CWEs

## Footer

© 2025 GitHub, Inc.

### Footer navigation

* [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)
* [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)
* [Security](https://github.com/security)
* [Status](https://www.githubstatus.com/)
* [Docs](https://docs.github.com/)
* [Contact](https://support.github.com?tags=dotcom-footer)
* Manage cookies
* Do not share my personal information

You can’t perform that action at this time.



=== Content from github.com_b1d5c744_20250114_210439.html ===

[Skip to content](#start-of-content)

## Navigation Menu

Toggle navigation

[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fcommit%2F0a365c029e437be0349c31f8d4c9926b69fa3fa1)

* Product

  + [GitHub Copilot
    Write better code with AI](https://github.com/features/copilot)
  + [Security
    Find and fix vulnerabilities](https://github.com/features/security)
  + [Actions
    Automate any workflow](https://github.com/features/actions)
  + [Codespaces
    Instant dev environments](https://github.com/features/codespaces)
  + [Issues
    Plan and track work](https://github.com/features/issues)
  + [Code Review
    Manage code changes](https://github.com/features/code-review)
  + [Discussions
    Collaborate outside of code](https://github.com/features/discussions)
  + [Code Search
    Find more, search less](https://github.com/features/code-search)

  Explore
  + [All features](https://github.com/features)
  + [Documentation](https://docs.github.com)
  + [GitHub Skills](https://skills.github.com)
  + [Blog](https://github.blog)
* Solutions

  By company size
  + [Enterprises](https://github.com/enterprise)
  + [Small and medium teams](https://github.com/team)
  + [Startups](https://github.com/enterprise/startups)
  By use case
  + [DevSecOps](/solutions/use-case/devsecops)
  + [DevOps](/solutions/use-case/devops)
  + [CI/CD](/solutions/use-case/ci-cd)
  + [View all use cases](/solutions/use-case)

  By industry
  + [Healthcare](/solutions/industry/healthcare)
  + [Financial services](/solutions/industry/financial-services)
  + [Manufacturing](/solutions/industry/manufacturing)
  + [Government](/solutions/industry/government)
  + [View all industries](/solutions/industry)

  [View all solutions](/solutions)
* Resources

  Topics
  + [AI](/resources/articles/ai)
  + [DevOps](/resources/articles/devops)
  + [Security](/resources/articles/security)
  + [Software Development](/resources/articles/software-development)
  + [View all](/resources/articles)

  Explore
  + [Learning Pathways](https://resources.github.com/learn/pathways)
  + [White papers, Ebooks, Webinars](https://resources.github.com)
  + [Customer Stories](https://github.com/customer-stories)
  + [Partners](https://partner.github.com)
  + [Executive Insights](https://github.com/solutions/executive-insights)
* Open Source

  + [GitHub Sponsors
    Fund open source developers](/sponsors)
  + [The ReadME Project
    GitHub community articles](https://github.com/readme)
  Repositories
  + [Topics](https://github.com/topics)
  + [Trending](https://github.com/trending)
  + [Collections](https://github.com/collections)
* Enterprise

  + [Enterprise platform
    AI-powered developer platform](/enterprise)
  Available add-ons
  + [Advanced Security
    Enterprise-grade security features](https://github.com/enterprise/advanced-security)
  + [GitHub Copilot
    Enterprise-grade AI features](/features/copilot#enterprise)
  + [Premium Support
    Enterprise-grade 24/7 support](/premium-support)
* [Pricing](https://github.com/pricing)

Search or jump to...

# Search code, repositories, users, issues, pull requests...

Search

Clear

[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)

# Provide feedback

We read every piece of feedback, and take your input very seriously.

Include my email address so I can be contacted

  Cancel

 Submit feedback

# Saved searches

## Use saved searches to filter your results more quickly

Name

Query

To see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).

  Cancel

 Create saved search

[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fcommit%2F0a365c029e437be0349c31f8d4c9926b69fa3fa1)

[Sign up](/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E%2Fvoltron%2Fcommit_fragments%2Frepo_layout&source=header-repo&source_repo=tensorflow%2Ftensorflow)
Reseting focus

You signed in with another tab or window. Reload to refresh your session.
You signed out in another tab or window. Reload to refresh your session.
You switched accounts on another tab or window. Reload to refresh your session.

Dismiss alert

{{ message }}

[tensorflow](/tensorflow)
/
**[tensorflow](/tensorflow/tensorflow)**
Public

* [Notifications](/login?return_to=%2Ftensorflow%2Ftensorflow) You must be signed in to change notification settings
* [Fork
  74.4k](/login?return_to=%2Ftensorflow%2Ftensorflow)
* [Star
   187k](/login?return_to=%2Ftensorflow%2Ftensorflow)

* [Code](/tensorflow/tensorflow)
* [Issues
  827](/tensorflow/tensorflow/issues)
* [Pull requests
  5k+](/tensorflow/tensorflow/pulls)
* [Actions](/tensorflow/tensorflow/actions)
* [Projects
  2](/tensorflow/tensorflow/projects)
* [Security](/tensorflow/tensorflow/security)
* [Insights](/tensorflow/tensorflow/pulse)

Additional navigation options

* [Code](/tensorflow/tensorflow)
* [Issues](/tensorflow/tensorflow/issues)
* [Pull requests](/tensorflow/tensorflow/pulls)
* [Actions](/tensorflow/tensorflow/actions)
* [Projects](/tensorflow/tensorflow/projects)
* [Security](/tensorflow/tensorflow/security)
* [Insights](/tensorflow/tensorflow/pulse)

## Commit

[Permalink](/tensorflow/tensorflow/commit/0a365c029e437be0349c31f8d4c9926b69fa3fa1)

This commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.

Prevent null pointer dereference in constant folding.

[Browse files](/tensorflow/tensorflow/tree/0a365c029e437be0349c31f8d4c9926b69fa3fa1)
Browse the repository at this point in the history

```
Under certain conditions, an invalid protobuf saved model with invalid nodes would be loaded. During optimization phase, Grappler optimizer will then dereference a null pointer.

PiperOrigin-RevId: 409683530
Change-Id: I1f10340a7ec384bc9bc587300390f1078cf5caa0
```

* Loading branch information

[![@mihaimaruseac](https://avatars.githubusercontent.com/u/323199?s=40&v=4)](/mihaimaruseac) [![@tensorflower-gardener](https://avatars.githubusercontent.com/u/17151892?s=40&v=4)](/tensorflower-gardener)

[mihaimaruseac](/tensorflow/tensorflow/commits?author=mihaimaruseac "View all commits by mihaimaruseac")
authored and
[tensorflower-gardener](/tensorflow/tensorflow/commits?author=tensorflower-gardener "View all commits by tensorflower-gardener")
committed
Nov 13, 2021

1 parent
[7a6a727](/tensorflow/tensorflow/commit/7a6a7276fa49f933156f39fc707b2f67a7da93d0)

commit 0a365c0

Showing
**1 changed file**
with
**3 additions**
and
**0 deletions**.

* Whitespace
* Ignore whitespace

* Split
* Unified

## There are no files selected for viewing

3 changes: 3 additions & 0 deletions

3
[tensorflow/core/grappler/optimizers/constant\_folding.cc](#diff-663636d4972e75e590efa74039aebbdd26f36eeabbf81ab6691915aa5995a349 "tensorflow/core/grappler/optimizers/constant_folding.cc")

Show comments

[View file](/tensorflow/tensorflow/blob/0a365c029e437be0349c31f8d4c9926b69fa3fa1/tensorflow/core/grappler/optimizers/constant_folding.cc)
Edit file

Delete file

This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters.
[Learn more about bidirectional Unicode characters](https://github.co/hiddenchars)

  [Show hidden characters](%7B%7B%20revealButtonHref%20%7D%7D)

| Original file line number | Diff line number | Diff line change |
| --- | --- | --- |
| Expand Up | | @@ -3505,6 +3505,9 @@ bool ConstantFolding::MulConvPushDown(GraphDef\* optimized\_graph, NodeDef\* node, |
|  |  |  |
|  |  | NodeDef\* mul\_left\_child = node\_map\_->GetNode(node->input(0)); |
|  |  | NodeDef\* mul\_right\_child = node\_map\_->GetNode(node->input(1)); |
|  |  | if (mul\_left\_child == nullptr || mul\_right\_child == nullptr) { |
|  |  | return false; |
|  |  | } |
|  |  | // One child must be constant, and the second must be Conv op. |
|  |  | const bool left\_child\_is\_constant = IsReallyConstant(\*mul\_left\_child); |
|  |  | const bool right\_child\_is\_constant = IsReallyConstant(\*mul\_right\_child); |
| Expand Down | |  |

Toggle all file notes
Toggle all file annotations

### 0 comments on commit `0a365c0`

Please
[sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fcommit%2F0a365c029e437be0349c31f8d4c9926b69fa3fa1) to comment.

## Footer

© 2025 GitHub, Inc.

### Footer navigation

* [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)
* [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)
* [Security](https://github.com/security)
* [Status](https://www.githubstatus.com/)
* [Docs](https://docs.github.com/)
* [Contact](https://support.github.com?tags=dotcom-footer)
* Manage cookies
* Do not share my personal information

You can’t perform that action at this time.


