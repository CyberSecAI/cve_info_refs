=== Content from github.com_72daaeec_20250115_084559.html ===

[Skip to content](#start-of-content)

## Navigation Menu

Toggle navigation

[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fblob%2F266558ac4c1f361e9a178ee9d3f0ce2e648ae499%2Ftensorflow%2Flite%2Fkernels%2Finternal%2Freference%2Freference_ops.h)

* Product

  + [GitHub Copilot
    Write better code with AI](https://github.com/features/copilot)
  + [Security
    Find and fix vulnerabilities](https://github.com/features/security)
  + [Actions
    Automate any workflow](https://github.com/features/actions)
  + [Codespaces
    Instant dev environments](https://github.com/features/codespaces)
  + [Issues
    Plan and track work](https://github.com/features/issues)
  + [Code Review
    Manage code changes](https://github.com/features/code-review)
  + [Discussions
    Collaborate outside of code](https://github.com/features/discussions)
  + [Code Search
    Find more, search less](https://github.com/features/code-search)

  Explore
  + [All features](https://github.com/features)
  + [Documentation](https://docs.github.com)
  + [GitHub Skills](https://skills.github.com)
  + [Blog](https://github.blog)
* Solutions

  By company size
  + [Enterprises](https://github.com/enterprise)
  + [Small and medium teams](https://github.com/team)
  + [Startups](https://github.com/enterprise/startups)
  By use case
  + [DevSecOps](/solutions/use-case/devsecops)
  + [DevOps](/solutions/use-case/devops)
  + [CI/CD](/solutions/use-case/ci-cd)
  + [View all use cases](/solutions/use-case)

  By industry
  + [Healthcare](/solutions/industry/healthcare)
  + [Financial services](/solutions/industry/financial-services)
  + [Manufacturing](/solutions/industry/manufacturing)
  + [Government](/solutions/industry/government)
  + [View all industries](/solutions/industry)

  [View all solutions](/solutions)
* Resources

  Topics
  + [AI](/resources/articles/ai)
  + [DevOps](/resources/articles/devops)
  + [Security](/resources/articles/security)
  + [Software Development](/resources/articles/software-development)
  + [View all](/resources/articles)

  Explore
  + [Learning Pathways](https://resources.github.com/learn/pathways)
  + [White papers, Ebooks, Webinars](https://resources.github.com)
  + [Customer Stories](https://github.com/customer-stories)
  + [Partners](https://partner.github.com)
  + [Executive Insights](https://github.com/solutions/executive-insights)
* Open Source

  + [GitHub Sponsors
    Fund open source developers](/sponsors)
  + [The ReadME Project
    GitHub community articles](https://github.com/readme)
  Repositories
  + [Topics](https://github.com/topics)
  + [Trending](https://github.com/trending)
  + [Collections](https://github.com/collections)
* Enterprise

  + [Enterprise platform
    AI-powered developer platform](/enterprise)
  Available add-ons
  + [Advanced Security
    Enterprise-grade security features](https://github.com/enterprise/advanced-security)
  + [GitHub Copilot
    Enterprise-grade AI features](/features/copilot#enterprise)
  + [Premium Support
    Enterprise-grade 24/7 support](/premium-support)
* [Pricing](https://github.com/pricing)

Search or jump to...

# Search code, repositories, users, issues, pull requests...

Search

Clear

[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)

# Provide feedback

We read every piece of feedback, and take your input very seriously.

Include my email address so I can be contacted

  Cancel

 Submit feedback

# Saved searches

## Use saved searches to filter your results more quickly

Name

Query

To see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).

  Cancel

 Create saved search

[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fblob%2F266558ac4c1f361e9a178ee9d3f0ce2e648ae499%2Ftensorflow%2Flite%2Fkernels%2Finternal%2Freference%2Freference_ops.h)

[Sign up](/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E%2Fblob%2Fshow&source=header-repo&source_repo=tensorflow%2Ftensorflow)
Reseting focus

You signed in with another tab or window. Reload to refresh your session.
You signed out in another tab or window. Reload to refresh your session.
You switched accounts on another tab or window. Reload to refresh your session.

Dismiss alert

{{ message }}

[tensorflow](/tensorflow)
/
**[tensorflow](/tensorflow/tensorflow)**
Public

* [Notifications](/login?return_to=%2Ftensorflow%2Ftensorflow) You must be signed in to change notification settings
* [Fork
  74.4k](/login?return_to=%2Ftensorflow%2Ftensorflow)
* [Star
   187k](/login?return_to=%2Ftensorflow%2Ftensorflow)

* [Code](/tensorflow/tensorflow)
* [Issues
  826](/tensorflow/tensorflow/issues)
* [Pull requests
  5k+](/tensorflow/tensorflow/pulls)
* [Actions](/tensorflow/tensorflow/actions)
* [Projects
  2](/tensorflow/tensorflow/projects)
* [Security](/tensorflow/tensorflow/security)
* [Insights](/tensorflow/tensorflow/pulse)

Additional navigation options

* [Code](/tensorflow/tensorflow)
* [Issues](/tensorflow/tensorflow/issues)
* [Pull requests](/tensorflow/tensorflow/pulls)
* [Actions](/tensorflow/tensorflow/actions)
* [Projects](/tensorflow/tensorflow/projects)
* [Security](/tensorflow/tensorflow/security)
* [Insights](/tensorflow/tensorflow/pulse)

## Files

 266558a
## Breadcrumbs

1. [tensorflow](/tensorflow/tensorflow/tree/266558ac4c1f361e9a178ee9d3f0ce2e648ae499)
2. /[tensorflow](/tensorflow/tensorflow/tree/266558ac4c1f361e9a178ee9d3f0ce2e648ae499/tensorflow)
3. /[lite](/tensorflow/tensorflow/tree/266558ac4c1f361e9a178ee9d3f0ce2e648ae499/tensorflow/lite)
4. /[kernels](/tensorflow/tensorflow/tree/266558ac4c1f361e9a178ee9d3f0ce2e648ae499/tensorflow/lite/kernels)
5. /[internal](/tensorflow/tensorflow/tree/266558ac4c1f361e9a178ee9d3f0ce2e648ae499/tensorflow/lite/kernels/internal)
6. /[reference](/tensorflow/tensorflow/tree/266558ac4c1f361e9a178ee9d3f0ce2e648ae499/tensorflow/lite/kernels/internal/reference)
/
# reference\_ops.h

Copy path Blame  Blame
## Latest commit

## History

[History](/tensorflow/tensorflow/commits/266558ac4c1f361e9a178ee9d3f0ce2e648ae499/tensorflow/lite/kernels/internal/reference/reference_ops.h)1153 lines (1052 loc) · 46.3 KB 266558a
## Breadcrumbs

1. [tensorflow](/tensorflow/tensorflow/tree/266558ac4c1f361e9a178ee9d3f0ce2e648ae499)
2. /[tensorflow](/tensorflow/tensorflow/tree/266558ac4c1f361e9a178ee9d3f0ce2e648ae499/tensorflow)
3. /[lite](/tensorflow/tensorflow/tree/266558ac4c1f361e9a178ee9d3f0ce2e648ae499/tensorflow/lite)
4. /[kernels](/tensorflow/tensorflow/tree/266558ac4c1f361e9a178ee9d3f0ce2e648ae499/tensorflow/lite/kernels)
5. /[internal](/tensorflow/tensorflow/tree/266558ac4c1f361e9a178ee9d3f0ce2e648ae499/tensorflow/lite/kernels/internal)
6. /[reference](/tensorflow/tensorflow/tree/266558ac4c1f361e9a178ee9d3f0ce2e648ae499/tensorflow/lite/kernels/internal/reference)
/
# reference\_ops.h

Top
## File metadata and controls

* Code
* Blame

1153 lines (1052 loc) · 46.3 KB[Raw](https://github.com/tensorflow/tensorflow/raw/266558ac4c1f361e9a178ee9d3f0ce2e648ae499/tensorflow/lite/kernels/internal/reference/reference_ops.h)1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798991001011021031041051061071081091101111121131141151161171181191201211221231241251261271281291301311321331341351361371381391401411421431441451461471481491501511521531541551561571581591601611621631641651661671681691701711721731741751761771781791801811821831841851861871881891901911921931941951961971981992002012022032042052062072082092102112122132142152162172182192202212222232242252262272282292302312322332342352362372382392402412422432442452462472482492502512522532542552562572582592602612622632642652662672682692702712722732742752762772782792802812822832842852862872882892902912922932942952962972982993003013023033043053063073083093103113123133143153163173183193203213223233243253263273283293303313323333343353363373383393403413423433443453463473483493503513523533543553563573583593603613623633643653663673683693703713723733743753763773783793803813823833843853863873883893903913923933943953963973983994004014024034044054064074084094104114124134144154164174184194204214224234244254264274284294304314324334344354364374384394404414424434444454464474484494504514524534544554564574584594604614624634644654664674684694704714724734744754764774784794804814824834844854864874884894904914924934944954964974984995005015025035045055065075085095105115125135145155165175185195205215225235245255265275285295305315325335345355365375385395405415425435445455465475485495505515525535545555565575585595605615625635645655665675685695705715725735745755765775785795805815825835845855865875885895905915925935945955965975985996006016026036046056066076086096106116126136146156166176186196206216226236246256266276286296306316326336346356366376386396406416426436446456466476486496506516526536546556566576586596606616626636646656666676686696706716726736746756766776786796806816826836846856866876886896906916926936946956966976986997007017027037047057067077087097107117127137147157167177187197207217227237247257267277287297307317327337347357367377387397407417427437447457467477487497507517527537547557567577587597607617627637647657667677687697707717727737747757767777787797807817827837847857867877887897907917927937947957967977987998008018028038048058068078088098108118128138148158168178188198208218228238248258268278288298308318328338348358368378388398408418428438448458468478488498508518528538548558568578588598608618628638648658668678688698708718728738748758768778788798808818828838848858868878888898908918928938948958968978988999009019029039049059069079089099109119129139149159169179189199209219229239249259269279289299309319329339349359369379389399409419429439449459469479489499509519529539549559569579589599609619629639649659669679689699709719729739749759769779789799809819829839849859869879889899909919929939949959969979989991000/\* Copyright 2021 The TensorFlow Authors. All Rights Reserved.
Licensed under the Apache License, Version 2.0 (the "License");you may not use this file except in compliance with the License.You may obtain a copy of the License at
 http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an "AS IS" BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.==============================================================================\*/#ifndef TENSORFLOW\_LITE\_KERNELS\_INTERNAL\_REFERENCE\_REFERENCE\_OPS\_H\_#define TENSORFLOW\_LITE\_KERNELS\_INTERNAL\_REFERENCE\_REFERENCE\_OPS\_H\_
#include <stdint.h>#include <sys/types.h>
#include <algorithm>#include <cmath>#include <cstring>#include <functional>#include <limits>#include <memory>#include <type\_traits>
#include "third\_party/eigen3/Eigen/Core"#include "fixedpoint/fixedpoint.h"#include "ruy/profiler/instrumentation.h" // from @ruy#include "tensorflow/lite/c/c\_api\_types.h"#include "tensorflow/lite/c/common.h"#include "tensorflow/lite/kernels/internal/common.h"#include "tensorflow/lite/kernels/internal/quantization\_util.h"#include "tensorflow/lite/kernels/internal/reference/add.h"#include "tensorflow/lite/kernels/internal/reference/add\_n.h"#include "tensorflow/lite/kernels/internal/reference/arg\_min\_max.h"#include "tensorflow/lite/kernels/internal/reference/batch\_matmul.h"#include "tensorflow/lite/kernels/internal/reference/batch\_to\_space\_nd.h"#include "tensorflow/lite/kernels/internal/reference/binary\_function.h"#include "tensorflow/lite/kernels/internal/reference/cast.h"#include "tensorflow/lite/kernels/internal/reference/ceil.h"#include "tensorflow/lite/kernels/internal/reference/comparisons.h"#include "tensorflow/lite/kernels/internal/reference/concatenation.h"#include "tensorflow/lite/kernels/internal/reference/conv.h"#include "tensorflow/lite/kernels/internal/reference/depth\_to\_space.h"#include "tensorflow/lite/kernels/internal/reference/dequantize.h"#include "tensorflow/lite/kernels/internal/reference/div.h"#include "tensorflow/lite/kernels/internal/reference/elu.h"#include "tensorflow/lite/kernels/internal/reference/exp.h"#include "tensorflow/lite/kernels/internal/reference/fill.h"#include "tensorflow/lite/kernels/internal/reference/floor.h"#include "tensorflow/lite/kernels/internal/reference/floor\_div.h"#include "tensorflow/lite/kernels/internal/reference/floor\_mod.h"#include "tensorflow/lite/kernels/internal/reference/fully\_connected.h"#include "tensorflow/lite/kernels/internal/reference/gather.h"#include "tensorflow/lite/kernels/internal/reference/hard\_swish.h"#include "tensorflow/lite/kernels/internal/reference/l2normalization.h"#include "tensorflow/lite/kernels/internal/reference/leaky\_relu.h"#include "tensorflow/lite/kernels/internal/reference/log\_softmax.h"#include "tensorflow/lite/kernels/internal/reference/logistic.h"#include "tensorflow/lite/kernels/internal/reference/lstm\_cell.h"#include "tensorflow/lite/kernels/internal/reference/maximum\_minimum.h"#include "tensorflow/lite/kernels/internal/reference/mul.h"#include "tensorflow/lite/kernels/internal/reference/neg.h"#include "tensorflow/lite/kernels/internal/reference/pad.h"#include "tensorflow/lite/kernels/internal/reference/pooling.h"#include "tensorflow/lite/kernels/internal/reference/prelu.h"#include "tensorflow/lite/kernels/internal/reference/process\_broadcast\_shapes.h"#include "tensorflow/lite/kernels/internal/reference/quantize.h"#include "tensorflow/lite/kernels/internal/reference/reduce.h"#include "tensorflow/lite/kernels/internal/reference/requantize.h"#include "tensorflow/lite/kernels/internal/reference/resize\_bilinear.h"#include "tensorflow/lite/kernels/internal/reference/resize\_nearest\_neighbor.h"#include "tensorflow/lite/kernels/internal/reference/round.h"#include "tensorflow/lite/kernels/internal/reference/slice.h"#include "tensorflow/lite/kernels/internal/reference/softmax.h"#include "tensorflow/lite/kernels/internal/reference/space\_to\_batch\_nd.h"#include "tensorflow/lite/kernels/internal/reference/space\_to\_depth.h"#include "tensorflow/lite/kernels/internal/reference/strided\_slice.h"#include "tensorflow/lite/kernels/internal/reference/string\_comparisons.h"#include "tensorflow/lite/kernels/internal/reference/sub.h"#include "tensorflow/lite/kernels/internal/reference/tanh.h"#include "tensorflow/lite/kernels/internal/reference/transpose.h"#include "tensorflow/lite/kernels/internal/reference/transpose\_conv.h"#include "tensorflow/lite/kernels/internal/strided\_slice\_logic.h"#include "tensorflow/lite/kernels/internal/tensor.h"#include "tensorflow/lite/kernels/internal/types.h"namespace tflite {
namespace reference\_ops {
template <typename T>inline void Relu(const RuntimeShape& input\_shape, const T\* input\_data, const RuntimeShape& output\_shape, T\* output\_data) { const int flat\_size = MatchingFlatSize(input\_shape, output\_shape); for (int i = 0; i < flat\_size; ++i) { const T val = input\_data[i]; const T lower = 0; const T clamped = val < lower ? lower : val; output\_data[i] = clamped; }}
template <typename T>inline void Relu0To1(const RuntimeShape& input\_shape, const T\* input\_data, const RuntimeShape& output\_shape, T\* output\_data) { ruy::profiler::ScopeLabel label("Relu0To1 (not fused)"); const int flat\_size = MatchingFlatSize(input\_shape, output\_shape); for (int i = 0; i < flat\_size; ++i) { const T val = input\_data[i]; const T upper = 1; const T lower = 0; const T clamped = val > upper ? upper : val < lower ? lower : val; output\_data[i] = clamped; }}
template <typename T>inline void Relu1(const RuntimeShape& input\_shape, const T\* input\_data, const RuntimeShape& output\_shape, T\* output\_data) { ruy::profiler::ScopeLabel label("Relu1 (not fused)"); const int flat\_size = MatchingFlatSize(input\_shape, output\_shape); for (int i = 0; i < flat\_size; ++i) { const T val = input\_data[i]; const T upper = 1; const T lower = -1; const T clamped = val > upper ? upper : val < lower ? lower : val; output\_data[i] = clamped; }}
inline void Relu6(const RuntimeShape& input\_shape, const float\* input\_data, const RuntimeShape& output\_shape, float\* output\_data) { ruy::profiler::ScopeLabel label("Relu6 (not fused)"); const int flat\_size = MatchingFlatSize(input\_shape, output\_shape); for (int i = 0; i < flat\_size; ++i) { const float val = input\_data[i]; const float upper = 6; const float lower = 0; const float clamped = val > upper ? upper : val < lower ? lower : val; output\_data[i] = clamped; }}
template <typename T>inline void ReluX(const tflite::ReluParams& params, const RuntimeShape& input\_shape, const T\* input\_data, const RuntimeShape& output\_shape, T\* output\_data) { ruy::profiler::ScopeLabel label("Quantized ReluX (not fused)"); const int flat\_size = MatchingFlatSize(input\_shape, output\_shape); for (int i = 0; i < flat\_size; ++i) { const int32 val = static\_cast<int32\_t>(input\_data[i]); int32 clamped = params.output\_offset + MultiplyByQuantizedMultiplier(val - params.input\_offset, params.output\_multiplier, params.output\_shift); clamped = std::max(params.quantized\_activation\_min, clamped); clamped = std::min(params.quantized\_activation\_max, clamped); output\_data[i] = static\_cast<T>(clamped); }}
template <typename T>inline void ReluX(const tflite::ActivationParams& params, const RuntimeShape& input\_shape, const T\* input\_data, const RuntimeShape& output\_shape, T\* output\_data) { ruy::profiler::ScopeLabel label("Quantized ReluX (not fused)"); const int flat\_size = MatchingFlatSize(input\_shape, output\_shape); const T max\_value = params.quantized\_activation\_max; const T min\_value = params.quantized\_activation\_min; for (int i = 0; i < flat\_size; ++i) { const T val = input\_data[i]; const T clamped = val > max\_value ? max\_value : val < min\_value ? min\_value : val; output\_data[i] = clamped; }}
// TODO(jiawen): We can implement BroadcastMul on buffers of arbitrary// dimensionality if the runtime code does a single loop over one dimension// that handles broadcasting as the base case. The code generator would then// generate max(D1, D2) nested for loops.inline void BroadcastMulFivefold(const ArithmeticParams& unswitched\_params, const RuntimeShape& unswitched\_input1\_shape, const uint8\* unswitched\_input1\_data, const RuntimeShape& unswitched\_input2\_shape, const uint8\* unswitched\_input2\_data, const RuntimeShape& output\_shape, uint8\* output\_data) { ArithmeticParams switched\_params = unswitched\_params; switched\_params.input1\_offset = unswitched\_params.input2\_offset; switched\_params.input2\_offset = unswitched\_params.input1\_offset;
 const bool use\_unswitched = unswitched\_params.broadcast\_category == tflite::BroadcastableOpCategory::kFirstInputBroadcastsFast;
 const ArithmeticParams& params = use\_unswitched ? unswitched\_params : switched\_params; const uint8\* input1\_data = use\_unswitched ? unswitched\_input1\_data : unswitched\_input2\_data; const uint8\* input2\_data = use\_unswitched ? unswitched\_input2\_data : unswitched\_input1\_data;
 // Fivefold nested loops. The second input resets its position for each // iteration of the second loop. The first input resets its position at the // beginning of the fourth loop. The innermost loop is an elementwise Mul of // sections of the arrays. uint8\* output\_data\_ptr = output\_data; const uint8\* input1\_data\_ptr = input1\_data; const uint8\* input2\_data\_reset = input2\_data; int y0 = params.broadcast\_shape[0]; int y1 = params.broadcast\_shape[1]; int y2 = params.broadcast\_shape[2]; int y3 = params.broadcast\_shape[3]; int y4 = params.broadcast\_shape[4]; for (int i0 = 0; i0 < y0; ++i0) { const uint8\* input2\_data\_ptr; for (int i1 = 0; i1 < y1; ++i1) { input2\_data\_ptr = input2\_data\_reset; for (int i2 = 0; i2 < y2; ++i2) { for (int i3 = 0; i3 < y3; ++i3) { MulElementwise(y4, params, input1\_data\_ptr, input2\_data\_ptr, output\_data\_ptr); input2\_data\_ptr += y4; output\_data\_ptr += y4; } input1\_data\_ptr += y4; } } input2\_data\_reset = input2\_data\_ptr; }}
inline void Mul(const ArithmeticParams& params, const RuntimeShape& input1\_shape, const int16\* input1\_data, const RuntimeShape& input2\_shape, const int16\* input2\_data, const RuntimeShape& output\_shape, int16\* output\_data) { ruy::profiler::ScopeLabel label("Mul/Int16");
 const int flat\_size = MatchingElementsSize(input1\_shape, input2\_shape, output\_shape);
 for (int i = 0; i < flat\_size; i++) { // F0 uses 0 integer bits, range [-1, 1]. using F0 = gemmlowp::FixedPoint<std::int16\_t, 0>;
 F0 unclamped\_result = F0::FromRaw(input1\_data[i]) \* F0::FromRaw(input2\_data[i]); output\_data[i] = unclamped\_result.raw(); }}
inline void Mul(const ArithmeticParams& params, const RuntimeShape& input1\_shape, const int16\* input1\_data, const RuntimeShape& input2\_shape, const int16\* input2\_data, const RuntimeShape& output\_shape, uint8\* output\_data) { ruy::profiler::ScopeLabel label("Mul/Int16Uint8"); int32 output\_offset = params.output\_offset; int32 output\_activation\_min = params.quantized\_activation\_min; int32 output\_activation\_max = params.quantized\_activation\_max; TFLITE\_DCHECK\_LE(output\_activation\_min, output\_activation\_max);
 const int flat\_size = MatchingElementsSize(input1\_shape, input2\_shape, output\_shape);
 for (int i = 0; i < flat\_size; i++) { // F0 uses 0 integer bits, range [-1, 1]. using F0 = gemmlowp::FixedPoint<std::int16\_t, 0>;
 F0 unclamped\_result = F0::FromRaw(input1\_data[i]) \* F0::FromRaw(input2\_data[i]); int16 rescaled\_result = gemmlowp::RoundingDivideByPOT(unclamped\_result.raw(), 8); int16 clamped\_result = std::min<int16>(output\_activation\_max - output\_offset, rescaled\_result); clamped\_result = std::max<int16>(output\_activation\_min - output\_offset, clamped\_result); output\_data[i] = output\_offset + clamped\_result; }}
inline void Sub16(const ArithmeticParams& params, const RuntimeShape& input1\_shape, const int16\_t\* input1\_data, const RuntimeShape& input2\_shape, const int16\_t\* input2\_data, const RuntimeShape& output\_shape, int16\_t\* output\_data) { ruy::profiler::ScopeLabel label("Sub/Int16"); const int input1\_shift = params.input1\_shift; const int flat\_size = MatchingElementsSize(input1\_shape, input2\_shape, output\_shape); const int16 output\_activation\_min = params.quantized\_activation\_min; const int16 output\_activation\_max = params.quantized\_activation\_max;
 TFLITE\_DCHECK(input1\_shift == 0 || params.input2\_shift == 0); TFLITE\_DCHECK\_LE(input1\_shift, 0); TFLITE\_DCHECK\_LE(params.input2\_shift, 0); const int16\* not\_shift\_input = input1\_shift == 0 ? input1\_data : input2\_data; const int16\* shift\_input = input1\_shift == 0 ? input2\_data : input1\_data; const int input\_right\_shift = input1\_shift == 0 ? -params.input2\_shift : -input1\_shift;
 if (input1\_shift == 0) { // F0 uses 0 integer bits, range [-1, 1]. using F0 = gemmlowp::FixedPoint<std::int16\_t, 0>; for (int i = 0; i < flat\_size; ++i) { F0 input\_ready\_scaled = F0::FromRaw(not\_shift\_input[i]); F0 scaled\_input = F0::FromRaw( gemmlowp::RoundingDivideByPOT(shift\_input[i], input\_right\_shift)); F0 result = SaturatingSub(input\_ready\_scaled, scaled\_input); const int16 raw\_output = result.raw(); const int16 clamped\_output = std::min( output\_activation\_max, std::max(output\_activation\_min, raw\_output)); output\_data[i] = clamped\_output; } } else { // F0 uses 0 integer bits, range [-1, 1]. using F0 = gemmlowp::FixedPoint<std::int16\_t, 0>; for (int i = 0; i < flat\_size; ++i) { F0 input\_ready\_scaled = F0::FromRaw(not\_shift\_input[i]); F0 scaled\_input = F0::FromRaw( gemmlowp::RoundingDivideByPOT(shift\_input[i], input\_right\_shift)); F0 result = SaturatingSub(scaled\_input, input\_ready\_scaled); const int16 raw\_output = result.raw(); const int16 clamped\_output = std::min( output\_activation\_max, std::max(output\_activation\_min, raw\_output)); output\_data[i] = clamped\_output; } }}
template <typename Scalar>void Pack(const PackParams& params, const RuntimeShape\* const\* input\_shapes, const Scalar\* const\* input\_data, const RuntimeShape& output\_shape, Scalar\* output\_data) { ruy::profiler::ScopeLabel label("Pack"); const int dimensions = output\_shape.DimensionsCount(); int axis = params.axis; int inputs\_count = params.inputs\_count;
 int outer\_size = 1; for (int i = 0; i < axis; i++) { outer\_size \*= output\_shape.Dims(i); } int copy\_size = 1; for (int i = params.axis + 1; i < dimensions; i++) { copy\_size \*= output\_shape.Dims(i); } TFLITE\_DCHECK\_EQ((\*\*input\_shapes).FlatSize(), copy\_size \* outer\_size);
 for (int i = 0; i < inputs\_count; ++i) { for (int k = 0; k < outer\_size; k++) { const Scalar\* input\_ptr = input\_data[i] + copy\_size \* k; int loc = k \* inputs\_count \* copy\_size + i \* copy\_size; memcpy(output\_data + loc, input\_ptr, copy\_size \* sizeof(Scalar)); } }}
template <typename Scalar>void Unpack(const UnpackParams& params, const RuntimeShape& input\_shape, const Scalar\* input\_data, const RuntimeShape& output\_shape, Scalar\* const\* output\_datas) { ruy::profiler::ScopeLabel label("Unpack"); const int dimensions = input\_shape.DimensionsCount(); const int outputs\_count = params.num\_split;
 int outer\_size = 1; int axis = params.axis; if (axis < 0) { axis += dimensions; } TFLITE\_DCHECK\_GE(axis, 0); TFLITE\_DCHECK\_LT(axis, dimensions); for (int i = 0; i < axis; ++i) { outer\_size \*= input\_shape.Dims(i); } int copy\_size = 1; for (int i = axis + 1; i < dimensions; ++i) { copy\_size \*= input\_shape.Dims(i); } TFLITE\_DCHECK\_EQ(output\_shape.FlatSize(), copy\_size \* outer\_size);
 for (int i = 0; i < outputs\_count; ++i) { for (int k = 0; k < outer\_size; k++) { Scalar\* output\_ptr = output\_datas[i] + copy\_size \* k; int loc = k \* outputs\_count \* copy\_size + i \* copy\_size; memcpy(output\_ptr, input\_data + loc, copy\_size \* sizeof(Scalar)); } }}
template <typename Scalar>void PackWithScaling(const PackParams& params, const RuntimeShape\* const\* input\_shapes, const uint8\* const\* input\_data, const RuntimeShape& output\_shape, uint8\* output\_data) { ruy::profiler::ScopeLabel label("PackWithScaling"); const int dimensions = output\_shape.DimensionsCount(); int axis = params.axis; const int32\* input\_zeropoint = params.input\_zeropoint; const float\* input\_scale = params.input\_scale; int inputs\_count = params.inputs\_count; const int32 output\_zeropoint = params.output\_zeropoint; const float output\_scale = params.output\_scale;
 int outer\_size = 1; for (int i = 0; i < axis; i++) { outer\_size \*= output\_shape.Dims(i); } int copy\_size = 1; for (int i = axis + 1; i < dimensions; i++) { copy\_size \*= output\_shape.Dims(i); } TFLITE\_DCHECK\_EQ((\*\*input\_shapes).FlatSize(), copy\_size \* outer\_size);
 Scalar\* output\_ptr = output\_data; const float inverse\_output\_scale = 1.f / output\_scale; for (int k = 0; k < outer\_size; k++) { for (int i = 0; i < inputs\_count; ++i) { if (input\_zeropoint[i] == output\_zeropoint && input\_scale[i] == output\_scale) { memcpy(output\_ptr, input\_data[i] + k \* copy\_size, copy\_size \* sizeof(Scalar)); } else { assert(false); const float scale = input\_scale[i] \* inverse\_output\_scale; const float bias = -input\_zeropoint[i] \* scale; auto input\_ptr = input\_data[i]; for (int j = 0; j < copy\_size; ++j) { const int32\_t value = static\_cast<int32\_t>(std::round(input\_ptr[j] \* scale + bias)) + output\_zeropoint; output\_ptr[j] = static\_cast<uint8\_t>(std::max(std::min(255, value), 0)); } } output\_ptr += copy\_size; } }}
template <typename Scalar>void DepthConcatenation(const ConcatenationParams& params, const RuntimeShape\* const\* input\_shapes, const Scalar\* const\* input\_data, const RuntimeShape& output\_shape, Scalar\* output\_data) { ruy::profiler::ScopeLabel label("DepthConcatenation"); auto params\_copy = params; params\_copy.axis = 3; Concatenation(params\_copy, input\_shapes, input\_data, output\_shape, output\_data);}
template <typename Scalar>void Split(const SplitParams& params, const RuntimeShape& input\_shape, const Scalar\* input\_data, const RuntimeShape\* const\* output\_shapes, Scalar\* const\* output\_data) { ruy::profiler::ScopeLabel label("Split"); const int split\_dimensions = input\_shape.DimensionsCount(); int axis = params.axis < 0 ? params.axis + split\_dimensions : params.axis; int outputs\_count = params.num\_split; TFLITE\_DCHECK\_LT(axis, split\_dimensions);
 int64\_t split\_size = 0; for (int i = 0; i < outputs\_count; i++) { TFLITE\_DCHECK\_EQ(output\_shapes[i]->DimensionsCount(), split\_dimensions); for (int j = 0; j < split\_dimensions; j++) { if (j != axis) { MatchingDim(\*output\_shapes[i], j, input\_shape, j); } } split\_size += output\_shapes[i]->Dims(axis); } TFLITE\_DCHECK\_EQ(split\_size, input\_shape.Dims(axis)); int64\_t outer\_size = 1; for (int i = 0; i < axis; ++i) { outer\_size \*= input\_shape.Dims(i); } // For all output arrays, // FlatSize() = outer\_size \* Dims(axis) \* base\_inner\_size; int64\_t base\_inner\_size = 1; for (int i = axis + 1; i < split\_dimensions; ++i) { base\_inner\_size \*= input\_shape.Dims(i); }
 const Scalar\* input\_ptr = input\_data; for (int k = 0; k < outer\_size; k++) { for (int i = 0; i < outputs\_count; ++i) { const int copy\_size = output\_shapes[i]->Dims(axis) \* base\_inner\_size; memcpy(output\_data[i] + k \* copy\_size, input\_ptr, copy\_size \* sizeof(Scalar)); input\_ptr += copy\_size; } }}
inline int NodeOffset(int b, int h, int w, int height, int width) { return (b \* height + h) \* width + w;}
inline void LocalResponseNormalization( const tflite::LocalResponseNormalizationParams& op\_params, const RuntimeShape& input\_shape, const float\* input\_data, const RuntimeShape& output\_shape, float\* output\_data) { const int trailing\_dim = input\_shape.DimensionsCount() - 1; const int outer\_size = MatchingFlatSizeSkipDim(input\_shape, trailing\_dim, output\_shape); const int depth = MatchingDim(input\_shape, trailing\_dim, output\_shape, trailing\_dim);
 for (int i = 0; i < outer\_size; ++i) { for (int c = 0; c < depth; ++c) { const int begin\_input\_c = std::max(0, c - op\_params.range); const int end\_input\_c = std::min(depth, c + op\_params.range); float accum = 0.f; for (int input\_c = begin\_input\_c; input\_c < end\_input\_c; ++input\_c) { const float input\_val = input\_data[i \* depth + input\_c]; accum += input\_val \* input\_val; } const float multiplier = std::pow(op\_params.bias + op\_params.alpha \* accum, -op\_params.beta); output\_data[i \* depth + c] = input\_data[i \* depth + c] \* multiplier; } }}
inline void Dequantize(const RuntimeShape& input\_shape, const Eigen::half\* input\_data, const RuntimeShape& output\_shape, float\* output\_data) { const int flat\_size = MatchingFlatSize(input\_shape, output\_shape); for (int i = 0; i < flat\_size; i++) { output\_data[i] = static\_cast<float>(input\_data[i]); }}
inline void FakeQuant(const tflite::FakeQuantParams& op\_params, const RuntimeShape& input\_shape, const float\* input\_data, const RuntimeShape& output\_shape, float\* output\_data) { ruy::profiler::ScopeLabel label("FakeQuant"); float rmin = op\_params.minmax.min; float rmax = op\_params.minmax.max; int num\_bits = op\_params.num\_bits; // 0 should always be a representable value. Let's assume that the initial // min,max range contains 0. TFLITE\_DCHECK\_LE(rmin, 0.0f); TFLITE\_DCHECK\_GE(rmax, 0.0f); TFLITE\_DCHECK\_LT(rmin, rmax);
 // Code matches tensorflow's FakeQuantWithMinMaxArgsFunctor. int quant\_min = 0; int quant\_max = (1 << num\_bits) - 1; float nudged\_min, nudged\_max, nudged\_scale; NudgeQuantizationRange(rmin, rmax, quant\_min, quant\_max, &nudged\_min, &nudged\_max, &nudged\_scale); const int flat\_size = MatchingFlatSize(input\_shape, output\_shape); FakeQuantizeArray(nudged\_scale, nudged\_min, nudged\_max, input\_data, output\_data, flat\_size);}
// Common subroutine for both `GatherNd` and `GatherNdString`.struct GatherNdHelperResult { int n\_slices; int slice\_size; int indices\_nd; std::vector<int> dims\_to\_count;};
// Returns common values being used on both `GatherNd` and `GatherNdString`.inline GatherNdHelperResult GatherNdHelper(const RuntimeShape& params\_shape, const RuntimeShape& indices\_shape) { GatherNdHelperResult ret; ret.n\_slices = 1; ret.slice\_size = 1; const int indices\_dims = indices\_shape.DimensionsCount(); ret.indices\_nd = indices\_shape.Dims(indices\_dims - 1); const int params\_dims = params\_shape.DimensionsCount(); for (int i = 0; i < indices\_dims - 1; ++i) { ret.n\_slices \*= indices\_shape.Dims(i); } if (ret.n\_slices == 0) return ret;
 for (int i = ret.indices\_nd; i < params\_dims; ++i) { ret.slice\_size \*= params\_shape.Dims(i); }
 int remain\_flat\_size = params\_shape.FlatSize(); ret.dims\_to\_count = std::vector<int>(ret.indices\_nd, 0); for (int i = 0; i < ret.indices\_nd; ++i) { ret.dims\_to\_count[i] = remain\_flat\_size / params\_shape.Dims(i); remain\_flat\_size = ret.dims\_to\_count[i]; }
 return ret;}
// Implements GatherNd.// Returns an error if any of the indices\_data would cause an out of bounds// memory read.template <typename ParamsT, typename IndicesT = int32>inline TfLiteStatus GatherNd(const RuntimeShape& params\_shape, const ParamsT\* params\_data, const RuntimeShape& indices\_shape, const IndicesT\* indices\_data, const RuntimeShape& output\_shape, ParamsT\* output\_data) { ruy::profiler::ScopeLabel label("GatherNd");
 const GatherNdHelperResult res = GatherNdHelper(params\_shape, indices\_shape); for (int i = 0; i < res.n\_slices; ++i) { int64\_t from\_pos = 0; for (int j = 0; j < res.indices\_nd; ++j) { from\_pos += indices\_data[i \* res.indices\_nd + j] \* res.dims\_to\_count[j]; } if (from\_pos < 0 || from\_pos + res.slice\_size > params\_shape.FlatSize()) { return kTfLiteError; } std::memcpy(output\_data + i \* res.slice\_size, params\_data + from\_pos, sizeof(ParamsT) \* res.slice\_size); } return kTfLiteOk;}
#ifndef TF\_LITE\_STATIC\_MEMORY// Implements GatherNd on strings.// Returns an error if any of the indices\_data would cause an out of bounds// memory read.template <typename IndicesT = int32>inline TfLiteStatus GatherNdString(const RuntimeShape& params\_shape, const TfLiteTensor\* params\_data, const RuntimeShape& indices\_shape, const IndicesT\* indices\_data, const RuntimeShape& output\_shape, TfLiteTensor\* output\_data) { ruy::profiler::ScopeLabel label("GatherNdString");
 const GatherNdHelperResult res = GatherNdHelper(params\_shape, indices\_shape); DynamicBuffer buffer; for (int i = 0; i < res.n\_slices; ++i) { int64\_t from\_pos = 0; for (int j = 0; j < res.indices\_nd; ++j) { from\_pos += indices\_data[i \* res.indices\_nd + j] \* res.dims\_to\_count[j]; } if (from\_pos < 0 || from\_pos + res.slice\_size > params\_shape.FlatSize()) { return kTfLiteError; } for (int j = 0; j < res.slice\_size; ++j) { buffer.AddString(GetString(params\_data, from\_pos + j)); } } buffer.WriteToTensor(output\_data, /\*new\_shape=\*/nullptr); return kTfLiteOk;}#endif
template <typename IndicesT, typename UpdatesT>inline void ScatterNd(const RuntimeShape& indices\_shape, const IndicesT\* indices\_data, const RuntimeShape& updates\_shape, const UpdatesT\* updates\_data, const RuntimeShape& output\_shape, UpdatesT\* output\_data) { ruy::profiler::ScopeLabel label("ScatterNd");
 int n\_slices = 1; int slice\_size = 1; const int outer\_dims = indices\_shape.DimensionsCount() - 1; const int indices\_nd = indices\_shape.Dims(outer\_dims); const int updates\_dims = updates\_shape.DimensionsCount(); for (int i = 0; i < outer\_dims; ++i) { n\_slices \*= indices\_shape.Dims(i); } for (int i = outer\_dims; i < updates\_dims; ++i) { slice\_size \*= updates\_shape.Dims(i); }
 int output\_flat\_size = output\_shape.FlatSize(); int remain\_flat\_size = output\_flat\_size; std::vector<int> dims\_to\_count(indices\_nd, 0); for (int i = 0; i < indices\_nd; ++i) { dims\_to\_count[i] = remain\_flat\_size / output\_shape.Dims(i); remain\_flat\_size = dims\_to\_count[i]; }
 memset(output\_data, 0, sizeof(UpdatesT) \* output\_flat\_size); for (int i = 0; i < n\_slices; ++i) { int to\_pos = 0; for (int j = 0; j < indices\_nd; ++j) { IndicesT idx = indices\_data[i \* indices\_nd + j]; TFLITE\_DCHECK(0 <= idx && idx < output\_shape.Dims(j)); to\_pos += idx \* dims\_to\_count[j]; } for (int j = 0; j < slice\_size; j++) { output\_data[to\_pos + j] += updates\_data[i \* slice\_size + j]; } }}
template <typename T>void Minimum(const RuntimeShape& input1\_shape, const T\* input1\_data, const T\* input2\_data, const RuntimeShape& output\_shape, T\* output\_data) { const int flat\_size = MatchingFlatSize(input1\_shape, output\_shape);
 auto min\_value = input2\_data[0]; for (int i = 0; i < flat\_size; i++) { output\_data[i] = input1\_data[i] > min\_value ? min\_value : input1\_data[i]; }}
// Convenience version that allows, for example, generated-code calls to be// the same as other binary ops.template <typename T>inline void Minimum(const RuntimeShape& input1\_shape, const T\* input1\_data, const RuntimeShape&, const T\* input2\_data, const RuntimeShape& output\_shape, T\* output\_data) { // Drop shape of second input: not needed. Minimum(input1\_shape, input1\_data, input2\_data, output\_shape, output\_data);}
template <typename T>void Maximum(const RuntimeShape& input1\_shape, const T\* input1\_data, const T\* input2\_data, const RuntimeShape& output\_shape, T\* output\_data) { const int flat\_size = MatchingFlatSize(input1\_shape, output\_shape);
 auto max\_value = input2\_data[0]; for (int i = 0; i < flat\_size; i++) { output\_data[i] = input1\_data[i] < max\_value ? max\_value : input1\_data[i]; }}
// Convenience version that allows, for example, generated-code calls to be// the same as other binary ops.template <typename T>inline void Maximum(const RuntimeShape& input1\_shape, const T\* input1\_data, const RuntimeShape&, const T\* input2\_data, const RuntimeShape& output\_shape, T\* output\_data) { // Drop shape of second input: not needed. Maximum(input1\_shape, input1\_data, input2\_data, output\_shape, output\_data);}
template <typename T1, typename T2, typename T3>void ArgMax(const RuntimeShape& input1\_shape, const T1\* input1\_data, const T3\* input2\_data, const RuntimeShape& output\_shape, T2\* output\_data) { ArgMinMax(input1\_shape, input1\_data, input2\_data, output\_shape, output\_data, std::greater<T1>());}
// Convenience version that allows, for example, generated-code calls to be// the same as other binary ops.template <typename T1, typename T2, typename T3>inline void ArgMax(const RuntimeShape& input1\_shape, const T1\* input1\_data, const RuntimeShape& input2\_shape, const T3\* input2\_data, const RuntimeShape& output\_shape, T2\* output\_data) { // Drop shape of second input: not needed. ArgMax(input1\_shape, input1\_data, input2\_data, output\_shape, output\_data);}
template <typename D, typename T>void Select(const RuntimeShape& input\_condition\_shape, const D\* input\_condition\_data, const RuntimeShape& input\_x\_shape, const T\* input\_x\_data, const RuntimeShape& input\_y\_shape, const T\* input\_y\_data, const RuntimeShape& output\_shape, T\* output\_data) { ruy::profiler::ScopeLabel label("Select"); int64\_t flatsize; // Allow select operator executions on mixed scalar tensors and one element // tensors. if (input\_condition\_shape.FlatSize() == 1 && input\_x\_shape.FlatSize() == 1 && input\_y\_shape.FlatSize() == 1 && output\_shape.FlatSize() == 1) { flatsize = 1; } else { flatsize = MatchingFlatSize(input\_condition\_shape, input\_x\_shape, input\_y\_shape, output\_shape); } for (int64\_t i = 0; i < flatsize; ++i) { output\_data[i] = input\_condition\_data[i] ? input\_x\_data[i] : input\_y\_data[i]; }}
template <typename D, typename T>void RankOneSelect(const RuntimeShape& input\_condition\_shape, const D\* input\_condition\_data, const RuntimeShape& input\_x\_shape, const T\* input\_x\_data, const RuntimeShape& input\_y\_shape, const T\* input\_y\_data, const RuntimeShape& output\_shape, T\* output\_data) { ruy::profiler::ScopeLabel label("Select/RankOneSelect"); const int64\_t outer\_size = input\_condition\_shape.FlatSize(); int64\_t inner\_size; if (input\_condition\_shape.DimensionsCount() == 0) { inner\_size = MatchingFlatSize(input\_x\_shape, input\_y\_shape, output\_shape); } else { TFLITE\_DCHECK\_EQ( MatchingDim(input\_x\_shape, 0, input\_y\_shape, 0, output\_shape, 0), outer\_size); inner\_size = MatchingFlatSizeSkipDim(input\_x\_shape, 0, input\_y\_shape, output\_shape); }
 int64\_t offset = 0; for (int64\_t i = 0; i < outer\_size; i++) { const T\* input\_data = input\_condition\_data[i] ? input\_x\_data : input\_y\_data; memcpy(output\_data + offset, input\_data + offset, inner\_size \* sizeof(T)); offset += inner\_size; }}
template <typename D, typename T>void BroadcastSelect5DSlow(const RuntimeShape& input\_condition\_shape, const D\* input\_condition\_data, const RuntimeShape& input\_x\_shape, const T\* input\_x\_data, const RuntimeShape& input\_y\_shape, const T\* input\_y\_data, const RuntimeShape& output\_shape, T\* output\_data) { ruy::profiler::ScopeLabel label("Select/BroadcastSelectSlow"); TFLITE\_DCHECK\_LE(input\_condition\_shape.DimensionsCount(), 5); TFLITE\_DCHECK\_LE(input\_x\_shape.DimensionsCount(), 5); TFLITE\_DCHECK\_LE(input\_y\_shape.DimensionsCount(), 5); TFLITE\_DCHECK\_LE(output\_shape.DimensionsCount(), 5);
 NdArrayDesc<5> desc\_condition; NdArrayDesc<5> desc\_x; NdArrayDesc<5> desc\_y; NdArrayDesc<5> desc\_output; const RuntimeShape extended\_output\_shape = RuntimeShape::ExtendedShape(5, output\_shape); CopyDimsToDesc(extended\_output\_shape, &desc\_output); NdArrayDescsForElementwiseBroadcast(input\_condition\_shape, input\_x\_shape, input\_y\_shape, &desc\_condition, &desc\_x, &desc\_y);
 // In Tensorflow, the dimensions are canonically named (batch\_number, row, // col, channel), with extents (batches, height, width, depth), with the // trailing dimension changing most rapidly (channels has the smallest // stride, typically 1 element). // // In generated C code, we store arrays with the dimensions reversed. The // first dimension has smallest stride. // // We name our variables by their Tensorflow convention, but generate C code // nesting loops such that the innermost loop has the smallest stride for // the best cache behavior. for (int n = 0; n < desc\_output.extents[0]; ++n) { int out\_idx\_n = desc\_output.extents[1] \* n; int cond\_idx\_n = desc\_condition.strides[0] \* n; int in\_idx1\_n = desc\_x.strides[0] \* n; int in\_idx2\_n = desc\_y.strides[0] \* n; for (int b = 0; b < desc\_output.extents[1]; ++b) { int out\_idx\_b = (out\_idx\_n + b) \* desc\_output.extents[2]; int cond\_idx\_b = cond\_idx\_n + desc\_condition.strides[1] \* b; int in\_idx1\_b = in\_idx1\_n + desc\_x.strides[1] \* b; int in\_idx2\_b = in\_idx2\_n + desc\_y.strides[1] \* b; for (int y = 0; y < desc\_output.extents[2]; ++y) { int out\_idx\_y = (out\_idx\_b + y) \* desc\_output.extents[3]; int cond\_idx\_y = cond\_idx\_b + desc\_condition.strides[2] \* y; int in\_idx1\_y = in\_idx1\_b + desc\_x.strides[2] \* y; int in\_idx2\_y = in\_idx2\_b + desc\_y.strides[2] \* y; for (int x = 0; x < desc\_output.extents[3]; ++x) { int out\_idx = (out\_idx\_y + x) \* desc\_output.extents[4]; int cond\_idx = cond\_idx\_y + desc\_condition.strides[3] \* x; int in\_idx1 = in\_idx1\_y + desc\_x.strides[3] \* x; int in\_idx2 = in\_idx2\_y + desc\_y.strides[3] \* x; for (int c = 0; c < desc\_output.extents[4]; ++c) { output\_data[out\_idx] = input\_condition\_data[cond\_idx] ? input\_x\_data[in\_idx1] : input\_y\_data[in\_idx2]; out\_idx++; cond\_idx += desc\_condition.strides[4]; in\_idx1 += desc\_x.strides[4]; in\_idx2 += desc\_y.strides[4]; } } } } }}
template <typename D, typename T>void SelectTrueCoords(const RuntimeShape& input\_condition\_shape, const D\* input\_condition\_data, T\* output\_data) { const size\_t size = input\_condition\_shape.FlatSize(); if (size == 0) { // Dimension is zero, in which case we don't need to output. return; } const size\_t cond\_rank = input\_condition\_shape.DimensionsCount();
 std::vector<int> dims\_to\_count(cond\_rank, 0); int cur\_flat\_size = size; for (int i = 0; i < cond\_rank; ++i) { dims\_to\_count[i] = cur\_flat\_size / input\_condition\_shape.Dims(i); cur\_flat\_size = dims\_to\_count[i]; }
 int output\_index = 0; for (int i = 0; i < size; ++i) { if (input\_condition\_data[i] != D(0)) { // Insert the coordinate of the current item (row major) into output. int flat\_index = i; for (int j = 0; j < cond\_rank; ++j) { int coord\_j = flat\_index / dims\_to\_count[j]; output\_data[output\_index \* cond\_rank + j] = coord\_j; flat\_index %= dims\_to\_count[j]; } output\_index++; } }}
// For easy implementation, the indices is always a vector of size-4 vectors.template <typename T, typename TI>inline void SparseToDense(const std::vector<std::vector<TI>>& indices, const T\* values, T default\_value, bool value\_is\_scalar, const RuntimeShape& unextended\_output\_shape, T\* output\_data) { TFLITE\_DCHECK\_LE(unextended\_output\_shape.DimensionsCount(), 4); const RuntimeShape output\_shape = RuntimeShape::ExtendedShape(4, unextended\_output\_shape); const int value\_count = indices.size();
 // First fill the output\_data with default value. const int num\_elements = output\_shape.FlatSize(); for (int i = 0; i < num\_elements; ++i) { output\_data[i] = default\_value; }
 // Special handle for value is scalar case to avoid checking the boolean // condition within the loop every time. if (value\_is\_scalar) { for (int i = 0; i < value\_count; ++i) { const std::vector<TI>& index = indices[i]; TFLITE\_DCHECK\_EQ(index.size(), 4); const T value = \*values; // just use the first value. output\_data[Offset(output\_shape, index[0], index[1], index[2], index[3])] = value; } return; }
 // Go through the values and indices to fill the sparse values. for (int i = 0; i < value\_count; ++i) { const std::vector<TI>& index = indices[i]; TFLITE\_DCHECK\_EQ(index.size(), 4); const T value = values[i]; output\_data[Offset(output\_shape, index[0], index[1], index[2], index[3])] = value; }}
template <typename T>inline void Pow(const RuntimeShape& input1\_shape, const T\* input1\_data, const RuntimeShape& input2\_shape, const T\* input2\_data, const RuntimeShape& output\_shape, T\* output\_data) { const int flat\_size = MatchingFlatSize(input1\_shape, input2\_shape, output\_shape); for (int i = 0; i < flat\_size; ++i) { output\_data[i] = std::pow(input1\_data[i], input2\_data[i]); }}
template <typename T>inline void BroadcastPow4DSlow(const RuntimeShape& unextended\_input1\_shape, const T\* input1\_data, const RuntimeShape& unextended\_input2\_shape, const T\* input2\_data, const RuntimeShape& unextended\_output\_shape, T\* output\_data) { TFLITE\_DCHECK\_LE(unextended\_input1\_shape.DimensionsCount(), 4); TFLITE\_DCHECK\_LE(unextended\_input2\_shape.DimensionsCount(), 4); TFLITE\_DCHECK\_LE(unextended\_output\_shape.DimensionsCount(), 4); const RuntimeShape output\_shape = RuntimeShape::ExtendedShape(4, unextended\_output\_shape);
 NdArrayDesc<4> desc1; NdArrayDesc<4> desc2; NdArrayDescsForElementwiseBroadcast(unextended\_input1\_shape, unextended\_input2\_shape, &desc1, &desc2);
 for (int b = 0; b < output\_shape.Dims(0); ++b) { for (int y = 0; y < output\_shape.Dims(1); ++y) { for (int x = 0; x < output\_shape.Dims(2); ++x) { for (int c = 0; c < output\_shape.Dims(3); ++c) { auto out\_idx = Offset(output\_shape, b, y, x, c); auto in1\_idx = SubscriptToIndex(desc1, b, y, x, c); auto in2\_idx = SubscriptToIndex(desc2, b, y, x, c); auto in1\_val = input1\_data[in1\_idx]; auto in2\_val = input2\_data[in2\_idx]; output\_data[out\_idx] = std::pow(in1\_val, in2\_val); } } } }}
[View remainder of file in raw view](https://github.com/tensorflow/tensorflow/raw/266558ac4c1f361e9a178ee9d3f0ce2e648ae499/tensorflow/lite/kernels/internal/reference/reference_ops.h)

## Footer

© 2025 GitHub, Inc.

### Footer navigation

* [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)
* [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)
* [Security](https://github.com/security)
* [Status](https://www.githubstatus.com/)
* [Docs](https://docs.github.com/)
* [Contact](https://support.github.com?tags=dotcom-footer)
* Manage cookies
* Do not share my personal information

You can’t perform that action at this time.



=== Content from github.com_e0c619b3_20250115_084603.html ===

[Skip to content](#start-of-content)

## Navigation Menu

Toggle navigation

[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fsecurity%2Fadvisories%2FGHSA-ffjm-4qwc-7cmf)

* Product

  + [GitHub Copilot
    Write better code with AI](https://github.com/features/copilot)
  + [Security
    Find and fix vulnerabilities](https://github.com/features/security)
  + [Actions
    Automate any workflow](https://github.com/features/actions)
  + [Codespaces
    Instant dev environments](https://github.com/features/codespaces)
  + [Issues
    Plan and track work](https://github.com/features/issues)
  + [Code Review
    Manage code changes](https://github.com/features/code-review)
  + [Discussions
    Collaborate outside of code](https://github.com/features/discussions)
  + [Code Search
    Find more, search less](https://github.com/features/code-search)

  Explore
  + [All features](https://github.com/features)
  + [Documentation](https://docs.github.com)
  + [GitHub Skills](https://skills.github.com)
  + [Blog](https://github.blog)
* Solutions

  By company size
  + [Enterprises](https://github.com/enterprise)
  + [Small and medium teams](https://github.com/team)
  + [Startups](https://github.com/enterprise/startups)
  By use case
  + [DevSecOps](/solutions/use-case/devsecops)
  + [DevOps](/solutions/use-case/devops)
  + [CI/CD](/solutions/use-case/ci-cd)
  + [View all use cases](/solutions/use-case)

  By industry
  + [Healthcare](/solutions/industry/healthcare)
  + [Financial services](/solutions/industry/financial-services)
  + [Manufacturing](/solutions/industry/manufacturing)
  + [Government](/solutions/industry/government)
  + [View all industries](/solutions/industry)

  [View all solutions](/solutions)
* Resources

  Topics
  + [AI](/resources/articles/ai)
  + [DevOps](/resources/articles/devops)
  + [Security](/resources/articles/security)
  + [Software Development](/resources/articles/software-development)
  + [View all](/resources/articles)

  Explore
  + [Learning Pathways](https://resources.github.com/learn/pathways)
  + [White papers, Ebooks, Webinars](https://resources.github.com)
  + [Customer Stories](https://github.com/customer-stories)
  + [Partners](https://partner.github.com)
  + [Executive Insights](https://github.com/solutions/executive-insights)
* Open Source

  + [GitHub Sponsors
    Fund open source developers](/sponsors)
  + [The ReadME Project
    GitHub community articles](https://github.com/readme)
  Repositories
  + [Topics](https://github.com/topics)
  + [Trending](https://github.com/trending)
  + [Collections](https://github.com/collections)
* Enterprise

  + [Enterprise platform
    AI-powered developer platform](/enterprise)
  Available add-ons
  + [Advanced Security
    Enterprise-grade security features](https://github.com/enterprise/advanced-security)
  + [GitHub Copilot
    Enterprise-grade AI features](/features/copilot#enterprise)
  + [Premium Support
    Enterprise-grade 24/7 support](/premium-support)
* [Pricing](https://github.com/pricing)

Search or jump to...

# Search code, repositories, users, issues, pull requests...

Search

Clear

[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)

# Provide feedback

We read every piece of feedback, and take your input very seriously.

Include my email address so I can be contacted

  Cancel

 Submit feedback

# Saved searches

## Use saved searches to filter your results more quickly

Name

Query

To see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).

  Cancel

 Create saved search

[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fsecurity%2Fadvisories%2FGHSA-ffjm-4qwc-7cmf)

[Sign up](/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E%2Frepos%2Fadvisories%2Fshow&source=header-repo&source_repo=tensorflow%2Ftensorflow)
Reseting focus

You signed in with another tab or window. Reload to refresh your session.
You signed out in another tab or window. Reload to refresh your session.
You switched accounts on another tab or window. Reload to refresh your session.

Dismiss alert

{{ message }}

[tensorflow](/tensorflow)
/
**[tensorflow](/tensorflow/tensorflow)**
Public

* [Notifications](/login?return_to=%2Ftensorflow%2Ftensorflow) You must be signed in to change notification settings
* [Fork
  74.4k](/login?return_to=%2Ftensorflow%2Ftensorflow)
* [Star
   187k](/login?return_to=%2Ftensorflow%2Ftensorflow)

* [Code](/tensorflow/tensorflow)
* [Issues
  826](/tensorflow/tensorflow/issues)
* [Pull requests
  5k+](/tensorflow/tensorflow/pulls)
* [Actions](/tensorflow/tensorflow/actions)
* [Projects
  2](/tensorflow/tensorflow/projects)
* [Security](/tensorflow/tensorflow/security)
* [Insights](/tensorflow/tensorflow/pulse)

Additional navigation options

* [Code](/tensorflow/tensorflow)
* [Issues](/tensorflow/tensorflow/issues)
* [Pull requests](/tensorflow/tensorflow/pulls)
* [Actions](/tensorflow/tensorflow/actions)
* [Projects](/tensorflow/tensorflow/projects)
* [Security](/tensorflow/tensorflow/security)
* [Insights](/tensorflow/tensorflow/pulse)

# OOB write in `scatter\_nd` op in TF Lite

High

[pak-laura](/pak-laura)
published
GHSA-ffjm-4qwc-7cmf
Sep 15, 2022

## Package

pip

tensorflow, tensorflow-cpu, tensorflow-gpu
([pip](/advisories?query=ecosystem%3Apip))

## Affected versions

< 2.10.0

## Patched versions

2.7.4, 2.8.3, 2.9.2, 2.10.0

## Description

### Impact

The [`ScatterNd`](https://github.com/tensorflow/tensorflow/blob/266558ac4c1f361e9a178ee9d3f0ce2e648ae499/tensorflow/lite/kernels/internal/reference/reference_ops.h#L659-L698) function takes an input argument that determines the indices of of the output tensor. An input index greater than the output tensor or less than zero will either write content at the wrong index or trigger a crash.

### Patches

We have patched the issue in GitHub commit [b4d4b4cb019bd7240a52daa4ba61e3cc814f0384](https://github.com/tensorflow/tensorflow/commit/b4d4b4cb019bd7240a52daa4ba61e3cc814f0384).

The fix will be included in TensorFlow 2.10.0. We will also cherrypick this commit on TensorFlow 2.9.1, TensorFlow 2.8.1, and TensorFlow 2.7.2, as these are also affected and still in supported range.

### For more information

Please consult [our security guide](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.

### Attribution

This vulnerability has been reported by Hui Peng from Baidu Security.

### Severity

High

### CVE ID

CVE-2022-35939

### Weaknesses

No CWEs

## Footer

© 2025 GitHub, Inc.

### Footer navigation

* [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)
* [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)
* [Security](https://github.com/security)
* [Status](https://www.githubstatus.com/)
* [Docs](https://docs.github.com/)
* [Contact](https://support.github.com?tags=dotcom-footer)
* Manage cookies
* Do not share my personal information

You can’t perform that action at this time.



=== Content from github.com_402114db_20250115_084601.html ===

[Skip to content](#start-of-content)

## Navigation Menu

Toggle navigation

[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fcommit%2Fb4d4b4cb019bd7240a52daa4ba61e3cc814f0384)

* Product

  + [GitHub Copilot
    Write better code with AI](https://github.com/features/copilot)
  + [Security
    Find and fix vulnerabilities](https://github.com/features/security)
  + [Actions
    Automate any workflow](https://github.com/features/actions)
  + [Codespaces
    Instant dev environments](https://github.com/features/codespaces)
  + [Issues
    Plan and track work](https://github.com/features/issues)
  + [Code Review
    Manage code changes](https://github.com/features/code-review)
  + [Discussions
    Collaborate outside of code](https://github.com/features/discussions)
  + [Code Search
    Find more, search less](https://github.com/features/code-search)

  Explore
  + [All features](https://github.com/features)
  + [Documentation](https://docs.github.com)
  + [GitHub Skills](https://skills.github.com)
  + [Blog](https://github.blog)
* Solutions

  By company size
  + [Enterprises](https://github.com/enterprise)
  + [Small and medium teams](https://github.com/team)
  + [Startups](https://github.com/enterprise/startups)
  By use case
  + [DevSecOps](/solutions/use-case/devsecops)
  + [DevOps](/solutions/use-case/devops)
  + [CI/CD](/solutions/use-case/ci-cd)
  + [View all use cases](/solutions/use-case)

  By industry
  + [Healthcare](/solutions/industry/healthcare)
  + [Financial services](/solutions/industry/financial-services)
  + [Manufacturing](/solutions/industry/manufacturing)
  + [Government](/solutions/industry/government)
  + [View all industries](/solutions/industry)

  [View all solutions](/solutions)
* Resources

  Topics
  + [AI](/resources/articles/ai)
  + [DevOps](/resources/articles/devops)
  + [Security](/resources/articles/security)
  + [Software Development](/resources/articles/software-development)
  + [View all](/resources/articles)

  Explore
  + [Learning Pathways](https://resources.github.com/learn/pathways)
  + [White papers, Ebooks, Webinars](https://resources.github.com)
  + [Customer Stories](https://github.com/customer-stories)
  + [Partners](https://partner.github.com)
  + [Executive Insights](https://github.com/solutions/executive-insights)
* Open Source

  + [GitHub Sponsors
    Fund open source developers](/sponsors)
  + [The ReadME Project
    GitHub community articles](https://github.com/readme)
  Repositories
  + [Topics](https://github.com/topics)
  + [Trending](https://github.com/trending)
  + [Collections](https://github.com/collections)
* Enterprise

  + [Enterprise platform
    AI-powered developer platform](/enterprise)
  Available add-ons
  + [Advanced Security
    Enterprise-grade security features](https://github.com/enterprise/advanced-security)
  + [GitHub Copilot
    Enterprise-grade AI features](/features/copilot#enterprise)
  + [Premium Support
    Enterprise-grade 24/7 support](/premium-support)
* [Pricing](https://github.com/pricing)

Search or jump to...

# Search code, repositories, users, issues, pull requests...

Search

Clear

[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)

# Provide feedback

We read every piece of feedback, and take your input very seriously.

Include my email address so I can be contacted

  Cancel

 Submit feedback

# Saved searches

## Use saved searches to filter your results more quickly

Name

Query

To see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).

  Cancel

 Create saved search

[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fcommit%2Fb4d4b4cb019bd7240a52daa4ba61e3cc814f0384)

[Sign up](/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E%2Fvoltron%2Fcommit_fragments%2Frepo_layout&source=header-repo&source_repo=tensorflow%2Ftensorflow)
Reseting focus

You signed in with another tab or window. Reload to refresh your session.
You signed out in another tab or window. Reload to refresh your session.
You switched accounts on another tab or window. Reload to refresh your session.

Dismiss alert

{{ message }}

[tensorflow](/tensorflow)
/
**[tensorflow](/tensorflow/tensorflow)**
Public

* [Notifications](/login?return_to=%2Ftensorflow%2Ftensorflow) You must be signed in to change notification settings
* [Fork
  74.4k](/login?return_to=%2Ftensorflow%2Ftensorflow)
* [Star
   187k](/login?return_to=%2Ftensorflow%2Ftensorflow)

* [Code](/tensorflow/tensorflow)
* [Issues
  826](/tensorflow/tensorflow/issues)
* [Pull requests
  5k+](/tensorflow/tensorflow/pulls)
* [Actions](/tensorflow/tensorflow/actions)
* [Projects
  2](/tensorflow/tensorflow/projects)
* [Security](/tensorflow/tensorflow/security)
* [Insights](/tensorflow/tensorflow/pulse)

Additional navigation options

* [Code](/tensorflow/tensorflow)
* [Issues](/tensorflow/tensorflow/issues)
* [Pull requests](/tensorflow/tensorflow/pulls)
* [Actions](/tensorflow/tensorflow/actions)
* [Projects](/tensorflow/tensorflow/projects)
* [Security](/tensorflow/tensorflow/security)
* [Insights](/tensorflow/tensorflow/pulse)

## Commit

[Permalink](/tensorflow/tensorflow/commit/b4d4b4cb019bd7240a52daa4ba61e3cc814f0384)

This commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.

Check bounds for reads and writes in scatter\_nd

[Browse files](/tensorflow/tensorflow/tree/b4d4b4cb019bd7240a52daa4ba61e3cc814f0384)
Browse the repository at this point in the history

```
PiperOrigin-RevId: 463365479
```

* Loading branch information

[![@alankelly](https://avatars.githubusercontent.com/u/5112267?s=40&v=4)](/alankelly) [![@tensorflower-gardener](https://avatars.githubusercontent.com/u/17151892?s=40&v=4)](/tensorflower-gardener)

[alankelly](/tensorflow/tensorflow/commits?author=alankelly "View all commits by alankelly")
authored and
[tensorflower-gardener](/tensorflow/tensorflow/commits?author=tensorflower-gardener "View all commits by tensorflower-gardener")
committed
Jul 26, 2022

1 parent
[b6d1794](/tensorflow/tensorflow/commit/b6d17941071244562ce3eb4cba235a91d4b4d326)

commit b4d4b4c

 Show file tree

 Hide file tree

Showing
**3 changed files**
with
**60 additions**
and
**14 deletions**.

* Whitespace
* Ignore whitespace

* Split
* Unified

* tensorflow/lite/kernels

  + internal/reference

    - tensorflow/lite/kernels/internal/reference/reference\_ops.h
      [reference\_ops.h](#diff-ace91ec9c4d8bcb9c3da92349d7875fe22d33d42ed341db22bd6cb6c939c030d)
  + tensorflow/lite/kernels/scatter\_nd.cc
    [scatter\_nd.cc](#diff-2d4f550aa904028ca9ec671d3fe0fc9e290ec12af3634d388e9cb9e932c80e35)
  + tensorflow/lite/kernels/scatter\_nd\_test.cc
    [scatter\_nd\_test.cc](#diff-0ea7953df1096059068616b7cebfdef584a780e6e9ca580943b0a49d956d5105)

## There are no files selected for viewing

19 changes: 13 additions & 6 deletions

19
[tensorflow/lite/kernels/internal/reference/reference\_ops.h](#diff-ace91ec9c4d8bcb9c3da92349d7875fe22d33d42ed341db22bd6cb6c939c030d "tensorflow/lite/kernels/internal/reference/reference_ops.h")

Show comments

[View file](/tensorflow/tensorflow/blob/b4d4b4cb019bd7240a52daa4ba61e3cc814f0384/tensorflow/lite/kernels/internal/reference/reference_ops.h)
Edit file

Delete file

This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters.
[Learn more about bidirectional Unicode characters](https://github.co/hiddenchars)

  [Show hidden characters](%7B%7B%20revealButtonHref%20%7D%7D)

| Original file line number | Diff line number | Diff line change |
| --- | --- | --- |
| Expand Up | | @@ -656,11 +656,12 @@ inline TfLiteStatus GatherNdString(const RuntimeShape& params\_shape, |
|  |  | #endif |
|  |  |  |
|  |  | template <typename IndicesT, typename UpdatesT> |
|  |  | inline void ScatterNd(const RuntimeShape& indices\_shape, |
|  |  | const IndicesT\* indices\_data, |
|  |  | const RuntimeShape& updates\_shape, |
|  |  | const UpdatesT\* updates\_data, |
|  |  | const RuntimeShape& output\_shape, UpdatesT\* output\_data) { |
|  |  | inline TfLiteStatus ScatterNd(const RuntimeShape& indices\_shape, |
|  |  | const IndicesT\* indices\_data, |
|  |  | const RuntimeShape& updates\_shape, |
|  |  | const UpdatesT\* updates\_data, |
|  |  | const RuntimeShape& output\_shape, |
|  |  | UpdatesT\* output\_data) { |
|  |  | ruy::profiler::ScopeLabel label("ScatterNd"); |
|  |  |  |
|  |  | int n\_slices = 1; |
| Expand All | | @@ -683,18 +684,24 @@ inline void ScatterNd(const RuntimeShape& indices\_shape, |
|  |  | remain\_flat\_size = dims\_to\_count[i]; |
|  |  | } |
|  |  |  |
|  |  | if (n\_slices \* slice\_size > updates\_shape.FlatSize()) { |
|  |  | return kTfLiteError; |
|  |  | } |
|  |  | memset(output\_data, 0, sizeof(UpdatesT) \* output\_flat\_size); |
|  |  | for (int i = 0; i < n\_slices; ++i) { |
|  |  | int to\_pos = 0; |
|  |  | for (int j = 0; j < indices\_nd; ++j) { |
|  |  | IndicesT idx = indices\_data[i \* indices\_nd + j]; |
|  |  | TFLITE\_DCHECK(0 <= idx && idx < output\_shape.Dims(j)); |
|  |  | to\_pos += idx \* dims\_to\_count[j]; |
|  |  | } |
|  |  | if (to\_pos < 0 || to\_pos + slice\_size > output\_flat\_size) { |
|  |  | return kTfLiteError; |
|  |  | } |
|  |  | for (int j = 0; j < slice\_size; j++) { |
|  |  | output\_data[to\_pos + j] += updates\_data[i \* slice\_size + j]; |
|  |  | } |
|  |  | } |
|  |  | return kTfLiteOk; |
|  |  | } |
|  |  |  |
|  |  | template <typename T> |
| Expand Down | |  |

26 changes: 18 additions & 8 deletions

26
[tensorflow/lite/kernels/scatter\_nd.cc](#diff-2d4f550aa904028ca9ec671d3fe0fc9e290ec12af3634d388e9cb9e932c80e35 "tensorflow/lite/kernels/scatter_nd.cc")

Show comments

[View file](/tensorflow/tensorflow/blob/b4d4b4cb019bd7240a52daa4ba61e3cc814f0384/tensorflow/lite/kernels/scatter_nd.cc)
Edit file

Delete file

This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters.
[Learn more about bidirectional Unicode characters](https://github.co/hiddenchars)

  [Show hidden characters](%7B%7B%20revealButtonHref%20%7D%7D)

| Original file line number | Diff line number | Diff line change |
| --- | --- | --- |
| Expand Up | | @@ -129,11 +129,10 @@ TfLiteStatus Prepare(TfLiteContext\* context, TfLiteNode\* node) { |
|  |  | template <typename IndicesT, typename UpdatesT> |
|  |  | TfLiteStatus ScatterNd(const TfLiteTensor\* indices, const TfLiteTensor\* updates, |
|  |  | TfLiteTensor\* output) { |
|  |  | reference\_ops::ScatterNd( |
|  |  | return reference\_ops::ScatterNd( |
|  |  | GetTensorShape(indices), GetTensorData<IndicesT>(indices), |
|  |  | GetTensorShape(updates), GetTensorData<UpdatesT>(updates), |
|  |  | GetTensorShape(output), GetTensorData<UpdatesT>(output)); |
|  |  | return kTfLiteOk; |
|  |  | } |
|  |  |  |
|  |  | template <typename IndicesT> |
| Expand All | | @@ -149,25 +148,36 @@ TfLiteStatus EvalScatterNd(TfLiteContext\* context, const TfLiteTensor\* indices, |
|  |  | ResizeOutputTensor<IndicesT>(context, shape, output)); |
|  |  | } |
|  |  |  |
|  |  | TfLiteStatus status = kTfLiteError; |
|  |  | switch (updates->type) { |
|  |  | case kTfLiteFloat32: |
|  |  | return ScatterNd<IndicesT, float>(indices, updates, output); |
|  |  | status = ScatterNd<IndicesT, float>(indices, updates, output); |
|  |  | break; |
|  |  | case kTfLiteUInt8: |
|  |  | return ScatterNd<IndicesT, uint8\_t>(indices, updates, output); |
|  |  | status = ScatterNd<IndicesT, uint8\_t>(indices, updates, output); |
|  |  | break; |
|  |  | case kTfLiteBool: |
|  |  | return ScatterNd<IndicesT, bool>(indices, updates, output); |
|  |  | status = ScatterNd<IndicesT, bool>(indices, updates, output); |
|  |  | break; |
|  |  | case kTfLiteInt8: |
|  |  | return ScatterNd<IndicesT, int8\_t>(indices, updates, output); |
|  |  | status = ScatterNd<IndicesT, int8\_t>(indices, updates, output); |
|  |  | break; |
|  |  | case kTfLiteInt32: |
|  |  | return ScatterNd<IndicesT, int32\_t>(indices, updates, output); |
|  |  | status = ScatterNd<IndicesT, int32\_t>(indices, updates, output); |
|  |  | break; |
|  |  | case kTfLiteInt64: |
|  |  | return ScatterNd<IndicesT, int64\_t>(indices, updates, output); |
|  |  | status = ScatterNd<IndicesT, int64\_t>(indices, updates, output); |
|  |  | break; |
|  |  | default: |
|  |  | TF\_LITE\_KERNEL\_LOG( |
|  |  | context, "Updates of type '%s' are not supported by scatter\_nd.", |
|  |  | TfLiteTypeGetName(updates->type)); |
|  |  | return kTfLiteError; |
|  |  | } |
|  |  | if (status != kTfLiteOk) { |
|  |  | TF\_LITE\_KERNEL\_LOG(context, "scatter\_nd index out of bounds"); |
|  |  | } |
|  |  | return status; |
|  |  | } |
|  |  |  |
|  |  | TfLiteStatus Eval(TfLiteContext\* context, TfLiteNode\* node) { |
| Expand Down | |  |

29 changes: 29 additions & 0 deletions

29
[tensorflow/lite/kernels/scatter\_nd\_test.cc](#diff-0ea7953df1096059068616b7cebfdef584a780e6e9ca580943b0a49d956d5105 "tensorflow/lite/kernels/scatter_nd_test.cc")

Show comments

[View file](/tensorflow/tensorflow/blob/b4d4b4cb019bd7240a52daa4ba61e3cc814f0384/tensorflow/lite/kernels/scatter_nd_test.cc)
Edit file

Delete file

This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters.
[Learn more about bidirectional Unicode characters](https://github.co/hiddenchars)

  [Show hidden characters](%7B%7B%20revealButtonHref%20%7D%7D)

| Original file line number | Diff line number | Diff line change |
| --- | --- | --- |
| Expand Up | | @@ -361,5 +361,34 @@ TEST(ScatterNdOpTest, DynamicShape) { |
|  |  | /\*2, 3\*/ 1, 2, 3, 4, 5})); |
|  |  | } |
|  |  |  |
|  |  | TEST(ScatterNdOpTest, ReadAndWriteArrayLimits) { |
|  |  | ScatterNdOpModel m({TensorType\_INT32, {5, 1}}, {TensorType\_INT32, {5}}, |
|  |  | {TensorType\_INT32, {1}}); |
|  |  | m.SetIndices<int32\_t>({4, 3, 1, 0, 2}); |
|  |  | m.SetUpdates<int32\_t>({1, 2, 3, 7, 9}); |
|  |  | m.SetShape<int32\_t>({5}); |
|  |  | ASSERT\_EQ(m.Invoke(), kTfLiteOk); |
|  |  | EXPECT\_THAT(m.GetOutputShape(), ElementsAreArray({5})); |
|  |  | EXPECT\_THAT(m.GetOutput<int32\_t>(), ElementsAreArray({7, 3, 9, 2, 1})); |
|  |  | } |
|  |  |  |
|  |  | TEST(ScatterNdOpTest, OOBRead) { |
|  |  | ScatterNdOpModel m({TensorType\_INT32, {1, 1}}, {TensorType\_INT32, {1}}, |
|  |  | {TensorType\_INT32, {1}}); |
|  |  | m.SetIndices<int32\_t>({4}); |
|  |  | m.SetUpdates<int32\_t>({1}); |
|  |  | m.SetShape<int32\_t>({1}); |
|  |  | ASSERT\_EQ(m.Invoke(), kTfLiteError); |
|  |  | } |
|  |  |  |
|  |  | TEST(ScatterNdOpTest, OOBWrites) { |
|  |  | ScatterNdOpModel m({TensorType\_INT32, {5, 1}}, {TensorType\_INT32, {5}}, |
|  |  | {TensorType\_INT32, {1}}); |
|  |  | m.SetIndices<int32\_t>({4, 3, 1, -0x38, 0x38}); |
|  |  | m.SetUpdates<int32\_t>({1, 2, 3, 0x44444444, 0x55555555}); |
|  |  | m.SetShape<int32\_t>({1}); |
|  |  | ASSERT\_EQ(m.Invoke(), kTfLiteError); |
|  |  | } |
|  |  |  |
|  |  | } // namespace |
|  |  | } // namespace tflite |

Toggle all file notes
Toggle all file annotations

### 0 comments on commit `b4d4b4c`

Please
[sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fcommit%2Fb4d4b4cb019bd7240a52daa4ba61e3cc814f0384) to comment.

## Footer

© 2025 GitHub, Inc.

### Footer navigation

* [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)
* [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)
* [Security](https://github.com/security)
* [Status](https://www.githubstatus.com/)
* [Docs](https://docs.github.com/)
* [Contact](https://support.github.com?tags=dotcom-footer)
* Manage cookies
* Do not share my personal information

You can’t perform that action at this time.


