=== Content from docs.vmware.com_eda197bd_20250115_202032.html ===


MENU

* [Products](https://www.broadcom.com/products/)
* [Solutions](https://www.broadcom.com/solutions/)
* [Support and Services](https://www.broadcom.com/support/)
* [Company](https://www.broadcom.com/company/)
* [How To Buy](https://www.broadcom.com/how-to-buy/)
* Login

  myBroadcom Account:
  [Login](https://login.broadcom.com/app/broadcomincexternal_techdocspublish_1/exk1fi5x8nkYLbeev1d8/sso/saml)
  [Register](https://profile.broadcom.com/web/registration)

  [Forgot
  Username/Password?](https://profile.broadcom.com/web/forgot-password)
* Username

  [Edit My
  Profile](https://portal.broadcom.com/group/user/editprofile)
  [myBroadcom](https://support.broadcom.com/user/user_redirect?dest=user)
  [Logout](/system/sling/logout.html)
* Language

  + [English](https://www.broadcom.com)
  + [日本語](https://jp.broadcom.com)
  + [中文](https://www.broadcom.cn)

* Login

  myBroadcom Account:
  [Login](https://login.broadcom.com/app/broadcomincexternal_techdocspublish_1/exk1fi5x8nkYLbeev1d8/sso/saml)
  [Register](https://profile.broadcom.com/web/registration)

  [Forgot
  Username/Password?](https://profile.broadcom.com/web/forgot-password)
* Username

  [Edit My
  Profile](https://portal.broadcom.com/group/user/editprofile)
  [myBroadcom](https://support.broadcom.com/user/user_redirect?dest=user)
  [Logout](/system/sling/logout.html)
* English
  [日本語](https://jp.broadcom.com)
  [中文](https://www.broadcom.cn)

* [Home](/)
* [VMware® Cloud Infrastructure Software](/us/en/vmware-cis.html)
* [VMware vSphere](/us/en/vmware-cis/vsphere.html)
* [VMware vSphere 8.0](/us/en/vmware-cis/vsphere/vsphere/8-0.html)
* [Release Notes](/us/en/vmware-cis/vsphere/vsphere/8-0/release-notes.html)
* [ESXi Update and Patch Release Notes](/us/en/vmware-cis/vsphere/vsphere/8-0/release-notes/esxi-update-and-patch-release-notes.html)
* [VMware ESXi 8.0a Release Notes](/us/en/vmware-cis/vsphere/vsphere/8-0/release-notes/esxi-update-and-patch-release-notes/vsphere-esxi-80a-release-notes.html)

# [VMware vSphere 8.0](/us/en/vmware-cis/vsphere/vsphere/8-0.html)

Version

[8.0](/us/en/vmware-cis/vsphere/vsphere/8-0/release-notes/esxi-update-and-patch-release-notes/vsphere-esxi-80a-release-notes.html)
[7.0](/us/en/vmware-cis/vsphere/vsphere/7-0.html)
[6.7](/us/en/vmware-cis/vsphere/vsphere/6-7.html)
[6.5](/us/en/vmware-cis/vsphere/vsphere/6-5.html)

Open/Close Topics Navigation

Product Menu

### Topics

# VMware ESXi 8.0a Release Notes

This document contains the following sections

* [Introduction](/us/en/vmware-cis/vsphere/vsphere/8-0/release-notes/esxi-update-and-patch-release-notes/vsphere-esxi-80a-release-notes.html#GUID-88e2de87-fea9-45d2-8fec-7dd1dbb2a6d2-en_id-58b6e270-0ab4-4838-9923-a56941b00579)
* [What's New](/us/en/vmware-cis/vsphere/vsphere/8-0/release-notes/esxi-update-and-patch-release-notes/vsphere-esxi-80a-release-notes.html#GUID-88e2de87-fea9-45d2-8fec-7dd1dbb2a6d2-en_id-af0f5303-6120-4ed4-a509-17899a7d9017)
* [Patches Contained in This Release](/us/en/vmware-cis/vsphere/vsphere/8-0/release-notes/esxi-update-and-patch-release-notes/vsphere-esxi-80a-release-notes.html#GUID-88e2de87-fea9-45d2-8fec-7dd1dbb2a6d2-en_id-1450efa1-92f4-44fd-9f45-e3e5c86f530e)
* [Resolved Issues](/us/en/vmware-cis/vsphere/vsphere/8-0/release-notes/esxi-update-and-patch-release-notes/vsphere-esxi-80a-release-notes.html#GUID-88e2de87-fea9-45d2-8fec-7dd1dbb2a6d2-en_id-8cee4404-3686-4c5c-922a-b1d950b47888)
* [Known Issues from Previous Releases](/us/en/vmware-cis/vsphere/vsphere/8-0/release-notes/esxi-update-and-patch-release-notes/vsphere-esxi-80a-release-notes.html#GUID-88e2de87-fea9-45d2-8fec-7dd1dbb2a6d2-en_id-73d4e49b-2882-41e0-9909-162af342aefc)

## Introduction

| VMware ESXi 8.0a | 08 DEC 2022 | Build 20842819 Check for additions and updates to these release notes. |
| --- |

## What's New

This patch addresses TPM-related security enhancements. These enhancements are necessary only for the upcoming CPU support in newer Lenovo and HPE platforms.
## Patches Contained in This Release

| **Download Filename**: | VMware-ESXi-8.0a-20842819-depot |
| --- | --- |
| **Build**: | 20842819 |
| **Download Size**: | 603.9 MB |
| **sha256checksum**: | 7d67771e856dab1f64c966748322576ddfd3c5a5bd293265367851a21e67f8ee |
| **Host Reboot Required**: | Yes |
| **Virtual Machine Migration or Shutdown Required**: | Yes |

**Components**

| **Component** | **Bulletin** | **Category** | **Severity** |
| --- | --- | --- | --- |
| ESXi | ESXi\_8.0.0-1.10.20842819 | Bugfix | Critical |
| --- | --- | --- | --- |
| ESXi Install/Upgrade Component | esx-update\_8.0.0-1.10.20842819 | Bugfix | Critical |
| ESXi Install/Upgrade Component | esxio-update\_8.0.0-1.10.20842819 | Bugfix | Critical |

**Rollup Bulletin**This rollup bulletin contains the latest VIBs with all the fixes since the initial release of ESXi 8.0.

| **Bulletin ID** | **Category** | **Severity** |
| --- | --- | --- |
| ESXi80a-20842819 | Bugfix | Critical |
| --- | --- | --- |

**Image Profiles**VMware patch and update releases contain general and critical image profiles. Application of the general release image profile applies to new bug fixes.

| **Image Profile Name** |
| --- |
| ESXi-8.0a-20842819-standard |
| --- |
| ESXi-8.0a-20842819-no-tools |

**ESXi Image**

| Name and Version | Release Date | Category | Detail |
| --- | --- | --- | --- |
| ESXi80a-20842819 | 12/08/2022 | Enhancement | Bugfix image |
| --- | --- | --- | --- |

For information about the individual components and bulletins, see the [Build Details](https://docs.vmware.com/en/VMware-vSphere/8.0/rn/vsphere-esxi-80a-release-notes/index.html#id-0a1aec7d-3794-439d-d1fd-e33fed5d4b9b) and the [Resolved Issues](https://docs.vmware.com/en/VMware-vSphere/8.0/rn/vsphere-esxi-80a-release-notes/index.html#Release-Note-Section-10261) section.For internationalization, compatibility, installation, upgrade, open source components and product support notices, see the [VMware vSphere 8.0 Release Notes](https://docs.vmware.com/en/VMware-vSphere/8.0/rn/vmware-vsphere-80-release-notes/index.html).**Patch Download and Installation**Log in to the [Broadcom Support Portal](https://support.broadcom.com/) to download this [patch](https://support.broadcom.com/web/ecx/solutiondetails?patchId=1256).
## Resolved Issues

## ESXi\_8.0.0-1.10.20842819

| **Patch Category** | Bugfix |
| --- | --- |
| **Patch Severity** | Critical |
| **Host Reboot Required** | Yes |
| **Virtual Machine Migration or Shutdown Required** | Yes |
| **Affected Hardware** | N/A |
| **Affected Software** | N/A |
| **VIBs Included** | * VMware\_bootbank\_vsan\_8.0.0-1.10.20842819 * VMware\_bootbank\_esx-xserver\_8.0.0-1.10.20842819 * VMware\_bootbank\_crx\_8.0.0-1.10.20842819 * VMware\_bootbank\_clusterstore\_8.0.0-1.10.20842819 * VMware\_bootbank\_gc-esxio\_8.0.0-1.10.20842819 * VMware\_bootbank\_esxio-base\_8.0.0-1.10.20842819 * VMware\_bootbank\_esxio-combiner-esxio\_8.0.0-1.10.20842819 * VMware\_bootbank\_vdfs\_8.0.0-1.10.20842819 * VMware\_bootbank\_esx-dvfilter-generic-fastpath\_8.0.0-1.10.20842819 * VMware\_bootbank\_drivervm-gpu\_8.0.0-1.10.20842819 * VMware\_bootbank\_esxio-combiner\_8.0.0-1.10.20842819 * VMware\_bootbank\_trx\_8.0.0-1.10.20842819 * VMware\_bootbank\_bmcal-esxio\_8.0.0-1.10.20842819 * VMware\_bootbank\_bmcal\_8.0.0-1.10.20842819 * VMware\_bootbank\_gc\_8.0.0-1.10.20842819 * VMware\_bootbank\_esxio\_8.0.0-1.10.20842819 * VMware\_bootbank\_esxio-dvfilter-generic-fastpath\_8.0.0-1.10.20842819 * VMware\_bootbank\_native-misc-drivers\_8.0.0-1.10.20842819 * VMware\_bootbank\_cpu-microcode\_8.0.0-1.10.20842819 * VMware\_bootbank\_native-misc-drivers-esxio\_8.0.0-1.10.20842819 * VMware\_bootbank\_esx-base\_8.0.0-1.10.20842819 * VMware\_bootbank\_vsanhealth\_8.0.0-1.10.20842819 |
| **PRs Fixed** | 3041101 |
| **CVE numbers** | N/A |

The ESXi and esx-update bulletins are dependent on each other. Always include both in a single ESXi host patch baseline or include the rollup bulletin in the baseline to avoid failure during host patching.Updates the **esx-ui, esx-xserver, cpu-microcode, trx, vsanhealth, esx-base, esx-dvfilter-generic-fastpath, esxio-dvfilter-generic-fastpath, gc, esxio-combiner, native-misc-drivers, native-misc-drivers-esxio, bmcal, vsan, vdfs, bmcal-esxio, clusterstore, gc-esxio, esxio-base, drivervm-gpu,** and **crx** VIBs to resolve the following issue:**When TPM 2.0 is enabled with TXT on an ESXi host, attempts to power-on a virtual machine might fail**When TXT is enabled on an ESX host, attempts to power-on a VM might fail with an error. In the vSphere Client, you see a message such as **This host supports Intel VT-x, but Intel VT-x is restricted. Intel VT-x might be restricted because 'trusted execution' has been enabled in the BIOS/firmware settings or because the host has not been power-cycled since changing this setting**.This issue is resolved in this release.
## esx-update\_8.0.0-1.10.20842819

| **Patch Category** | Bugfix |
| --- | --- |
| **Patch Severity** | Critical |
| **Host Reboot Required** | Yes |
| **Virtual Machine Migration or Shutdown Required** | Yes |
| **Affected Hardware** | N/A |
| **Affected Software** | N/A |
| **VIBs Included** | * VMware\_bootbank\_loadesx\_8.0.0-1.10.20842819 * VMware\_bootbank\_esx-update\_8.0.0-1.10.20842819 |
| **PRs Fixed** | N/A |
| **CVE numbers** | N/A |

Updates the **loadesx** and **esx-update** VIBs.
## esxio-update\_8.0.0-1.10.20842819

| **Patch Category** | Bugfix |
| --- | --- |
| **Patch Severity** | Critical |
| **Host Reboot Required** | Yes |
| **Virtual Machine Migration or Shutdown Required** | Yes |
| **Affected Hardware** | N/A |
| **Affected Software** | N/A |
| **VIBs Included** | * VMware\_bootbank\_loadesxio\_8.0.0-1.10.20842819 * VMware\_bootbank\_esxio-update\_8.0.0-1.10.20842819 |
| **PRs Fixed** | N/A |
| **CVE numbers** | N/A |

Updates the**loadesxio**and **esxio-update** VIBs.
## ESXi-8.0a-20842819-standard

| **Profile Name** | ESXi-8.0a-20842819-standard |
| --- | --- |
| **Build** | For build information, see [Build Details](https://docs.vmware.com/en/VMware-vSphere/8.0/rn/vsphere-esxi-80a-release-notes/index.html#Release-Note-Section-10890). |
| **Vendor** | VMware, Inc. |
| **Release Date** | December 8, 2022 |
| **Acceptance Level** | PartnerSupported |
| **Affected Hardware** | N/A |
| **Affected Software** | N/A |
| **Affected VIBs** | * VMware\_bootbank\_vsan\_8.0.0-1.10.20842819 * VMware\_bootbank\_esx-xserver\_8.0.0-1.10.20842819 * VMware\_bootbank\_crx\_8.0.0-1.10.20842819 * VMware\_bootbank\_clusterstore\_8.0.0-1.10.20842819 * VMware\_bootbank\_gc-esxio\_8.0.0-1.10.20842819 * VMware\_bootbank\_esxio-base\_8.0.0-1.10.20842819 * VMware\_bootbank\_esxio-combiner-esxio\_8.0.0-1.10.20842819 * VMware\_bootbank\_vdfs\_8.0.0-1.10.20842819 * VMware\_bootbank\_esx-dvfilter-generic-fastpath\_8.0.0-1.10.20842819 * VMware\_bootbank\_drivervm-gpu\_8.0.0-1.10.20842819 * VMware\_bootbank\_esxio-combiner\_8.0.0-1.10.20842819 * VMware\_bootbank\_trx\_8.0.0-1.10.20842819 * VMware\_bootbank\_bmcal-esxio\_8.0.0-1.10.20842819 * VMware\_bootbank\_bmcal\_8.0.0-1.10.20842819 * VMware\_bootbank\_gc\_8.0.0-1.10.20842819 * VMware\_bootbank\_esxio\_8.0.0-1.10.20842819 * VMware\_bootbank\_esxio-dvfilter-generic-fastpath\_8.0.0-1.10.20842819 * VMware\_bootbank\_native-misc-drivers\_8.0.0-1.10.20842819 * VMware\_bootbank\_cpu-microcode\_8.0.0-1.10.20842819 * VMware\_bootbank\_native-misc-drivers-esxio\_8.0.0-1.10.20842819 * VMware\_bootbank\_esx-base\_8.0.0-1.10.20842819 * VMware\_bootbank\_vsanhealth\_8.0.0-1.10.20842819 * VMware\_bootbank\_loadesx\_8.0.0-1.10.20842819 * VMware\_bootbank\_esx-update\_8.0.0-1.10.20842819 * VMware\_bootbank\_loadesxio\_8.0.0-1.10.20842819 * VMware\_bootbank\_esxio-update\_8.0.0-1.10.20842819 |
| **PRs Fixed** | 3041101 |
| **Related CVE numbers** | N/A |

**This patch updates the following issue:**

* **When TPM 2.0 is enabled with TXT on an ESXi host, attempts to power-on a virtual machine might fail**When TXT is enabled on an ESX host, attempts to power-on a VM might fail with an error. In the vSphere Client, you see a message such as **This host supports Intel VT-x, but Intel VT-x is restricted. Intel VT-x might be restricted because 'trusted execution' has been enabled in the BIOS/firmware settings or because the host has not been power-cycled since changing this setting**.This issue is resolved in this release.
## `ESXi-8.0a-20842819-no-tools

| **Profile Name** | ESXi-8.0a-20842819-no-tools |
| --- | --- |
| **Build** | For build information, see [Build Details](https://docs.vmware.com/en/VMware-vSphere/8.0/rn/vsphere-esxi-80a-release-notes/index.html#Release-Note-Section-10890). |
| **Vendor** | VMware, Inc. |
| **Release Date** | December 8, 2022 |
| **Acceptance Level** | PartnerSupported |
| **Affected Hardware** | N/A |
| **Affected Software** | N/A |
| **Affected VIBs** | * VMware\_bootbank\_vsan\_8.0.0-1.10.20842819 * VMware\_bootbank\_esx-xserver\_8.0.0-1.10.20842819 * VMware\_bootbank\_crx\_8.0.0-1.10.20842819 * VMware\_bootbank\_clusterstore\_8.0.0-1.10.20842819 * VMware\_bootbank\_gc-esxio\_8.0.0-1.10.20842819 * VMware\_bootbank\_esxio-base\_8.0.0-1.10.20842819 * VMware\_bootbank\_esxio-combiner-esxio\_8.0.0-1.10.20842819 * VMware\_bootbank\_vdfs\_8.0.0-1.10.20842819 * VMware\_bootbank\_esx-dvfilter-generic-fastpath\_8.0.0-1.10.20842819 * VMware\_bootbank\_drivervm-gpu\_8.0.0-1.10.20842819 * VMware\_bootbank\_esxio-combiner\_8.0.0-1.10.20842819 * VMware\_bootbank\_trx\_8.0.0-1.10.20842819 * VMware\_bootbank\_bmcal-esxio\_8.0.0-1.10.20842819 * VMware\_bootbank\_bmcal\_8.0.0-1.10.20842819 * VMware\_bootbank\_gc\_8.0.0-1.10.20842819 * VMware\_bootbank\_esxio\_8.0.0-1.10.20842819 * VMware\_bootbank\_esxio-dvfilter-generic-fastpath\_8.0.0-1.10.20842819 * VMware\_bootbank\_native-misc-drivers\_8.0.0-1.10.20842819 * VMware\_bootbank\_cpu-microcode\_8.0.0-1.10.20842819 * VMware\_bootbank\_native-misc-drivers-esxio\_8.0.0-1.10.20842819 * VMware\_bootbank\_esx-base\_8.0.0-1.10.20842819 * VMware\_bootbank\_vsanhealth\_8.0.0-1.10.20842819 * VMware\_bootbank\_loadesx\_8.0.0-1.10.20842819 * VMware\_bootbank\_esx-update\_8.0.0-1.10.20842819 * VMware\_bootbank\_loadesxio\_8.0.0-1.10.20842819 * VMware\_bootbank\_esxio-update\_8.0.0-1.10.20842819 |
| **PRs Fixed** | 3041101 |
| **Related CVE numbers** | N/A |

**This patch updates the following issue:**

* **When TPM 2.0 is enabled with TXT on an ESXi host, attempts to power-on a virtual machine might fail**When TXT is enabled on an ESX host, attempts to power-on a VM might fail with an error. In the vSphere Client, you see a message such as **This host supports Intel VT-x, but Intel VT-x is restricted. Intel VT-x might be restricted because 'trusted execution' has been enabled in the BIOS/firmware settings or because the host has not been power-cycled since changing this setting**.This issue is resolved in this release.
## ESXi80a-20842819

| **Name** | ESXi |
| --- | --- |
| **Version** | ESXi80a-20842819 |
| **Release Date** | December 8, 2022 |
| **Category** | Bugfix |
| **Affected Components** | * ESXi Component - core ESXi VIBs * ESXi Install/Upgrade Component |
| **PRs Fixed** | 3041101 |
| **Related CVE numbers** | N/A |

## Known Issues from Previous Releases

## Installation, Upgrade, and Migration Issues

**Second stage of vCenter Server restore procedure freezes at 90%**When you use the vCenter Server GUI installer or the vCenter Server Appliance Management Interface (VAMI) to restore a vCenter from a file-based backup, the restore workflow might freeze at 90% with an error 401 Unable to authenticate user, even though the task completes successfully in the backend. The issue occurs if the deployed machine has a different time than the NTP server, which requires a time sync. As a result of the time sync, clock skew might fail the running session of the GUI or VAMI.Workaround: If you use the GUI installer, you can get the restore status by using the restore.job.get command from the appliancesh shell. If you use VAMI, refresh your browser.**After upgrade to ESXi 8.0, you might lose some nmlx5\_core driver module settings due to obsolete parameters**Some module parameters for the nmlx5\_core driver, such as device\_rss, drss and rss, are deprecated in ESXi 8.0 and any custom values, different from the default values, are not kept after an upgrade to ESXi 8.0.Workaround: Replace the values of the device\_rss, drss and rss parameters as follows:

* device\_rss: Use the DRSS parameter.
* drss: Use the DRSS parameter.
* rss: Use the RSS parameter.
**If a vCenter Server Security Token Service (STS) refresh happens during upgrade to ESXi 8.0, the upgrade might fail**In vSphere 8.0, vCenter Single Sign-On automatically renews a VMCA-generated STS signing certificate. The auto-renewal occurs before the STS signing certificate expires and before triggering the 90-day expiration alarm. However, in long-running upgrade or remediation tasks by using a vSphere Lifecycle Manager image on multiple ESXi hosts in a cluster, vSphere Lifecycle Manager might create a cache of STS certificates internally. In very rare cases, if an STS certificates refresh task starts in parallel with the long-running upgrade or remediation task, the upgrade task might fail as the STS certificates in the internal cache might be different from the refreshed certificates. After the upgrade task fails, some ESXi hosts might remain in maintenance mode.Workaround: Manually exit any ESXi hosts in maintenance mode and retry the upgrade or remediation. Refreshing or importing and replacing the STS signing certificates happens automatically and does not require a vCenter Server restart, to avoid downtime.**VMNICs might be down after an upgrade to ESXi 8.0**If the peer physical switch of a VMNIC does not support the auto negotiate option, or the option is deactivated, and the VMNIC link is set down and then up, the link remains down after upgrade to or installation of ESXi 8.0.Workaround: Use either of these 2 options:

- Enable the option media-auto-detect in the BIOS settings by navigating to System Setup Main Menu, usually by pressing **F2** or opening a virtual console, and then **Device Settings** > *<specific broadcom NIC>* > **Device Configuration Menu** > **Media Auto Detect**. Reboot the host.
- Alternatively, use an ESXCLI command similar to: esxcli network nic set -S <your speed> -D full -n <your nic>. With this option, you also set a fixed speed to the link, and it does not require a reboot.
**You cannot use ESXi hosts of version 8.0 as a reference host for existing host profiles of earlier ESXi versions**Validation of existing host profiles for ESXi versions 7.x, 6.7.x and 6.5.x fails when only an 8.0 reference host is available in the inventory.Workaround: Make sure you have a reference host of the respective version in the inventory. For example, use an ESXi 7.0 Update 2 reference host to update or edit an ESXi 7.0 Update 2 host profile.**If you apply a host profile using a software FCoE configuration to an ESXi 8.0 host, the operation fails with a validation error**Starting from vSphere 7.0, software FCoE is deprecated, and in vSphere 8.0 software FCoE profiles are not supported. If you try to apply a host profile from an earlier version to an ESXi 8.0 host, for example to edit the host customization, the operation fails. In the vSphere Client, you see an error such as Host Customizations validation error.Workaround: Disable the Software FCoE Configuration subprofile in the host profile.
## Miscellaneous Issues

**Reset or restore of the ESXi system configuration in a vSphere system with DPUs might cause invalid state of the DPUs**If you reset or restore the ESXi system configuration in a vSphere system with DPUs, for example, by selecting **Reset System Configuration** in the direct console, the operation might cause invalid state of the DPUs. In the DCUI, you might see errors such as Failed to reset system configuration. Note that this operation cannot be performed when a managed DPU is present. A backend call to the -f force reboot option is not supported for ESXi installations with a DPU. Although ESXi 8.0 supports the -f force reboot option, if you use reboot -f on an ESXi configuration with a DPU, the forceful reboot might cause an invalid state.Workaround: Reinstall ESXi. Avoid resetting the ESXi system configuration in a vSphere system with DPUs.**In the vSphere API Explorer, VMware Datacenter CLI (DCLI) and PowerCLI, you see an API option "contentinternal" that is not functional**You see an API option **contentinternal** in the metadata of either the vSphere API Explorer, DCLI and PowerCLI. For example, when you open https://<your vCenter IP>/ui/app/devcenter/api-explorer, you see the option in the **select API** drop-down menu. This option is not functional.Workaround: Ignore the **contentinternal** API option and do not use it.**If you configure a VM at HW version earlier than 20 with a Vendor Device Group, such VMs might not work as expected**Vendor Device Groups, which enable binding of high-speed networking devices and the GPU, are supported only on VMs with HW version 20 and later, but you are not prevented to configure a VM at HW version earlier than 20 with a Vendor Device Group. Such VMs might not work as expected: for example, fail to power-on.Workaround: Ensure that VM HW version is of version 20 before you configure a Vendor Device Group in that VM.**If you deploy a virtual machine from an OVF file or from the Content Library, the number of cores per socket for the VM is set to 1**If you deploy a virtual machine from an OVF file or from the Content Library, instead of ESXi automatically selecting the number of cores per socket, the number is pre-set to 1.Workaround: You can manually set the number of cores per socket by using the vSphere Client.**You cannot remove a PCI passthrough device assigned to a virtual Non-Uniform Memory Access (NUMA) node from a virtual machine with CPU Hot Add enabled**Although by default when you enable CPU Hot Add to allow the addition of vCPUs to a running virtual machine, virtual NUMA topology is deactivated, if you have a PCI passthrough device assigned to a NUMA node, attempts to remove the device end with an error. In the vSphere Client, you see messages such as Invalid virtual machine configuration. Virtual NUMA cannot be configured when CPU hotadd is enabled.Workaround: None.**In the Virtual Appliance Management Interface (VAMI), you see a warning message during the pre-upgrade stage**Moving vSphere plug-ins to a remote plug-in architecture, vSphere 8.0 deprecates support for local plug-ins. If your 8.0 vSphere environment has local plug-ins, some breaking changes for such plug-ins might cause the pre-upgrade check by using VAMI to fail.In the Pre-Update Check Results screen, you see an error such as:Warning message: The compatibility of plug-in package(s) %s with the new vCenter Server version cannot be validated. They may not function properly after vCenter Server upgrade.Resolution: Please contact the plug-in vendor and make sure the package is compatible with the new vCenter Server version.Workaround: Refer to the [VMware Compatibility Guide](https://www.vmware.com/resources/compatibility/search.php) and [VMware Product Interoperability Matrix](http://partnerweb.vmware.com/comp_guide2/sim/interop_matrix.php) or contact the plug-in vendors for recommendations to make sure local plug-ins in your environment are compatible with vCenter Server 8.0 before you continue with the upgrade. For more information, see the blog [Deprecating the Local Plugins :- The Next Step in vSphere Client Extensibility Evolution](https://core.vmware.com/blog/deprecating-local-plugins-next-step-vsphere-client-extensibility-evolution) and VMware knowledge base article [87880](https://kb.vmware.com/s/article/87880).**Some ionic\_en driver uplinks might work with just a single receive queue and you see slower performance in native mode**Pensando Distributed Services Platform (DSC) adapters have 2 high speed ethernet controllers (for example vmnic6 and vmnic7) and one management controller (for example vmnic8)::~] esxcfg-nics -lvmnic6 0000:39:00.0 ionic\_en\_unstable Up 25000Mbps Full 00:ae:cd:09:c9:48 1500 Pensando Systems DSC-25 10/25G 2-port 4G RAM 8G eMMC G1 Services Card, Ethernet Controllervmnic7 0000:3a:00.0 ionic\_en\_unstable Up 25000Mbps Full 00:ae:cd:09:c9:49 1500 Pensando Systems DSC-25 10/25G 2-port 4G RAM 8G eMMC G1 Services Card, Ethernet Controller:~] esxcfg-nics -lSvmnic8 0000:3b:00.0 ionic\_en\_unstable Up 1000Mbps Full 00:ae:cd:09:c9:4a 1500 Pensando Systems DSC-25 10/25G 2-port 4G RAM 8G eMMC G1 Services Card, Management ControllerThe high-speed ethernet controllers vmnic6 and vmnic7 register first and operate with RSS set to 16 receive queues.:~] localcli --plugin-dir /usr/lib/vmware/esxcli/int networkinternal nic privstats get -n vmnic6…Num of RSS-Q=16, ntxq\_descs=2048, nrxq\_descs=1024, log\_level=3, vlan\_tx\_insert=1, vlan\_rx\_strip=1, geneve\_offload=1 }However, in rare cases, if the management controller vmnic8 registers first with the vSphere Distributed Switch, the high-speed ethernet controllers vmnic6 or vmnic7 uplink might end up operating with RSS set to 1 receive queue.:~] localcli --plugin-dir /usr/lib/vmware/esxcli/int networkinternal nic privstats get -n vmnic6…Num of RSS-Q=1, ntxq\_descs=2048, nrxq\_descs=1024, log\_level=3, vlan\_tx\_insert=1, vlan\_rx\_strip=1, geneve\_offload=1 }As a result, you might see slower performance in native mode.Workaround: Reload the ionic\_en driver on ESXi by using the following commands::~] esxcfg-module -u ionic\_en:~] esxcfg-module ionic\_en:~] localcli --plugin-dir /usr/lib/vmware/esxcli/int/ deviceInternal bind.**If an NVIDIA BlueField DPU is in hardware offload mode disabled, virtual machines with configured SR-IOV virtual function cannot power on**NVIDIA BlueField DPUs must be in hardware offload mode enabled to allow virtual machines with configured SR-IOV virtual function to power on and operate.Workaround: Always use the default hardware offload mode enabled for NVIDIA BlueField DPUs when you have VMs with configured SR-IOV virtual function connected to a virtual switch.**If you have an USB interface enabled in a remote management application that you use to install ESXi 8.0, you see an additional standard switch vSwitchBMC with uplink vusb0**Starting with vSphere 8.0, in both Integrated Dell Remote Access Controller (iDRAC) and HP Integrated Lights Out (ILO), when you have an USB interface enabled, vUSB or vNIC respectively, an additional standard switch vSwitchBMC with uplink vusb0 gets created on the ESXi host. This is expected, in view of the introduction of data processing units (DPUs) on some servers but might cause the VMware Cloud Foundation Bring-Up process to fail.Workaround: Before vSphere 8.0 installation, disable the USB interface in the remote management application that you use by following vendor documentation.After vSphere 8.0 installation, use the ESXCLI command esxcfg-advcfg -s 0 /Net/BMCNetworkEnable to prevent the creation of a virtual switch vSwitchBMC and associated portgroups on the next reboot of host.See this script as an example:~# esxcfg-advcfg -s 0 /Net/BMCNetworkEnableThe value of BMCNetworkEnable is 0 and the service is disabled.~# rebootOn host reboot, no virtual switch, PortGroup and VMKNIC are created in the host related to remote management application network.**You might see 10 min delay in rebooting an ESXi host on HPE server with pre-installed Pensando DPU**In rare cases, HPE servers with pre-installed Pensando DPU might take more than 10 minutes to reboot in case of a failure of the DPU. As a result, ESXi hosts might fail with a purple diagnostic screen and the default wait time is 10 minutes.Workaround: None.**In a vCenter Server system with DPUs, if IPv6 is disabled, you cannot manage DPUs**Although the vSphere Client allows the operation, if you disable IPv6 on an ESXi host with DPUs, you cannot use the DPUs, because the internal communication between the host and the devices depends on IPv6. The issue affects only ESXi hosts with DPUs.Workaround: Make sure IPv6 is enabled on ESXi hosts with DPUs.**If a PCI passthrough is active on a DPU during the shutdown or restart of an ESXi host, the host fails with a purple diagnostic screen** If an active virtual machine has a PCI passthrough (SR-IOV or UPT) to a DPU at the time of shutdown or reboot of an ESXi host, the host fails with a purple diagnostic screen. The issue is specific for systems with DPUs and only in case of VMs that use PCI passthrough to the DPU.Workaround: Before shutdown or reboot of an ESXi host, make sure the host is in maintenance mode, or that no VMs that use PCI passthrough to a DPU are running. If you use auto start options for a virtual machine, the Autostart manager stops such VMs before shutdown or reboot of a host.
## Networking Issues

**Pensando DPUs do not support Link Layer Discovery Protocol (LLDP) on physical switch ports of ESXi hosts**When you enable LLDP on an ESXi host with a DPU, the host cannot receive LLDP packets.Workaround: None.**You cannot use Mellanox ConnectX-5, ConnectX-6 cards Model 1 Level 2 and Model 2 for Enhanced Network Stack (ENS) mode in vSphere 8.0**Due to hardware limitations, Model 1 Level 2, and Model 2 for Enhanced Network Stack (ENS) mode in vSphere 8.0 is not supported in ConnectX-5 and ConnectX-6 adapter cards.Workaround: Use Mellanox ConnectX-6 Lx and ConnectX-6 Dx or later cards that support ENS Model 1 Level 2, and Model 2A.**If you do not reboot an ESXi host after you enable or disable SR-IOV with the icen driver, when you configure a transport node in ENS Interrupt mode on that host, some virtual machines might not get DHCP addresses**If you enable or disable SR-IOV with the icen driver on an ESXi host and configure a transport node in ENS Interrupt mode, some Rx (receive) queues might not work if you do not reboot the host. As a result, some virtual machines might not get DHCP addresses.Workaround: Either add a transport node profile directly, without enabling SR-IOV, or reboot the ESXi host after you enable or disable SR-IOV.**VMware NSX installation or upgrade in a vSphere environment with DPUs might fail with a connectivity error**An intermittent timing issue on the ESXi host side might cause NSX installation or upgrade in a vSphere environment with DPUs to fail. In the nsxapi.log file you see logs such as Failed to get SFHC response. MessageType MT\_SOFTWARE\_STATUS.Workaround: Wait for 10 min and retry the NSX install or upgrade.**You see link flapping on NICs that use the ntg3 driver of version 4.1.3 and later**When two NICs that use the ntg3 driver of versions 4.1.3 and later are connected directly, not to a physical switch port, link flapping might occur. The issue does not occur on ntg3 drivers of versions earlier than 4.1.3 or the tg3 driver. This issue is not related to the occasional Energy Efficient Ethernet (EEE) link flapping on such NICs. The fix for the EEE issue is to use a ntg3 driver of version 4.1.7 or later, or disable EEE on physical switch ports.Workaround: Upgrade the ntg3 driver to version 4.1.8 and set the new module parameter noPhyStateSet to 1. The noPhyStateSet parameter defaults to 0 and is not required in most environments, except they face the issue.**You cannot set the Maximum Transmission Unit (MTU) on a VMware vSphere Distributed Switch to a value larger than 9174 on a Pensando DPU**If you have the vSphere Distributed Services Engine feature with a Pensando DPU enabled on your ESXi 8.0 system, you cannot set the Maximum Transmission Unit (MTU) on a vSphere Distributed Switch to a value larger than 9174.Workaround: None.
## Storage Issues

**You cannot create snapshots of virtual machines due to an error in the Content Based Read Cache (CBRC) that a digest operation has failed**A rare race condition when assigning a content ID during the update of the CBRC digest file might cause a discrepancy between the content ID in the data disk and the digest disk. As a result, you cannot create virtual machine snapshots. You see an error such as An error occurred while saving the snapshot: A digest operation has failed in the backtrace. The snapshot creation task completes upon retry.Workaround: Retry the snapshot creation task.**vSphere Storage vMotion operations might fail in a vSAN environment due to an unauthenticated session of the Network File Copy (NFC) manager**Migrations to a vSAN datastore by using vSphere Storage vMotion of virtual machines that have at least one snapshot and more than one virtual disk with different storage policy might fail. The issue occurs due to an unauthenticated session of the NFC manager because the Simple Object Access Protocol (SOAP) body exceeds the allowed size.Workaround: First migrate the VM home namespace and just one of the virtual disks. After the operation completes, perform a disk only migration of the remaining 2 disks.**VASA API version does not automatically refresh after upgrade to vCenter Server 8.0**vCenter Server 8.0 supports VASA API version 4.0. However, after you upgrade your vCenter Server system to version 8.0, the VASA API version might not automatically change to 4.0. You see the issue in 2 cases:

- If a VASA provider that supports VASA API version 4.0 is registered with a previous version of VMware vCenter, the VASA API version remains unchanged after you upgrade to VMware vCenter 8.0. For example, if you upgrade a VMware vCenter system of version 7.x with a registered VASA provider that supports both VASA API versions 3.5 and 4.0, the VASA API version does not automatically change to 4.0, even though the VASA provider supports VASA API version 4.0. After the upgrade, when you navigate to **vCenter Server** > **Configure** > **Storage Providers** and expand the **General** tab of the registered VASA provider, you still see VASA API version 3.5.
- If you register a VASA provider that supports VASA API version 3.5 with a VMware vCenter 8.0 system and upgrade the VASA API version to 4.0, even after the upgrade, you still see VASA API version 3.5.
Workaround: Unregister and re-register the VASA provider on the VMware vCenter 8.0 system.
## vCenter Server and vSphere Client Issues

**If you use custom update repository with untrusted certificates, vCenter Server upgrade or update by using vCenter Lifecycle Manager workflows to vSphere 8.0 might fail**If you use a custom update repository with self-signed certificates that the VMware Certificate Authority (VMCA) does not trust, vCenter Lifecycle Manager fails to download files from such a repository. As a result, vCenter Server upgrade or update operations by using vCenter Lifecycle Manager workflows fail with the error Failed to load the repository manifest data for the configured upgrade.Workaround: Use CLI, the GUI installer, or the Virtual Appliance Management Interface (VAMI) to perform the upgrade. For more information, see VMware knowledge base article [90259](https://kb.vmware.com/s/article/90259).**ESXi hosts might become unresponsive, and you see a vpxa dump file due to a rare condition of insufficient file descriptors for the request queue on vpxa**In rare cases, when requests to the vpxa service take long, for example waiting for access to a slow datastore, the request queue on vpxa might exceed the limit of file descriptors. As a result, ESXi hosts might briefly become unresponsive, and you see a vpxa-zdump.00\* file in the /var/core directory. The vpxa logs contain the line Too many open files.Workaround: None. The vpxa service automatically restarts and corrects the issue.**You see an error for Cloud Native Storage (CNS) block volumes created by using API in a mixed vCenter environment**If your environment has vCenter Server systems of version 8.0 and 7.x, creating Cloud Native Storage (CNS) block volume by using API is successful, but you might see an error in the vSphere Client, when you navigate to see the CNS volume details. You see an error such as Failed to extract the requested data. Check vSphere Client logs for details. + TypeError: Cannot read properties of null (reading 'cluster'). The issue occurs only if you review volumes managed by the 7.x vCenter Server by using the vSphere Client of an 8.0 vCenter Server.Workaround: Log in to vSphere Client on a vCenter Server system of version 7.x to review the volume properties.**In the vSphere Client, you do not see banner notifications for historical data imports**Due to a backend issue, you do not see banner notifications for background migration of historical data in the vSphere Client.Workaround: Use the vCenter Server Management Interface as an alternative to the vSphere Client. For more information, see [Monitor and Manage Historical Data Migration](https://docs.vmware.com/en/VMware-vSphere/8.0/vsphere-vcenter-upgrade/GUID-82BF6D0C-211E-4862-AC6C-C7DEDA564D90.html).**If you load the vSphere virtual infrastructure to more than 90%, ESXi hosts might intermittently disconnect from vCenter Server**In rare occasions, if the vSphere virtual infrastructure is continuously using more than 90% of its hardware capacity, some ESXi hosts might intermittently disconnect from the vCenter Server. Connection typically restores within a few seconds.Workaround: If connection to vCenter Server accidentally does not restore in a few seconds, reconnect ESXi hosts manually by using vSphere Client.**The Utilization view of resource pools and clusters might not automatically refresh when you change the object**When you have already opened the **Utilization** view under the **Monitor** tab for a resource pool or a cluster and then you change the resource pool or cluster, the view might not automatically refresh. For example, when you open the **Utilization** view of one cluster and then select a different cluster, you might still see the statistics of the first cluster.Workaround: Click the refresh icon.
## Virtual Machine Management Issues

**When you add an existing virtual hard disk to a new virtual machine, you might see an error that the VM configuration is rejected**When you add an existing virtual hard disk to a new virtual machine by using the VMware Host Client, the operation might fail with an error such as The VM configuration was rejected. Please see browser Console. The issue occurs because the VMware Host Client might fail to get some properties, such as the hard disk controller.Workaround: After you select a hard disk and go to the **Ready to complete** page, do not click **Finish**. Instead, return one step back, wait for the page to load, and then click **Next** > **Finish**.
## Security Features Issues

**If you use an RSA key size smaller than 2048 bits, RSA signature generation fails**Starting from vSphere 8.0, ESXi uses the OpenSSL 3.0 FIPS provider. As part of the FIPS 186-4 requirement, the RSA key size must be at least 2048 bits for any signature generation, and signature generation with SHA1 is not supported.Workaround: Use RSA key size larger than 2048.
## vSphere Lifecycle Manager Issues

**If you use an ESXi host deployed from a host profile with enabled stateful install as an image to deploy other ESXi hosts in a cluster, the operation fails**If you extract an image of an ESXi host deployed from a host profile with enabled stateful install to deploy other ESXi hosts in a vSphere Lifecycle Manager cluster, the operation fails. In the vSphere Client, you see an error such as A general system error occurred: Failed to extract image from the host: no stored copy available for inactive VIB VMW\_bootbank\_xxx. Extraction of image from host xxx.eng.vmware.com failed.Workaround: Use a different host from the cluster to extract an image.**If a parallel remediation task fails, you do not see the correct number of ESXi hosts that passed or skipped the operation**With vSphere 8.0, you can enable vSphere Lifecycle Manager to remediate all hosts that are in maintenance mode in parallel instead of in sequence. However, if a parallel remediation task fails, in the vSphere Client you might not see the correct number of hosts that passed, failed, or skipped the operation, or even not see such counts at all. The issue does not affect the vSphere Lifecycle Manager functionality, but only the reporting in the vSphere Client.Workaround: None.**You see error messages when try to stage vSphere Lifecycle Manager Images on ESXi hosts of version earlier than 8.0**ESXi 8.0 introduces the option to explicitly stage desired state images, which is the process of downloading depot components from the vSphere Lifecycle Manager depot to the ESXi hosts without applying the software and firmware updates immediately. However, staging of images is only supported on an ESXi 8.0 or later hosts. Attempting to stage a vSphere Lifecycle Manager image on ESXi hosts of version earlier than 8.0 results in messages that the staging of such hosts fails, and the hosts are skipped. This is expected behavior and does not indicate any failed functionality as all ESXi 8.0 or later hosts are staged with the specified desired image.Workaround: None. After you confirm that the affected ESXi hosts are of version earlier than 8.0, ignore the errors.**A remediation task by using vSphere Lifecycle Manager might intermittently fail on ESXi hosts with DPUs**When you start a vSphere Lifecycle Manager remediation on an ESXi hosts with DPUs, the host upgrades and reboots as expected, but after the reboot, before completing the remediation task, you might see an error such as:A general system error occurred: After host … remediation completed, compliance check reported host as 'non-compliant'. The image on the host does not match the image set for the cluster. Retry the cluster remediation operation.This is a rare issue, caused by an intermittent timeout of the post-remediation scan on the DPU.Workaround: Reboot the ESXi host and re-run the vSphere Lifecycle Manager compliance check operation, which includes the post-remediation scan.
## VMware Host Client Issues

**VMware Host Client might display incorrect descriptions for severity event states**When you look in the VMware Host Client to see the descriptions of the severity event states of an ESXi host, they might differ from the descriptions you see by using Intelligent Platform Management Interface (IPMI) or Lenovo XClarity Controller (XCC). For example, in the VMware Host Client, the description of the severity event state for the PSU Sensors might be Transition to Non-critical from OK, while in the XCC and IPMI, the description is Transition to OK.Workaround: Verify the descriptions for severity event states by using the ESXCLI command esxcli hardware ipmi sdr list and Lenovo XCC.
## Guest OS Issues

**Linux guest operating system cannot complete booting if Direct Memory Access (DMA) remapping is enabled**If the advanced processor setting Enable IOMMU in this virtual machine is enabled on a virtual machine, and the guest operating system has enabled DMA remapping, the Linux guest operating system might fail to complete the booting process. This issue affects VMs with hardware version 20 and a Linux distribution that has specific patches introduced in Linux kernel 5.18 for a VMCI feature, including but not limited to up-to-date versions of RHEL 8.7, Ubuntu 22.04 and 22.10, and SLES15 SP3, and SP4.Workaround: Set the advanced option vmci.dmaDatagramSupport to FALSE or disable the Enable IOMMU in this virtual machine option. For more information, see VMware knowledge base article [89683.](https://kb.vmware.com/s/article/89683)**The guest operating system of a VM might become unresponsive due to lost communication over the Virtual Machine Communication Interface (VMCI)**In very specific circumstances, when a vSphere vMotion operation on a virtual machine runs in parallel with an operation that sends VMCI datagrams, services that use VMCI datagrams might see unexpected communication or loss of communication. Under the same conditions, the issue can also happen when restoring a memory snapshot, resuming a suspended VM or using CPU Hot Add. As a result, the guest operating system that depends on services communicating over VMCI might become unresponsive. The issue might also affect services that use vSockets over VMCI. This problem does not impact VMware Tools. The issue is specific for VMs on hardware version 20 with a Linux distribution that has specific patches introduced in Linux kernel 5.18 for a VMCI feature, including but not limited to up-to-date versions of RHEL 8.7, Ubuntu 22.04 and 22.10, and SLES15 SP3, and SP4.Workaround: Set the advanced option vmci.dmaDatagramSupport to FALSE. For more information, see VMware knowledge base article [89683.](https://kb.vmware.com/s/article/89683)
[ESXi Update and Patch Release Notes](/us/en/vmware-cis/vsphere/vsphere/8-0/release-notes/esxi-update-and-patch-release-notes.html)

---

[Content feedback and comments](/us/en/user-feedback.html)

* [Products](https://www.broadcom.com/products/)
* [Solutions](https://www.broadcom.com/solutions/)
* [Support and Services](https://www.broadcom.com/support/)
* [Company](https://www.broadcom.com/company/)
* [How To Buy](https://www.broadcom.com/how-to-buy/)

Copyright © 2005-2025 Broadcom. All Rights Reserved. The term “Broadcom” refers to Broadcom Inc. and/or its subsidiaries.

* [Privacy](https://www.broadcom.com/company/legal/privacy-policy)
* [Supplier Responsibility](https://www.broadcom.com/company/citizenship/supplier-responsibility)
* [Terms of Use](https://www.broadcom.com/company/legal/terms-of-use/)
* [Sitemap](https://www.broadcom.com/sitemap)



=== Content from kb.vmware.com_960ab569_20250115_202029.html ===


search

cancel

Search

### Remove the USB Controller on VMware Workstation and VMware Fusion

book
#### Article ID: 332353

calendar\_today
#### Updated On:

#### Products

VMware Desktop Hypervisor

Show More
Show Less

#### Issue/Introduction

The purpose of this article is to provide steps for removing USB controller on VMware Workstation and VMware Fusion.

#### Resolution

The following steps should be followed to remove USB controller on VMware Workstation and VMware Fusion.
### **Prerequisites**

Shut down or power off the virtual machine. You cannot change the setting while the virtual machine is powered on or suspended.

**For Fusion**:

1. Select **Window** > **Virtual Machine Library**.
2. Select a virtual machine in the **Virtual Machine Library** window and click **Settings**.
3. Under Removable Devices in the Settings window, click **USB & Bluetooth**.
4. Under Advanced USB options, click **Remove USB Controller**.
5. Click **Remove** in the confirmation dialog box.

**For Workstation**:

1. Select a virtual machine in the **Library** pane and select **VM** > **Settings**.
2. On the **Virtual Machine Settings** dialog go to the **Hardware tab**.
3. Select the **USB Controller** entry and click **Remove**.

#### Feedback

thumb\_up
Yes

thumb\_down
No

Powered by
[![Wolken Software](https://cdn.wolkenservicedesk.com/wolken-footer-image.png)](https://www.wolkensoftware.com/)



=== Content from kb.vmware.com_45f2c8a3_20250115_202016.html ===


search

cancel

Search

### Guidelines to address multiple security vulnerabilities for VCF 3.10.x, VCF 3.11.x, VCF 4.3.x, VCF 4.4.x and VCF 4.5 releases by VMSA-2022-0030 and VMSA-2022-0033

book
#### Article ID: 318847

calendar\_today
#### Updated On:

#### Products

VMware Cloud Foundation

Show More
Show Less

#### Issue/Introduction

[VMSA-2022-0030](https://www.vmware.com/security/advisories/VMSA-2022-0030.html)  details vulnerabilities in vCenter Server 6.7 & 7.0 and VMware ESXi 6.7 & 7.0. These vulnerabilities are remediated in the releases [vCenter Server 7.0 Update 3i](https://docs.vmware.com/en/VMware-vSphere/7.0/rn/vsphere-vcenter-server-70u3i-release-notes.html) and [ESXi 7.0 Update 3i](https://docs.vmware.com/en/VMware-vSphere/7.0/rn/vsphere-esxi-70u3i-release-notes.html).
These issues have also been fixed in [vCenter Server 6.7 Update 3s](https://docs.vmware.com/en/VMware-vSphere/6.7/rn/vsphere-vcenter-server-67u3s-release-notes.html) and [VMware ESXi 6.7, Patch Release ESXi670-202210001](https://docs.vmware.com/en/VMware-vSphere/6.7/rn/esxi670-202210001.html).

[VMSA-2022-0033](https://www.vmware.com/security/advisories/VMSA-2022-0033.html) details vulnerabilities in VMware ESXi 7.0. These are remediated in [VMware ESXi 7.0 Update 3i](https://docs.vmware.com/en/VMware-vSphere/7.0/rn/vsphere-esxi-70u3i-release-notes.html)

This KB documents the process should customers running VMware Cloud Foundation 3.10.x, 3.11.x or 4.x wish to update their vCenter Server and ESXi hosts.

Symptoms:
As documented in [VMSA-2022-0030](https://www.vmware.com/security/advisories/VMSA-2022-0030.html), vCenter Server 6.7 & 7.0 and Vmware ESXi 6.7 & 7.0 is affected by the vulnerabilities listed in the advisories.

As documented in [VMSA-2022-0033](https://www.vmware.com/security/advisories/VMSA-2022-0033.html), VMware ESXi 7.0 is affected by the vulnerabilities listed in the advisories.

Since the VMware Cloud Foundation (VCF) 3.10.x, 3.11.x and 4.x versions bundle the impacted releases of vCenter Server and VMware ESXi ; VCF 3.10.x, 3.11.x and VCF 4.x releases are similarly impacted by the vulnerabilities listed in the advisory.

#### Environment

VMware Cloud Foundation 4.4.x
VMware Cloud Foundation 3.0.x
Vmware Cloud Foundation 4.5
VMware Cloud Foundation 4.3.x

#### Resolution

The [vCenter Server 7.0 Update 3i](https://docs.vmware.com/en/VMware-vSphere/7.0/rn/vsphere-vcenter-server-70u3i-release-notes.html) and [ESXi 7.0 Update 3i](https://docs.vmware.com/en/VMware-vSphere/7.0/rn/vsphere-esxi-70u3i-release-notes.html) releases are available on [VCF Async Patch Tool](https://docs.vmware.com/en/VMware-Cloud-Foundation/services/rn/async-patch-tool-release-notes/index.html) for the VCF customers to readily upgrade their vCenter Server and ESXi hosts. Please follow the table below for further guidance :

| VCF Release | Resolution |
| --- | --- |
| VCF 3.10.x and 3.11.x | Refer [KB 89692](https://kb.vmware.com/s/article/89692) to Async Patch VMware ESXi 6.7, Patch Release ESXi670-202210001 Refer [KB 90407](https://kb.vmware.com/s/article/90407) to Async Patch vCenter Server 6.7 Update 3s |
| Prior to VCF 4.3.1.1 | Upgrade to VCF 4.3.1.1 or above and refer [KB 88287](https://kb.vmware.com/s/article/88287) |
| VCF 4.3.1.1 | Refer [KB 88287](https://kb.vmware.com/s/article/88287) |
| VCF 4.4 | Refer [KB 88287](https://kb.vmware.com/s/article/88287) |
| VCF 4.4.1 | Refer [KB 88287](https://kb.vmware.com/s/article/88287) |
| VCF 4.4.1.1 | Refer [KB 88287](https://kb.vmware.com/s/article/88287) |
| VCF 4.5 | Refer [KB 88287](https://kb.vmware.com/s/article/88287) |

#### Feedback

thumb\_up
Yes

thumb\_down
No

Powered by
[![Wolken Software](https://cdn.wolkenservicedesk.com/wolken-footer-image.png)](https://www.wolkensoftware.com/)



=== Content from www.vmware.com_25becc9b_20250114_224915.html ===


Menu

* [Products](https://www.broadcom.com/products/)
* [Solutions](https://www.broadcom.com/solutions)
* [Support and Services](https://www.broadcom.com/support)
* [Company](https://www.broadcom.com/company/about-us/)
* [How To Buy](https://www.broadcom.com/how-to-buy/#sales)

* Log in

  [Log In](/c/portal/login)
  [Register](https://profile.broadcom.com/web/registration)

[Register](https://profile.broadcom.com/web/registration)
[Login](/c/portal/login)

VMSA-2022-0033:VMware ESXi, Workstation, and Fusion updates address a heap out-of-bounds write vulnerability

Product/Component

VMware Cloud Foundation

2 more products

List of Products

3 Products

* VMware Cloud Foundation
* VMware Desktop Hypervisor
* VMware vSphere ESXi

Notification Id

23653

Last Updated

11 December 2022

Initial Publication Date

11 December 2022

Status

CLOSED

Severity

CRITICAL

CVSS Base Score

5.9-9.3

WorkAround

Affected CVE

CVE-2022-31705

             Advisory ID: VMSA-2022-0033   CVSSv3 Range: 5.9-9.3   Issue Date:2022-12-13   Updated On: 2022-12-13 (Initial Advisory)   CVE(s): CVE-2022-31705   Synopsis: VMware ESXi, Workstation, and Fusion updates address a heap out-of-bounds write vulnerability (CVE-2022-31705)

 [RSS Feed](https://www.vmware.com/security/advisories/VMSA-2022-0033.xml)

 Download PDF

 Download Text File

Share this page on social media:

##### **1. Impacted Products**

* VMware ESXi
* VMware Workstation Pro / Player (Workstation)
* VMware Fusion Pro / Fusion (Fusion)
* VMware Cloud Foundation

##### **2. Introduction**

A heap out-of-bounds write vulnerability in VMware ESXi, Workstation, and Fusion was privately reported to VMware. Updates and workarounds are available to remediate this vulnerability in affected VMware products.

##### **3. Heap out-of-bounds write vulnerability in EHCI controller (CVE-2022-31705)**

**Description**

VMware ESXi, Workstation, and Fusion contain a heap out-of-bounds write vulnerability in the USB 2.0 controller (EHCI). VMware has evaluated the severity of this issue to be in the [Critical severity range](https://www.vmware.com/support/policies/security_response.html) with a maximum CVSSv3 base score of [9.3](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:L/AC:L/PR:N/UI:N/S:C/C:H/I:H/A:H).

**Known Attack Vectors**

A malicious actor with local administrative privileges on a virtual machine may exploit this issue to execute code as the virtual machine's VMX process running on the host. On ESXi, the exploitation is contained within the VMX sandbox whereas, on Workstation and Fusion, this may lead to code execution on the machine where Workstation or Fusion is installed.

**Resolution**

To remediate CVE-2022-31705 apply the patches listed in the 'Fixed Version' column of the 'Response Matrix' found below.

**Workarounds**

Workarounds for CVE-2022-31705 have been listed in the 'Workarounds' column of the 'Response Matrix' below.

**Additional Documentation**

None.

**Acknowledgements**

VMware would like to thank the organizers of GeekPwn 2022 and Yuhao Jiang for reporting this issue to us.

**Notes**

None.

**Response Matrix:**

| Product | Version | Running On | CVE Identifier | CVSSv3 | Severity | Fixed Version | Workarounds | Additional Documentation |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| ESXi | 8.0 | Any | CVE-2022-31705 | [5.9](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:L/AC:L/PR:N/UI:N/S:U/C:L/I:L/A:L) | moderate | [ESXi80a-20842819](https://docs.vmware.com/en/VMware-vSphere/8.0/rn/vsphere-esxi-80a-release-notes/index.html) | [KB87617](https://kb.vmware.com/s/article/87617) | None |
| ESXi | 7.0 | Any | CVE-2022-31705 | [5.9](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:L/AC:L/PR:N/UI:N/S:U/C:L/I:L/A:L) | moderate | [ESXi70U3si-20841705](https://docs.vmware.com/en/VMware-vSphere/7.0/rn/vsphere-esxi-70u3i-release-notes.html) | [KB87617](https://kb.vmware.com/s/article/87617) | None |
| Fusion | 13.x | OS X | CVE-2022-31705 | N/A | N/A | Unaffected | N/A | N/A |
| Fusion | 12.x | OS X | CVE-2022-31705 | [9.3](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:L/AC:L/PR:N/UI:N/S:C/C:H/I:H/A:H) | critical | 12.2.5 | [KB79712](https://kb.vmware.com/s/article/79712) | None |
| Workstation | 17.x | Any | CVE-2022-31705 | N/A | N/A | Unaffected | N/A | N/A |
| Workstation | 16.x | Any | CVE-2022-31705 | [9.3](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:L/AC:L/PR:N/UI:N/S:C/C:H/I:H/A:H) | critical | 16.2.5 | [KB79712](https://kb.vmware.com/s/article/79712) | None |

**Impacted Product Suites that Deploy Response Matrix Components:**

| Product | Version | Running On | CVE Identifier | CVSSv3 | Severity | Fixed Version | Workarounds | Additional Documentation |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| Cloud Foundation (ESXi) | 4.x/3.x | Any | CVE-2022-31705 | [5.9](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:L/AC:L/PR:N/UI:N/S:U/C:L/I:L/A:L) | moderate | [KB90336](https://kb.vmware.com/s/article/90336) | [KB87617](https://kb.vmware.com/s/article/87617) | None |

##### **4. References**

**VMware ESXi 8.0 ESXi80a-20842819**Downloads and Documentation:<https://my.vmware.com/group/vmware/patch>
<https://docs.vmware.com/en/VMware-vSphere/8.0/rn/vsphere-esxi-80a-release-notes/index.html>

****VMware ESXi 7.0 ESXi70U3si-20841705****
Downloads and Documentation:
<https://my.vmware.com/group/vmware/patch>
<https://docs.vmware.com/en/VMware-vSphere/7.0/rn/vsphere-esxi-70u3i-release-notes.html>

**VMware Workstation 16.2.5**
[\https://customerconnect.vmware.com/en/downloads/info/slug/desktop\_end\_user\_computing/vmware\_workstation\_pro/16\_0](https://customerconnect.vmware.com/en/downloads/info/slug/desktop_end_user_computing/vmware_workstation_pro/16_0)
<https://docs.vmware.com/en/VMware-Workstation-Pro/16.2.5/rn/vmware-workstation-1625-pro-release-notes/index.html>

**VMware Fusion 12.2.5** Downloads and Documentation:
<https://customerconnect.vmware.com/downloads/info/slug/desktop_end_user_computing/vmware_fusion/12_0>
<https://docs.vmware.com/en/VMware-Fusion/12.2.5/rn/vmware-fusion-1225release-notes/index.html>

**KBs:** <https://kb.vmware.com/s/article/87617>
<https://kb.vmware.com/s/article/79712>
<https://kb.vmware.com/s/article/90336>

**Mitre CVE Dictionary Links:**
<https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-31705>

**FIRST CVSSv3 Calculator:** CVE-2022-31705
ESXi: <https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:L/AC:L/PR:N/UI:N/S:U/C:L/I:L/A:L>
Workstation/Fusion: <https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:L/AC:L/PR:N/UI:N/S:C/C:H/I:H/A:H>

##### **5. Change Log**

**2022-12-13 VMSA-2022-0033**
Initial security advisory.

##### **6. Contact**

E-mail list for product security notifications and announcements:

<https://lists.vmware.com/cgi-bin/mailman/listinfo/security-announce>

This Security Advisory is posted to the following lists:

[[email protected]
[email protected]](/cdn-cgi/l/email-protection#ddbba8b1b1b9b4aebeb1b2aea8afb89daeb8beb1b4aea9aef3b2afba)
[[email protected]](/cdn-cgi/l/email-protection#5234273e3e363b21313e3d21272037122137313e3b2126217c3d2035)

E-mail: [[email protected]](/cdn-cgi/l/email-protection#aeddcbcddbdcc7dad7eed8c3d9cfdccb80cdc1c3)

PGP key at:
<https://kb.vmware.com/kb/1055>

VMware Security Advisories
<https://www.vmware.com/security/advisories>

VMware Security Response Policy
<https://www.vmware.com/support/policies/security_response.html>

VMware Lifecycle Support Phases
<https://www.vmware.com/support/policies/lifecycle.html>

VMware Security & Compliance Blog
<https://blogs.vmware.com/security>

Twitter
<https://twitter.com/VMwareSRC>

Copyright 2022 VMware Inc. All rights reserved.

Hidden

#####

×

It appears your Broadcom Products and Services are

supported by one of our certified Support partners

Click below to be redirected to the appropriate Support

Partner Portal to request support

For non-product related issues (Support Portal / Licensing) Click HERE

Continue

#####

×

For **Technical Support** (issues with products or services)

1. Select **Technical** to be redirected to the My Entitlements page
2. Expand the product you require support on
3. Select the case icon from the case column
4. You will be redirected to the appropriate vendor portal where you can raise your technical request

For **Non-Technical Support** (issues with portal access, license keys, software downloads)

1. Select **Non-Technical** to be redirected to Broadcom's case management portal

Technical
Non-Technical

#####

×

# Access Denied

This feature has been disabled by your administrator.

#####

×

To prevent this message from showing again, please enable pop-up blockers for [support.broadcom.com](https://support.broadcom.com/)
or click Continue to proceed.

Continue

Top

* [Products](https://www.broadcom.com/products)
* [Solutions](https://www.broadcom.com/solutions)
* [Support and Services](https://www.broadcom.com/support)
* [Company](https://www.broadcom.com/)
* [How to Buy](https://www.broadcom.com/how-to-buy)

 Copyright © 2005-2024 Broadcom. All Rights Reserved. The term “Broadcom” refers to Broadcom Inc. and/or its subsidiaries.

* [Accessibility](https://www.broadcom.com/company/legal/accessibility)
* [Privacy](https://www.broadcom.com/company/legal/privacy)
* [Supplier Responsibility](https://www.broadcom.com/company/citizenship/supplier-responsibility)
* [Terms of Use](https://www.broadcom.com/company/legal/terms-of-use)
* [Site Map](https://www.broadcom.com/sitemap)



=== Content from kb.vmware.com_076c9eb8_20250115_202015.html ===


search

cancel

Search

### Steps to remove a USB controller from a VMware ESXi virtual machine

book
#### Article ID: 315467

calendar\_today
#### Updated On:

#### Products

VMware vSphere ESXi

Show More
Show Less

#### Issue/Introduction

This KB documents the steps required to remove a USB controller from a virtual machine.

Please note that while the ESXi host supports hot-removal of a USB controller, the guest operating system of the virtual machine must also support the hot-removal functionality. Please refer vendors documentation to determine whether this functionality is supported or not.
In the event that the guest operating system does not support hot removal of a USB controller, then the VM will need to be powered off.

In addition, please ensure that the USB controller is not in-use prior to removing it from the virtual machine.

The vSphere UI (vCenter Server, ESXi Embedded Host Client) only allows for the configuration of virtual USB 2.0 or virtual USB 3.0 controllers in VMs.
When a virtual USB 2.0 controller is added to a VM in vSphere, both a virtual USB 1.1 AND a virtual USB 2.0 controller are added to the VM by default.
Removing the virtual USB 2.0 controller will also remove the virtual USB 1.1 controller from the VM.

#### Resolution

(See the Related Information section below for Powercli options and examples)

**Using the vSphere Client (UI)**

1) Ensure that the USB controller is not in use

*2) Power off the Virtual machine (If required - No requirement to power off if the guestOS supports hot removal"*

3) Right-click the virtual machine and click "**Edit Settings**".

4) Remove all controllers from the VM.

![USB2.0.jpg](https://api-broadcomcms-software.wolkenservicedesk.com/attachment/get_attachment_content?uniqueFileId=1512269395087)

5) Click "**OK**" to apply the new virtual machine configuration.

![remove.jpg](https://api-broadcomcms-software.wolkenservicedesk.com/attachment/get_attachment_content?uniqueFileId=1512265959719)

#### Additional Information

**Powercli Options**

The command below can be used to list all the virtual machines with a USB controller.
Any VM reported should be investigated to determine if it can be safely removed

**Get-VM | ?{$\_.ExtensionData.Config.Hardware.Device.DeviceInfo.Label -match "USB"}**

The **"Remove\_USB\_Controller"** script attached to this KB is provided as an option to allow for the USB controller to be removed in an automated process.
However, please ensure that the virtual machine has been reviewed and that it is safe to remove the USB controller. In addition, it is important that the VM is powered off in the event that the guest operating system does not support hot removal of the USB controller

This script will

1)  Prompt for the FQDN or IP address of the vCenter system
2)  Prompt the user to allow for the USB controllers to be removed
3)  Find all virtual machines with a USB controller
4)  Attempt to remove the USB controller from the virtual machines
5)  Review the environment and provide a CSV file output detailing whether a virtual machine has an USB controller or not

The attachment is a text (\*.txt) file. Please rename this to a \*.ps1 (Remove\_USB\_Controller.ps1) before executing the script

Impact/Risks:

USB pass through functionality will be unavailable as a result of removing the USB controller from the virtual machine.
In addition, virtual/emulated USB devices, such as VMware virtual USB mouse and keyboard will not be available for use by the VM.

**IMPORTANT**:
Certain guest operating systems, including Mac OS do not support using a PS/2 mouse and keyboard. Without a USB controller. these guest operating systems will be left without a mouse and keyboard.

#### Attachments

Remove\_USB\_Controller
get\_app

#### Feedback

thumb\_up
Yes

thumb\_down
No

Powered by
[![Wolken Software](https://cdn.wolkenservicedesk.com/wolken-footer-image.png)](https://www.wolkensoftware.com/)



=== Content from docs.vmware.com_cc40e10f_20250115_202026.html ===
VMware ESXi 7.0 Update 3i Release Notes

ESXi 7.0 Update 3i | DEC 08 2022 | 20842708

Check for additions and updates to these release notes.

What's in the Release Notes

The release notes cover the following topics:

What's New

Earlier Releases of ESXi 7.0

Patches Contained in this Release

Product Support Notices

Resolved Issues

Known Issues

Known Issues from Previous Releases

IMPORTANT: If your source system contains hosts of versions between ESXi 7.0 Update 2 and Update 3c, and Intel drivers, before upgrading to ESXi 7.0 Update 3i,

see the What's New section of the VMware vCenter Server 7.0 Update 3c Release Notes, because all content in the section is also applicable for vSphere 7.0 Update

3i. Also, see the related VMware knowledge base articles: 86447, 87258, and 87308.

What's New

ESXi 7.0 Update 3i supports vSphere Quick Boot on the following server:

HPE ProLiant MicroServer Gen10 Plus v2

This release resolves CVE-2022-31696, and CVE-2022-31699. For more information on these vulnerabilities and their impact on VMware products, see VMSA-

2022-0030.

Earlier Releases of ESXi 7.0

New features, resolved, and known issues of ESXi are described in the release notes for each release. Release notes for earlier releases of ESXi 7.0 are:

VMware ESXi 7.0, ESXi 7.0 Update 3g Release Notes

VMware ESXi 7.0, ESXi 7.0 Update 3f Release Notes

VMware ESXi 7.0, ESXi 7.0 Update 3e Release Notes

VMware ESXi 7.0, ESXi 7.0 Update 3d Release Notes

VMware ESXi 7.0, ESXi 7.0 Update 2e Release Notes

VMware ESXi 7.0, ESXi 7.0 Update 1e Release Notes

VMware ESXi 7.0, ESXi 7.0 Update 3c Release Notes

VMware ESXi 7.0, ESXi 7.0 Update 2d Release Notes

VMware ESXi 7.0, ESXi 7.0 Update 2c Release Notes

VMware ESXi 7.0, ESXi 7.0 Update 2a Release Notes

VMware ESXi 7.0, ESXi 7.0 Update 2 Release Notes

VMware ESXi 7.0, ESXi 7.0 Update 1d Release Notes

VMware ESXi 7.0, ESXi 7.0 Update 1c Release Notes

VMware ESXi 7.0, ESXi 7.0 Update 1b Release Notes

VMware ESXi 7.0, ESXi 7.0 Update 1a Release Notes

VMware ESXi 7.0, ESXi 7.0 Update 1 Release Notes

VMware ESXi 7.0, ESXi 7.0b Release Notes

For internationalization, compatibility, and open source components, see the VMware vSphere 7.0 Release Notes.

Patches Contained in This Release

This release of ESXi 7.0 Update 3i delivers the following patches:

Build Details

Download Filename:

VMware-ESXi-7.0U3i-20842708-depot

Build:

Download Size:

md5sum:

sha256checksum:

20842708

570.5 MB

7ec976758701d4287393aebcc2c5d55c

26b26ddb4e0d3ba2bc4ac3b693addb111a0b1980abf03fce243473812ec460f7

Host Reboot Required:

Yes

Virtual Machine Migration or
Shutdown Required:

Yes

Components

Component

Bulletin

ESXi Component - core ESXi VIBs

ESXi_7.0.3-0.65.20842708

Category Severity

Bugfix

Critical

ESXi Install/Upgrade Component

esx-update_7.0.3-0.65.20842708

Bugfix

Critical

Broadcom NetXtreme I ESX VMKAPI ethernet
driver

Broadcom-ntg3_4.1.8.0-
4vmw.703.0.65.20842708

ESXi Component - core ESXi VIBs

ESXi_7.0.3-0.60.20841705

ESXi Install/Upgrade Component

esx-update_7.0.3-0.60.20841705

Bugfix

Critical

Security Critical

Security Critical

ESXi Tools Component

VMware-VM-Tools_12.1.0.20219665-20841705

Security Critical

IMPORTANT:

Starting with vSphere 7.0, VMware uses components for packaging VIBs along with bulletins. The ESXi and esx-update bulletins are dependent on each other.

Always include both in a single ESXi host patch baseline or include the rollup bulletin in the baseline to avoid failure during host patching.

When patching ESXi hosts by using VMware Update Manager from a version prior to ESXi 7.0 Update 2, it is strongly recommended to use the rollup bulletin in

the patch baseline. If you cannot use the rollup bulletin, be sure to include all of the following packages in the patching baseline. If the following packages are not

included in the baseline, the update operation fails:

VMware-vmkusb_0.1-1vmw.701.0.0.16850804 or higher

VMware-vmkata_0.1-1vmw.701.0.0.16850804 or higher

VMware-vmkfcoe_1.0.0.2-1vmw.701.0.0.16850804 or higher

VMware-NVMeoF-RDMA_1.0.1.2-1vmw.701.0.0.16850804 or higher

Rollup Bulletin

This rollup bulletin contains the latest VIBs with all the fixes after the initial release of ESXi 7.0.

Bulletin ID

Category

Severity

Detail

ESXi70U3i-20842708

ESXi70U3si-20841705

Bugfix

Security

Critical

Critical

Security and Bugfix

Security only

Image Profiles

VMware patch and update releases contain general and critical image profiles. Application of the general release image profile applies to new bug fixes.

Image Profile Name

ESXi-7.0U3i-20842708-standard

ESXi-7.0U3i-20842708-no-tools

ESXi-7.0U3si-20841705-standard

ESXi-7.0U3si-20841705-no-tools

ESXi Image

Name and Version

Release Date Category

Detail

ESXi_7.0.3-0.65.20842708 DEC 08 2022 Enhancement Security and Bugfix image

ESXi_7.0.3-0.60.20841705 DEC 08 2022 Enhancement Security only image

For information about the individual components and bulletins, see the Product Patches page and the Resolved Issues section.

Patch Download and Installation

In vSphere 7.x, the Update Manager plug-in, used for administering vSphere Update Manager, is replaced with the Lifecycle Manager plug-in. Administrative operations

for vSphere Update Manager are still available under the Lifecycle Manager plug-in, along with new capabilities for vSphere Lifecycle Manager.

The typical way to apply patches to ESXi 7.x hosts is by using the vSphere Lifecycle Manager. For details, see About vSphere Lifecycle Manager and vSphere Lifecycle

Manager Baselines and Images.

You can also update ESXi hosts without using the Lifecycle Manager plug-in, and use an image profile instead. To do this, you must manually download the patch

offline bundle ZIP file from VMware Customer Connect. From the Select a Product drop-down menu, select ESXi (Embedded and Installable) and from the Select a

Version drop-down menu, select 7.0. For more information, see the Upgrading Hosts by Using ESXCLI Commands and the VMware ESXi Upgrade guide.

Product Support Notices

Starting with vSphere 7.0 Update 3i, when you configure the reverse proxy on the vCenter Server system to enable smart card authentication, you must use port

3128, which is set and opened automatically, but you must be permitted access to the port on the respective vCenter Server. Check your perimeter firewalls to

ensure that access has been granted. Make sure you restart the STS service after you configure the rhttpproxy service.

For more information, see Configure the Reverse Proxy to Request Client Certificates and VMware knowledge base articles 78057 and 90542.

Resolved Issues

The resolved issues are grouped as follows.

ESXi_7.0.3-0.65.20842708

esx-update_7.0.3-0.65.20842708

Broadcom-ntg3_4.1.8.0-4vmw.703.0.65.20842708

ESXi_7.0.3-0.60.20841705

esx-update_7.0.3-0.60.20841705

VMware-VM-Tools_12.1.0.20219665-20841705

ESXi-7.0U3i-20842708-standard

ESXi-7.0U3i-20842708-no-tools

ESXi-7.0U3si-20841705-standard

ESXi-7.0U3si-20841705-no-tools

ESXi_7.0.3-0.65.20842708

ESXi_7.0.3-0.60.20841705

ESXi_7.0.3-0.65.20842708

Patch Category

Patch Severity

Host Reboot Required

Virtual Machine Migration or Shutdown
Required

Affected Hardware

Affected Software

Bugfix

Critical

Yes

Yes

N/A

N/A

VIBs Included

PRs Fixed

VMware_bootbank_esx-xserver_7.0.3-0.65.20842708
VMware_bootbank_gc_7.0.3-0.65.20842708
VMware_bootbank_vsan_7.0.3-0.65.20842708
VMware_bootbank_cpu-microcode_7.0.3-0.65.20842708
VMware_bootbank_crx_7.0.3-0.65.20842708
VMware_bootbank_esx-base_7.0.3-0.65.20842708
VMware_bootbank_esx-dvfilter-generic-fastpath_7.0.3-
0.65.20842708
VMware_bootbank_vsanhealth_7.0.3-0.65.20842708
VMware_bootbank_native-misc-drivers_7.0.3-0.65.20842708
VMware_bootbank_esx-ui_2.1.1-20188605
VMware_bootbank_trx_7.0.3-0.65.20842708
VMware_bootbank_bmcal_7.0.3-0.65.20842708
VMware_bootbank_vdfs_7.0.3-0.65.20842708
VMware_bootbank_esxio-combiner_7.0.3-0.65.20842708

2962719, 3021935, 2994966, 2983919, 3014323, 2996511, 2982013,
3011324, 3048788, 2983043, 3035934, 3015498, 3031708, 3041100,
3044475, 2990414, 3003866, 3011850, 3002779, 3033161, 3016132,
3015493, 3029194, 3007116, 2977496, 3021384, 2988179, 3025470,
3033159, 3011169, 3018832, 2977486, 2932056, 3013368, 2995471,
3006356, 2981272, 3000195, 3026623, 2981420, 3025469,
3042776, 2916160, 2996208, 3014374, 2977957, 2979281, 2976953,
2999968, 3031566, 3036859, 3029490, 2992407

CVE numbers

N/A

The ESXi and esx-update bulletins are dependent on each other. Always include both in a single ESXi host patch baseline or include the rollup bulletin in the baseline to

avoid failure during host patching.

Updates the esx-xserver, gc, vsan, cpu-microcode, esx-base, esx-dvfilter-generic-fastpath, vsanhealth, native-misc-drivers, esx-ui, trx, bmcal, vdfs, esxio-combiner,

and crx VIBs to resolve the following issues:

NEW PR 2962719: Virtual machines might intermittently experience soft lockups inside the guest kernel on Intel machines

Starting with 7.0 Update 2, ESXi supports Posted Interrupts (PI) on Intel CPUs for PCI passthrough devices to improve the overall system performance. In some

cases, a race between PIs and the VMkernel scheduling might occur. As a result, virtual machines that are configured with PCI passthrough devices with normal

or low latency sensitivity might experience soft lockups.

This issue is resolved in this release.

PR 3021935: VM events sometimes report the template property incorrectly

In rare cases, VM events might report the template property, which indicates if a virtual machine is marked as a template, incorrectly. As a result, you might see

the template property as true even if the VM is not a template VM or as false, when a VM is marked as a template.

This issue is resolved in this release.

PR 2994966: You do not see memory status info for ESXi hosts in the Managed Object Browser (MOB) interface

Due to a missing Memory Module Entity for Cisco servers in the Managed Object Browser, you might not see the memory status info of an ESXi host by using

MOB.

This issue is resolved in this release. The fix adds support for Memory Module Entity ID 8 (08h).

PR 2983919: Unresponsive Active Directory domain controllers might intermittently cause the hostd service to become unresponsive too

In very rare occasions, random issues with Active Directory environments that might lead to unresponsive state of the domain controllers, might also result in

unresponsiveness of the hostd service.

This issue is resolved in this release. The fix makes the hostd service resilient to intermittent Active Directory domain controller unresponsiveness.

PR 3014323: After creating or reverting to a VM snapshot, VMware Tools guest-related performance counters stop to update

Rarely, due to the fast suspend resume mechanism used to create or revert a VM to a snapshot, the internal state of the VMX process might reinitialize without

notification to the upper layers of the virtual infrastructure management stack. As a result, all guest-related performance counters that VMware Tools provides

stop updating. In all interfaces to the ESXi host, you continuously see the last recorded values.

This issue is resolved in this release.

PR 2996511: The rhttpproxy service occasionally fails with a coredump reporting a buffer overflow

When the rhttpproxy service performs multiple operations on incoming URIs, it might miscalculate the buffer offset of each connection, which potentially leads to

errors such as buffer overflows and negative reads. As a result, the service fails.

This issue is resolved in this release.

PR 2982013: You cannot modify a hypercall option when a virtual machine is powered on

By default, modifying hypercall options by using commands such as vm | Get-AdvancedSetting -Name isolation.tools.autoInstall.disable works only when the VM

is powered off. For powered on VMs such calls trigger the error The attempted operation cannot be performed in the current state (Powered on). This is expected.

This issue is resolved in this release.

PR 3011324: Update from ESXi 7.0 Update 3c to a later version might revert to default values all security advanced options of an ESXi host

After you update an ESXi 7.0 Update 3c host to a later version of 7.0.x or install or remove an ESXi 7.0 Update 3c VIB and reboot the host, you might see all

security advanced options on the host revert to their default values. The affected advanced settings are:

Security.AccountLockFailures

Security.AccountUnlockTime

Security.PasswordQualityControl

Security.PasswordHistory

Security.PasswordMaxDays

Security.SshSessionLimit

Security.DefaultShellAccess

This issue is resolved in this release.

PR 3048788: SR-IOV devices might report incorrect NUMA node for Virtual Functions

The command esxcli hardware pci list, which reports the NUMA node for ESXi host devices, returns the correct NUMA node for the Physical Functions (PF) of

an SR-IOV device, but returns zero for its Virtual Functions (VF).

This issue is resolved in this release. The fix makes sure that NUMA nodes are consistent for both PF and VF of SR-IOV devices.

PR 2983043: Windows Guest OS might fail with a blue diagnostic screen after migration of virtual machines to ESXi hosts of version 7.0 Update 2 or later

After an upgrade to ESXi 7.0 Update 2 or later, when you migrate Windows virtual machines to the upgraded hosts by using vSphere vMotion, some VMs might

fail with a blue diagnostic screen after the migration. In the screen, you see the error OS failed to boot with no operating system found. The issue occurs due to a

fault in the address optimization logic of the Virtual Machine File System (VMFS).

This issue is resolved in this release.

PR 3035934: ESXi hosts might become unresponsive when no registered default request handler is available and the execution of vmacore.dll fails

If the client application has no registered default request handler, requests with a path that is not present in the handler map might cause the execution of

vmacore.dll to fail. As a result, you see the ESXi host as disconnected from the vCenter Server system.

This issue is resolved in this release.

PR 3015498: ESXi hosts might fail with a purple diagnostic screen during resource allocation reservation operations

Some allocation reservation operations might go over the limit of 128 parallel reservation keys and exceed the allocated memory range of an ESXi host. As a

result, ESXi hosts might fail with a purple diagnostic screen during resource allocation reservation operations. In the error screen, you see messages such as PSOD

BlueScreen: #PF Exception 14 in world 2097700:SCSI period IP.

This issue is resolved in this release.

PR 3031708: ESXi hosts might fail with a purple diagnostic screen during device or path unclaim operations

If you run an unclaim command on a device or path while virtual machines on the device still have active I/Os, the ESXi host might fail with a purple diagnostic

screen. In the screen, you see a message such as PSOD at bora/modules/vmkernel/nmp/nmp_misc.c:3839 during load/unload of lpfc.

This issue is resolved in this release.

PR 3041100: When TPM 2.0 is enabled with TXT on an ESXi host, attempts to power-on a virtual machine might fail

When TXT is enabled on an ESX host, attempts to power-on a VM might fail with an error. In the vSphere Client, you see a message such as This host supports

Intel VT-x, but Intel VT-x is restricted. Intel VT-x might be restricted because 'trusted execution' has been enabled in the BIOS/firmware settings or because

the host has not been power-cycled since changing this setting.

This issue is resolved in this release.

PR 3044475: Virtual machines might fail with ESX unrecoverable error due to a rare issue with handling AVX2 instructions

Due to a rare issue with handling AVX2 instructions, a virtual machine of version ESX 7.0 Update 3f might fail with ESX unrecoverable error. In the vmware.log file,

you see a message such as: MONITOR PANIC: vcpu-0:VMM fault 6: src=MONITOR ....

The issue is specific for virtual machines with hardware versions 12 or earlier.

This issue is resolved in this release.

PR 2990414: Upgrade operations by using a vSphere Lifecycle Manager upgrade baseline with a vSphere ESXi Image Builder .iso custom image might fail

If you use an ESXi .iso image created by using the Image Builder to make a vSphere Lifecycle Manager upgrade baseline for ESXi hosts, upgrades by using such

baselines might fail. In the vSphere Client, you see an error such as Cannot execute upgrade script on host. On the impacted ESXi host, in the /var/log/vua*.log file,

you see an error such as ValueError: Should have base image when an addon exists.

The error occurs when the existing image of the ESXi host has an add-on, but the Image Builder-generated ISO provides no add-on.

This issue is resolved in this release.

PR 3003866: ESXi hosts might fail with a purple diagnostic screen due to insufficient XMAP space

Many parallel requests for memory regions by virtual machines using the Data Plane Development Kit (DPDK) on an ESXi host might exceed the XMAP memory

space on the host. As a result, the host fails with a purple diagnostic screen and an error such as: Panic Message: @BlueScreen: VERIFY

bora/vmkernel/hardware/pci/config.c:157.

This issue is resolved in this release. The fix sets the default memory regions available for all vmxnet3 adapters to 8192 MB.

PR 3011850: You do not see some performance reports for virtual machines with NVMe controllers on ESXi 7.0 Update 3 and later

After an upgrade of ESXi hosts to ESXi 7.0 Update 3 and later, you might no longer see some performance reports for virtual machines with NVMe controllers.

For example, you do not see the Virtual Disk - Aggregate of all Instances chart in the VMware Aria Operations.

This issue is resolved in this release.

PR 3002779: ESXi host with a Fault Tolerance encryption enabled secondary VM might fail with a purple diagnostic screen due to a decryption buffer

overflow

In rare cases, such as scheduled reboot of the primary VM with FT encryption that runs heavy workloads, the secondary VM might not have sufficient buffer to

decrypt more than 512 MB of dirty pages in a single FT checkpoint and experience a buffer overflow error. As a result, the ESXi host on which the secondary VM

resides might fail with a purple diagnostic screen.

This issue is resolved in this release.

PR 3033161: After upgrade or update to ESXi 7.0 Update 2 and later, vSAN storage marked as flash (SSD) might change to magnetic disk (HDD)

In rare cases, after upgrade or update to ESXi 7.0 Update 2 and later, the vSAN storage configuration might lose the tag mark_ssd and default to HDD.

This issue is resolved in this release.

PR 3016132: You see frequent logs for failed registration of detached LUNs

Even when a device or LUN is in a detached state, the Pluggable Storage Architecture (PSA) might still attempt to register the object. PSA files a log for each

path evaluation step at every path evaluation interval of such attempts. As a result, you might see multiple identical messages such as nmp_RegisterDeviceEvents

failed for device registration, which are not necessary while the device or LUN is detached.

This issue is resolved in this release.

PR 3015493: When you change a device configuration at runtime, a datastore might fail to mount after an ESXi host reboots

If you change a device configuration at runtime, changes might not be reflected in the ESXi ConfigStore that holds the configurations for an ESXi host. As a

result, the datastore might not mount after the ESXi host reboots.

This issue is resolved in this release. The fix makes sure that ConfigStore reflects device configuration changes at runtime.

PR 3029194: After upgrade to ESXi 7.0 Update 3 and later, ESXi hosts might fail with a purple diagnostic screen due to legacy I/O scheduler

Starting from ESXi 6.0, mClock is the default I/O scheduler for ESXi, but some environments might still use legacy schedulers of ESXi versions earlier than 6.0. As

a result, upgrades of such hosts to ESXi 7.0 Update 3 and later might fail with a purple diagnostic screen.

This issue is resolved in this release.

PR 3007116: If you upgrade ESXi hosts to ESXi 7.0 Update 3d or later by using a host profile with tokens indicating ALUA state, the host might fail with a

purple diagnostic screen

Starting with ESXi 7.0 Update 1, the configuration management of ESXi hosts moved from the /etc/vmware/esx.conf file to the ConfigStore framework, which

makes an explicit segregation of state and configuration. Tokens in the esx.conf file such as implicit_support or explicit_support that indicate a state, are not

recognized as valid tokens, and are ignored by the satp_alua module. As a result, when you upgrade ESXi hosts to ESXi 7.0 Update 3d or later by using a host

profile with tokens indicating ALUA state, the operation might fail with a purple diagnostic screen. In the screen, you see an error such as Failed modules:

/var/lib/vmware/configmanager/upgrade/lib/postLoadStore/libupgradepsadeviceconfig.so.

This issue is resolved in this release.

PR 2977496: An ESXi host might become unresponsive due to a rare file block (FB) allocation issue

A helper mechanism that caches FB resource allocation details working in background might accidentally stop and block FB resource allocation during I/O

operations to the ESXi host. In some cases, this issue might affect other processes working on the same file and block them. As a result, the ESXi host might

become unresponsive.

This issue is resolved in this release.

PR 3021384: vSAN File Service fails to enable when an isolated witness network is configured

vSAN File Service requires hosts to communicate with each other. File Service might incorrectly use an IP address in the witness network for inter-

communication. If you have configured an isolated witness network for vSAN, the host can communicate with a witness node over the witness network, but hosts

cannot communicate with each other over the witness network. Communication between hosts for vSAN File Service cannot be established.

This issue is resolved in this release.

PR 2988179: If an ESXi host is in a low memory state, insufficient heap allocation to a network module might cause the host to fail with a purple diagnostic

screen

If an ESXi host is in a low memory state, insufficient heap allocation to a network module might cause the port bitmap to be set to NULL. As a result, the ESXi host

might fail with a purple diagnostic screen when attempting to forward a packet.

This issue is resolved in this release. The fix makes sure that bit vectors in the portsBitmap property are set only when heap allocation is successful. However, you

still need to make sure that ESXi hosts have sufficient RAM to operate and forward packets successfully.

PR 3025470:  The Cluster Level Object Manager Daemon (CLOMD) fails during object format change

During object format change, some objects with old layout might get partially cleaned up, leaving the configuration in an invalid state. This problem can cause

CLOMD to fail whenever it attempts to process the object during reconfiguration.

You might see the following entries in clomd.log file:

2022-10-14T16:17:26.456Z PANIC: NOT_REACHED bora/lib/vsan/vsan_config_builder.c:744

2022-10-14T16:17:26.456Z Backtrace:

2022-10-14T16:17:26.456Z Backtrace[0] 0000030b4742c6a0 rip=000000bf0c7de98f rbx=0000030b4742c6a0

rbp=0000030b4742cad0 r12=000000bf0d677788 r13=0000030b4742cae8 r14=000000bf14ce052c r15=000000bf14ce3c2c

This issue is resolved in this release.

PR 3033159: ESXi hosts might fail with a purple diagnostic screen when a VM with bus sharing set to Physical issues a SCSI-2 reserve command

Windows 2012 and later use SCSI-3 reservation for resource arbitration to support Windows failover clustering (WSFC) on ESXi for cluster-across-box (CAB)

configurations. However, if you configure the bus sharing of the SCSI controller on that VM to Physical, the SCSI RESERVE command causes the ESXi host to fail

with a purple diagnostic screen. SCSI RESERVE is SCSI-2 semantic and is not supported with WSFC clusters on ESXi.

This issue is resolved in this release.

PR 3011169: vSAN cluster congestion due to cache buildup

vSAN might stop destaging data due to a counting issue of outstanding I/Os. If a VSAN disk group stops destaging data from the cache to the capacity tier, this

can cause data to accumulate in the cache tier. This problem leads to congestion, I/O throttling, and longer latency.

This issue is resolved in this release.

PR 3018832: If a cluster contains a 0-byte object, vSAN hosts might fail with a purple diagnostic screen

If a vSAN cluster with a 0-byte object receives a policy change request, the Cluster Level Object Manager (CLOM) might incorrectly set an invalid flag for one or

more components of the object. Such a flag can cause the host to send large writes that overload the system and cause the host to fail with a purple diagnostic

screen.

This issue is resolved in this release.

PR 2977486: Intermittent lock contention for VMFS journal blocks might delay VMFS rescan commands or cause mounting a datastore to fail

A rare issue with processing VMFS journal blocks might cause lock contention that results in delays of VMFS rescan operations or failed mounting of datastores.

In the vmkernel logs, you see errors such as Resource file for resource: 6 reached max limit 8192 and Resource file extension ('No space left on device').

This issue is resolved in this release.

PR 2932056: The vmx service might fail during cancellation of a vSphere Storage vMotion task and vCenter Server High Availability restarts virtual machines

In rare cases, the vmx service might fail during the cancellation of a vSphere Storage vMotion task. As a result, if your environment uses vCenter Server High

Availability, the service restarts the affected virtual machines.

This issue is resolved in this release.

PR 3013368: During booting, you see errors that support for SD Card and USB-only configurations is deprecated

When you use setups with only SD or USB devices to boot ESXi 7.x, you might see errors such as support for SD-Card/USB only configuration is being deprecated.

This message does not indicate an error, but only a warning that SD and USB devices are supported only for bootbank partitions, and for best performance, a

secondary persistent storage with a minimum of 32 GB must be provided for the /scratch and VMware Tools which reside in the OSData partition.

This issue is resolved in this release. The fix removes the message to avoid confusion, as deployments with no persistent storage are supported, although not a

best practice.

PR 2995471: vSAN disk encrypted and locked due to encryption key not available

This issue applies to vSAN hosts that use an external KMS for data-at-rest encryption. When you upgrade a vSAN host from 6.7 or earlier to 7.0 and later, the

KMS password is lost. The host's disks remain encrypted and locked.

This issue is resolved in this release.

PR 3006356: ESXi host fail with a purple diagnostic screen due to rebinding of virtual volumes

In rare cases, vSphere Virtual Volumes might attempt to rebind volumes on ESXi hosts that have SCSI Persistent Reservations. As a result, the ESXi hosts fail with

a purple diagnostic screen and an error such as Panic Message: @BlueScreen: PANIC bora/vmkernel/main/dlmalloc.c:4933 - Usage error in dlmalloc in the backtrace.

This issue is resolved in this release.

PR 2981272: vSphere Client displays 0 KB regardless of the actual VMDK size of a virtual machine

Due to a caching issue, in the vSphere Client you might see a VMDK size of 0 KB regardless of the actual size of virtual machines in a vSphere Virtual Volumes

environment.

This issue is resolved in this release.

PR 3000195: A cross site Advanced Cross vCenter vMotion operation might timeout and some virtual machines become unresponsive

During the storage migration part of a cross site Advanced Cross vCenter vMotion operation, some async I/Os at the storage stack might be trapped and not

properly time out. As a result, virtual machines remain waiting for a I/O response, which causes the Advanced Cross vCenter vMotion operation to time out and

the virtual machines to become unresponsive.

This issue is resolved in this release. The fix adds a timeout to every async I/O request to makes sure that a response is returned after the timeout.

PR 3026623: LLOG recovery might erroneously mark vSAN components as invalid

A problem during LLOG recovery can cause a vSAN component to be erroneously marked as invalid. This issue can lead to log build up and congestion.

This issue is resolved in this release.

PR 2981420: The Small-Footprint CIM Broker Daemon (SFCBD) intermittently fails

Due to insufficient resource pool allocation, some services that report to the SFCBD, such as sfcb-vmware_base and sfcb-vmw, might fail and generate zdump. In

the syslog.log file you see errors such as:

sfcb-vmware_base[2110110]: tool_mm_realloc_or_die: memory re-allocation failed(orig=364000 new=364800 msg=Cannot allocate memory, aborting

sfcb-vmw_ipmi[2291550]: tool_mm_realloc_or_die: memory re-allocation failed(orig=909200 new=909600 msg=Cannot allocate memory, aborting

This issue is resolved in this release. The fix increases the default pool size for sfcb-vmware_base service.

PR 3025469:  If the target policy is Raid 1, StripeWidth 1, policy reconfiguration of large objects in a vSAN cluster might stop with no progress

If the target policy is Raid 1, StripeWidth 1, when a vSAN cluster runs low on transient capacity, the Cluster Level Object Manager might keep reconfiguring the

same part of objects larger than 8TB. As a result, such objects remain in noncompliant state, and you might see some unnecessary resync operations.

This issue is resolved in this release.

PR 3042776: Storage I/O Control quickly generates a large volume of logs marked as critical that might also fill up the datastore

In VMware Aria Operations for Logs, formerly vRealize Log Insight, you might see a large volume of logs generated by Storage I/O Control such as Invalid share

value: 0. Using default. and Skipping device naa.xxxx either due to VSI read error or abnormal state. The volume of logs varies depending on the number of

ESXi hosts in a cluster and the number of devices in switched off state. When the issue occurs, the log volume generates quickly, within 24 hours, and VMware

Aria Operations for Logs might classify the messages as critical. However, such logs are harmless and do not impact the operations on other datastores that are

online.

This issue is resolved in this release. The fix moves such logs from error to trivia to prevent misleading logging.

PR 2916160: ESXi Managed Object Browser (MOB) shows the CPU status as unknown

If the storage sensor list of an ESXi host is empty, the CPU status that the Intelligent Platform Management Interface (IPMI) reports might reset. As a result, you

see the sensor data record with entity ID 3, which is the status of the processor, displayed incorrectly as Cannot report on the current status of the physical

element in the MOB.

This issue is resolved in this release. The fix makes sure that the CPU status resets only when refreshing the sensor state from IPMI fails.

PR 2996208: You see high read latency for objects in a stretch cluster

In stretch clusters, vSAN deploys each VMDK object with a specific format. When you change the policy of a VMDK object from hostFailuresToTolerate=0 to

hostFailuresToTolerate=1, the format might change in such a way that it can cause reads to transit the inter-site(cross-AZ) link. As a result, you see higher read

latency in such objects.

This issue is resolved in this release.

PR 3014374: In the vSphere Client, you see repeated options in the SCSI Bus Sharing drop-down menu

In the vSphere Client, when you create or reconfigure a virtual machine, under SCSI controller > SCSI Bus Sharing you might see doubling options in the drop-

down menu. The issue does not affect any of the VM create or configure workflows.

This issue is resolved in this release.

PR 2977957: After a migration operation, Windows 10 virtual machines might fail with a blue diagnostic screen and reports for a microcode revision mismatch

After a migration operation, Windows 10 virtual machines might fail with a blue diagnostic screen and report a microcode revision mismatch error such as:-

MICROCODE_REVISION_MISMATCH (17e). The issue occurs when a scan of the CPUs runs during the migration operation and the firmware of the source CPUs does not

match with the firmware of the destination CPUs.

This issue is resolved in this release.

PR 2979281: ESXi hosts might become unresponsive due to intermittent out of memory state in result of stale cache

In certain cases, clearing the cache of objects in a datastore volume on ESXi hosts fails, objects remain in the cache, and cause out of memory state. For

example, when connection with the underlying device of the volume drops. As a result, the ESXi host becomes unresponsive. In the logs, you see errors such as:

Cannot reconnect to xxxxx] or Failed to cleanup VMFS heartbeat on volume xxxxx: No connection. or

The volume on the device xxxxx locked, possibly because some remote host encountered an error during a volume operation and could not recover.

This issue is resolved in this release.

PR 2976953: The hostd service might fail and virtual machines shut down due to intermittent object cache exhaustion

Certain workflows like backup operations of ESXi hosts can open a large number of files which in turn could lead to object cache exhaustion. In such cases, you

might see the hostd service to fail, or virtual machines to shut down, or the VM to get into an invalid state that prevents it to power-on. In the logs, you see

warnings such as Cannot allocate memory.

This issue is resolved in this release. The fix doubles the object cache size to 183 MB to fit heavy workloads.

PR 2999968: vSAN health reports an error that the file server is restarting

In Monitor > Skyline Health > File Service > File Server Health, you might see the error File server is (re)starting.

The issue is caused by a cache overrun, which leads to failure of the VDFS daemon. In the /var/run/log/vdfsd-server.log file in an affected ESXi host, you see

messages such as NOT_IMPLEMENTED bora/vdfs/core/VDFSPhysicalLog.cpp.

This issue is resolved in this release.

PR 3031566: Changing the policy of a powered-on VM with an IDE controller throws an error that the attempted operation cannot be performed in the current

state

In the vSphere Client, when you change the policy of a powered-on VM with an IDE controller, you might see the error The attempted operation cannot be

performed in the current state ("Powered on").

This issue is resolved in this release.

PR 3036859: HCI Mesh fails to mount cluster after reenabling vSAN with data-in-transit encryption previously enabled

HCI Mesh cluster mount might fail after you deactivate vSAN with data-in-transit encryption, and then reenable vSAN.

This issue is resolved in this release.

PR 3029490: vSAN Skyline Health does not include Hardware Compatibility group

If NVMe drives used for vSAN have a duplicate PCI ID, and you restart the vSAN health service on vCenter Server, the Hardware Compatibility group is missing

from vSAN Skyline Health.

This issue is resolved in this release.

PR 2992407: TPM 2.0 attestation might fail on Lenovo servers with an insufficient buffer error

TPM 2.0 attestation on Lenovo servers returns the TPM error code: TSS2_SYS_RC_INSUFFICIENT_BUFFER.

This issue is resolved in this release.

esx-update_7.0.3-0.65.20842708

Patch Category

Patch Severity

Host Reboot Required

Bugfix

Critical

Yes

Virtual Machine Migration or Shutdown
Required

Affected Hardware

Affected Software

VIBs Included

PRs Fixed

CVE numbers

Yes

N/A

N/A

N/A

N/A

VMware_bootbank_esx-update_7.0.3-0.65.20842708
VMware_bootbank_loadesx_7.0.3-0.65.20842708

Updates the loadesx and esx-update VIBs.

Broadcom-ntg3_4.1.8.0-4vmw.703.0.65.20842708

Patch Category

Patch Severity

Host Reboot Required

Virtual Machine Migration or Shutdown
Required

Affected Hardware

Affected Software

VIBs Included

PRs Fixed

CVE numbers

Updates the ngt3 VIB to resolve the following issue:

Bugfix

Critical

Yes

Yes

N/A

N/A

VMW_bootbank_ntg3_4.1.8.0-4vmw.703.0.65.20842708

3007883

N/A

PR 3007883: You see link flapping on NICs that use the ntg3 driver of version 4.1.3 and later

When two NICs that use the ntg3 driver of versions 4.1.3 and later are connected directly, not to a physical switch port, link flapping might occur. The issue does

not occur on ntg3 drivers of versions earlier than 4.1.3 or the tg3 driver. This issue is not related to the occasional Energy Efficient Ethernet (EEE) link flapping on

such NICs. The fix for the EEE issue is to use an ntg3 driver of version 4.1.7 or later, or disable EEE on physical switch ports.

This issue is resolved in this release. ESXi 7.0 Update 3i comes with ntg3 driver version 4.1.8. However, after you upgrade the ntg3 driver to version 4.1.8, you

must set the new module parameter noPhyStateSet to 1. The noPhyStateSet parameter defaults to 0 and is not required in most environments, except they face the

issue.

ESXi_7.0.3-0.60.20841705

Patch Category

Patch Severity

Host Reboot Required

Virtual Machine Migration or Shutdown
Required

Affected Hardware

Affected Software

Security

Critical

Yes

Yes

N/A

N/A

VIBs Included

VMware_bootbank_esx-base_7.0.3-0.60.20841705
VMware_bootbank_trx_7.0.3-0.60.20841705
VMware_bootbank_vsanhealth_7.0.3-0.60.20841705
VMware_bootbank_cpu-microcode_7.0.3-0.60.20841705
VMware_bootbank_crx_7.0.3-0.60.20841705
VMware_bootbank_vsan_7.0.3-0.60.20841705
VMware_bootbank_native-misc-drivers_7.0.3-0.60.20841705
VMware_bootbank_esx-xserver_7.0.3-0.60.20841705
VMware_bootbank_esx-dvfilter-generic-fastpath_7.0.3-
0.60.20841705
VMware_bootbank_gc_7.0.3-0.60.20841705
VMware_bootbank_esx-ui_2.1.1-20188605
VMware_bootbank_vdfs_7.0.3-0.60.20841705
VMware_bootbank_bmcal_7.0.3-0.60.20841705
VMware_bootbank_esxio-combiner_7.0.3-0.60.20841705

PRs Fixed

CVE numbers

2993721, 3007957, 3007958, 3015560, 3034286, 3038621, 3030691

CVE-2020-28196, CVE-2022-31696, CVE-2022-31699

The ESXi and esx-update bulletins are dependent on each other. Always include both in a single ESXi host patch baseline or include the rollup bulletin in the baseline to

avoid failure during host patching.

Updates the esx-ui, esx-xserver, cpu-microcode, trx, vsanhealth, esx-base, esx-dvfilter-generic-fastpath, gc, esxio-combiner, native-misc-drivers, bmcal, vsan, vdfs,

and crx VIBs to resolve the following issues:

The cpu-microcode VIB includes the following Intel microcode:
Code Name

Plt ID MCU Rev

FMS

MCU Date

Brand Names

Nehalem EP

Clarkdale

Arrandale

Sandy Bridge DT

Westmere EP

0x106a5
(06/1a/5)

0x20652
(06/25/2)

0x20655
(06/25/5)

0x206a7
(06/2a/7)

0x206c2
(06/2c/2)

0x03 0x0000001d

5/11/2018

Intel Xeon 35xx Series;
Intel Xeon 55xx Series

0x12

0x00000011

5/8/2018

Intel i3/i5 Clarkdale Series;
Intel Xeon 34xx Clarkdale Series

0x92 0x00000007

4/23/2018 Intel Core i7-620LE Processor

0x12

0x0000002f

2/17/2019

0x03 0x0000001f

5/8/2018

Sandy Bridge EP

0x206d6
(06/2d/6)

0x6d 0x00000621

3/4/2020

Sandy Bridge EP

0x206d7
(06/2d/7)

0x6d 0x0000071a

3/24/2020

Nehalem EX

Westmere EX

0x206e6
(06/2e/6)

0x206f2
(06/2f/2)

0x04 0x0000000d

5/15/2018

0x05 0x0000003b

5/16/2018

Ivy Bridge DT

0x306a9
(06/3a/9)

0x12

0x00000021

2/13/2019

Haswell DT

0x306c3
(06/3c/3)

0x32 0x00000028

11/12/2019

Intel Xeon E3-1100 Series;
Intel Xeon E3-1200 Series;
Intel i7-2655-LE Series;
Intel i3-2100 Series

Intel Xeon 56xx Series;
Intel Xeon 36xx Series

Intel Pentium 1400 Series;
Intel Xeon E5-1400 Series;
Intel Xeon E5-1600 Series;
Intel Xeon E5-2400 Series;
Intel Xeon E5-2600 Series;
Intel Xeon E5-4600 Series

Intel Pentium 1400 Series;
Intel Xeon E5-1400 Series;
Intel Xeon E5-1600 Series;
Intel Xeon E5-2400 Series;
Intel Xeon E5-2600 Series;
Intel Xeon E5-4600 Series

Intel Xeon 65xx Series;
Intel Xeon 75xx Series

Intel Xeon E7-8800 Series;
Intel Xeon E7-4800 Series;
Intel Xeon E7-2800 Series

Intel i3-3200 Series;
Intel i7-3500-LE/UE;
Intel i7-3600-QE;
Intel Xeon E3-1200-v2 Series;
Intel Xeon E3-1100-C-v2 Series;
Intel Pentium B925C

Intel Xeon E3-1200-v3 Series;
Intel i7-4700-EQ Series;
Intel i5-4500-TE Series;
Intel i3-4300 Series

Intel Xeon E5-4600-v2 Series;
Intel Xeon E5-2600-v2 Series;
Intel Xeon E5-2400-v2 Series;
Intel Xeon E5-1600-v2 Series;
Intel Xeon E5-1400-v2 Series

Ivy Bridge EP

Ivy Bridge EX

Haswell EP

Haswell EX

Broadwell H

Avoton

0x306e4
(06/3e/4)

0x306e7
(06/3e/7)

0x306f2
(06/3f/2)

0x306f4
(06/3f/4)

0x40671
(06/47/1)

0x406d8
(06/4d/8)

0xed 0x0000042e

3/14/2019

0xed 0x00000715

3/14/2019

Intel Xeon E7-8800/4800/2800-v2
Series

0x6f

0x00000049

8/11/2021

Intel Xeon E5-4600-v3 Series;
Intel Xeon E5-2600-v3 Series;
Intel Xeon E5-2400-v3 Series;
Intel Xeon E5-1600-v3 Series;
Intel Xeon E5-1400-v3 Series

0x80 0x0000001a

5/24/2021 Intel Xeon E7-8800/4800-v3 Series

0x22 0x00000022

11/12/2019

0x01 0x0000012d

9/16/2019

Intel Core i7-5700EQ;
Intel Xeon E3-1200-v4 Series

Intel Atom C2300 Series;
Intel Atom C2500 Series;
Intel Atom C2700 Series

Intel Xeon E7-8800/4800-v4
Series;
Intel Xeon E5-4600-v4 Series;
Intel Xeon E5-2600-v4 Series;
Intel Xeon E5-1600-v4 Series

Broadwell EP/EX

0x406f1
(06/4f/1)

0xef

0x0b000040

5/19/2021

Code Name

FMS

Plt ID MCU Rev

MCU Date

Brand Names

Skylake SP

0x50654
(06/55/4)

0xb7 0x02006e05

3/8/2022

Cascade Lake B-
0

0x50656
(06/55/6)

0xbf

0x04003302

12/10/2021

Cascade Lake

0x50657
(06/55/7)

0xbf

0x05003302

12/10/2021

Intel Xeon Platinum 8100 Series;
Intel Xeon Gold 6100/5100, Silver
4100, Bronze 3100 Series;
Intel Xeon D-2100 Series;
Intel Xeon D-1600 Series;
Intel Xeon W-3100 Series;
Intel Xeon W-2100 Series

Intel Xeon Platinum 9200/8200
Series;
Intel Xeon Gold 6200/5200;
Intel Xeon Silver 4200/Bronze
3200;
Intel Xeon W-3200

Intel Xeon Platinum 9200/8200
Series;
Intel Xeon Gold 6200/5200;
Intel Xeon Silver 4200/Bronze
3200;
Intel Xeon W-3200

Intel Xeon Platinum 8300 Series;
Intel Xeon Gold 6300/5300

Cooper Lake

Broadwell DE

Broadwell DE

Broadwell DE

Broadwell NS

Skylake H/S

Denverton

Ice Lake SP

Ice Lake D

Snow Ridge

Snow Ridge

Kaby Lake H/S/X

Coffee Lake

Coffee Lake

Coffee Lake

Coffee Lake
Refresh

Rocket Lake S

0x5065b
(06/55/b)

0x50662
(06/56/2)

0x50663
(06/56/3)

0x50664
(06/56/4)

0x50665
(06/56/5)

0x506e3
(06/5e/3)

0x506f1
(06/5f/1)

0x606a6
(06/6a/6)

0x606c1
(06/6c/1)

0x80665
(06/86/5)

0x80667
(06/86/7)

0x906e9
(06/9e/9)

0x906ea
(06/9e/a)

0x906eb
(06/9e/b)

0x906ec
(06/9e/c)

0x906ed
(06/9e/d)

0xa0671
(06/a7/1)

0xbf

0x07002501

11/19/2021

0x10 0x0000001c

6/17/2019 Intel Xeon D-1500 Series

0x10 0x0700001c

6/12/2021 Intel Xeon D-1500 Series

0x10 0x0f00001a

6/12/2021 Intel Xeon D-1500 Series

0x10 0x0e000014

9/18/2021 Intel Xeon D-1600 Series

0x36 0x000000f0

11/12/2021

Intel Xeon E3-1500-v5 Series;
Intel Xeon E3-1200-v5 Series

0x01 0x00000038

12/2/2021 Intel Atom C3000 Series

0x87 0x0d000375

4/7/2022

Intel Xeon Silver 4300 Series;
Intel Xeon Gold 6300/5300 Series;
Intel Xeon Platinum 8300 Series

0x10 0x010001f0

6/24/2022 Intel Xeon D Series

0x01 0x4c000020

5/10/2022 Intel Atom P5000 Series

0x01 0x4c000020

5/10/2022 Intel Atom P5000 Series

0x2a 0x000000f0

11/12/2021

0x22 0x000000f0

11/15/2021

Intel Xeon E3-1200-v6 Series;
Intel Xeon E3-1500-v6 Series

Intel Xeon E-2100 Series;
Intel Xeon E-2200 Series (4 or 6
core)

0x02 0x000000f0

11/12/2021 Intel Xeon E-2100 Series

0x22 0x000000f0

11/15/2021 Intel Xeon E-2100 Series

0x22 0x000000f4

7/31/2022 Intel Xeon E-2200 Series (8 core)

0x02 0x00000056

8/2/2022 Intel Xeon E-2300 Series

ESXi 7.0 Update 3i provides the following security updates:

OpenSSL is updated to version 1.0.2zf.

Apache Thrift is updated to version 0.15.0.

The urllib3 client is updated to version 1.26.5.

cURL is updated to version 7.84.0.

The SQLite database is updated to version 3.39.2.

The Expat XML parser is updated to version 2.4.9.

This release resolves CVE-2022-31696, and CVE-2022-31699. For more information on these vulnerabilities and their impact on VMware products, see VMSA-

2022-0030.

esx-update_7.0.3-0.60.20841705

Patch Category

Patch Severity

Host Reboot Required

Virtual Machine Migration or Shutdown
Required

Affected Hardware

Affected Software

VIBs Included

PRs Fixed

CVE numbers

Updates the loadesx and esx-update VIBs.

VMware-VM-Tools_12.1.0.20219665-20841705

Patch Category

Patch Severity

Host Reboot Required

Virtual Machine Migration or Shutdown
Required

Affected Hardware

Affected Software

VIBs Included

PRs Fixed

CVE numbers

Updates the tools-light VIB.

Bugfix

Critical

Yes

Yes

N/A

N/A

N/A

N/A

VMware_bootbank_loadesx_7.0.3-0.60.20841705
VMware_bootbank_esx-update_7.0.3-0.60.20841705

Security

Critical

No

No

N/A

N/A

VMware_locker_tools-light_12.1.0.20219665-20841705

3015499

N/A

The following VMware Tools ISO images are bundled with ESXi 7.0 Update 3i:

windows.iso: VMware Tools 12.1.0 supports Windows 7 SP1 or Windows Server 2008 R2 SP1 and later.

linux.iso: VMware Tools 10.3.25 ISO image for Linux OS with glibc 2.11 or later.

The following VMware Tools ISO images are available for download:

VMware Tools 11.0.6:

windows.iso: for Windows Vista (SP2) and Windows Server 2008 Service Pack 2 (SP2).

VMware Tools 10.0.12:

winPreVista.iso: for Windows 2000, Windows XP, and Windows 2003.

linuxPreGLibc25.iso: supports Linux guest operating systems earlier than Red Hat Enterprise Linux (RHEL) 5, SUSE Linux Enterprise Server (SLES) 11,

Ubuntu 7.04, and other distributions with glibc version earlier than 2.5.

solaris.iso: VMware Tools image 10.3.10 for Solaris.

darwin.iso: Supports Mac OS X versions 10.11 and later.

Follow the procedures listed in the following documents to download VMware Tools for platforms not bundled with ESXi:

VMware Tools 12.1.0 Release Notes

Earlier versions of VMware Tools

What Every vSphere Admin Must Know About VMware Tools

VMware Tools for hosts provisioned with Auto Deploy

Updating VMware Tools

ESXi-7.0U3i-20842708-standard

Profile Name

ESXi-7.0U3i-20842708-standard

Build

Vendor

Release Date

Acceptance Level

For build information, see Patches Contained in this Release.

VMware, Inc.

December 8, 2022

PartnerSupported

Affected Hardware

N/A

Affected Software

N/A

VMware_bootbank_esx-xserver_7.0.3-0.65.20842708
VMware_bootbank_gc_7.0.3-0.65.20842708
VMware_bootbank_vsan_7.0.3-0.65.20842708
VMware_bootbank_cpu-microcode_7.0.3-0.65.20842708
VMware_bootbank_crx_7.0.3-0.65.20842708
VMware_bootbank_esx-base_7.0.3-0.65.20842708
VMware_bootbank_esx-dvfilter-generic-fastpath_7.0.3-0.65.20842708
VMware_bootbank_vsanhealth_7.0.3-0.65.20842708
VMware_bootbank_native-misc-drivers_7.0.3-0.65.20842708
VMware_bootbank_esx-ui_2.1.1-20188605
VMware_bootbank_trx_7.0.3-0.65.20842708
VMware_bootbank_bmcal_7.0.3-0.65.20842708
VMware_bootbank_vdfs_7.0.3-0.65.20842708
VMware_bootbank_esxio-combiner_7.0.3-0.65.20842708
VMware_bootbank_esx-update_7.0.3-0.65.20842708
VMware_bootbank_loadesx_7.0.3-0.65.20842708
VMW_bootbank_ntg3_4.1.8.0-4vmw.703.0.65.20842708
VMware_locker_tools-light_12.1.0.20219665-20841705

2962719, 3021935, 2994966, 2983919, 3014323, 2996511, 2982013, 3011324,
3048788, 2983043, 3035934, 3015498, 3031708, 3041100, 3044475,
2990414, 3003866, 3011850, 3002779, 3033161, 3016132, 3015493, 3029194,
3007116, 2977496, 3021384, 2988179, 3025470, 3033159, 3011169, 3018832,
2977486, 2932056, 3013368, 2995471, 3006356, 2981272, 3000195,
3026623, 2981420, 3025469, 3042776, 2916160, 2996208, 3014374,
2977957, 2979281, 2976953, 2999968, 3031566, 3036859, 3029490,
3007883, 2992407

Affected VIBs

PRs Fixed

Related CVE numbers

N/A

This patch updates the following issues:

Starting with 7.0 Update 2, ESXi supports Posted Interrupts (PI) on Intel CPUs for PCI passthrough devices to improve the overall system performance. In some

cases, a race between PIs and the VMkernel scheduling might occur. As a result, virtual machines that are configured with PCI passthrough devices with

normal or low latency sensitivity might experience soft lockups.

In rare cases, VM events might report the template property, which indicates if a virtual machine is marked as a template, incorrectly. As a result, you might see

the template property as true even if the VM is not a template VM or as false, when a VM is marked as a template.

Due to a missing Memory Module Entity for Cisco servers in the Managed Object Browser, you might not see the memory status info of an ESXi host by using

MOB.

In very rare occasions, random issues with Active Directory environments that might lead to unresponsive state of the domain controllers, might also result in

unresponsiveness of the hostd service.

Rarely, due to the fast suspend resume mechanism used to create or revert a VM to a snapshot, the internal state of the VMX process might reinitialize without

notification to the upper layers of the virtual infrastructure management stack. As a result, all guest-related performance counters that VMware Tools provides

stop updating. In all interfaces to the ESXi host, you continuously see the last recorded values.

When the rhttpproxy service performs multiple operations on incoming URIs, it might miscalculate the buffer offset of each connection, which potentially leads

to errors such as buffer overflows and negative reads. As a result, the service fails.

By default, modifying hypercall options by using commands such as vm | Get-AdvancedSetting -Name isolation.tools.autoInstall.disable works only when the

VM is powered off. For powered on VMs such calls trigger the error The attempted operation cannot be performed in the current state (Powered on). This is

expected.

After you update an ESXi 7.0 Update 3c host to a later version of 7.0.x or install or remove an ESXi 7.0 Update 3c VIB and reboot the host, you might see all

security advanced options on the host revert to their default values. The affected advanced settings are:

Security.AccountLockFailures

Security.AccountUnlockTime

Security.PasswordQualityControl

Security.PasswordHistory

Security.PasswordMaxDays

Security.SshSessionLimit

Security.DefaultShellAccess

The command esxcli hardware pci list, which reports the NUMA node for ESXi host devices, returns the correct NUMA node for the Physical Functions (PF) of

an SR-IOV device, but returns zero for its Virtual Functions (VF).

After an upgrade to ESXi 7.0 Update 2 or later, when you migrate Windows virtual machines to the upgraded hosts by using vSphere vMotion, some VMs

might fail with a blue diagnostic screen after the migration. In the screen, you see the error OS failed to boot with no operating system found. The issue occurs

due to a fault in the address optimization logic of the Virtual Machine File System (VMFS).

If the client application has no registered default request handler, requests with a path that is not present in the handler map might cause the execution

of vmacore.dll to fail. As a result, you see the ESXi host as disconnected from the vCenter Server system.

Some allocation reservation operations might go over the limit of 128 parallel reservation keys and exceed the allocated memory range of an ESXi host. As a

result, ESXi hosts might fail with a purple diagnostic screen during resource allocation reservation operations. In the error screen, you see messages such

as PSOD BlueScreen: #PF Exception 14 in world 2097700:SCSI period IP.

If you run an unclaim command on a device or path while virtual machines on the device still have active I/Os, the ESXi host might fail with a purple diagnostic

screen. In the screen, you see a message such as PSOD at bora/modules/vmkernel/nmp/nmp_misc.c:3839 during load/unload of lpfc.

When TXT is enabled on an ESX host, attempts to power-on a VM might fail with an error. In the vSphere Client, you see a message such as This host supports

Intel VT-x, but Intel VT-x is restricted. Intel VT-x might be restricted because 'trusted execution' has been enabled in the BIOS/firmware settings or because

the host has not been power-cycled since changing this setting.

Due to a rare issue with handling AVX2 instructions, a virtual machine of version ESX 7.0 Update 3f might fail with ESX unrecoverable error. In

the vmware.log file, you see a message such as: MONITOR PANIC: vcpu-0:VMM fault 6: src=MONITOR ....

The issue is specific for virtual machines with hardware versions 12 or earlier.

If you use an ESXi .iso image created by using the Image Builder to make a vSphere Lifecycle Manager upgrade baseline for ESXi hosts, upgrades by using

such baselines might fail. In the vSphere Client, you see an error such as Cannot execute upgrade script on host. On the impacted ESXi host, in

the /var/log/vua*.log file, you see an error such as ValueError: Should have base image when an addon exists.

The error occurs when the existing image of the ESXi host has an add-on, but the Image Builder-generated ISO provides no add-on.

Many parallel requests for memory regions by virtual machines using the Data Plane Development Kit (DPDK) on an ESXi host might exceed the XMAP

memory space on the host. As a result, the host fails with a purple diagnostic screen and an error such as: Panic Message: @BlueScreen: VERIFY

bora/vmkernel/hardware/pci/config.c:157.

After an upgrade of ESXi hosts to ESXi 7.0 Update 3 and later, you might no longer see some performance reports for virtual machines with NVMe controllers.

For example, you do not see the Virtual Disk - Aggregate of all Instances chart in the VMware Aria Operations.

In rare cases, such as scheduled reboot of the primary VM with FT encryption that runs heavy workloads, the secondary VM might not have sufficient buffer to

decrypt more than 512 MB of dirty pages in a single FT checkpoint and experience a buffer overflow error. As a result, the ESXi host on which the secondary

VM resides might fail with a purple diagnostic screen.

In rare cases, after upgrade or update to ESXi 7.0 Update 2 and later, the vSAN storage configuration might lose the tag mark_ssd and default to HDD.

Even when a device or LUN is in a detached state, the Pluggable Storage Architecture (PSA) might still attempt to register the object. PSA files a log for each

path evaluation step at every path evaluation interval of such attempts. As a result, you might see multiple identical messages such as nmp_RegisterDeviceEvents

failed for device registration, which are not necessary while the device or LUN is detached.

If you change a device configuration at runtime, changes might not be reflected in the ESXi ConfigStore that holds the configurations for an ESXi host. As a

result, the datastore might not mount after the ESXi host reboots.

Starting from ESXi 6.0, mClock is the default I/O scheduler for ESXi, but some environments might still use legacy schedulers of ESXi versions earlier than 6.0.

As a result, upgrades of such hosts to ESXi 7.0 Update 3 and later might fail with a purple diagnostic screen.

Starting with ESXi 7.0 Update 1, the configuration management of ESXi hosts moved from the /etc/vmware/esx.conf file to the ConfigStore framework, which

makes an explicit segregation of state and configuration. Tokens in the esx.conf file such as implicit_support or explicit_support that indicate a state, are not

recognized as valid tokens, and are ignored by the satp_alua module. As a result, when you upgrade ESXi hosts to ESXi 7.0 Update 3d or later by using a host

profile with tokens indicating ALUA state, the operation might fail with a purple diagnostic screen. In the screen, you see an error such as Failed modules:

/var/lib/vmware/configmanager/upgrade/lib/postLoadStore/libupgradepsadeviceconfig.so.

A helper mechanism that caches FB resource allocation details working in background might accidentally stop and block FB resource allocation during I/O

operations to the ESXi host. In some cases, this issue might affect other processes working on the same file and block them. As a result, the ESXi host might

become unresponsive.

vSAN File Service requires hosts to communicate with each other. File Service might incorrectly use an IP address in the witness network for inter-

communication. If you have configured an isolated witness network for vSAN, the host can communicate with a witness node over the witness network, but

hosts cannot communicate with each other over the witness network. Communication between hosts for vSAN File Service cannot be established.

If an ESXi host is in a low memory state, insufficient heap allocation to a network module might cause the port bitmap to be set to NULL. As a result, the ESXi

host might fail with a purple diagnostic screen when attempting to forward a packet.

During object format change, some objects with old layout might get partially cleaned up, leaving the configuration in an invalid state. This problem can cause

CLOMD to fail whenever it attempts to process the object during reconfiguration.

You might see the following entries in clomd.log file:

2022-10-14T16:17:26.456Z PANIC: NOT_REACHED bora/lib/vsan/vsan_config_builder.c:744

2022-10-14T16:17:26.456Z Backtrace:

2022-10-14T16:17:26.456Z Backtrace[0] 0000030b4742c6a0 rip=000000bf0c7de98f rbx=0000030b4742c6a0

rbp=0000030b4742cad0 r12=000000bf0d677788 r13=0000030b4742cae8 r14=000000bf14ce052c r15=000000bf14ce3c2c

Windows 2012 and later use SCSI-3 reservation for resource arbitration to support Windows failover clustering (WSFC) on ESXi for cluster-across-box (CAB)

configurations. However, if you configure the bus sharing of the SCSI controller on that VM to Physical, the SCSI RESERVE command causes the ESXi host to fail

with a purple diagnostic screen. SCSI RESERVE is SCSI-2 semantic and is not supported with WSFC clusters on ESXi.

vSAN might stop destaging data due to a counting issue of outstanding I/Os. If a VSAN disk group stops destaging data from the cache to the capacity tier,

this can cause data to accumulate in the cache tier. This problem leads to congestion, I/O throttling, and longer latency.

If a vSAN cluster with a 0-byte object receives a policy change request, the Cluster Level Object Manager (CLOM) might incorrectly set an invalid flag for one

or more components of the object. Such a flag can cause the host to send large writes that overload the system and cause the host to fail with a purple

diagnostic screen.

A rare issue with processing VMFS journal blocks might cause lock contention that results in delays of VMFS rescan operations or failed mounting of

datastores. In the vmkernel logs, you see errors such as Resource file for resource: 6 reached max limit 8192 and Resource file extension ('No space left on

device').

In rare cases, the vmx service might fail during the cancellation of a vSphere Storage vMotion task. As a result, if your environment uses vCenter Server High

Availability, the service restarts the affected virtual machines.

When you use setups with only SD or USB devices to boot ESXi 7.x, you might see errors such as support for SD-Card/USB only configuration is being

deprecated. This message does not indicate an error, but only a warning that SD and USB devices are supported only for bootbank partitions, and for best

performance, a secondary persistent storage with a minimum of 32 GB must be provided for the /scratch and VMware Tools which reside in the OSData

partition.

This issue applies to vSAN hosts that use an external KMS for data-at-rest encryption. When you upgrade a vSAN host from 6.7 or earlier to 7.0 and later, the

KMS password is lost. The host's disks remain encrypted and locked.

In rare cases, vSphere Virtual Volumes might attempt to rebind volumes on ESXi hosts that have SCSI Persistent Reservations. As a result, the ESXi hosts fail

with a purple diagnostic screen and an error such as Panic Message: @BlueScreen: PANIC bora/vmkernel/main/dlmalloc.c:4933 - Usage error in dlmalloc in the

backtrace.

Due to a caching issue, in the vSphere Client you might see a VMDK size of 0 KB regardless of the actual size of virtual machines in a vSphere Virtual Volumes

environment.

During the storage migration part of a cross site Advanced Cross vCenter vMotion operation, some async I/Os at the storage stack might be trapped and not

properly time out. As a result, virtual machines remain waiting for a I/O response, which causes the Advanced Cross vCenter vMotion operation to time out

and the virtual machines to become unresponsive.

A problem during LLOG recovery can cause a vSAN component to be erroneously marked as invalid. This issue can lead to log build up and congestion.

Due to insufficient resource pool allocation, some services that report to the SFCBD, such as sfcb-vmware_base and sfcb-vmw, might fail and generate zdump.

In the syslog.log file you see errors such as:

sfcb-vmware_base[2110110]: tool_mm_realloc_or_die: memory re-allocation failed(orig=364000 new=364800 msg=Cannot allocate memory, aborting

sfcb-vmw_ipmi[2291550]: tool_mm_realloc_or_die: memory re-allocation failed(orig=909200 new=909600 msg=Cannot allocate memory, aborting

If the target policy is Raid 1, StripeWidth 1, when a vSAN cluster runs low on transient capacity, the Cluster Level Object Manager might keep reconfiguring the

same part of objects larger than 8TB. As a result, such objects remain in noncompliant state, and you might see some unnecessary resync operations.

In VMware Aria Operations for Logs, formerly vRealize Log Insight, you might see a large volume of logs generated by Storage I/O Control such as Invalid

share value: 0. Using default. and Skipping device naa.xxxx either due to VSI read error or abnormal state. The volume of logs varies depending on the

number of ESXi hosts in a cluster and the number of devices in switched off state. When the issue occurs, the log volume generates quickly, within 24 hours,

and VMware Aria Operations for Logs might classify the messages as critical. However, such logs are harmless and do not impact the operations on other

datastores that are online.

If the storage sensor list of an ESXi host is empty, the CPU status that the Intelligent Platform Management Interface (IPMI) reports might reset. As a result, you

see the sensor data record with entity ID 3, which is the status of the processor, displayed incorrectly as Cannot report on the current status of the physical

element in the MOB.

In stretch clusters, vSAN deploys each VMDK object with a specific format. When you change the policy of a VMDK object

from hostFailuresToTolerate=0 to hostFailuresToTolerate=1, the format might change in such a way that it can cause reads to transit the inter-site(cross-AZ) link.

As a result, you see higher read latency in such objects.

In the vSphere Client, when you create or reconfigure a virtual machine, under SCSI controller > SCSI Bus Sharing you might see doubling options in the drop-

down menu. The issue does not affect any of the VM create or configure workflows.

After a migration operation, Windows 10 virtual machines might fail with a blue diagnostic screen and report a microcode revision mismatch error such as:-

 MICROCODE_REVISION_MISMATCH (17e). The issue occurs when a scan of the CPUs runs during the migration operation and the firmware of the source CPUs does

not match with the firmware of the destination CPUs.

In certain cases, clearing the cache of objects in a datastore volume on ESXi hosts fails, objects remain in the cache, and cause out of memory state. For

example, when connection with the underlying device of the volume drops. As a result, the ESXi host becomes unresponsive. In the logs, you see errors such

as:

Cannot reconnect to xxxxx] or Failed to cleanup VMFS heartbeat on volume xxxxx: No connection. or

The volume on the device xxxxx locked, possibly because some remote host encountered an error during a volume operation and could not recover.

Certain workflows like backup operations of ESXi hosts can open a large number of files which in turn could lead to object cache exhaustion. In such cases,

you might see the hostd service to fail, or virtual machines to shut down, or the VM to get into an invalid state that prevents it to power-on. In the logs, you see

warnings such as Cannot allocate memory.

In Monitor > Skyline Health > File Service > File Server Health, you might see the error File server is (re)starting.

The issue is caused by a cache overrun, which leads to failure of the VDFS daemon. In the /var/run/log/vdfsd-server.log file in an affected ESXi host, you see

messages such as NOT_IMPLEMENTED bora/vdfs/core/VDFSPhysicalLog.cpp.

In the vSphere Client, when you change the policy of a powered-on VM with an IDE controller, you might see the error The attempted operation cannot be

performed in the current state ("Powered on").

HCI Mesh cluster mount might fail after you deactivate vSAN with data-in-transit encryption, and then reenable vSAN.

If NVMe drives used for vSAN have a duplicate PCI ID, and you restart the vSAN health service on vCenter Server, the Hardware Compatibility group is missing

from vSAN Skyline Health.

When two NICs that use the ntg3 driver of versions 4.1.3 and later are connected directly, not to a physical switch port, link flapping might occur. The issue

does not occur on ntg3 drivers of versions earlier than 4.1.3 or the tg3 driver. This issue is not related to the occasional Energy Efficient Ethernet (EEE) link

flapping on such NICs. The fix for the EEE issue is to use an ntg3 driver of version 4.1.7 or later, or disable EEE on physical switch ports.

TPM 2.0 attestation on Lenovo servers returns the TPM error code: TSS2_SYS_RC_INSUFFICIENT_BUFFER.

ESXi-7.0U3i-20842708-no-tools

Profile Name

ESXi-7.0U3i-20842708-no-tools

Build

Vendor

Release Date

Acceptance Level

Affected Hardware

Affected Software

Affected VIBs

PRs Fixed

For build information, see Patches Contained in this Release.

VMware, Inc.

December 8, 2022

PartnerSupported

N/A

N/A

VMware_bootbank_esx-xserver_7.0.3-0.65.20842708
VMware_bootbank_gc_7.0.3-0.65.20842708
VMware_bootbank_vsan_7.0.3-0.65.20842708
VMware_bootbank_cpu-microcode_7.0.3-0.65.20842708
VMware_bootbank_crx_7.0.3-0.65.20842708
VMware_bootbank_esx-base_7.0.3-0.65.20842708
VMware_bootbank_esx-dvfilter-generic-fastpath_7.0.3-0.65.20842708
VMware_bootbank_vsanhealth_7.0.3-0.65.20842708
VMware_bootbank_native-misc-drivers_7.0.3-0.65.20842708
VMware_bootbank_esx-ui_2.1.1-20188605
VMware_bootbank_trx_7.0.3-0.65.20842708
VMware_bootbank_bmcal_7.0.3-0.65.20842708
VMware_bootbank_vdfs_7.0.3-0.65.20842708
VMware_bootbank_esxio-combiner_7.0.3-0.65.20842708
VMware_bootbank_esx-update_7.0.3-0.65.20842708
VMware_bootbank_loadesx_7.0.3-0.65.20842708
VMW_bootbank_ntg3_4.1.8.0-4vmw.703.0.65.20842708

2962719, 3021935, 2994966, 2983919, 3014323, 2996511, 2982013, 3011324,
3048788, 2983043, 3035934, 3015498, 3031708, 3041100, 3044475,
2990414, 3003866, 3011850, 3002779, 3033161, 3016132, 3015493, 3029194,
3007116, 2977496, 3021384, 2988179, 3025470, 3033159, 3011169, 3018832,
2977486, 2932056, 3013368, 2995471, 3006356, 2981272, 3000195,
3026623, 2981420, 3025469, 3042776, 2916160, 2996208, 3014374,
2977957, 2979281, 2976953, 2999968, 3031566, 3036859, 3029490,
3007883, 2992407

Related CVE numbers

N/A

This patch updates the following issues:

Starting with 7.0 Update 2, ESXi supports Posted Interrupts (PI) on Intel CPUs for PCI passthrough devices to improve the overall system performance. In some

cases, a race between PIs and the VMkernel scheduling might occur. As a result, virtual machines that are configured with PCI passthrough devices with

normal or low latency sensitivity might experience soft lockups.

In rare cases, VM events might report the template property, which indicates if a virtual machine is marked as a template, incorrectly. As a result, you might see

the template property as true even if the VM is not a template VM or as false, when a VM is marked as a template.

Due to a missing Memory Module Entity for Cisco servers in the Managed Object Browser, you might not see the memory status info of an ESXi host by using

MOB.

In very rare occasions, random issues with Active Directory environments that might lead to unresponsive state of the domain controllers, might also result in

unresponsiveness of the hostd service.

Rarely, due to the fast suspend resume mechanism used to create or revert a VM to a snapshot, the internal state of the VMX process might reinitialize without

notification to the upper layers of the virtual infrastructure management stack. As a result, all guest-related performance counters that VMware Tools provides

stop updating. In all interfaces to the ESXi host, you continuously see the last recorded values.

When the rhttpproxy service performs multiple operations on incoming URIs, it might miscalculate the buffer offset of each connection, which potentially leads

to errors such as buffer overflows and negative reads. As a result, the service fails.

By default, modifying hypercall options by using commands such as vm | Get-AdvancedSetting -Name isolation.tools.autoInstall.disable works only when the

VM is powered off. For powered on VMs such calls trigger the error The attempted operation cannot be performed in the current state (Powered on). This is

expected.

After you update an ESXi 7.0 Update 3c host to a later version of 7.0.x or install or remove an ESXi 7.0 Update 3c VIB and reboot the host, you might see all

security advanced options on the host revert to their default values. The affected advanced settings are:

Security.AccountLockFailures

Security.AccountUnlockTime

Security.PasswordQualityControl

Security.PasswordHistory

Security.PasswordMaxDays

Security.SshSessionLimit

Security.DefaultShellAccess

The command esxcli hardware pci list, which reports the NUMA node for ESXi host devices, returns the correct NUMA node for the Physical Functions (PF) of

an SR-IOV device, but returns zero for its Virtual Functions (VF).

After an upgrade to ESXi 7.0 Update 2 or later, when you migrate Windows virtual machines to the upgraded hosts by using vSphere vMotion, some VMs

might fail with a blue diagnostic screen after the migration. In the screen, you see the error OS failed to boot with no operating system found. The issue occurs

due to a fault in the address optimization logic of the Virtual Machine File System (VMFS).

If the client application has no registered default request handler, requests with a path that is not present in the handler map might cause the execution

of vmacore.dll to fail. As a result, you see the ESXi host as disconnected from the vCenter Server system.

Some allocation reservation operations might go over the limit of 128 parallel reservation keys and exceed the allocated memory range of an ESXi host. As a

result, ESXi hosts might fail with a purple diagnostic screen during resource allocation reservation operations. In the error screen, you see messages such

as PSOD BlueScreen: #PF Exception 14 in world 2097700:SCSI period IP.

If you run an unclaim command on a device or path while virtual machines on the device still have active I/Os, the ESXi host might fail with a purple diagnostic

screen. In the screen, you see a message such as PSOD at bora/modules/vmkernel/nmp/nmp_misc.c:3839 during load/unload of lpfc.

When TXT is enabled on an ESX host, attempts to power-on a VM might fail with an error. In the vSphere Client, you see a message such as This host supports

Intel VT-x, but Intel VT-x is restricted. Intel VT-x might be restricted because 'trusted execution' has been enabled in the BIOS/firmware settings or because

the host has not been power-cycled since changing this setting.

Due to a rare issue with handling AVX2 instructions, a virtual machine of version ESX 7.0 Update 3f might fail with ESX unrecoverable error. In

the vmware.log file, you see a message such as: MONITOR PANIC: vcpu-0:VMM fault 6: src=MONITOR ....

The issue is specific for virtual machines with hardware versions 12 or earlier.

If you use an ESXi .iso image created by using the Image Builder to make a vSphere Lifecycle Manager upgrade baseline for ESXi hosts, upgrades by using

such baselines might fail. In the vSphere Client, you see an error such as Cannot execute upgrade script on host. On the impacted ESXi host, in

the /var/log/vua*.log file, you see an error such as ValueError: Should have base image when an addon exists.

The error occurs when the existing image of the ESXi host has an add-on, but the Image Builder-generated ISO provides no add-on.

Many parallel requests for memory regions by virtual machines using the Data Plane Development Kit (DPDK) on an ESXi host might exceed the XMAP

memory space on the host. As a result, the host fails with a purple diagnostic screen and an error such as: Panic Message: @BlueScreen: VERIFY

bora/vmkernel/hardware/pci/config.c:157.

After an upgrade of ESXi hosts to ESXi 7.0 Update 3 and later, you might no longer see some performance reports for virtual machines with NVMe controllers.

For example, you do not see the Virtual Disk - Aggregate of all Instances chart in the VMware Aria Operations.

In rare cases, such as scheduled reboot of the primary VM with FT encryption that runs heavy workloads, the secondary VM might not have sufficient buffer to

decrypt more than 512 MB of dirty pages in a single FT checkpoint and experience a buffer overflow error. As a result, the ESXi host on which the secondary

VM resides might fail with a purple diagnostic screen.

In rare cases, after upgrade or update to ESXi 7.0 Update 2 and later, the vSAN storage configuration might lose the tag mark_ssd and default to HDD.

Even when a device or LUN is in a detached state, the Pluggable Storage Architecture (PSA) might still attempt to register the object. PSA files a log for each

path evaluation step at every path evaluation interval of such attempts. As a result, you might see multiple identical messages such as nmp_RegisterDeviceEvents

failed for device registration, which are not necessary while the device or LUN is detached.

If you change a device configuration at runtime, changes might not be reflected in the ESXi ConfigStore that holds the configurations for an ESXi host. As a

result, the datastore might not mount after the ESXi host reboots.

Starting from ESXi 6.0, mClock is the default I/O scheduler for ESXi, but some environments might still use legacy schedulers of ESXi versions earlier than 6.0.

As a result, upgrades of such hosts to ESXi 7.0 Update 3 and later might fail with a purple diagnostic screen.

Starting with ESXi 7.0 Update 1, the configuration management of ESXi hosts moved from the /etc/vmware/esx.conf file to the ConfigStore framework, which

makes an explicit segregation of state and configuration. Tokens in the esx.conf file such as implicit_support or explicit_support that indicate a state, are not

recognized as valid tokens, and are ignored by the satp_alua module. As a result, when you upgrade ESXi hosts to ESXi 7.0 Update 3d or later by using a host

profile with tokens indicating ALUA state, the operation might fail with a purple diagnostic screen. In the screen, you see an error such as Failed modules:

/var/lib/vmware/configmanager/upgrade/lib/postLoadStore/libupgradepsadeviceconfig.so.

A helper mechanism that caches FB resource allocation details working in background might accidentally stop and block FB resource allocation during I/O

operations to the ESXi host. In some cases, this issue might affect other processes working on the same file and block them. As a result, the ESXi host might

become unresponsive.

vSAN File Service requires hosts to communicate with each other. File Service might incorrectly use an IP address in the witness network for inter-

communication. If you have configured an isolated witness network for vSAN, the host can communicate with a witness node over the witness network, but

hosts cannot communicate with each other over the witness network. Communication between hosts for vSAN File Service cannot be established.

If an ESXi host is in a low memory state, insufficient heap allocation to a network module might cause the port bitmap to be set to NULL. As a result, the ESXi

host might fail with a purple diagnostic screen when attempting to forward a packet.

During object format change, some objects with old layout might get partially cleaned up, leaving the configuration in an invalid state. This problem can cause

CLOMD to fail whenever it attempts to process the object during reconfiguration.

You might see the following entries in clomd.log file:

2022-10-14T16:17:26.456Z PANIC: NOT_REACHED bora/lib/vsan/vsan_config_builder.c:744

2022-10-14T16:17:26.456Z Backtrace:

2022-10-14T16:17:26.456Z Backtrace[0] 0000030b4742c6a0 rip=000000bf0c7de98f rbx=0000030b4742c6a0

rbp=0000030b4742cad0 r12=000000bf0d677788 r13=0000030b4742cae8 r14=000000bf14ce052c r15=000000bf14ce3c2c

Windows 2012 and later use SCSI-3 reservation for resource arbitration to support Windows failover clustering (WSFC) on ESXi for cluster-across-box (CAB)

configurations. However, if you configure the bus sharing of the SCSI controller on that VM to Physical, the SCSI RESERVE command causes the ESXi host to fail

with a purple diagnostic screen. SCSI RESERVE is SCSI-2 semantic and is not supported with WSFC clusters on ESXi.

vSAN might stop destaging data due to a counting issue of outstanding I/Os. If a VSAN disk group stops destaging data from the cache to the capacity tier,

this can cause data to accumulate in the cache tier. This problem leads to congestion, I/O throttling, and longer latency.

If a vSAN cluster with a 0-byte object receives a policy change request, the Cluster Level Object Manager (CLOM) might incorrectly set an invalid flag for one

or more components of the object. Such a flag can cause the host to send large writes that overload the system and cause the host to fail with a purple

diagnostic screen.

A rare issue with processing VMFS journal blocks might cause lock contention that results in delays of VMFS rescan operations or failed mounting of

datastores. In the vmkernel logs, you see errors such as Resource file for resource: 6 reached max limit 8192 and Resource file extension ('No space left on

device').

In rare cases, the vmx service might fail during the cancellation of a vSphere Storage vMotion task. As a result, if your environment uses vCenter Server High

Availability, the service restarts the affected virtual machines.

When you use setups with only SD or USB devices to boot ESXi 7.x, you might see errors such as support for SD-Card/USB only configuration is being

deprecated. This message does not indicate an error, but only a warning that SD and USB devices are supported only for bootbank partitions, and for best

performance, a secondary persistent storage with a minimum of 32 GB must be provided for the /scratch and VMware Tools which reside in the OSData

partition.

This issue applies to vSAN hosts that use an external KMS for data-at-rest encryption. When you upgrade a vSAN host from 6.7 or earlier to 7.0 and later, the

KMS password is lost. The host's disks remain encrypted and locked.

In rare cases, vSphere Virtual Volumes might attempt to rebind volumes on ESXi hosts that have SCSI Persistent Reservations. As a result, the ESXi hosts fail

with a purple diagnostic screen and an error such as Panic Message: @BlueScreen: PANIC bora/vmkernel/main/dlmalloc.c:4933 - Usage error in dlmalloc in the

backtrace.

Due to a caching issue, in the vSphere Client you might see a VMDK size of 0 KB regardless of the actual size of virtual machines in a vSphere Virtual Volumes

environment.

During the storage migration part of a cross site Advanced Cross vCenter vMotion operation, some async I/Os at the storage stack might be trapped and not

properly time out. As a result, virtual machines remain waiting for a I/O response, which causes the Advanced Cross vCenter vMotion operation to time out

and the virtual machines to become unresponsive.

A problem during LLOG recovery can cause a vSAN component to be erroneously marked as invalid. This issue can lead to log build up and congestion.

Due to insufficient resource pool allocation, some services that report to the SFCBD, such as sfcb-vmware_base and sfcb-vmw, might fail and generate zdump.

In the syslog.log file you see errors such as:

sfcb-vmware_base[2110110]: tool_mm_realloc_or_die: memory re-allocation failed(orig=364000 new=364800 msg=Cannot allocate memory, aborting

sfcb-vmw_ipmi[2291550]: tool_mm_realloc_or_die: memory re-allocation failed(orig=909200 new=909600 msg=Cannot allocate memory, aborting

If the target policy is Raid 1, StripeWidth 1, when a vSAN cluster runs low on transient capacity, the Cluster Level Object Manager might keep reconfiguring the

same part of objects larger than 8TB. As a result, such objects remain in noncompliant state, and you might see some unnecessary resync operations.

In VMware Aria Operations for Logs, formerly vRealize Log Insight, you might see a large volume of logs generated by Storage I/O Control such as Invalid

share value: 0. Using default. and Skipping device naa.xxxx either due to VSI read error or abnormal state. The volume of logs varies depending on the

number of ESXi hosts in a cluster and the number of devices in switched off state. When the issue occurs, the log volume generates quickly, within 24 hours,

and VMware Aria Operations for Logs might classify the messages as critical. However, such logs are harmless and do not impact the operations on other

datastores that are online.

If the storage sensor list of an ESXi host is empty, the CPU status that the Intelligent Platform Management Interface (IPMI) reports might reset. As a result, you

see the sensor data record with entity ID 3, which is the status of the processor, displayed incorrectly as Cannot report on the current status of the physical

element in the MOB.

In stretch clusters, vSAN deploys each VMDK object with a specific format. When you change the policy of a VMDK object

from hostFailuresToTolerate=0 to hostFailuresToTolerate=1, the format might change in such a way that it can cause reads to transit the inter-site(cross-AZ) link.

As a result, you see higher read latency in such objects.

In the vSphere Client, when you create or reconfigure a virtual machine, under SCSI controller > SCSI Bus Sharing you might see doubling options in the drop-

down menu. The issue does not affect any of the VM create or configure workflows.

After a migration operation, Windows 10 virtual machines might fail with a blue diagnostic screen and report a microcode revision mismatch error such as:-

 MICROCODE_REVISION_MISMATCH (17e). The issue occurs when a scan of the CPUs runs during the migration operation and the firmware of the source CPUs does

not match with the firmware of the destination CPUs.

In certain cases, clearing the cache of objects in a datastore volume on ESXi hosts fails, objects remain in the cache, and cause out of memory state. For

example, when connection with the underlying device of the volume drops. As a result, the ESXi host becomes unresponsive. In the logs, you see errors such

as:

Cannot reconnect to xxxxx] or Failed to cleanup VMFS heartbeat on volume xxxxx: No connection. or

The volume on the device xxxxx locked, possibly because some remote host encountered an error during a volume operation and could not recover.

Certain workflows like backup operations of ESXi hosts can open a large number of files which in turn could lead to object cache exhaustion. In such cases,

you might see the hostd service to fail, or virtual machines to shut down, or the VM to get into an invalid state that prevents it to power-on. In the logs, you see

warnings such as Cannot allocate memory.

In Monitor > Skyline Health > File Service > File Server Health, you might see the error File server is (re)starting.

The issue is caused by a cache overrun, which leads to failure of the VDFS daemon. In the /var/run/log/vdfsd-server.log file in an affected ESXi host, you see

messages such as NOT_IMPLEMENTED bora/vdfs/core/VDFSPhysicalLog.cpp.

In the vSphere Client, when you change the policy of a powered-on VM with an IDE controller, you might see the error The attempted operation cannot be

performed in the current state ("Powered on").

HCI Mesh cluster mount might fail after you deactivate vSAN with data-in-transit encryption, and then reenable vSAN.

If NVMe drives used for vSAN have a duplicate PCI ID, and you restart the vSAN health service on vCenter Server, the Hardware Compatibility group is missing

from vSAN Skyline Health.

When two NICs that use the ntg3 driver of versions 4.1.3 and later are connected directly, not to a physical switch port, link flapping might occur. The issue

does not occur on ntg3 drivers of versions earlier than 4.1.3 or the tg3 driver. This issue is not related to the occasional Energy Efficient Ethernet (EEE) link

flapping on such NICs. The fix for the EEE issue is to use an ntg3 driver of version 4.1.7 or later, or disable EEE on physical switch ports.

TPM 2.0 attestation on Lenovo servers returns the TPM error code: TSS2_SYS_RC_INSUFFICIENT_BUFFER.

ESXi-7.0U3si-20841705-standard

Profile Name

ESXi-7.0U3si-20841705-standard

Build

Vendor

Release Date

Acceptance Level

For build information, see Patches Contained in this Release.

VMware, Inc.

December 8, 2022

PartnerSupported

Affected Hardware

Affected Software

N/A

N/A

Affected VIBs

VMware_bootbank_esx-base_7.0.3-0.60.20841705
VMware_bootbank_trx_7.0.3-0.60.20841705
VMware_bootbank_vsanhealth_7.0.3-0.60.20841705
VMware_bootbank_cpu-microcode_7.0.3-0.60.20841705
VMware_bootbank_crx_7.0.3-0.60.20841705
VMware_bootbank_vsan_7.0.3-0.60.20841705
VMware_bootbank_native-misc-drivers_7.0.3-0.60.20841705
VMware_bootbank_esx-xserver_7.0.3-0.60.20841705
VMware_bootbank_esx-dvfilter-generic-fastpath_7.0.3-0.60.20841705
VMware_bootbank_gc_7.0.3-0.60.20841705
VMware_bootbank_esx-ui_2.1.1-20188605
VMware_bootbank_vdfs_7.0.3-0.60.20841705
VMware_bootbank_bmcal_7.0.3-0.60.20841705
VMware_bootbank_esxio-combiner_7.0.3-0.60.20841705
VMware_bootbank_loadesx_7.0.3-0.60.20841705
VMware_bootbank_esx-update_7.0.3-0.60.20841705
VMware_locker_tools-light_12.1.0.20219665-20841705

PRs Fixed

2993721, 3007957, 3007958, 3015560, 3034286, 3038621,
3030691, 3015499

Related CVE numbers

CVE-2020-28196, CVE-2022-31696, CVE-2022-31699

This patch updates the following issues:

The cpu-microcode VIB includes the following Intel microcode:
Code Name

Plt ID MCU Rev

FMS

MCU Date Brand Names

Nehalem EP

Clarkdale

Arrandale

0x106a5
(06/1a/5)

0x20652
(06/25/2)

0x20655
(06/25/5)

0x03 0x0000001d

5/11/2018

0x12 0x00000011

5/8/2018

0x92 0x00000007 4/23/2018

Sandy Bridge
DT

0x206a7
(06/2a/7)

0x12 0x0000002f

2/17/2019

Westmere EP

0x206c2
(06/2c/2)

0x03 0x0000001f

5/8/2018

Sandy Bridge
EP

0x206d6
(06/2d/6)

0x6d 0x00000621

3/4/2020

Sandy Bridge
EP

0x206d7
(06/2d/7)

0x6d 0x0000071a

3/24/2020

Nehalem EX

0x206e6
(06/2e/6)

0x04 0x0000000d 5/15/2018

Westmere EX

0x206f2
(06/2f/2)

0x05 0x0000003b

5/16/2018

Ivy Bridge DT

0x306a9
(06/3a/9)

0x12 0x00000021

2/13/2019

Intel Xeon 35xx Series;
Intel Xeon 55xx Series

Intel i3/i5 Clarkdale Series;
Intel Xeon 34xx Clarkdale
Series

Intel Core i7-620LE
Processor

Intel Xeon E3-1100 Series;
Intel Xeon E3-1200 Series;
Intel i7-2655-LE Series;
Intel i3-2100 Series

Intel Xeon 56xx Series;
Intel Xeon 36xx Series

Intel Pentium 1400 Series;
Intel Xeon E5-1400 Series;
Intel Xeon E5-1600 Series;
Intel Xeon E5-2400
Series;
Intel Xeon E5-2600
Series;
Intel Xeon E5-4600 Series

Intel Pentium 1400 Series;
Intel Xeon E5-1400 Series;
Intel Xeon E5-1600 Series;
Intel Xeon E5-2400
Series;
Intel Xeon E5-2600
Series;
Intel Xeon E5-4600 Series

Intel Xeon 65xx Series;
Intel Xeon 75xx Series

Intel Xeon E7-8800
Series;
Intel Xeon E7-4800
Series;
Intel Xeon E7-2800 Series

Intel i3-3200 Series;
Intel i7-3500-LE/UE;
Intel i7-3600-QE;
Intel Xeon E3-1200-v2
Series;
Intel Xeon E3-1100-C-v2
Series;
Intel Pentium B925C

Code Name

FMS

Plt ID MCU Rev

MCU Date Brand Names

Haswell DT

0x306c3
(06/3c/3)

0x32 0x00000028

11/12/2019

Ivy Bridge EP

0x306e4
(06/3e/4)

0xed 0x0000042e

3/14/2019

Ivy Bridge EX

0x306e7
(06/3e/7)

0xed 0x00000715

3/14/2019

Haswell EP

0x306f2
(06/3f/2)

0x6f 0x00000049

8/11/2021

Haswell EX

Broadwell H

Avoton

0x306f4
(06/3f/4)

0x40671
(06/47/1)

0x406d8
(06/4d/8)

0x80 0x0000001a

5/24/2021

0x22 0x00000022

11/12/2019

0x01 0x0000012d

9/16/2019

Broadwell
EP/EX

0x406f1
(06/4f/1)

0xef 0x0b000040 5/19/2021

Skylake SP

0x50654
(06/55/4)

0xb7 0x02006e05

3/8/2022

Cascade
Lake B-0

0x50656
(06/55/6)

0xbf 0x04003302 12/10/2021

Cascade
Lake

0x50657
(06/55/7)

0xbf 0x05003302

12/10/2021

Cooper Lake

0x5065b
(06/55/b)

0xbf 0x07002501

11/19/2021

Intel Xeon E3-1200-v3
Series;
Intel i7-4700-EQ Series;
Intel i5-4500-TE Series;
Intel i3-4300 Series

Intel Xeon E5-4600-v2
Series;
Intel Xeon E5-2600-v2
Series;
Intel Xeon E5-2400-v2
Series;
Intel Xeon E5-1600-v2
Series;
Intel Xeon E5-1400-v2
Series

Intel Xeon E7-
8800/4800/2800-v2
Series

Intel Xeon E5-4600-v3
Series;
Intel Xeon E5-2600-v3
Series;
Intel Xeon E5-2400-v3
Series;
Intel Xeon E5-1600-v3
Series;
Intel Xeon E5-1400-v3
Series

Intel Xeon E7-8800/4800-
v3 Series

Intel Core i7-5700EQ;
Intel Xeon E3-1200-v4
Series

Intel Atom C2300 Series;
Intel Atom C2500 Series;
Intel Atom C2700 Series

Intel Xeon E7-8800/4800-
v4 Series;
Intel Xeon E5-4600-v4
Series;
Intel Xeon E5-2600-v4
Series;
Intel Xeon E5-1600-v4
Series

Intel Xeon Platinum 8100
Series;
Intel Xeon Gold
6100/5100, Silver 4100,
Bronze 3100 Series;
Intel Xeon D-2100 Series;
Intel Xeon D-1600 Series;
Intel Xeon W-3100 Series;
Intel Xeon W-2100 Series

Intel Xeon Platinum
9200/8200 Series;
Intel Xeon Gold
6200/5200;
Intel Xeon Silver
4200/Bronze 3200;
Intel Xeon W-3200

Intel Xeon Platinum
9200/8200 Series;
Intel Xeon Gold
6200/5200;
Intel Xeon Silver
4200/Bronze 3200;
Intel Xeon W-3200

Intel Xeon Platinum 8300
Series;
Intel Xeon Gold
6300/5300

Code Name

FMS

Plt ID MCU Rev

MCU Date Brand Names

Broadwell DE

Broadwell DE

Broadwell DE

Broadwell NS

Skylake H/S

0x50662
(06/56/2)

0x50663
(06/56/3)

0x50664
(06/56/4)

0x50665
(06/56/5)

0x506e3
(06/5e/3)

Denverton

0x506f1
(06/5f/1)

0x10 0x0000001c

6/17/2019 Intel Xeon D-1500 Series

0x10 0x0700001c

6/12/2021 Intel Xeon D-1500 Series

0x10 0x0f00001a

6/12/2021 Intel Xeon D-1500 Series

0x10 0x0e000014

9/18/2021 Intel Xeon D-1600 Series

0x36 0x000000f0

11/12/2021

Intel Xeon E3-1500-v5
Series;
Intel Xeon E3-1200-v5
Series

0x01 0x00000038

12/2/2021 Intel Atom C3000 Series

Ice Lake SP

0x606a6
(06/6a/6)

0x87 0x0d000375

4/7/2022

Intel Xeon Silver 4300
Series;
Intel Xeon Gold
6300/5300 Series;
Intel Xeon Platinum 8300
Series

Ice Lake D

Snow Ridge

Snow Ridge

0x606c1
(06/6c/1)

0x80665
(06/86/5)

0x80667
(06/86/7)

0x10 0x010001f0

6/24/2022 Intel Xeon D Series

0x01 0x4c000020 5/10/2022 Intel Atom P5000 Series

0x01 0x4c000020 5/10/2022 Intel Atom P5000 Series

Kaby Lake
H/S/X

0x906e9
(06/9e/9)

0x2a 0x000000f0

11/12/2021

Coffee Lake

Coffee Lake

Coffee Lake

0x906ea
(06/9e/a)

0x906eb
(06/9e/b)

0x906ec
(06/9e/c)

Coffee Lake
Refresh

0x906ed
(06/9e/d)

Rocket Lake
S

0xa0671
(06/a7/1)

0x22 0x000000f0

11/15/2021

0x02 0x000000f0

11/12/2021 Intel Xeon E-2100 Series

0x22 0x000000f0

11/15/2021 Intel Xeon E-2100 Series

0x22 0x000000f4

7/31/2022

Intel Xeon E-2200 Series
(8 core)

0x02 0x00000056

8/2/2022 Intel Xeon E-2300 Series

Intel Xeon E3-1200-v6
Series;
Intel Xeon E3-1500-v6
Series

Intel Xeon E-2100 Series;
Intel Xeon E-2200 Series
(4 or 6 core)

ESXi 7.0 Update 3i provides the following security updates:

OpenSSL is updated to version 1.0.2zf.

Apache Thrift is updated to version 0.15.0.

The urllib3 client is updated to version 1.26.5.

cURL is updated to version 7.84.0.

The SQLite database is updated to version 3.39.2.

The Expat XML parser is updated to version 2.4.9.

This release resolves CVE-2022-31696, and CVE-2022-31699. For more information on these vulnerabilities and their impact on VMware products, see VMSA-

2022-0030.

The following VMware Tools ISO images are bundled with ESXi 7.0 Update 3i:

windows.iso: VMware Tools 12.1.0 supports Windows 7 SP1 or Windows Server 2008 R2 SP1 and later.

linux.iso: VMware Tools 10.3.25 ISO image for Linux OS with glibc 2.11 or later.

The following VMware Tools ISO images are available for download:

VMware Tools 11.0.6:

windows.iso: for Windows Vista (SP2) and Windows Server 2008 Service Pack 2 (SP2).

VMware Tools 10.0.12:

winPreVista.iso: for Windows 2000, Windows XP, and Windows 2003.

linuxPreGLibc25.iso: supports Linux guest operating systems earlier than Red Hat Enterprise Linux (RHEL) 5, SUSE Linux Enterprise Server (SLES) 11,

Ubuntu 7.04, and other distributions with glibc version earlier than 2.5.

solaris.iso: VMware Tools image 10.3.10 for Solaris.

darwin.iso: Supports Mac OS X versions 10.11 and later.

Follow the procedures listed in the following documents to download VMware Tools for platforms not bundled with ESXi:

VMware Tools 12.1.0 Release Notes

Earlier versions of VMware Tools

What Every vSphere Admin Must Know About VMware Tools

VMware Tools for hosts provisioned with Auto Deploy

Updating VMware Tools

ESXi-7.0U3si-20841705-no-tools

Profile Name

ESXi-7.0U3si-20841705-no-tools

Build

Vendor

Release Date

Acceptance Level

Affected Hardware

Affected Software

Affected VIBs

For build information, see Patches Contained in this Release.

VMware, Inc.

December 8, 2022

PartnerSupported

N/A

N/A

VMware_bootbank_esx-base_7.0.3-0.60.20841705
VMware_bootbank_trx_7.0.3-0.60.20841705
VMware_bootbank_vsanhealth_7.0.3-0.60.20841705
VMware_bootbank_cpu-microcode_7.0.3-0.60.20841705
VMware_bootbank_crx_7.0.3-0.60.20841705
VMware_bootbank_vsan_7.0.3-0.60.20841705
VMware_bootbank_native-misc-drivers_7.0.3-0.60.20841705
VMware_bootbank_esx-xserver_7.0.3-0.60.20841705
VMware_bootbank_esx-dvfilter-generic-fastpath_7.0.3-0.60.20841705
VMware_bootbank_gc_7.0.3-0.60.20841705
VMware_bootbank_esx-ui_2.1.1-20188605
VMware_bootbank_vdfs_7.0.3-0.60.20841705
VMware_bootbank_bmcal_7.0.3-0.60.20841705
VMware_bootbank_esxio-combiner_7.0.3-0.60.20841705
VMware_bootbank_loadesx_7.0.3-0.60.20841705
VMware_bootbank_esx-update_7.0.3-0.60.20841705

PRs Fixed

2993721, 3007957, 3007958, 3015560, 3034286, 3038621, 3030691

Related CVE numbers

CVE-2020-28196, CVE-2022-31696, CVE-2022-31699

This patch updates the following issues:

The cpu-microcode VIB includes the following Intel microcode:
Code Name

Plt ID MCU Rev

FMS

MCU Date Brand Names

Nehalem EP

Clarkdale

Arrandale

0x106a5
(06/1a/5)

0x20652
(06/25/2)

0x20655
(06/25/5)

0x03 0x0000001d

5/11/2018

0x12 0x00000011

5/8/2018

0x92 0x00000007 4/23/2018

Sandy Bridge
DT

0x206a7
(06/2a/7)

0x12 0x0000002f

2/17/2019

Westmere EP

0x206c2
(06/2c/2)

0x03 0x0000001f

5/8/2018

Sandy Bridge
EP

0x206d6
(06/2d/6)

0x6d 0x00000621

3/4/2020

Intel Xeon 35xx Series;
Intel Xeon 55xx Series

Intel i3/i5 Clarkdale Series;
Intel Xeon 34xx Clarkdale
Series

Intel Core i7-620LE
Processor

Intel Xeon E3-1100 Series;
Intel Xeon E3-1200 Series;
Intel i7-2655-LE Series;
Intel i3-2100 Series

Intel Xeon 56xx Series;
Intel Xeon 36xx Series

Intel Pentium 1400 Series;
Intel Xeon E5-1400 Series;
Intel Xeon E5-1600 Series;
Intel Xeon E5-2400
Series;
Intel Xeon E5-2600
Series;
Intel Xeon E5-4600 Series

Code Name

FMS

Plt ID MCU Rev

MCU Date Brand Names

Sandy Bridge
EP

0x206d7
(06/2d/7)

0x6d 0x0000071a

3/24/2020

Nehalem EX

0x206e6
(06/2e/6)

0x04 0x0000000d 5/15/2018

Westmere EX

0x206f2
(06/2f/2)

0x05 0x0000003b

5/16/2018

Ivy Bridge DT

0x306a9
(06/3a/9)

0x12 0x00000021

2/13/2019

Haswell DT

0x306c3
(06/3c/3)

0x32 0x00000028

11/12/2019

Ivy Bridge EP

0x306e4
(06/3e/4)

0xed 0x0000042e

3/14/2019

Ivy Bridge EX

0x306e7
(06/3e/7)

0xed 0x00000715

3/14/2019

Haswell EP

0x306f2
(06/3f/2)

0x6f 0x00000049

8/11/2021

Haswell EX

Broadwell H

Avoton

0x306f4
(06/3f/4)

0x40671
(06/47/1)

0x406d8
(06/4d/8)

0x80 0x0000001a

5/24/2021

0x22 0x00000022

11/12/2019

0x01 0x0000012d

9/16/2019

Broadwell
EP/EX

0x406f1
(06/4f/1)

0xef 0x0b000040 5/19/2021

Intel Pentium 1400 Series;
Intel Xeon E5-1400 Series;
Intel Xeon E5-1600 Series;
Intel Xeon E5-2400
Series;
Intel Xeon E5-2600
Series;
Intel Xeon E5-4600 Series

Intel Xeon 65xx Series;
Intel Xeon 75xx Series

Intel Xeon E7-8800
Series;
Intel Xeon E7-4800
Series;
Intel Xeon E7-2800 Series

Intel i3-3200 Series;
Intel i7-3500-LE/UE;
Intel i7-3600-QE;
Intel Xeon E3-1200-v2
Series;
Intel Xeon E3-1100-C-v2
Series;
Intel Pentium B925C

Intel Xeon E3-1200-v3
Series;
Intel i7-4700-EQ Series;
Intel i5-4500-TE Series;
Intel i3-4300 Series

Intel Xeon E5-4600-v2
Series;
Intel Xeon E5-2600-v2
Series;
Intel Xeon E5-2400-v2
Series;
Intel Xeon E5-1600-v2
Series;
Intel Xeon E5-1400-v2
Series

Intel Xeon E7-
8800/4800/2800-v2
Series

Intel Xeon E5-4600-v3
Series;
Intel Xeon E5-2600-v3
Series;
Intel Xeon E5-2400-v3
Series;
Intel Xeon E5-1600-v3
Series;
Intel Xeon E5-1400-v3
Series

Intel Xeon E7-8800/4800-
v3 Series

Intel Core i7-5700EQ;
Intel Xeon E3-1200-v4
Series

Intel Atom C2300 Series;
Intel Atom C2500 Series;
Intel Atom C2700 Series

Intel Xeon E7-8800/4800-
v4 Series;
Intel Xeon E5-4600-v4
Series;
Intel Xeon E5-2600-v4
Series;
Intel Xeon E5-1600-v4
Series

Code Name

FMS

Plt ID MCU Rev

MCU Date Brand Names

Skylake SP

0x50654
(06/55/4)

0xb7 0x02006e05

3/8/2022

Cascade
Lake B-0

0x50656
(06/55/6)

0xbf 0x04003302 12/10/2021

Cascade
Lake

0x50657
(06/55/7)

0xbf 0x05003302

12/10/2021

Cooper Lake

0x5065b
(06/55/b)

0xbf 0x07002501

11/19/2021

Intel Xeon Platinum 8100
Series;
Intel Xeon Gold
6100/5100, Silver 4100,
Bronze 3100 Series;
Intel Xeon D-2100 Series;
Intel Xeon D-1600 Series;
Intel Xeon W-3100 Series;
Intel Xeon W-2100 Series

Intel Xeon Platinum
9200/8200 Series;
Intel Xeon Gold
6200/5200;
Intel Xeon Silver
4200/Bronze 3200;
Intel Xeon W-3200

Intel Xeon Platinum
9200/8200 Series;
Intel Xeon Gold
6200/5200;
Intel Xeon Silver
4200/Bronze 3200;
Intel Xeon W-3200

Intel Xeon Platinum 8300
Series;
Intel Xeon Gold
6300/5300

Broadwell DE

Broadwell DE

Broadwell DE

Broadwell NS

Skylake H/S

0x50662
(06/56/2)

0x50663
(06/56/3)

0x50664
(06/56/4)

0x50665
(06/56/5)

0x506e3
(06/5e/3)

Denverton

0x506f1
(06/5f/1)

0x10 0x0000001c

6/17/2019 Intel Xeon D-1500 Series

0x10 0x0700001c

6/12/2021 Intel Xeon D-1500 Series

0x10 0x0f00001a

6/12/2021 Intel Xeon D-1500 Series

0x10 0x0e000014

9/18/2021 Intel Xeon D-1600 Series

0x36 0x000000f0

11/12/2021

Intel Xeon E3-1500-v5
Series;
Intel Xeon E3-1200-v5
Series

0x01 0x00000038

12/2/2021 Intel Atom C3000 Series

Ice Lake SP

0x606a6
(06/6a/6)

0x87 0x0d000375

4/7/2022

Intel Xeon Silver 4300
Series;
Intel Xeon Gold
6300/5300 Series;
Intel Xeon Platinum 8300
Series

Ice Lake D

Snow Ridge

Snow Ridge

0x606c1
(06/6c/1)

0x80665
(06/86/5)

0x80667
(06/86/7)

0x10 0x010001f0

6/24/2022 Intel Xeon D Series

0x01 0x4c000020 5/10/2022 Intel Atom P5000 Series

0x01 0x4c000020 5/10/2022 Intel Atom P5000 Series

Kaby Lake
H/S/X

0x906e9
(06/9e/9)

0x2a 0x000000f0

11/12/2021

Coffee Lake

Coffee Lake

Coffee Lake

0x906ea
(06/9e/a)

0x906eb
(06/9e/b)

0x906ec
(06/9e/c)

Coffee Lake
Refresh

0x906ed
(06/9e/d)

0x22 0x000000f0

11/15/2021

0x02 0x000000f0

11/12/2021 Intel Xeon E-2100 Series

0x22 0x000000f0

11/15/2021 Intel Xeon E-2100 Series

0x22 0x000000f4

7/31/2022

Intel Xeon E-2200 Series
(8 core)

Intel Xeon E3-1200-v6
Series;
Intel Xeon E3-1500-v6
Series

Intel Xeon E-2100 Series;
Intel Xeon E-2200 Series
(4 or 6 core)

Code Name

FMS

Plt ID MCU Rev

MCU Date Brand Names

Rocket Lake
S

0xa0671
(06/a7/1)

0x02 0x00000056

8/2/2022 Intel Xeon E-2300 Series

ESXi 7.0 Update 3i provides the following security updates:

OpenSSL is updated to version 1.0.2zf.

Apache Thrift is updated to version 0.15.0.

The urllib3 client is updated to version 1.26.5.

cURL is updated to version 7.84.0.

The SQLite database is updated to version 3.39.2.

The Expat XML parser is updated to version 2.4.9.

This release resolves CVE-2022-31696, and CVE-2022-31699. For more information on these vulnerabilities and their impact on VMware products, see VMSA-

2022-0030.

The following VMware Tools ISO images are bundled with ESXi 7.0 Update 3i:

windows.iso: VMware Tools 12.1.0 supports Windows 7 SP1 or Windows Server 2008 R2 SP1 and later.

linux.iso: VMware Tools 10.3.25 ISO image for Linux OS with glibc 2.11 or later.

The following VMware Tools ISO images are available for download:

VMware Tools 11.0.6:

windows.iso: for Windows Vista (SP2) and Windows Server 2008 Service Pack 2 (SP2).

VMware Tools 10.0.12:

winPreVista.iso: for Windows 2000, Windows XP, and Windows 2003.

linuxPreGLibc25.iso: supports Linux guest operating systems earlier than Red Hat Enterprise Linux (RHEL) 5, SUSE Linux Enterprise Server (SLES) 11,

Ubuntu 7.04, and other distributions with glibc version earlier than 2.5.

solaris.iso: VMware Tools image 10.3.10 for Solaris.

darwin.iso: Supports Mac OS X versions 10.11 and later.

Follow the procedures listed in the following documents to download VMware Tools for platforms not bundled with ESXi:

VMware Tools 12.1.0 Release Notes

Earlier versions of VMware Tools

What Every vSphere Admin Must Know About VMware Tools

VMware Tools for hosts provisioned with Auto Deploy

Updating VMware Tools

ESXi_7.0.3-0.65.20842708

Name

Version

Release Date

Category

Affected Components

PRs Fixed

Related CVE numbers

ESXi_7.0.3-0.60.20841705

Name

Version

Release Date

Category

Affected Components

PRs Fixed

ESXi

ESXi_7.0.3-0.65.20842708

December 8, 2022

Bugfix

ESXi Component - core ESXi VIBs
ESXi Install/Upgrade Component
Broadcom NetXtreme I ESX VMKAPI ethernet driver

N/A

ESXi

ESXi_7.0.3-0.60.20841705

December 8, 2022

Security

ESXi Component - core ESXi VIBs
ESXi Install/Upgrade Component
ESXi Tools Component

Related CVE numbers

N/A

Known Issues

The known issues are grouped as follows.

Installation, Upgrade and Migration Issues

Known Issues from Previous Releases

Installation, Upgrade and Migration Issues

The vlanid property in custom installation scripts might not work

If you use a custom installation script that sets the vlanid property to specify a desired VLAN, the property might not take effect on newly installed ESXi hosts.

The issue occurs only when a physical NIC is already connected to DHCP when the installation starts. The vlanid property works properly when you use a newly

connected NIC.

Workaround: Manually set the VLAN from the Direct Console User Interface after you boot the ESXi host. Alternatively, disable the physical NIC and then boot

the host.

HPE servers with Trusted Platform Module (TPM) boot, but remote attestation fails

Some HPE servers do not have enough event log space to properly finish TPM remote attestation. As a result, the VMkernel boots, but remote attestation fails

due to the truncated log.

Workaround: None.

Known Issues from Previous Releases

To view a list of previous known issues, click here.

Copyright © Broadcom


