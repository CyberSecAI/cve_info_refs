=== Content from github.com_ce1ccd88_20250114_211737.html ===

[Skip to content](#start-of-content)

## Navigation Menu

Toggle navigation

[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fblob%2Fmaster%2Ftensorflow%2Fpython%2Fkeras%2Flosses.py)

* Product

  + [GitHub Copilot
    Write better code with AI](https://github.com/features/copilot)
  + [Security
    Find and fix vulnerabilities](https://github.com/features/security)
  + [Actions
    Automate any workflow](https://github.com/features/actions)
  + [Codespaces
    Instant dev environments](https://github.com/features/codespaces)
  + [Issues
    Plan and track work](https://github.com/features/issues)
  + [Code Review
    Manage code changes](https://github.com/features/code-review)
  + [Discussions
    Collaborate outside of code](https://github.com/features/discussions)
  + [Code Search
    Find more, search less](https://github.com/features/code-search)

  Explore
  + [All features](https://github.com/features)
  + [Documentation](https://docs.github.com)
  + [GitHub Skills](https://skills.github.com)
  + [Blog](https://github.blog)
* Solutions

  By company size
  + [Enterprises](https://github.com/enterprise)
  + [Small and medium teams](https://github.com/team)
  + [Startups](https://github.com/enterprise/startups)
  By use case
  + [DevSecOps](/solutions/use-case/devsecops)
  + [DevOps](/solutions/use-case/devops)
  + [CI/CD](/solutions/use-case/ci-cd)
  + [View all use cases](/solutions/use-case)

  By industry
  + [Healthcare](/solutions/industry/healthcare)
  + [Financial services](/solutions/industry/financial-services)
  + [Manufacturing](/solutions/industry/manufacturing)
  + [Government](/solutions/industry/government)
  + [View all industries](/solutions/industry)

  [View all solutions](/solutions)
* Resources

  Topics
  + [AI](/resources/articles/ai)
  + [DevOps](/resources/articles/devops)
  + [Security](/resources/articles/security)
  + [Software Development](/resources/articles/software-development)
  + [View all](/resources/articles)

  Explore
  + [Learning Pathways](https://resources.github.com/learn/pathways)
  + [White papers, Ebooks, Webinars](https://resources.github.com)
  + [Customer Stories](https://github.com/customer-stories)
  + [Partners](https://partner.github.com)
  + [Executive Insights](https://github.com/solutions/executive-insights)
* Open Source

  + [GitHub Sponsors
    Fund open source developers](/sponsors)
  + [The ReadME Project
    GitHub community articles](https://github.com/readme)
  Repositories
  + [Topics](https://github.com/topics)
  + [Trending](https://github.com/trending)
  + [Collections](https://github.com/collections)
* Enterprise

  + [Enterprise platform
    AI-powered developer platform](/enterprise)
  Available add-ons
  + [Advanced Security
    Enterprise-grade security features](https://github.com/enterprise/advanced-security)
  + [GitHub Copilot
    Enterprise-grade AI features](/features/copilot#enterprise)
  + [Premium Support
    Enterprise-grade 24/7 support](/premium-support)
* [Pricing](https://github.com/pricing)

Search or jump to...

# Search code, repositories, users, issues, pull requests...

Search

Clear

[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)

# Provide feedback

We read every piece of feedback, and take your input very seriously.

Include my email address so I can be contacted

  Cancel

 Submit feedback

# Saved searches

## Use saved searches to filter your results more quickly

Name

Query

To see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).

  Cancel

 Create saved search

[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fblob%2Fmaster%2Ftensorflow%2Fpython%2Fkeras%2Flosses.py)

[Sign up](/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E%2Fblob%2Fshow&source=header-repo&source_repo=tensorflow%2Ftensorflow)
Reseting focus

You signed in with another tab or window. Reload to refresh your session.
You signed out in another tab or window. Reload to refresh your session.
You switched accounts on another tab or window. Reload to refresh your session.

Dismiss alert

{{ message }}

[tensorflow](/tensorflow)
/
**[tensorflow](/tensorflow/tensorflow)**
Public

* [Notifications](/login?return_to=%2Ftensorflow%2Ftensorflow) You must be signed in to change notification settings
* [Fork
  74.4k](/login?return_to=%2Ftensorflow%2Ftensorflow)
* [Star
   187k](/login?return_to=%2Ftensorflow%2Ftensorflow)

* [Code](/tensorflow/tensorflow)
* [Issues
  827](/tensorflow/tensorflow/issues)
* [Pull requests
  5k+](/tensorflow/tensorflow/pulls)
* [Actions](/tensorflow/tensorflow/actions)
* [Projects
  2](/tensorflow/tensorflow/projects)
* [Security](/tensorflow/tensorflow/security)
* [Insights](/tensorflow/tensorflow/pulse)

Additional navigation options

* [Code](/tensorflow/tensorflow)
* [Issues](/tensorflow/tensorflow/issues)
* [Pull requests](/tensorflow/tensorflow/pulls)
* [Actions](/tensorflow/tensorflow/actions)
* [Projects](/tensorflow/tensorflow/projects)
* [Security](/tensorflow/tensorflow/security)
* [Insights](/tensorflow/tensorflow/pulse)

## Files

 master
## Breadcrumbs

1. [tensorflow](/tensorflow/tensorflow/tree/master)
2. /[tensorflow](/tensorflow/tensorflow/tree/master/tensorflow)
3. /[python](/tensorflow/tensorflow/tree/master/tensorflow/python)
4. /[keras](/tensorflow/tensorflow/tree/master/tensorflow/python/keras)
/
# losses.py

Copy path Blame  Blame
## Latest commit

## History

[History](/tensorflow/tensorflow/commits/master/tensorflow/python/keras/losses.py)2106 lines (1695 loc) · 75.3 KB master
## Breadcrumbs

1. [tensorflow](/tensorflow/tensorflow/tree/master)
2. /[tensorflow](/tensorflow/tensorflow/tree/master/tensorflow)
3. /[python](/tensorflow/tensorflow/tree/master/tensorflow/python)
4. /[keras](/tensorflow/tensorflow/tree/master/tensorflow/python/keras)
/
# losses.py

Top
## File metadata and controls

* Code
* Blame

2106 lines (1695 loc) · 75.3 KB[Raw](https://github.com/tensorflow/tensorflow/raw/refs/heads/master/tensorflow/python/keras/losses.py)1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798991001011021031041051061071081091101111121131141151161171181191201211221231241251261271281291301311321331341351361371381391401411421431441451461471481491501511521531541551561571581591601611621631641651661671681691701711721731741751761771781791801811821831841851861871881891901911921931941951961971981992002012022032042052062072082092102112122132142152162172182192202212222232242252262272282292302312322332342352362372382392402412422432442452462472482492502512522532542552562572582592602612622632642652662672682692702712722732742752762772782792802812822832842852862872882892902912922932942952962972982993003013023033043053063073083093103113123133143153163173183193203213223233243253263273283293303313323333343353363373383393403413423433443453463473483493503513523533543553563573583593603613623633643653663673683693703713723733743753763773783793803813823833843853863873883893903913923933943953963973983994004014024034044054064074084094104114124134144154164174184194204214224234244254264274284294304314324334344354364374384394404414424434444454464474484494504514524534544554564574584594604614624634644654664674684694704714724734744754764774784794804814824834844854864874884894904914924934944954964974984995005015025035045055065075085095105115125135145155165175185195205215225235245255265275285295305315325335345355365375385395405415425435445455465475485495505515525535545555565575585595605615625635645655665675685695705715725735745755765775785795805815825835845855865875885895905915925935945955965975985996006016026036046056066076086096106116126136146156166176186196206216226236246256266276286296306316326336346356366376386396406416426436446456466476486496506516526536546556566576586596606616626636646656666676686696706716726736746756766776786796806816826836846856866876886896906916926936946956966976986997007017027037047057067077087097107117127137147157167177187197207217227237247257267277287297307317327337347357367377387397407417427437447457467477487497507517527537547557567577587597607617627637647657667677687697707717727737747757767777787797807817827837847857867877887897907917927937947957967977987998008018028038048058068078088098108118128138148158168178188198208218228238248258268278288298308318328338348358368378388398408418428438448458468478488498508518528538548558568578588598608618628638648658668678688698708718728738748758768778788798808818828838848858868878888898908918928938948958968978988999009019029039049059069079089099109119129139149159169179189199209219229239249259269279289299309319329339349359369379389399409419429439449459469479489499509519529539549559569579589599609619629639649659669679689699709719729739749759769779789799809819829839849859869879889899909919929939949959969979989991000# Copyright 2015 The TensorFlow Authors. All Rights Reserved.## Licensed under the Apache License, Version 2.0 (the "License");# you may not use this file except in compliance with the License.# You may obtain a copy of the License at## http://www.apache.org/licenses/LICENSE-2.0## Unless required by applicable law or agreed to in writing, software# distributed under the License is distributed on an "AS IS" BASIS,# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.# See the License for the specific language governing permissions and# limitations under the License.# ==============================================================================# pylint: disable=g-classes-have-attributes"""Built-in loss functions."""
import abcimport functools
from tensorflow.python.autograph.core import ag\_ctxfrom tensorflow.python.autograph.impl import api as autographfrom tensorflow.python.distribute import distribute\_libfrom tensorflow.python.eager import contextfrom tensorflow.python.framework import constant\_opfrom tensorflow.python.framework import opsfrom tensorflow.python.framework import smart\_condfrom tensorflow.python.framework import tensor\_conversionfrom tensorflow.python.framework import tensor\_specfrom tensorflow.python.framework import tensor\_utilfrom tensorflow.python.keras import backendfrom tensorflow.python.keras.utils import losses\_utilsfrom tensorflow.python.keras.utils import tf\_utilsfrom tensorflow.python.keras.utils.generic\_utils import deserialize\_keras\_objectfrom tensorflow.python.keras.utils.generic\_utils import serialize\_keras\_objectfrom tensorflow.python.ops import array\_opsfrom tensorflow.python.ops import condfrom tensorflow.python.ops import math\_opsfrom tensorflow.python.ops import nnfrom tensorflow.python.ops.losses import losses\_implfrom tensorflow.python.ops.ragged import ragged\_map\_opsfrom tensorflow.python.ops.ragged import ragged\_tensorfrom tensorflow.python.ops.ragged import ragged\_utilfrom tensorflow.python.util import dispatchfrom tensorflow.tools.docs import doc\_controls
class Loss: """Loss base class.
 To be implemented by subclasses: \* `call()`: Contains the logic for loss calculation using `y\_true`, `y\_pred`.
 Example subclass implementation:
 ```python class MeanSquaredError(Loss):
 def call(self, y\_true, y\_pred): y\_pred = tf.convert\_to\_tensor\_v2(y\_pred) y\_true = tf.cast(y\_true, y\_pred.dtype) return tf.reduce\_mean(math\_ops.square(y\_pred - y\_true), axis=-1) ```
 When used with `tf.distribute.Strategy`, outside of built-in training loops such as `tf.keras` `compile` and `fit`, please use 'SUM' or 'NONE' reduction types, and reduce losses explicitly in your training loop. Using 'AUTO' or 'SUM\_OVER\_BATCH\_SIZE' will raise an error.
 Please see this custom training [tutorial]( https://www.tensorflow.org/tutorials/distribute/custom\_training) for more details on this.
 You can implement 'SUM\_OVER\_BATCH\_SIZE' using global batch size like:
 ```python with strategy.scope(): loss\_obj = tf.keras.losses.CategoricalCrossentropy( reduction=tf.keras.losses.Reduction.NONE) .... loss = (tf.reduce\_sum(loss\_obj(labels, predictions)) \* (1. / global\_batch\_size)) ``` """
 def \_\_init\_\_(self, reduction=losses\_utils.ReductionV2.AUTO, name=None): """Initializes `Loss` class.
 Args: reduction: Type of `tf.keras.losses.Reduction` to apply to loss. Default value is `AUTO`. `AUTO` indicates that the reduction option will be determined by the usage context. For almost all cases this defaults to `SUM\_OVER\_BATCH\_SIZE`. When used with `tf.distribute.Strategy`, outside of built-in training loops such as `tf.keras` `compile` and `fit`, using `AUTO` or `SUM\_OVER\_BATCH\_SIZE` will raise an error. Please see this custom training [tutorial]( https://www.tensorflow.org/tutorials/distribute/custom\_training) for more details. name: Optional name for the instance. """ losses\_utils.ReductionV2.validate(reduction) self.reduction = reduction self.name = name # SUM\_OVER\_BATCH is only allowed in losses managed by `fit` or # CannedEstimators. self.\_allow\_sum\_over\_batch\_size = False self.\_set\_name\_scope()
 def \_set\_name\_scope(self): """Creates a valid `name\_scope` name.""" if self.name is None: self.\_name\_scope = self.\_\_class\_\_.\_\_name\_\_ elif self.name == '<lambda>': self.\_name\_scope = 'lambda' else: # E.g. '\_my\_loss' => 'my\_loss' self.\_name\_scope = self.name.strip('\_')
 def \_\_call\_\_(self, y\_true, y\_pred, sample\_weight=None): """Invokes the `Loss` instance.
 Args: y\_true: Ground truth values. shape = `[batch\_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch\_size, d0, .. dN-1]` y\_pred: The predicted values. shape = `[batch\_size, d0, .. dN]` sample\_weight: Optional `sample\_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample\_weight` is a tensor of size `[batch\_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample\_weight` vector. If the shape of `sample\_weight` is `[batch\_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `y\_pred` is scaled by the corresponding value of `sample\_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.)
 Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch\_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.)
 Raises: ValueError: If the shape of `sample\_weight` is invalid. """ # If we are wrapping a lambda function strip '<>' from the name as it is not # accepted in scope name. graph\_ctx = tf\_utils.graph\_context\_for\_symbolic\_tensors( y\_true, y\_pred, sample\_weight) with backend.name\_scope(self.\_name\_scope), graph\_ctx: if context.executing\_eagerly(): call\_fn = self.call else: call\_fn = autograph.tf\_convert(self.call, ag\_ctx.control\_status\_ctx()) losses = call\_fn(y\_true, y\_pred) return losses\_utils.compute\_weighted\_loss( losses, sample\_weight, reduction=self.\_get\_reduction())
 @classmethod def from\_config(cls, config): """Instantiates a `Loss` from its config (output of `get\_config()`).
 Args: config: Output of `get\_config()`.
 Returns: A `Loss` instance. """ return cls(\*\*config)
 def get\_config(self): """Returns the config dictionary for a `Loss` instance.""" return {'reduction': self.reduction, 'name': self.name}
 @abc.abstractmethod @doc\_controls.for\_subclass\_implementers def call(self, y\_true, y\_pred): """Invokes the `Loss` instance.
 Args: y\_true: Ground truth values. shape = `[batch\_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch\_size, d0, .. dN-1]` y\_pred: The predicted values. shape = `[batch\_size, d0, .. dN]`
 Returns: Loss values with the shape `[batch\_size, d0, .. dN-1]`. """ raise NotImplementedError('Must be implemented in subclasses.')
 def \_get\_reduction(self): """Handles `AUTO` reduction cases and returns the reduction value.""" if (not self.\_allow\_sum\_over\_batch\_size and distribute\_lib.has\_strategy() and (self.reduction == losses\_utils.ReductionV2.AUTO or self.reduction == losses\_utils.ReductionV2.SUM\_OVER\_BATCH\_SIZE)): raise ValueError( 'Please use `tf.keras.losses.Reduction.SUM` or ' '`tf.keras.losses.Reduction.NONE` for loss reduction when losses are ' 'used with `tf.distribute.Strategy` outside of the built-in training ' 'loops. You can implement ' '`tf.keras.losses.Reduction.SUM\_OVER\_BATCH\_SIZE` using global batch ' 'size like:\n```\nwith strategy.scope():\n' ' loss\_obj = tf.keras.losses.CategoricalCrossentropy(' 'reduction=tf.keras.losses.Reduction.NONE)\n....\n' ' loss = tf.reduce\_sum(loss\_obj(labels, predictions)) \* ' '(1. / global\_batch\_size)\n```\nPlease see ' 'https://www.tensorflow.org/tutorials/distribute/custom\_training' ' for more details.')
 if self.reduction == losses\_utils.ReductionV2.AUTO: return losses\_utils.ReductionV2.SUM\_OVER\_BATCH\_SIZE return self.reduction
class LossFunctionWrapper(Loss): """Wraps a loss function in the `Loss` class."""
 def \_\_init\_\_(self, fn, reduction=losses\_utils.ReductionV2.AUTO, name=None, \*\*kwargs): """Initializes `LossFunctionWrapper` class.
 Args: fn: The loss function to wrap, with signature `fn(y\_true, y\_pred, \*\*kwargs)`. reduction: Type of `tf.keras.losses.Reduction` to apply to loss. Default value is `AUTO`. `AUTO` indicates that the reduction option will be determined by the usage context. For almost all cases this defaults to `SUM\_OVER\_BATCH\_SIZE`. When used with `tf.distribute.Strategy`, outside of built-in training loops such as `tf.keras` `compile` and `fit`, using `AUTO` or `SUM\_OVER\_BATCH\_SIZE` will raise an error. Please see this custom training [tutorial]( https://www.tensorflow.org/tutorials/distribute/custom\_training) for more details. name: Optional name for the instance. \*\*kwargs: The keyword arguments that are passed on to `fn`. """ super().\_\_init\_\_(reduction=reduction, name=name) self.fn = fn self.\_fn\_kwargs = kwargs
 def call(self, y\_true, y\_pred): """Invokes the `LossFunctionWrapper` instance.
 Args: y\_true: Ground truth values. y\_pred: The predicted values.
 Returns: Loss values per sample. """ if tensor\_util.is\_tf\_type(y\_pred) and tensor\_util.is\_tf\_type(y\_true): y\_pred, y\_true = losses\_utils.squeeze\_or\_expand\_dimensions(y\_pred, y\_true)
 ag\_fn = autograph.tf\_convert(self.fn, ag\_ctx.control\_status\_ctx()) return ag\_fn(y\_true, y\_pred, \*\*self.\_fn\_kwargs)
 def get\_config(self): config = {} for k, v in self.\_fn\_kwargs.items(): config[k] = backend.eval(v) if tf\_utils.is\_tensor\_or\_variable(v) else v base\_config = super().get\_config() return dict(list(base\_config.items()) + list(config.items()))
class MeanSquaredError(LossFunctionWrapper): """Computes the mean of squares of errors between labels and predictions.
 `loss = square(y\_true - y\_pred)`
 Standalone usage:
 >>> y\_true = [[0., 1.], [0., 0.]] >>> y\_pred = [[1., 1.], [1., 0.]] >>> # Using 'auto'/'sum\_over\_batch\_size' reduction type. >>> mse = tf.keras.losses.MeanSquaredError() >>> mse(y\_true, y\_pred).numpy() 0.5
 >>> # Calling with 'sample\_weight'. >>> mse(y\_true, y\_pred, sample\_weight=[0.7, 0.3]).numpy() 0.25
 >>> # Using 'sum' reduction type. >>> mse = tf.keras.losses.MeanSquaredError( ... reduction=tf.keras.losses.Reduction.SUM) >>> mse(y\_true, y\_pred).numpy() 1.0
 >>> # Using 'none' reduction type. >>> mse = tf.keras.losses.MeanSquaredError( ... reduction=tf.keras.losses.Reduction.NONE) >>> mse(y\_true, y\_pred).numpy() array([0.5, 0.5], dtype=float32)
 Usage with the `compile()` API:
 ```python model.compile(optimizer='sgd', loss=tf.keras.losses.MeanSquaredError()) ``` """
 def \_\_init\_\_(self, reduction=losses\_utils.ReductionV2.AUTO, name='mean\_squared\_error'): """Initializes `MeanSquaredError` instance.
 Args: reduction: Type of `tf.keras.losses.Reduction` to apply to loss. Default value is `AUTO`. `AUTO` indicates that the reduction option will be determined by the usage context. For almost all cases this defaults to `SUM\_OVER\_BATCH\_SIZE`. When used with `tf.distribute.Strategy`, outside of built-in training loops such as `tf.keras` `compile` and `fit`, using `AUTO` or `SUM\_OVER\_BATCH\_SIZE` will raise an error. Please see this custom training [tutorial]( https://www.tensorflow.org/tutorials/distribute/custom\_training) for more details. name: Optional name for the instance. Defaults to 'mean\_squared\_error'. """ super().\_\_init\_\_(mean\_squared\_error, name=name, reduction=reduction)
class MeanAbsoluteError(LossFunctionWrapper): """Computes the mean of absolute difference between labels and predictions.
 `loss = abs(y\_true - y\_pred)`
 Standalone usage:
 >>> y\_true = [[0., 1.], [0., 0.]] >>> y\_pred = [[1., 1.], [1., 0.]] >>> # Using 'auto'/'sum\_over\_batch\_size' reduction type. >>> mae = tf.keras.losses.MeanAbsoluteError() >>> mae(y\_true, y\_pred).numpy() 0.5
 >>> # Calling with 'sample\_weight'. >>> mae(y\_true, y\_pred, sample\_weight=[0.7, 0.3]).numpy() 0.25
 >>> # Using 'sum' reduction type. >>> mae = tf.keras.losses.MeanAbsoluteError( ... reduction=tf.keras.losses.Reduction.SUM) >>> mae(y\_true, y\_pred).numpy() 1.0
 >>> # Using 'none' reduction type. >>> mae = tf.keras.losses.MeanAbsoluteError( ... reduction=tf.keras.losses.Reduction.NONE) >>> mae(y\_true, y\_pred).numpy() array([0.5, 0.5], dtype=float32)
 Usage with the `compile()` API:
 ```python model.compile(optimizer='sgd', loss=tf.keras.losses.MeanAbsoluteError()) ``` """
 def \_\_init\_\_(self, reduction=losses\_utils.ReductionV2.AUTO, name='mean\_absolute\_error'): """Initializes `MeanAbsoluteError` instance.
 Args: reduction: Type of `tf.keras.losses.Reduction` to apply to loss. Default value is `AUTO`. `AUTO` indicates that the reduction option will be determined by the usage context. For almost all cases this defaults to `SUM\_OVER\_BATCH\_SIZE`. When used with `tf.distribute.Strategy`, outside of built-in training loops such as `tf.keras` `compile` and `fit`, using `AUTO` or `SUM\_OVER\_BATCH\_SIZE` will raise an error. Please see this custom training [tutorial]( https://www.tensorflow.org/tutorials/distribute/custom\_training) for more details. name: Optional name for the instance. Defaults to 'mean\_absolute\_error'. """ super().\_\_init\_\_(mean\_absolute\_error, name=name, reduction=reduction)
class MeanAbsolutePercentageError(LossFunctionWrapper): """Computes the mean absolute percentage error between `y\_true` and `y\_pred`.
 `loss = 100 \* abs(y\_true - y\_pred) / y\_true`
 Standalone usage:
 >>> y\_true = [[2., 1.], [2., 3.]] >>> y\_pred = [[1., 1.], [1., 0.]] >>> # Using 'auto'/'sum\_over\_batch\_size' reduction type. >>> mape = tf.keras.losses.MeanAbsolutePercentageError() >>> mape(y\_true, y\_pred).numpy() 50.
 >>> # Calling with 'sample\_weight'. >>> mape(y\_true, y\_pred, sample\_weight=[0.7, 0.3]).numpy() 20.
 >>> # Using 'sum' reduction type. >>> mape = tf.keras.losses.MeanAbsolutePercentageError( ... reduction=tf.keras.losses.Reduction.SUM) >>> mape(y\_true, y\_pred).numpy() 100.
 >>> # Using 'none' reduction type. >>> mape = tf.keras.losses.MeanAbsolutePercentageError( ... reduction=tf.keras.losses.Reduction.NONE) >>> mape(y\_true, y\_pred).numpy() array([25., 75.], dtype=float32)
 Usage with the `compile()` API:
 ```python model.compile(optimizer='sgd', loss=tf.keras.losses.MeanAbsolutePercentageError()) ``` """
 def \_\_init\_\_(self, reduction=losses\_utils.ReductionV2.AUTO, name='mean\_absolute\_percentage\_error'): """Initializes `MeanAbsolutePercentageError` instance.
 Args: reduction: Type of `tf.keras.losses.Reduction` to apply to loss. Default value is `AUTO`. `AUTO` indicates that the reduction option will be determined by the usage context. For almost all cases this defaults to `SUM\_OVER\_BATCH\_SIZE`. When used with `tf.distribute.Strategy`, outside of built-in training loops such as `tf.keras` `compile` and `fit`, using `AUTO` or `SUM\_OVER\_BATCH\_SIZE` will raise an error. Please see this custom training [tutorial]( https://www.tensorflow.org/tutorials/distribute/custom\_training) for more details. name: Optional name for the instance. Defaults to 'mean\_absolute\_percentage\_error'. """ super().\_\_init\_\_( mean\_absolute\_percentage\_error, name=name, reduction=reduction)
class MeanSquaredLogarithmicError(LossFunctionWrapper): """Computes the mean squared logarithmic error between `y\_true` and `y\_pred`.
 `loss = square(log(y\_true + 1.) - log(y\_pred + 1.))`
 Standalone usage:
 >>> y\_true = [[0., 1.], [0., 0.]] >>> y\_pred = [[1., 1.], [1., 0.]] >>> # Using 'auto'/'sum\_over\_batch\_size' reduction type. >>> msle = tf.keras.losses.MeanSquaredLogarithmicError() >>> msle(y\_true, y\_pred).numpy() 0.240
 >>> # Calling with 'sample\_weight'. >>> msle(y\_true, y\_pred, sample\_weight=[0.7, 0.3]).numpy() 0.120
 >>> # Using 'sum' reduction type. >>> msle = tf.keras.losses.MeanSquaredLogarithmicError( ... reduction=tf.keras.losses.Reduction.SUM) >>> msle(y\_true, y\_pred).numpy() 0.480
 >>> # Using 'none' reduction type. >>> msle = tf.keras.losses.MeanSquaredLogarithmicError( ... reduction=tf.keras.losses.Reduction.NONE) >>> msle(y\_true, y\_pred).numpy() array([0.240, 0.240], dtype=float32)
 Usage with the `compile()` API:
 ```python model.compile(optimizer='sgd', loss=tf.keras.losses.MeanSquaredLogarithmicError()) ``` """
 def \_\_init\_\_(self, reduction=losses\_utils.ReductionV2.AUTO, name='mean\_squared\_logarithmic\_error'): """Initializes `MeanSquaredLogarithmicError` instance.
 Args: reduction: Type of `tf.keras.losses.Reduction` to apply to loss. Default value is `AUTO`. `AUTO` indicates that the reduction option will be determined by the usage context. For almost all cases this defaults to `SUM\_OVER\_BATCH\_SIZE`. When used with `tf.distribute.Strategy`, outside of built-in training loops such as `tf.keras` `compile` and `fit`, using `AUTO` or `SUM\_OVER\_BATCH\_SIZE` will raise an error. Please see this custom training [tutorial]( https://www.tensorflow.org/tutorials/distribute/custom\_training) for more details. name: Optional name for the instance. Defaults to 'mean\_squared\_logarithmic\_error'. """ super().\_\_init\_\_( mean\_squared\_logarithmic\_error, name=name, reduction=reduction)
class BinaryCrossentropy(LossFunctionWrapper): """Computes the cross-entropy loss between true labels and predicted labels.
 Use this cross-entropy loss for binary (0 or 1) classification applications. The loss function requires the following inputs:
 - `y\_true` (true label): This is either 0 or 1. - `y\_pred` (predicted value): This is the model's prediction, i.e, a single floating-point value which either represents a [logit](https://en.wikipedia.org/wiki/Logit), (i.e, value in [-inf, inf] when `from\_logits=True`) or a probability (i.e, value in [0., 1.] when `from\_logits=False`).
 \*\*Recommended Usage:\*\* (set `from\_logits=True`)
 With `tf.keras` API:
 ```python model.compile( loss=tf.keras.losses.BinaryCrossentropy(from\_logits=True), .... ) ```
 As a standalone function:
 >>> # Example 1: (batch\_size = 1, number of samples = 4) >>> y\_true = [0, 1, 0, 0] >>> y\_pred = [-18.6, 0.51, 2.94, -12.8] >>> bce = tf.keras.losses.BinaryCrossentropy(from\_logits=True) >>> bce(y\_true, y\_pred).numpy() 0.865
 >>> # Example 2: (batch\_size = 2, number of samples = 4) >>> y\_true = [[0, 1], [0, 0]] >>> y\_pred = [[-18.6, 0.51], [2.94, -12.8]] >>> # Using default 'auto'/'sum\_over\_batch\_size' reduction type. >>> bce = tf.keras.losses.BinaryCrossentropy(from\_logits=True) >>> bce(y\_true, y\_pred).numpy() 0.865 >>> # Using 'sample\_weight' attribute >>> bce(y\_true, y\_pred, sample\_weight=[0.8, 0.2]).numpy() 0.243 >>> # Using 'sum' reduction` type. >>> bce = tf.keras.losses.BinaryCrossentropy(from\_logits=True, ... reduction=tf.keras.losses.Reduction.SUM) >>> bce(y\_true, y\_pred).numpy() 1.730 >>> # Using 'none' reduction type. >>> bce = tf.keras.losses.BinaryCrossentropy(from\_logits=True, ... reduction=tf.keras.losses.Reduction.NONE) >>> bce(y\_true, y\_pred).numpy() array([0.235, 1.496], dtype=float32)
 \*\*Default Usage:\*\* (set `from\_logits=False`)
 >>> # Make the following updates to the above "Recommended Usage" section >>> # 1. Set `from\_logits=False` >>> tf.keras.losses.BinaryCrossentropy() # OR ...('from\_logits=False') >>> # 2. Update `y\_pred` to use probabilities instead of logits >>> y\_pred = [0.6, 0.3, 0.2, 0.8] # OR [[0.6, 0.3], [0.2, 0.8]] """
 def \_\_init\_\_(self, from\_logits=False, label\_smoothing=0, axis=-1, reduction=losses\_utils.ReductionV2.AUTO, name='binary\_crossentropy'): """Initializes `BinaryCrossentropy` instance.
 Args: from\_logits: Whether to interpret `y\_pred` as a tensor of [logit](https://en.wikipedia.org/wiki/Logit) values. By default, we assume that `y\_pred` contains probabilities (i.e., values in [0, 1]). label\_smoothing: Float in [0, 1]. When 0, no smoothing occurs. When > 0, we compute the loss between the predicted labels and a smoothed version of the true labels, where the smoothing squeezes the labels towards 0.5. Larger values of `label\_smoothing` correspond to heavier smoothing. axis: The axis along which to compute crossentropy (the features axis). Defaults to -1. reduction: Type of `tf.keras.losses.Reduction` to apply to loss. Default value is `AUTO`. `AUTO` indicates that the reduction option will be determined by the usage context. For almost all cases this defaults to `SUM\_OVER\_BATCH\_SIZE`. When used with `tf.distribute.Strategy`, outside of built-in training loops such as `tf.keras` `compile` and `fit`, using `AUTO` or `SUM\_OVER\_BATCH\_SIZE` will raise an error. Please see this custom training [tutorial]( https://www.tensorflow.org/tutorials/distribute/custom\_training) for more details. name: Name for the op. Defaults to 'binary\_crossentropy'. """ super().\_\_init\_\_( binary\_crossentropy, name=name, reduction=reduction, from\_logits=from\_logits, label\_smoothing=label\_smoothing, axis=axis) self.from\_logits = from\_logits
class CategoricalCrossentropy(LossFunctionWrapper): """Computes the crossentropy loss between the labels and predictions.
 Use this crossentropy loss function when there are two or more label classes. We expect labels to be provided in a `one\_hot` representation. If you want to provide labels as integers, please use `SparseCategoricalCrossentropy` loss. There should be `# classes` floating point values per feature.
 In the snippet below, there is `# classes` floating pointing values per example. The shape of both `y\_pred` and `y\_true` are `[batch\_size, num\_classes]`.
 Standalone usage:
 >>> y\_true = [[0, 1, 0], [0, 0, 1]] >>> y\_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]] >>> # Using 'auto'/'sum\_over\_batch\_size' reduction type. >>> cce = tf.keras.losses.CategoricalCrossentropy() >>> cce(y\_true, y\_pred).numpy() 1.177
 >>> # Calling with 'sample\_weight'. >>> cce(y\_true, y\_pred, sample\_weight=tf.constant([0.3, 0.7])).numpy() 0.814
 >>> # Using 'sum' reduction type. >>> cce = tf.keras.losses.CategoricalCrossentropy( ... reduction=tf.keras.losses.Reduction.SUM) >>> cce(y\_true, y\_pred).numpy() 2.354
 >>> # Using 'none' reduction type. >>> cce = tf.keras.losses.CategoricalCrossentropy( ... reduction=tf.keras.losses.Reduction.NONE) >>> cce(y\_true, y\_pred).numpy() array([0.0513, 2.303], dtype=float32)
 Usage with the `compile()` API:
 ```python model.compile(optimizer='sgd', loss=tf.keras.losses.CategoricalCrossentropy()) ``` """
 def \_\_init\_\_(self, from\_logits=False, label\_smoothing=0, axis=-1, reduction=losses\_utils.ReductionV2.AUTO, name='categorical\_crossentropy'): """Initializes `CategoricalCrossentropy` instance.
 Args: from\_logits: Whether `y\_pred` is expected to be a logits tensor. By default, we assume that `y\_pred` encodes a probability distribution. label\_smoothing: Float in [0, 1]. When > 0, label values are smoothed, meaning the confidence on label values are relaxed. For example, if `0.1`, use `0.1 / num\_classes` for non-target labels and `0.9 + 0.1 / num\_classes` for target labels. axis: The axis along which to compute crossentropy (the features axis). Defaults to -1. reduction: Type of `tf.keras.losses.Reduction` to apply to loss. Default value is `AUTO`. `AUTO` indicates that the reduction option will be determined by the usage context. For almost all cases this defaults to `SUM\_OVER\_BATCH\_SIZE`. When used with `tf.distribute.Strategy`, outside of built-in training loops such as `tf.keras` `compile` and `fit`, using `AUTO` or `SUM\_OVER\_BATCH\_SIZE` will raise an error. Please see this custom training [tutorial]( https://www.tensorflow.org/tutorials/distribute/custom\_training) for more details. name: Optional name for the instance. Defaults to 'categorical\_crossentropy'. """ super().\_\_init\_\_( categorical\_crossentropy, name=name, reduction=reduction, from\_logits=from\_logits, label\_smoothing=label\_smoothing, axis=axis)
class SparseCategoricalCrossentropy(LossFunctionWrapper): """Computes the crossentropy loss between the labels and predictions.
 Use this crossentropy loss function when there are two or more label classes. We expect labels to be provided as integers. If you want to provide labels using `one-hot` representation, please use `CategoricalCrossentropy` loss. There should be `# classes` floating point values per feature for `y\_pred` and a single floating point value per feature for `y\_true`.
 In the snippet below, there is a single floating point value per example for `y\_true` and `# classes` floating pointing values per example for `y\_pred`. The shape of `y\_true` is `[batch\_size]` and the shape of `y\_pred` is `[batch\_size, num\_classes]`.
 Standalone usage:
 >>> y\_true = [1, 2] >>> y\_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]] >>> # Using 'auto'/'sum\_over\_batch\_size' reduction type. >>> scce = tf.keras.losses.SparseCategoricalCrossentropy() >>> scce(y\_true, y\_pred).numpy() 1.177
 >>> # Calling with 'sample\_weight'. >>> scce(y\_true, y\_pred, sample\_weight=tf.constant([0.3, 0.7])).numpy() 0.814
 >>> # Using 'sum' reduction type. >>> scce = tf.keras.losses.SparseCategoricalCrossentropy( ... reduction=tf.keras.losses.Reduction.SUM) >>> scce(y\_true, y\_pred).numpy() 2.354
 >>> # Using 'none' reduction type. >>> scce = tf.keras.losses.SparseCategoricalCrossentropy( ... reduction=tf.keras.losses.Reduction.NONE) >>> scce(y\_true, y\_pred).numpy() array([0.0513, 2.303], dtype=float32)
 Usage with the `compile()` API:
 ```python model.compile(optimizer='sgd', loss=tf.keras.losses.SparseCategoricalCrossentropy()) ``` """
 def \_\_init\_\_(self, from\_logits=False, reduction=losses\_utils.ReductionV2.AUTO, name='sparse\_categorical\_crossentropy'): """Initializes `SparseCategoricalCrossentropy` instance.
 Args: from\_logits: Whether `y\_pred` is expected to be a logits tensor. By default, we assume that `y\_pred` encodes a probability distribution. reduction: Type of `tf.keras.losses.Reduction` to apply to loss. Default value is `AUTO`. `AUTO` indicates that the reduction option will be determined by the usage context. For almost all cases this defaults to `SUM\_OVER\_BATCH\_SIZE`. When used with `tf.distribute.Strategy`, outside of built-in training loops such as `tf.keras` `compile` and `fit`, using `AUTO` or `SUM\_OVER\_BATCH\_SIZE` will raise an error. Please see this custom training [tutorial]( https://www.tensorflow.org/tutorials/distribute/custom\_training) for more details. name: Optional name for the instance. Defaults to 'sparse\_categorical\_crossentropy'. """ super().\_\_init\_\_( sparse\_categorical\_crossentropy, name=name, reduction=reduction, from\_logits=from\_logits)
class Hinge(LossFunctionWrapper): """Computes the hinge loss between `y\_true` and `y\_pred`.
 `loss = maximum(1 - y\_true \* y\_pred, 0)`
 `y\_true` values are expected to be -1 or 1. If binary (0 or 1) labels are provided we will convert them to -1 or 1.
 Standalone usage:
 >>> y\_true = [[0., 1.], [0., 0.]] >>> y\_pred = [[0.6, 0.4], [0.4, 0.6]] >>> # Using 'auto'/'sum\_over\_batch\_size' reduction type. >>> h = tf.keras.losses.Hinge() >>> h(y\_true, y\_pred).numpy() 1.3
 >>> # Calling with 'sample\_weight'. >>> h(y\_true, y\_pred, sample\_weight=[1, 0]).numpy() 0.55
 >>> # Using 'sum' reduction type. >>> h = tf.keras.losses.Hinge( ... reduction=tf.keras.losses.Reduction.SUM) >>> h(y\_true, y\_pred).numpy() 2.6
 >>> # Using 'none' reduction type. >>> h = tf.keras.losses.Hinge( ... reduction=tf.keras.losses.Reduction.NONE) >>> h(y\_true, y\_pred).numpy() array([1.1, 1.5], dtype=float32)
 Usage with the `compile()` API:
 ```python model.compile(optimizer='sgd', loss=tf.keras.losses.Hinge()) ``` """
 def \_\_init\_\_(self, reduction=losses\_utils.ReductionV2.AUTO, name='hinge'): """Initializes `Hinge` instance.
 Args: reduction: Type of `tf.keras.losses.Reduction` to apply to loss. Default value is `AUTO`. `AUTO` indicates that the reduction option will be determined by the usage context. For almost all cases this defaults to `SUM\_OVER\_BATCH\_SIZE`. When used with `tf.distribute.Strategy`, outside of built-in training loops such as `tf.keras` `compile` and `fit`, using `AUTO` or `SUM\_OVER\_BATCH\_SIZE` will raise an error. Please see this custom training [tutorial]( https://www.tensorflow.org/tutorials/distribute/custom\_training) for more details. name: Optional name for the instance. Defaults to 'hinge'. """ super().\_\_init\_\_(hinge, name=name, reduction=reduction)
class SquaredHinge(LossFunctionWrapper): """Computes the squared hinge loss between `y\_true` and `y\_pred`.
 `loss = square(maximum(1 - y\_true \* y\_pred, 0))`
 `y\_true` values are expected to be -1 or 1. If binary (0 or 1) labels are provided we will convert them to -1 or 1.
 Standalone usage:
 >>> y\_true = [[0., 1.], [0., 0.]] >>> y\_pred = [[0.6, 0.4], [0.4, 0.6]] >>> # Using 'auto'/'sum\_over\_batch\_size' reduction type. >>> h = tf.keras.losses.SquaredHinge() >>> h(y\_true, y\_pred).numpy() 1.86
 >>> # Calling with 'sample\_weight'. >>> h(y\_true, y\_pred, sample\_weight=[1, 0]).numpy() 0.73
 >>> # Using 'sum' reduction type. >>> h = tf.keras.losses.SquaredHinge( ... reduction=tf.keras.losses.Reduction.SUM) >>> h(y\_true, y\_pred).numpy() 3.72
 >>> # Using 'none' reduction type. >>> h = tf.keras.losses.SquaredHinge( ... reduction=tf.keras.losses.Reduction.NONE) >>> h(y\_true, y\_pred).numpy() array([1.46, 2.26], dtype=float32)
 Usage with the `compile()` API:
 ```python model.compile(optimizer='sgd', loss=tf.keras.losses.SquaredHinge()) ``` """
 def \_\_init\_\_(self, reduction=losses\_utils.ReductionV2.AUTO, name='squared\_hinge'): """Initializes `SquaredHinge` instance.
 Args: reduction: Type of `tf.keras.losses.Reduction` to apply to loss. Default value is `AUTO`. `AUTO` indicates that the reduction option will be determined by the usage context. For almost all cases this defaults to `SUM\_OVER\_BATCH\_SIZE`. When used with `tf.distribute.Strategy`, outside of built-in training loops such as `tf.keras` `compile` and `fit`, using `AUTO` or `SUM\_OVER\_BATCH\_SIZE` will raise an error. Please see this custom training [tutorial]( https://www.tensorflow.org/tutorials/distribute/custom\_training) for more details. name: Optional name for the instance. Defaults to 'squared\_hinge'. """ super().\_\_init\_\_(squared\_hinge, name=name, reduction=reduction)
class CategoricalHinge(LossFunctionWrapper): """Computes the categorical hinge loss between `y\_true` and `y\_pred`.
 `loss = maximum(neg - pos + 1, 0)` where `neg=maximum((1-y\_true)\*y\_pred) and pos=sum(y\_true\*y\_pred)`
 Standalone usage:
 >>> y\_true = [[0, 1], [0, 0]] >>> y\_pred = [[0.6, 0.4], [0.4, 0.6]] >>> # Using 'auto'/'sum\_over\_batch\_size' reduction type. >>> h = tf.keras.losses.CategoricalHinge() >>> h(y\_true, y\_pred).numpy() 1.4
 >>> # Calling with 'sample\_weight'. >>> h(y\_true, y\_pred, sample\_weight=[1, 0]).numpy() 0.6
 >>> # Using 'sum' reduction type. >>> h = tf.keras.losses.CategoricalHinge( ... reduction=tf.keras.losses.Reduction.SUM) >>> h(y\_true, y\_pred).numpy() 2.8
 >>> # Using 'none' reduction type. >>> h = tf.keras.losses.CategoricalHinge( ... reduction=tf.keras.losses.Reduction.NONE) >>> h(y\_true, y\_pred).numpy() array([1.2, 1.6], dtype=float32)
 Usage with the `compile()` API:
 ```python model.compile(optimizer='sgd', loss=tf.keras.losses.CategoricalHinge()) ``` """
 def \_\_init\_\_(self, reduction=losses\_utils.ReductionV2.AUTO, name='categorical\_hinge'): """Initializes `CategoricalHinge` instance.
 Args: reduction: Type of `tf.keras.losses.Reduction` to apply to loss. Default value is `AUTO`. `AUTO` indicates that the reduction option will be determined by the usage context. For almost all cases this defaults to `SUM\_OVER\_BATCH\_SIZE`. When used with `tf.distribute.Strategy`, outside of built-in training loops such as `tf.keras` `compile` and `fit`, using `AUTO` or `SUM\_OVER\_BATCH\_SIZE` will raise an error. Please see this custom training [tutorial]( https://www.tensorflow.org/tutorials/distribute/custom\_training) for more details. name: Optional name for the instance. Defaults to 'categorical\_hinge'. """ super().\_\_init\_\_(categorical\_hinge, name=name, reduction=reduction)
class Poisson(LossFunctionWrapper): """Computes the Poisson loss between `y\_true` and `y\_pred`.
 `loss = y\_pred - y\_true \* log(y\_pred)`
 Standalone usage:
 >>> y\_true = [[0., 1.], [0., 0.]] >>> y\_pred = [[1., 1.], [0., 0.]] >>> # Using 'auto'/'sum\_over\_batch\_size' reduction type. >>> p = tf.keras.losses.Poisson() >>> p(y\_true, y\_pred).numpy() 0.5
 >>> # Calling with 'sample\_weight'. >>> p(y\_true, y\_pred, sample\_weight=[0.8, 0.2]).numpy() 0.4
 >>> # Using 'sum' reduction type. >>> p = tf.keras.losses.Poisson( ... reduction=tf.keras.losses.Reduction.SUM) >>> p(y\_true, y\_pred).numpy() 0.999
 >>> # Using 'none' reduction type. >>> p = tf.keras.losses.Poisson( ... reduction=tf.keras.losses.Reduction.NONE) >>> p(y\_true, y\_pred).numpy() array([0.999, 0.], dtype=float32)
 Usage with the `compile()` API:
 ```python model.compile(optimizer='sgd', loss=tf.keras.losses.Poisson()) ``` """
 def \_\_init\_\_(self, reduction=losses\_utils.ReductionV2.AUTO, name='poisson'): """Initializes `Poisson` instance.
 Args: reduction: Type of `tf.keras.losses.Reduction` to apply to loss. Default value is `AUTO`. `AUTO` indicates that the reduction option will be determined by the usage context. For almost all cases this defaults to `SUM\_OVER\_BATCH\_SIZE`. When used with `tf.distribute.Strategy`, outside of built-in training loops such as `tf.keras` `compile` and `fit`, using `AUTO` or `SUM\_OVER\_BATCH\_SIZE` will raise an error. Please see this custom training [tutorial]( https://www.tensorflow.org/tutorials/distribute/custom\_training) for more details. name: Optional name for the instance. Defaults to 'poisson'. """ super().\_\_init\_\_(poisson, name=name, reduction=reduction)
class LogCosh(LossFunctionWrapper): """Computes the logarithm of the hyperbolic cosine of the prediction error.
 `logcosh = log((exp(x) + exp(-x))/2)`, where x is the error `y\_pred - y\_true`.
 Standalone usage:
 >>> y\_true = [[0., 1.], [0., 0.]][View remainder of file in raw view](https://github.com/tensorflow/tensorflow/raw/refs/heads/master/tensorflow/python/keras/losses.py)

## Footer

© 2025 GitHub, Inc.

### Footer navigation

* [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)
* [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)
* [Security](https://github.com/security)
* [Status](https://www.githubstatus.com/)
* [Docs](https://docs.github.com/)
* [Contact](https://support.github.com?tags=dotcom-footer)
* Manage cookies
* Do not share my personal information

You can’t perform that action at this time.



=== Content from github.com_5322c4af_20250114_211738.html ===

[Skip to content](#start-of-content)

## Navigation Menu

Toggle navigation

[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fcommit%2Fc5b30379ba87cbe774b08ac50c1f6d36df4ebb7c)

* Product

  + [GitHub Copilot
    Write better code with AI](https://github.com/features/copilot)
  + [Security
    Find and fix vulnerabilities](https://github.com/features/security)
  + [Actions
    Automate any workflow](https://github.com/features/actions)
  + [Codespaces
    Instant dev environments](https://github.com/features/codespaces)
  + [Issues
    Plan and track work](https://github.com/features/issues)
  + [Code Review
    Manage code changes](https://github.com/features/code-review)
  + [Discussions
    Collaborate outside of code](https://github.com/features/discussions)
  + [Code Search
    Find more, search less](https://github.com/features/code-search)

  Explore
  + [All features](https://github.com/features)
  + [Documentation](https://docs.github.com)
  + [GitHub Skills](https://skills.github.com)
  + [Blog](https://github.blog)
* Solutions

  By company size
  + [Enterprises](https://github.com/enterprise)
  + [Small and medium teams](https://github.com/team)
  + [Startups](https://github.com/enterprise/startups)
  By use case
  + [DevSecOps](/solutions/use-case/devsecops)
  + [DevOps](/solutions/use-case/devops)
  + [CI/CD](/solutions/use-case/ci-cd)
  + [View all use cases](/solutions/use-case)

  By industry
  + [Healthcare](/solutions/industry/healthcare)
  + [Financial services](/solutions/industry/financial-services)
  + [Manufacturing](/solutions/industry/manufacturing)
  + [Government](/solutions/industry/government)
  + [View all industries](/solutions/industry)

  [View all solutions](/solutions)
* Resources

  Topics
  + [AI](/resources/articles/ai)
  + [DevOps](/resources/articles/devops)
  + [Security](/resources/articles/security)
  + [Software Development](/resources/articles/software-development)
  + [View all](/resources/articles)

  Explore
  + [Learning Pathways](https://resources.github.com/learn/pathways)
  + [White papers, Ebooks, Webinars](https://resources.github.com)
  + [Customer Stories](https://github.com/customer-stories)
  + [Partners](https://partner.github.com)
  + [Executive Insights](https://github.com/solutions/executive-insights)
* Open Source

  + [GitHub Sponsors
    Fund open source developers](/sponsors)
  + [The ReadME Project
    GitHub community articles](https://github.com/readme)
  Repositories
  + [Topics](https://github.com/topics)
  + [Trending](https://github.com/trending)
  + [Collections](https://github.com/collections)
* Enterprise

  + [Enterprise platform
    AI-powered developer platform](/enterprise)
  Available add-ons
  + [Advanced Security
    Enterprise-grade security features](https://github.com/enterprise/advanced-security)
  + [GitHub Copilot
    Enterprise-grade AI features](/features/copilot#enterprise)
  + [Premium Support
    Enterprise-grade 24/7 support](/premium-support)
* [Pricing](https://github.com/pricing)

Search or jump to...

# Search code, repositories, users, issues, pull requests...

Search

Clear

[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)

# Provide feedback

We read every piece of feedback, and take your input very seriously.

Include my email address so I can be contacted

  Cancel

 Submit feedback

# Saved searches

## Use saved searches to filter your results more quickly

Name

Query

To see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).

  Cancel

 Create saved search

[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fcommit%2Fc5b30379ba87cbe774b08ac50c1f6d36df4ebb7c)

[Sign up](/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E%2Fvoltron%2Fcommit_fragments%2Frepo_layout&source=header-repo&source_repo=tensorflow%2Ftensorflow)
Reseting focus

You signed in with another tab or window. Reload to refresh your session.
You signed out in another tab or window. Reload to refresh your session.
You switched accounts on another tab or window. Reload to refresh your session.

Dismiss alert

{{ message }}

[tensorflow](/tensorflow)
/
**[tensorflow](/tensorflow/tensorflow)**
Public

* [Notifications](/login?return_to=%2Ftensorflow%2Ftensorflow) You must be signed in to change notification settings
* [Fork
  74.4k](/login?return_to=%2Ftensorflow%2Ftensorflow)
* [Star
   187k](/login?return_to=%2Ftensorflow%2Ftensorflow)

* [Code](/tensorflow/tensorflow)
* [Issues
  827](/tensorflow/tensorflow/issues)
* [Pull requests
  5k+](/tensorflow/tensorflow/pulls)
* [Actions](/tensorflow/tensorflow/actions)
* [Projects
  2](/tensorflow/tensorflow/projects)
* [Security](/tensorflow/tensorflow/security)
* [Insights](/tensorflow/tensorflow/pulse)

Additional navigation options

* [Code](/tensorflow/tensorflow)
* [Issues](/tensorflow/tensorflow/issues)
* [Pull requests](/tensorflow/tensorflow/pulls)
* [Actions](/tensorflow/tensorflow/actions)
* [Projects](/tensorflow/tensorflow/projects)
* [Security](/tensorflow/tensorflow/security)
* [Insights](/tensorflow/tensorflow/pulse)

## Commit

[Permalink](/tensorflow/tensorflow/commit/c5b30379ba87cbe774b08ac50c1f6d36df4ebb7c)

This commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.

Fix cwise dimension overflow issue again.

[Browse files](/tensorflow/tensorflow/tree/c5b30379ba87cbe774b08ac50c1f6d36df4ebb7c)
Browse the repository at this point in the history

```
If resulting dimensions overflow an int32, we were seeing an overflow and
crash due to size mismatch during broadcast assignment.  The cause is a simple
dimension type mismatch.

Note that actual tests for this are currently impractical, since successful
operations require more than 2^32 elements and OOM on most machines.

PiperOrigin-RevId: 479336566
```

* Loading branch information

[![@cantonios](https://avatars.githubusercontent.com/u/2538739?s=40&v=4)](/cantonios) [![@tensorflower-gardener](https://avatars.githubusercontent.com/u/17151892?s=40&v=4)](/tensorflower-gardener)

[cantonios](/tensorflow/tensorflow/commits?author=cantonios "View all commits by cantonios")
authored and
[tensorflower-gardener](/tensorflow/tensorflow/commits?author=tensorflower-gardener "View all commits by tensorflower-gardener")
committed
Oct 6, 2022

1 parent
[e6e7ca5](/tensorflow/tensorflow/commit/e6e7ca5fd4908472939959130af0721b559d6101)

commit c5b3037

Showing
**1 changed file**
with
**10 additions**
and
**8 deletions**.

* Whitespace
* Ignore whitespace

* Split
* Unified

## There are no files selected for viewing

18 changes: 10 additions & 8 deletions

18
[tensorflow/core/kernels/cwise\_ops\_common.h](#diff-a5b798510da2493ca4df71be59c8dd66ebfb086fbc31872a381f523d7ab00e41 "tensorflow/core/kernels/cwise_ops_common.h")

Show comments

[View file](/tensorflow/tensorflow/blob/c5b30379ba87cbe774b08ac50c1f6d36df4ebb7c/tensorflow/core/kernels/cwise_ops_common.h)
Edit file

Delete file

This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters.
[Learn more about bidirectional Unicode characters](https://github.co/hiddenchars)

  [Show hidden characters](%7B%7B%20revealButtonHref%20%7D%7D)

| Original file line number | Diff line number | Diff line change |
| --- | --- | --- |
| Expand Up | | @@ -450,13 +450,15 @@ struct BinaryFunctor<CPUDevice, Functor, 2, false> { |
|  |  | Assign(d, out, in.unaryExpr(Unary(scalar.data()))); |
|  |  | } |
|  |  |  |
|  |  | inline Eigen::IndexList<int, Eigen::type2index<1>> NByOne(int n) { |
|  |  | Eigen::IndexList<int, Eigen::type2index<1>> ret; |
|  |  | inline Eigen::IndexList<Eigen::DenseIndex, Eigen::type2index<1>> NByOne( |
|  |  | Eigen::DenseIndex n) { |
|  |  | Eigen::IndexList<Eigen::DenseIndex, Eigen::type2index<1>> ret; |
|  |  | ret.set(0, n); |
|  |  | return ret; |
|  |  | } |
|  |  | inline Eigen::IndexList<Eigen::type2index<1>, int> OneByM(int m) { |
|  |  | Eigen::IndexList<Eigen::type2index<1>, int> ret; |
|  |  | inline Eigen::IndexList<Eigen::type2index<1>, Eigen::DenseIndex> OneByM( |
|  |  | Eigen::DenseIndex m) { |
|  |  | Eigen::IndexList<Eigen::type2index<1>, Eigen::DenseIndex> ret; |
|  |  | ret.set(1, m); |
|  |  | return ret; |
|  |  | } |
| Expand Down  Expand Up | | @@ -487,10 +489,10 @@ struct BinaryFunctor<CPUDevice, Functor, 2, false> { |
|  |  | // use\_broadcast\_optimization<T> are compile-time constant, gcc |
|  |  | // does a decent job avoiding generating code when conditions |
|  |  | // are not met. |
|  |  | const int a = in0.dimension(0); // in0 is shape [a, b] |
|  |  | const int b = in0.dimension(1); |
|  |  | const int c = in1.dimension(0); // in1 is shape [c, d] |
|  |  | const int d = in1.dimension(1); |
|  |  | const Eigen::DenseIndex a = in0.dimension(0); // in0 is shape [a, b] |
|  |  | const Eigen::DenseIndex b = in0.dimension(1); |
|  |  | const Eigen::DenseIndex c = in1.dimension(0); // in1 is shape [c, d] |
|  |  | const Eigen::DenseIndex d = in1.dimension(1); |
|  |  | if ((a == 1) && (d == 1)) { |
|  |  | auto lhs = in0.reshape(OneByM(b)).broadcast(NByOne(c)); |
|  |  | auto rhs = in1.reshape(NByOne(c)).broadcast(OneByM(b)); |
| Expand Down | |  |

Toggle all file notes
Toggle all file annotations

### 0 comments on commit `c5b3037`

Please
[sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fcommit%2Fc5b30379ba87cbe774b08ac50c1f6d36df4ebb7c) to comment.

## Footer

© 2025 GitHub, Inc.

### Footer navigation

* [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)
* [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)
* [Security](https://github.com/security)
* [Status](https://www.githubstatus.com/)
* [Docs](https://docs.github.com/)
* [Contact](https://support.github.com?tags=dotcom-footer)
* Manage cookies
* Do not share my personal information

You can’t perform that action at this time.



=== Content from github.com_61bb2d86_20250114_211738.html ===

[Skip to content](#start-of-content)

## Navigation Menu

Toggle navigation

[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fsecurity%2Fadvisories%2FGHSA-8fvv-46hw-vpg3)

* Product

  + [GitHub Copilot
    Write better code with AI](https://github.com/features/copilot)
  + [Security
    Find and fix vulnerabilities](https://github.com/features/security)
  + [Actions
    Automate any workflow](https://github.com/features/actions)
  + [Codespaces
    Instant dev environments](https://github.com/features/codespaces)
  + [Issues
    Plan and track work](https://github.com/features/issues)
  + [Code Review
    Manage code changes](https://github.com/features/code-review)
  + [Discussions
    Collaborate outside of code](https://github.com/features/discussions)
  + [Code Search
    Find more, search less](https://github.com/features/code-search)

  Explore
  + [All features](https://github.com/features)
  + [Documentation](https://docs.github.com)
  + [GitHub Skills](https://skills.github.com)
  + [Blog](https://github.blog)
* Solutions

  By company size
  + [Enterprises](https://github.com/enterprise)
  + [Small and medium teams](https://github.com/team)
  + [Startups](https://github.com/enterprise/startups)
  By use case
  + [DevSecOps](/solutions/use-case/devsecops)
  + [DevOps](/solutions/use-case/devops)
  + [CI/CD](/solutions/use-case/ci-cd)
  + [View all use cases](/solutions/use-case)

  By industry
  + [Healthcare](/solutions/industry/healthcare)
  + [Financial services](/solutions/industry/financial-services)
  + [Manufacturing](/solutions/industry/manufacturing)
  + [Government](/solutions/industry/government)
  + [View all industries](/solutions/industry)

  [View all solutions](/solutions)
* Resources

  Topics
  + [AI](/resources/articles/ai)
  + [DevOps](/resources/articles/devops)
  + [Security](/resources/articles/security)
  + [Software Development](/resources/articles/software-development)
  + [View all](/resources/articles)

  Explore
  + [Learning Pathways](https://resources.github.com/learn/pathways)
  + [White papers, Ebooks, Webinars](https://resources.github.com)
  + [Customer Stories](https://github.com/customer-stories)
  + [Partners](https://partner.github.com)
  + [Executive Insights](https://github.com/solutions/executive-insights)
* Open Source

  + [GitHub Sponsors
    Fund open source developers](/sponsors)
  + [The ReadME Project
    GitHub community articles](https://github.com/readme)
  Repositories
  + [Topics](https://github.com/topics)
  + [Trending](https://github.com/trending)
  + [Collections](https://github.com/collections)
* Enterprise

  + [Enterprise platform
    AI-powered developer platform](/enterprise)
  Available add-ons
  + [Advanced Security
    Enterprise-grade security features](https://github.com/enterprise/advanced-security)
  + [GitHub Copilot
    Enterprise-grade AI features](/features/copilot#enterprise)
  + [Premium Support
    Enterprise-grade 24/7 support](/premium-support)
* [Pricing](https://github.com/pricing)

Search or jump to...

# Search code, repositories, users, issues, pull requests...

Search

Clear

[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)

# Provide feedback

We read every piece of feedback, and take your input very seriously.

Include my email address so I can be contacted

  Cancel

 Submit feedback

# Saved searches

## Use saved searches to filter your results more quickly

Name

Query

To see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).

  Cancel

 Create saved search

[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fsecurity%2Fadvisories%2FGHSA-8fvv-46hw-vpg3)

[Sign up](/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E%2Frepos%2Fadvisories%2Fshow&source=header-repo&source_repo=tensorflow%2Ftensorflow)
Reseting focus

You signed in with another tab or window. Reload to refresh your session.
You signed out in another tab or window. Reload to refresh your session.
You switched accounts on another tab or window. Reload to refresh your session.

Dismiss alert

{{ message }}

[tensorflow](/tensorflow)
/
**[tensorflow](/tensorflow/tensorflow)**
Public

* [Notifications](/login?return_to=%2Ftensorflow%2Ftensorflow) You must be signed in to change notification settings
* [Fork
  74.4k](/login?return_to=%2Ftensorflow%2Ftensorflow)
* [Star
   187k](/login?return_to=%2Ftensorflow%2Ftensorflow)

* [Code](/tensorflow/tensorflow)
* [Issues
  827](/tensorflow/tensorflow/issues)
* [Pull requests
  5k+](/tensorflow/tensorflow/pulls)
* [Actions](/tensorflow/tensorflow/actions)
* [Projects
  2](/tensorflow/tensorflow/projects)
* [Security](/tensorflow/tensorflow/security)
* [Insights](/tensorflow/tensorflow/pulse)

Additional navigation options

* [Code](/tensorflow/tensorflow)
* [Issues](/tensorflow/tensorflow/issues)
* [Pull requests](/tensorflow/tensorflow/pulls)
* [Actions](/tensorflow/tensorflow/actions)
* [Projects](/tensorflow/tensorflow/projects)
* [Security](/tensorflow/tensorflow/security)
* [Insights](/tensorflow/tensorflow/pulse)

# Overflow in `tf.keras.losses.poisson`

Low

[pak-laura](/pak-laura)
published
GHSA-8fvv-46hw-vpg3
Nov 18, 2022

## Package

pip

tensorflow, tensorflow-cpu, tensorflow-gpu
([pip](/advisories?query=ecosystem%3Apip))

## Affected versions

< 2.11.0

## Patched versions

2.9.3, 2.10.1, 2.11.0

## Description

### Impact

[`tf.keras.losses.poisson`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/losses.py) receives a `y_pred` and `y_true` that are passed through `functor::mul` in [`BinaryOp`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/cwise_ops_common.h). If the resulting dimensions overflow an `int32`, TensorFlow will crash due to a size mismatch during broadcast assignment.

```
import numpy as np
import tensorflow as tf

true_value = tf.reshape(shape=[1, 2500000000], tensor = tf.zeros(dtype=tf.bool, shape=[50000, 50000]))
pred_value = np.array([[[-2]], [[8]]], dtype = np.float64)

tf.keras.losses.poisson(y_true=true_value,y_pred=pred_value)
```
### Patches

We have patched the issue in GitHub commit [c5b30379ba87cbe774b08ac50c1f6d36df4ebb7c](https://github.com/tensorflow/tensorflow/commit/c5b30379ba87cbe774b08ac50c1f6d36df4ebb7c).

The fix will be included in TensorFlow 2.11. We will also cherrypick this commit on TensorFlow 2.10.1 and 2.9.3, as these are also affected and still in supported range. However, we will not cherrypick this commit into TensorFlow 2.8.x, as it depends on Eigen behavior that changed between 2.8 and 2.9.

### For more information

Please consult [our security guide](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md) for more information regarding the security model and how to contact us with issues and questions.

### Attribution

This vulnerability has been reported by Pattarakrit Rattankul.

### Severity

Low

### CVE ID

CVE-2022-41887

### Weaknesses

No CWEs

## Footer

© 2025 GitHub, Inc.

### Footer navigation

* [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)
* [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)
* [Security](https://github.com/security)
* [Status](https://www.githubstatus.com/)
* [Docs](https://docs.github.com/)
* [Contact](https://support.github.com?tags=dotcom-footer)
* Manage cookies
* Do not share my personal information

You can’t perform that action at this time.



=== Content from github.com_a6163aee_20250114_211734.html ===

[Skip to content](#start-of-content)

## Navigation Menu

Toggle navigation

[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fblob%2Fmaster%2Ftensorflow%2Fcore%2Fkernels%2Fcwise_ops_common.h)

* Product

  + [GitHub Copilot
    Write better code with AI](https://github.com/features/copilot)
  + [Security
    Find and fix vulnerabilities](https://github.com/features/security)
  + [Actions
    Automate any workflow](https://github.com/features/actions)
  + [Codespaces
    Instant dev environments](https://github.com/features/codespaces)
  + [Issues
    Plan and track work](https://github.com/features/issues)
  + [Code Review
    Manage code changes](https://github.com/features/code-review)
  + [Discussions
    Collaborate outside of code](https://github.com/features/discussions)
  + [Code Search
    Find more, search less](https://github.com/features/code-search)

  Explore
  + [All features](https://github.com/features)
  + [Documentation](https://docs.github.com)
  + [GitHub Skills](https://skills.github.com)
  + [Blog](https://github.blog)
* Solutions

  By company size
  + [Enterprises](https://github.com/enterprise)
  + [Small and medium teams](https://github.com/team)
  + [Startups](https://github.com/enterprise/startups)
  By use case
  + [DevSecOps](/solutions/use-case/devsecops)
  + [DevOps](/solutions/use-case/devops)
  + [CI/CD](/solutions/use-case/ci-cd)
  + [View all use cases](/solutions/use-case)

  By industry
  + [Healthcare](/solutions/industry/healthcare)
  + [Financial services](/solutions/industry/financial-services)
  + [Manufacturing](/solutions/industry/manufacturing)
  + [Government](/solutions/industry/government)
  + [View all industries](/solutions/industry)

  [View all solutions](/solutions)
* Resources

  Topics
  + [AI](/resources/articles/ai)
  + [DevOps](/resources/articles/devops)
  + [Security](/resources/articles/security)
  + [Software Development](/resources/articles/software-development)
  + [View all](/resources/articles)

  Explore
  + [Learning Pathways](https://resources.github.com/learn/pathways)
  + [White papers, Ebooks, Webinars](https://resources.github.com)
  + [Customer Stories](https://github.com/customer-stories)
  + [Partners](https://partner.github.com)
  + [Executive Insights](https://github.com/solutions/executive-insights)
* Open Source

  + [GitHub Sponsors
    Fund open source developers](/sponsors)
  + [The ReadME Project
    GitHub community articles](https://github.com/readme)
  Repositories
  + [Topics](https://github.com/topics)
  + [Trending](https://github.com/trending)
  + [Collections](https://github.com/collections)
* Enterprise

  + [Enterprise platform
    AI-powered developer platform](/enterprise)
  Available add-ons
  + [Advanced Security
    Enterprise-grade security features](https://github.com/enterprise/advanced-security)
  + [GitHub Copilot
    Enterprise-grade AI features](/features/copilot#enterprise)
  + [Premium Support
    Enterprise-grade 24/7 support](/premium-support)
* [Pricing](https://github.com/pricing)

Search or jump to...

# Search code, repositories, users, issues, pull requests...

Search

Clear

[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)

# Provide feedback

We read every piece of feedback, and take your input very seriously.

Include my email address so I can be contacted

  Cancel

 Submit feedback

# Saved searches

## Use saved searches to filter your results more quickly

Name

Query

To see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).

  Cancel

 Create saved search

[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fblob%2Fmaster%2Ftensorflow%2Fcore%2Fkernels%2Fcwise_ops_common.h)

[Sign up](/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E%2Fblob%2Fshow&source=header-repo&source_repo=tensorflow%2Ftensorflow)
Reseting focus

You signed in with another tab or window. Reload to refresh your session.
You signed out in another tab or window. Reload to refresh your session.
You switched accounts on another tab or window. Reload to refresh your session.

Dismiss alert

{{ message }}

[tensorflow](/tensorflow)
/
**[tensorflow](/tensorflow/tensorflow)**
Public

* [Notifications](/login?return_to=%2Ftensorflow%2Ftensorflow) You must be signed in to change notification settings
* [Fork
  74.4k](/login?return_to=%2Ftensorflow%2Ftensorflow)
* [Star
   187k](/login?return_to=%2Ftensorflow%2Ftensorflow)

* [Code](/tensorflow/tensorflow)
* [Issues
  827](/tensorflow/tensorflow/issues)
* [Pull requests
  5k+](/tensorflow/tensorflow/pulls)
* [Actions](/tensorflow/tensorflow/actions)
* [Projects
  2](/tensorflow/tensorflow/projects)
* [Security](/tensorflow/tensorflow/security)
* [Insights](/tensorflow/tensorflow/pulse)

Additional navigation options

* [Code](/tensorflow/tensorflow)
* [Issues](/tensorflow/tensorflow/issues)
* [Pull requests](/tensorflow/tensorflow/pulls)
* [Actions](/tensorflow/tensorflow/actions)
* [Projects](/tensorflow/tensorflow/projects)
* [Security](/tensorflow/tensorflow/security)
* [Insights](/tensorflow/tensorflow/pulse)

## Files

 master
## Breadcrumbs

1. [tensorflow](/tensorflow/tensorflow/tree/master)
2. /[tensorflow](/tensorflow/tensorflow/tree/master/tensorflow)
3. /[core](/tensorflow/tensorflow/tree/master/tensorflow/core)
4. /[kernels](/tensorflow/tensorflow/tree/master/tensorflow/core/kernels)
/
# cwise\_ops\_common.h

Copy path Blame  Blame
## Latest commit

## History

[History](/tensorflow/tensorflow/commits/master/tensorflow/core/kernels/cwise_ops_common.h)683 lines (615 loc) · 26.9 KB master
## Breadcrumbs

1. [tensorflow](/tensorflow/tensorflow/tree/master)
2. /[tensorflow](/tensorflow/tensorflow/tree/master/tensorflow)
3. /[core](/tensorflow/tensorflow/tree/master/tensorflow/core)
4. /[kernels](/tensorflow/tensorflow/tree/master/tensorflow/core/kernels)
/
# cwise\_ops\_common.h

Top
## File metadata and controls

* Code
* Blame

683 lines (615 loc) · 26.9 KB[Raw](https://github.com/tensorflow/tensorflow/raw/refs/heads/master/tensorflow/core/kernels/cwise_ops_common.h)123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580581582583584585586587588589590591592593594595596597598599600601602603604605606607608609610611612613614615616617618619620621622623624625626627628629630631632633634635636637638639640641642643644645646647648649650651652653654655656657658659660661662663664665666667668669670671672673674675676677678679680681682683/\* Copyright 2015 The TensorFlow Authors. All Rights Reserved.
Licensed under the Apache License, Version 2.0 (the "License");you may not use this file except in compliance with the License.You may obtain a copy of the License at
 http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an "AS IS" BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.==============================================================================\*/
#ifndef TENSORFLOW\_CORE\_KERNELS\_CWISE\_OPS\_COMMON\_H\_#define TENSORFLOW\_CORE\_KERNELS\_CWISE\_OPS\_COMMON\_H\_
// See docs in ../ops/math\_ops.cc.#define \_USE\_MATH\_DEFINES#include <cmath>
#define EIGEN\_USE\_THREADS
#include "tensorflow/core/platform/bfloat16.h"
#include "tensorflow/core/framework/op.h"#include "tensorflow/core/framework/op\_kernel.h"#include "tensorflow/core/framework/tensor\_types.h"#include "tensorflow/core/framework/variant\_op\_registry.h"#include "tensorflow/core/kernels/cwise\_ops.h"#include "tensorflow/core/kernels/cwise\_ops\_gradients.h"#include "tensorflow/core/kernels/fill\_functor.h"#include "tensorflow/core/platform/logging.h"#include "tensorflow/core/util/bcast.h"
namespace tensorflow {
typedef Eigen::ThreadPoolDevice CPUDevice;typedef Eigen::GpuDevice GPUDevice;
class BinaryOpShared : public OpKernel { public: explicit BinaryOpShared(OpKernelConstruction\* ctx, DataType out, DataType in);
 protected: struct BinaryOpState { // Sets up bcast with the shape of in0 and in1, ensures that the bcast // is valid, and if so, set out, either by allocating a new buffer using // ctx->output(...) or by creating an alias for an owned input buffer for // in-place computation. // Caller must check ctx->status() upon return for non-ok status. // If ctx->status().ok() is true, then out is guaranteed to be allocated. explicit BinaryOpState(OpKernelContext\* ctx);
 const Tensor& in0; const Tensor& in1;
 BCast bcast; Tensor\* out = nullptr; int64\_t out\_num\_elements;
 int64\_t in0\_num\_elements; int64\_t in1\_num\_elements;
 int ndims; bool result; };
 void SetUnimplementedError(OpKernelContext\* ctx); void SetComputeError(OpKernelContext\* ctx);};
// Coefficient-wise binary operations:// Device: E.g., CPUDevice, GPUDevice.// Functor: defined in cwise\_ops.h. E.g., functor::add.template <typename Device, typename Functor>class BinaryOp : public BinaryOpShared { public: typedef typename Functor::in\_type Tin; // Input scalar data type. typedef typename Functor::out\_type Tout; // Output scalar data type.
 explicit BinaryOp(OpKernelConstruction\* ctx) : BinaryOpShared(ctx, DataTypeToEnum<Tout>::v(), DataTypeToEnum<Tin>::v()) {}
 void Compute(OpKernelContext\* ctx) override { const Tensor& input\_0 = ctx->input(0); OP\_REQUIRES(ctx, input\_0.dtype() == DataTypeToEnum<Tin>::v(), errors::InvalidArgument( "Expected tensor of type ", DataTypeString(DataTypeToEnum<Tin>::v()), " but got type ", DataTypeString(input\_0.dtype()))); const Tensor& input\_1 = ctx->input(1); OP\_REQUIRES(ctx, input\_1.dtype() == DataTypeToEnum<Tin>::v(), errors::InvalidArgument( "Expected tensor of type ", DataTypeString(DataTypeToEnum<Tin>::v()), " but got type ", DataTypeString(input\_1.dtype()))); const Device& eigen\_device = ctx->eigen\_device<Device>(); bool error = false; bool\* const error\_ptr = Functor::has\_errors ? &error : nullptr;
 // NOTE: Handle three simple cases before building the BinaryOpState, which // is relatively expensive for small operations. if (input\_0.shape() == input\_1.shape()) { // tensor op tensor with no broadcasting. Tensor\* out; OP\_REQUIRES\_OK(ctx, ctx->forward\_input\_or\_allocate\_output( {0, 1}, 0, input\_0.shape(), &out)); functor::BinaryFunctor<Device, Functor, 1>()( eigen\_device, out->template flat<Tout>(), input\_0.template flat<Tin>(), input\_1.template flat<Tin>(), error\_ptr); if (Functor::has\_errors && error) { SetComputeError(ctx); } return; } else if (input\_0.shape().dims() == 0) { // scalar op tensor. Tensor\* out; OP\_REQUIRES\_OK(ctx, ctx->forward\_input\_or\_allocate\_output( {1}, 0, input\_1.shape(), &out));
 functor::BinaryFunctor<Device, Functor, 1>().Left( eigen\_device, out->template flat<Tout>(), input\_0.template scalar<Tin>(), input\_1.template flat<Tin>(), error\_ptr); if (Functor::has\_errors && error) { SetComputeError(ctx); } return; } else if (input\_1.shape().dims() == 0) { // tensor op scalar. Tensor\* out; OP\_REQUIRES\_OK(ctx, ctx->forward\_input\_or\_allocate\_output( {0}, 0, input\_0.shape(), &out)); functor::BinaryFunctor<Device, Functor, 1>().Right( eigen\_device, out->template flat<Tout>(), input\_0.template flat<Tin>(), input\_1.template scalar<Tin>(), error\_ptr); if (Functor::has\_errors && error) { SetComputeError(ctx); } return; }
 // 'state': Shared helper not dependent on T to reduce code size BinaryOpState state(ctx); if (ctx->status().code() == error::RESOURCE\_EXHAUSTED) { // Stop when BinaryOpState's constructor failed due to OOM. return; } auto& bcast = state.bcast; Tensor\* out = state.out; if (!bcast.IsValid()) { if (ctx->status().ok()) { if (state.result) { functor::SetOneFunctor<Device, bool>()(eigen\_device, out->flat<bool>()); } else { functor::SetZeroFunctor<Device, bool>()(eigen\_device, out->flat<bool>()); } } return; }
 auto& in0 = state.in0; auto& in1 = state.in1; if (state.out\_num\_elements == 0) { return; }
 const int ndims = state.ndims; if (ndims <= 1) { auto out\_flat = out->flat<Tout>(); if (state.in1\_num\_elements == 1) { // tensor op scalar functor::BinaryFunctor<Device, Functor, 1>().Right( eigen\_device, out\_flat, in0.template flat<Tin>(), in1.template scalar<Tin>(), error\_ptr); } else if (state.in0\_num\_elements == 1) { // scalar op tensor functor::BinaryFunctor<Device, Functor, 1>().Left( eigen\_device, out\_flat, in0.template scalar<Tin>(), in1.template flat<Tin>(), error\_ptr); } else { functor::BinaryFunctor<Device, Functor, 1>()( eigen\_device, out\_flat, in0.template flat<Tin>(), in1.template flat<Tin>(), error\_ptr); } } else if (ndims == 2) { functor::BinaryFunctor<Device, Functor, 2>().BCast( eigen\_device, out->shaped<Tout, 2>(bcast.result\_shape()), in0.template shaped<Tin, 2>(bcast.x\_reshape()), BCast::ToIndexArray<2>(bcast.x\_bcast()), in1.template shaped<Tin, 2>(bcast.y\_reshape()), BCast::ToIndexArray<2>(bcast.y\_bcast()), error\_ptr); } else if (ndims == 3) { functor::BinaryFunctor<Device, Functor, 3>().BCast( eigen\_device, out->shaped<Tout, 3>(bcast.result\_shape()), in0.template shaped<Tin, 3>(bcast.x\_reshape()), BCast::ToIndexArray<3>(bcast.x\_bcast()), in1.template shaped<Tin, 3>(bcast.y\_reshape()), BCast::ToIndexArray<3>(bcast.y\_bcast()), error\_ptr); } else if (ndims == 4) { functor::BinaryFunctor<Device, Functor, 4>().BCast( eigen\_device, out->shaped<Tout, 4>(bcast.result\_shape()), in0.template shaped<Tin, 4>(bcast.x\_reshape()), BCast::ToIndexArray<4>(bcast.x\_bcast()), in1.template shaped<Tin, 4>(bcast.y\_reshape()), BCast::ToIndexArray<4>(bcast.y\_bcast()), error\_ptr); } else if (ndims == 5) { functor::BinaryFunctor<Device, Functor, 5>().BCast( eigen\_device, out->shaped<Tout, 5>(bcast.result\_shape()), in0.template shaped<Tin, 5>(bcast.x\_reshape()), BCast::ToIndexArray<5>(bcast.x\_bcast()), in1.template shaped<Tin, 5>(bcast.y\_reshape()), BCast::ToIndexArray<5>(bcast.y\_bcast()), error\_ptr); } else { SetUnimplementedError(ctx); } if (Functor::has\_errors && error) { SetComputeError(ctx); } }};
template <typename Device, typename T>class ApproximateEqualOp : public OpKernel { public: explicit ApproximateEqualOp(OpKernelConstruction\* context) : OpKernel(context) { float tolerance; OP\_REQUIRES\_OK(context, context->GetAttr("tolerance", &tolerance)); tolerance\_ = T(tolerance); } void Compute(OpKernelContext\* context) override { const Tensor& x\_input = context->input(0); const Tensor& y\_input = context->input(1); OP\_REQUIRES( context, x\_input.shape() == y\_input.shape(), errors::InvalidArgument("x and y must be of the same shape. ", "x shape: ", x\_input.shape().DebugString(), ". y shape: ", y\_input.shape().DebugString())); Tensor\* z\_output = nullptr; OP\_REQUIRES\_OK(context, context->allocate\_output(0, x\_input.shape(), &z\_output)); const Device& d = context->eigen\_device<Device>(); typename TTypes<T>::ConstFlat x(x\_input.flat<T>()); typename TTypes<T>::ConstFlat y(y\_input.flat<T>()); typename TTypes<bool>::Flat z(z\_output->flat<bool>()); functor::ApproximateEqual<Device, T>()(d, x, y, tolerance\_, z); }
 private: T tolerance\_;};
// Basic coefficient-wise binary operations that are known to not require// any broadcasting. This is the case for example of the gradients of// unary operations.// Device: E.g., CPUDevice, GPUDevice.// Functor: defined above. E.g., functor::tanh\_grad.template <typename Device, typename Functor>class SimpleBinaryOp : public OpKernel { public: typedef typename Functor::in\_type Tin; // Input scalar data type. typedef typename Functor::out\_type Tout; // Output scalar data type.
 explicit SimpleBinaryOp(OpKernelConstruction\* ctx) : OpKernel(ctx) {}
 void Compute(OpKernelContext\* ctx) override { const Tensor& in0 = ctx->input(0); const Tensor& in1 = ctx->input(1); OP\_REQUIRES( ctx, in0.NumElements() == in1.NumElements(), errors::InvalidArgument("The two arguments to a cwise op must have " "same number of elements, got ", in0.NumElements(), " and ", in1.NumElements())); auto in0\_flat = in0.flat<Tin>(); auto in1\_flat = in1.flat<Tin>(); const Device& eigen\_device = ctx->eigen\_device<Device>();
 Tensor\* out = nullptr; if (std::is\_same<Tin, Tout>::value) { OP\_REQUIRES\_OK(ctx, ctx->forward\_input\_or\_allocate\_output( {0, 1}, 0, in0.shape(), &out)); } else { OP\_REQUIRES\_OK(ctx, ctx->allocate\_output(0, in0.shape(), &out)); } auto out\_flat = out->flat<Tout>(); functor::SimpleBinaryFunctor<Device, Functor>()(eigen\_device, out\_flat, in0\_flat, in1\_flat); }};
// Coefficient-wise unary operations:// Device: E.g., CPUDevice, GPUDevice.// Functor: defined in cwise\_ops.h. E.g., functor::sqrt.template <typename Device, typename Functor>class UnaryOp : public OpKernel { public: typedef typename Functor::in\_type Tin; // Input scalar data type. typedef typename Functor::out\_type Tout; // Output scalar data type. // Tin may be different from Tout. E.g., abs: complex64 -> float
 explicit UnaryOp(OpKernelConstruction\* ctx) : OpKernel(ctx) { auto in = DataTypeToEnum<Tin>::v(); auto out = DataTypeToEnum<Tout>::v(); OP\_REQUIRES\_OK(ctx, ctx->MatchSignature({in}, {out})); }
 void Compute(OpKernelContext\* ctx) override { const Tensor& inp = ctx->input(0); Tensor\* out = nullptr; if (std::is\_same<Tin, Tout>::value) { OP\_REQUIRES\_OK(ctx, ctx->forward\_input\_or\_allocate\_output( {0}, 0, inp.shape(), &out)); } else { OP\_REQUIRES\_OK(ctx, ctx->allocate\_output(0, inp.shape(), &out)); } functor::UnaryFunctor<Device, Functor>()( ctx->eigen\_device<Device>(), out->flat<Tout>(), inp.flat<Tin>()); }};
template <typename Device, VariantUnaryOp OpEnum>class UnaryVariantOp : public OpKernel { public: explicit UnaryVariantOp(OpKernelConstruction\* ctx) : OpKernel(ctx) {}
 void Compute(OpKernelContext\* ctx) override { const Tensor& inp = ctx->input(0); OP\_REQUIRES( ctx, TensorShapeUtils::IsScalar(inp.shape()), errors::InvalidArgument("Non-scalar variants are not supported.")); const Variant& v = inp.scalar<Variant>()(); Variant v\_out; OP\_REQUIRES\_OK(ctx, UnaryOpVariant<Device>(ctx, OpEnum, v, &v\_out)); int numa\_node = ctx->device()->NumaNode(); Tensor out(cpu\_allocator(numa\_node), DT\_VARIANT, TensorShape()); out.scalar<Variant>()() = std::move(v\_out); ctx->set\_output(0, std::move(out)); }};
namespace functor {
template <typename D, typename Out, typename Rhs>void Assign(const D& d, Out out, Rhs rhs) { out.device(d) = rhs;}
// Partial specialization of BinaryFunctor<Device=CPUDevice, Functor, NDIMS>// for functors with no error checking.template <typename Functor, int NDIMS>struct BinaryFunctor<CPUDevice, Functor, NDIMS, false> { void operator()(const CPUDevice& d, typename Functor::tout\_type out, typename Functor::tin\_type in0, typename Functor::tin\_type in1, bool\* error) { Assign(d, out, in0.binaryExpr(in1, typename Functor::func())); }
 void Left(const CPUDevice& d, typename Functor::tout\_type out, typename Functor::tscalar\_type scalar, typename Functor::tin\_type in, bool\* error) { typedef typename Functor::out\_type Tout; typedef typename Functor::in\_type Tin; typedef typename Functor::func Binary; typedef typename Eigen::internal::scalar\_left<Tout, Tin, Binary> Unary; Assign(d, out, in.unaryExpr(Unary(scalar.data()))); }
 void Right(const CPUDevice& d, typename Functor::tout\_type out, typename Functor::tin\_type in, typename Functor::tscalar\_type scalar, bool\* error) { typedef typename Functor::out\_type Tout; typedef typename Functor::in\_type Tin; typedef typename Functor::func Binary; typedef typename Eigen::internal::scalar\_right<Tout, Tin, Binary> Unary; Assign(d, out, in.unaryExpr(Unary(scalar.data()))); }
 void BCast(const CPUDevice& dev, typename TTypes<typename Functor::out\_type, NDIMS>::Tensor out, typename TTypes<typename Functor::in\_type, NDIMS>::ConstTensor in0, typename Eigen::array<Eigen::DenseIndex, NDIMS> bcast0, typename TTypes<typename Functor::in\_type, NDIMS>::ConstTensor in1, typename Eigen::array<Eigen::DenseIndex, NDIMS> bcast1, bool\* error) { typename Functor::func func; if (AllOne<NDIMS>(bcast0) && AllOne<NDIMS>(bcast1)) { Assign(dev, out, in0.binaryExpr(in1, func)); } else if (AllOne<NDIMS>(bcast0)) { auto rhs = in1.broadcast(bcast1); Assign(dev, out, in0.binaryExpr(rhs, func)); } else if (AllOne<NDIMS>(bcast1)) { auto lhs = in0.broadcast(bcast0); Assign(dev, out, lhs.binaryExpr(in1, func)); } else { auto lhs = in0.broadcast(bcast0); auto rhs = in1.broadcast(bcast1); Assign(dev, out, lhs.binaryExpr(rhs, func)); } }};
// Partial specialization of BinaryFunctor<Device=CPUDevice, Functor, 2>// for functors with no error checking.template <typename Functor>struct BinaryFunctor<CPUDevice, Functor, 2, false> { enum { NDIMS = 2 };
 void operator()(const CPUDevice& d, typename Functor::tout\_type out, typename Functor::tin\_type in0, typename Functor::tin\_type in1, bool\* error) { Assign(d, out, in0.binaryExpr(in1, typename Functor::func())); }
 void Left(const CPUDevice& d, typename Functor::tout\_type out, typename Functor::tscalar\_type scalar, typename Functor::tin\_type in, bool\* error) { typedef typename Functor::out\_type Tout; typedef typename Functor::in\_type Tin; typedef typename Functor::func Binary; typedef typename Eigen::internal::scalar\_left<Tout, Tin, Binary> Unary; Assign(d, out, in.unaryExpr(Unary(scalar.data()))); }
 void Right(const CPUDevice& d, typename Functor::tout\_type out, typename Functor::tin\_type in, typename Functor::tscalar\_type scalar, bool\* error) { typedef typename Functor::out\_type Tout; typedef typename Functor::in\_type Tin; typedef typename Functor::func Binary; typedef typename Eigen::internal::scalar\_right<Tout, Tin, Binary> Unary; Assign(d, out, in.unaryExpr(Unary(scalar.data()))); }
 inline Eigen::IndexList<Eigen::DenseIndex, Eigen::type2index<1>> NByOne( Eigen::DenseIndex n) { Eigen::IndexList<Eigen::DenseIndex, Eigen::type2index<1>> ret; ret.set(0, n); return ret; } inline Eigen::IndexList<Eigen::type2index<1>, Eigen::DenseIndex> OneByM( Eigen::DenseIndex m) { Eigen::IndexList<Eigen::type2index<1>, Eigen::DenseIndex> ret; ret.set(1, m); return ret; }
 void BCast(const CPUDevice& dev, typename TTypes<typename Functor::out\_type, NDIMS>::Tensor out, typename TTypes<typename Functor::in\_type, NDIMS>::ConstTensor in0, typename Eigen::array<Eigen::DenseIndex, NDIMS> bcast0, typename TTypes<typename Functor::in\_type, NDIMS>::ConstTensor in1, typename Eigen::array<Eigen::DenseIndex, NDIMS> bcast1, bool\* error) { typedef typename Functor::in\_type T; typename Functor::func func; if (Functor::use\_bcast\_optimization && use\_bcast\_optimization<T>::value) { // Optimize for speed by using Eigen::type2index and avoid // .broadcast() when we know it's a no-op. // // Here, we need to handle 6 cases depending on how many "1" // exist in in0 and in1's shapes (4 numbers in total). It's not // possible that two shapes have more than 2 1s because those // are simplified to NDIMS==1 case. // // Because this optimization increases the binary size for each // Functor (+, -, \*, /, <, <=, etc.), type and ndim combination. // we only apply such optimization for selected ops/types/ndims. // // Because NDIMS, Functor::use\_broadcast\_optimization and // use\_broadcast\_optimization<T> are compile-time constant, gcc // does a decent job avoiding generating code when conditions // are not met. const Eigen::DenseIndex a = in0.dimension(0); // in0 is shape [a, b] const Eigen::DenseIndex b = in0.dimension(1); const Eigen::DenseIndex c = in1.dimension(0); // in1 is shape [c, d] const Eigen::DenseIndex d = in1.dimension(1); if ((a == 1) && (d == 1)) { auto lhs = in0.reshape(OneByM(b)).broadcast(NByOne(c)); auto rhs = in1.reshape(NByOne(c)).broadcast(OneByM(b)); Assign(dev, out, lhs.binaryExpr(rhs, func)); return; } if ((b == 1) && (c == 1)) { auto lhs = in0.reshape(NByOne(a)).broadcast(OneByM(d)); auto rhs = in1.reshape(OneByM(d)).broadcast(NByOne(a)); Assign(dev, out, lhs.binaryExpr(rhs, func)); return; } if (a == 1) { auto lhs = in0.reshape(OneByM(b)).broadcast(NByOne(c)); auto rhs = in1; Assign(dev, out, lhs.binaryExpr(rhs, func)); return; } if (b == 1) { auto lhs = in0.reshape(NByOne(a)).broadcast(OneByM(d)); auto rhs = in1; Assign(dev, out, lhs.binaryExpr(rhs, func)); return; } if (c == 1) { auto lhs = in0; auto rhs = in1.reshape(OneByM(d)).broadcast(NByOne(a)); Assign(dev, out, lhs.binaryExpr(rhs, func)); return; } if (d == 1) { auto lhs = in0; auto rhs = in1.reshape(NByOne(c)).broadcast(OneByM(b)); Assign(dev, out, lhs.binaryExpr(rhs, func)); return; }
 const bool bcast0\_all\_one = AllOne<NDIMS>(bcast0); const bool bcast1\_all\_one = AllOne<NDIMS>(bcast1); if (bcast0\_all\_one && !bcast1\_all\_one) { auto lhs = in0; // No need to do broadcast for in0 auto rhs = in1.broadcast(bcast1); Assign(dev, out, lhs.binaryExpr(rhs, func)); return; }
 if (!bcast0\_all\_one && bcast1\_all\_one) { auto lhs = in0.broadcast(bcast0); auto rhs = in1; // No need to do broadcast for in1 Assign(dev, out, lhs.binaryExpr(rhs, func)); return; } }
 // Fallback path. Always works and probably slower. auto lhs = in0.broadcast(bcast0); auto rhs = in1.broadcast(bcast1); Assign(dev, out, lhs.binaryExpr(rhs, func)); }};
// Version of BinaryFunctor with error handling.template <typename Functor, int NDIMS>struct BinaryFunctor<CPUDevice, Functor, NDIMS, true> { void operator()(const CPUDevice& d, typename Functor::tout\_type out, typename Functor::tin\_type in0, typename Functor::tin\_type in1, bool\* error) { Assign(d, out, in0.binaryExpr(in1, typename Functor::func(error))); }
 void Left(const CPUDevice& d, typename Functor::tout\_type out, typename Functor::tscalar\_type scalar, typename Functor::tin\_type in, bool\* error) { typedef typename Functor::out\_type Tout; typedef typename Functor::in\_type Tin; typedef typename Functor::func Binary; typedef typename Eigen::internal::scalar\_left<Tout, Tin, Binary> Unary; Assign(d, out, in.unaryExpr(Unary(scalar.data(), error))); }
 void Right(const CPUDevice& d, typename Functor::tout\_type out, typename Functor::tin\_type in, typename Functor::tscalar\_type scalar, bool\* error) { typedef typename Functor::out\_type Tout; typedef typename Functor::in\_type Tin; typedef typename Functor::func Binary; typedef typename Eigen::internal::scalar\_right<Tout, Tin, Binary> Unary; Assign(d, out, in.unaryExpr(Unary(scalar.data(), error))); }
 void BCast(const CPUDevice& dev, typename TTypes<typename Functor::out\_type, NDIMS>::Tensor out, typename TTypes<typename Functor::in\_type, NDIMS>::ConstTensor in0, typename Eigen::array<Eigen::DenseIndex, NDIMS> bcast0, typename TTypes<typename Functor::in\_type, NDIMS>::ConstTensor in1, typename Eigen::array<Eigen::DenseIndex, NDIMS> bcast1, bool\* error) { typename Functor::func func(error); auto lhs = in0.broadcast(bcast0); auto rhs = in1.broadcast(bcast1); Assign(dev, out, lhs.binaryExpr(rhs, func)); }};
// Partial specialization of UnaryFunctor<Device=CPUDevice, Functor>.template <typename Functor>struct UnaryFunctor<CPUDevice, Functor> { void operator()(const CPUDevice& d, typename Functor::tout\_type out, typename Functor::tin\_type in) { Assign(d, out, in.unaryExpr(typename Functor::func())); }};
template <typename Functor, typename Targ>struct UnaryFunctorWithArg<CPUDevice, Functor, Targ> { void operator()(const CPUDevice& d, typename Functor::tout\_type out, typename Functor::tin\_type in, Targ val) { Assign(d, out, in.unaryExpr(typename Functor::func(val))); }};
// Partial specialization of ApproximateEqual<Device=CPUDevice, T>.template <typename T>struct ApproximateEqual<CPUDevice, T> { void operator()(const CPUDevice& d, typename TTypes<T>::ConstFlat x, typename TTypes<T>::ConstFlat y, T tolerance, typename TTypes<bool>::Flat z) { auto diff = x - y; z.device(d) = diff.abs() <= tolerance; }};
} // end namespace functor
#define REGISTER(OP, D, N, F, T) \ REGISTER\_KERNEL\_BUILDER(Name(N).Device(DEVICE\_##D).TypeConstraint<T>("T"), \ OP<D##Device, F<T>>);
#define REGISTER\_VARIANT(OP, D, N, ENUM) \ REGISTER\_KERNEL\_BUILDER( \ Name(N).Device(DEVICE\_##D).TypeConstraint<Variant>("T"), \ OP<D##Device, ENUM>);
// Macros to register kernels for multiple types (T0, T1, etc.) on// device type "D" (CPU or GPU) for operation "N" (e.g., sqrt) using// the functor "F" (e.g., functor::sqrt).
#if defined(\_\_ANDROID\_TYPES\_SLIM\_\_)// Note that \_\_ANDROID\_TYPES\_SLIM\_\_ is also checked in the cwise\_ops\*.cc files.// Normally Android TensorFlow is built with a reduced number of types (float).// Override on the command-line using "--copt=-D\_\_ANDROID\_TYPES\_FULL\_\_"// to generate a library with full type support with a consequent increase in// code size.#define REGISTER2(OP, D, N, F, T0, T1) REGISTER(OP, D, N, F, T0)#define REGISTER3(OP, D, N, F, T0, T1, T2) REGISTER(OP, D, N, F, T0)#define REGISTER4(OP, D, N, F, T0, T1, T2, T3) REGISTER(OP, D, N, F, T0)#define REGISTER5(OP, D, N, F, T0, T1, T2, T3, T4) REGISTER(OP, D, N, F, T0)#define REGISTER6(OP, D, N, F, T0, T1, T2, T3, T4, T5) REGISTER(OP, D, N, F, T0)#define REGISTER7(OP, D, N, F, T0, T1, T2, T3, T4, T5, T6) \ REGISTER(OP, D, N, F, T0)#define REGISTER8(OP, D, N, F, T0, T1, T2, T3, T4, T5, T6, T7) \ REGISTER(OP, D, N, F, T0)#define REGISTER9(OP, D, N, F, T0, T1, T2, T3, T4, T5, T6, T7, T8) \ REGISTER(OP, D, N, F, T0)#else // !defined(\_\_ANDROID\_TYPES\_SLIM\_\_)#define REGISTER2(OP, D, N, F, T0, T1) \ REGISTER(OP, D, N, F, T0) \ REGISTER(OP, D, N, F, T1)#define REGISTER3(OP, D, N, F, T0, T1, T2) \ REGISTER2(OP, D, N, F, T0, T1) \ REGISTER(OP, D, N, F, T2)#define REGISTER4(OP, D, N, F, T0, T1, T2, T3) \ REGISTER2(OP, D, N, F, T0, T1) \ REGISTER2(OP, D, N, F, T2, T3)#define REGISTER5(OP, D, N, F, T0, T1, T2, T3, T4) \ REGISTER3(OP, D, N, F, T0, T1, T2) \ REGISTER2(OP, D, N, F, T3, T4)#define REGISTER6(OP, D, N, F, T0, T1, T2, T3, T4, T5) \ REGISTER3(OP, D, N, F, T0, T1, T2) \ REGISTER3(OP, D, N, F, T3, T4, T5)#define REGISTER7(OP, D, N, F, T0, T1, T2, T3, T4, T5, T6) \ REGISTER4(OP, D, N, F, T0, T1, T2, T3) \ REGISTER3(OP, D, N, F, T4, T5, T6)#define REGISTER8(OP, D, N, F, T0, T1, T2, T3, T4, T5, T6, T7) \ REGISTER4(OP, D, N, F, T0, T1, T2, T3) \ REGISTER4(OP, D, N, F, T4, T5, T6, T7)#define REGISTER9(OP, D, N, F, T0, T1, T2, T3, T4, T5, T6, T7, T8) \ REGISTER5(OP, D, N, F, T0, T1, T2, T3, T4) \ REGISTER4(OP, D, N, F, T5, T6, T7, T8)
// Instead of adding REGISTER10, etc., shard the .cc files - see// cwise\_op\_equal\_to\_\*.cc for an example.
#endif // defined(\_\_ANDROID\_TYPES\_SLIM\_\_)
} // end namespace tensorflow
#endif // TENSORFLOW\_CORE\_KERNELS\_CWISE\_OPS\_COMMON\_H\_

## Footer

© 2025 GitHub, Inc.

### Footer navigation

* [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)
* [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)
* [Security](https://github.com/security)
* [Status](https://www.githubstatus.com/)
* [Docs](https://docs.github.com/)
* [Contact](https://support.github.com?tags=dotcom-footer)
* Manage cookies
* Do not share my personal information

You can’t perform that action at this time.


