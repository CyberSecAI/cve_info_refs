=== Content from github.com_ca277a89_20250125_052127.html ===

[Skip to content](#start-of-content)

## Navigation Menu

Toggle navigation

[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fscrapy%2Fscrapy%2Fissues%2F482)

* Product

  + [GitHub Copilot
    Write better code with AI](https://github.com/features/copilot)
  + [Security
    Find and fix vulnerabilities](https://github.com/features/security)
  + [Actions
    Automate any workflow](https://github.com/features/actions)
  + [Codespaces
    Instant dev environments](https://github.com/features/codespaces)
  + [Issues
    Plan and track work](https://github.com/features/issues)
  + [Code Review
    Manage code changes](https://github.com/features/code-review)
  + [Discussions
    Collaborate outside of code](https://github.com/features/discussions)
  + [Code Search
    Find more, search less](https://github.com/features/code-search)

  Explore
  + [All features](https://github.com/features)
  + [Documentation](https://docs.github.com)
  + [GitHub Skills](https://skills.github.com)
  + [Blog](https://github.blog)
* Solutions

  By company size
  + [Enterprises](https://github.com/enterprise)
  + [Small and medium teams](https://github.com/team)
  + [Startups](https://github.com/enterprise/startups)
  + [Nonprofits](/solutions/industry/nonprofits)
  By use case
  + [DevSecOps](/solutions/use-case/devsecops)
  + [DevOps](/solutions/use-case/devops)
  + [CI/CD](/solutions/use-case/ci-cd)
  + [View all use cases](/solutions/use-case)

  By industry
  + [Healthcare](/solutions/industry/healthcare)
  + [Financial services](/solutions/industry/financial-services)
  + [Manufacturing](/solutions/industry/manufacturing)
  + [Government](/solutions/industry/government)
  + [View all industries](/solutions/industry)

  [View all solutions](/solutions)
* Resources

  Topics
  + [AI](/resources/articles/ai)
  + [DevOps](/resources/articles/devops)
  + [Security](/resources/articles/security)
  + [Software Development](/resources/articles/software-development)
  + [View all](/resources/articles)

  Explore
  + [Learning Pathways](https://resources.github.com/learn/pathways)
  + [White papers, Ebooks, Webinars](https://resources.github.com)
  + [Customer Stories](https://github.com/customer-stories)
  + [Partners](https://partner.github.com)
  + [Executive Insights](https://github.com/solutions/executive-insights)
* Open Source

  + [GitHub Sponsors
    Fund open source developers](/sponsors)
  + [The ReadME Project
    GitHub community articles](https://github.com/readme)
  Repositories
  + [Topics](https://github.com/topics)
  + [Trending](https://github.com/trending)
  + [Collections](https://github.com/collections)
* Enterprise

  + [Enterprise platform
    AI-powered developer platform](/enterprise)
  Available add-ons
  + [Advanced Security
    Enterprise-grade security features](https://github.com/enterprise/advanced-security)
  + [GitHub Copilot
    Enterprise-grade AI features](/features/copilot#enterprise)
  + [Premium Support
    Enterprise-grade 24/7 support](/premium-support)
* [Pricing](https://github.com/pricing)

Search or jump to...

# Search code, repositories, users, issues, pull requests...

Search

Clear

[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)

# Provide feedback

We read every piece of feedback, and take your input very seriously.

Include my email address so I can be contacted

  Cancel

 Submit feedback

# Saved searches

## Use saved searches to filter your results more quickly

Name

Query

To see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).

  Cancel

 Create saved search

[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fscrapy%2Fscrapy%2Fissues%2F482)

[Sign up](/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E%2Fvoltron%2Fissues_fragments%2Fissue_layout&source=header-repo&source_repo=scrapy%2Fscrapy)
Reseting focus

You signed in with another tab or window. Reload to refresh your session.
You signed out in another tab or window. Reload to refresh your session.
You switched accounts on another tab or window. Reload to refresh your session.

Dismiss alert

{{ message }}

[scrapy](/scrapy)
/
**[scrapy](/scrapy/scrapy)**
Public

* [Notifications](/login?return_to=%2Fscrapy%2Fscrapy) You must be signed in to change notification settings
* [Fork
  10.6k](/login?return_to=%2Fscrapy%2Fscrapy)
* [Star
   53.9k](/login?return_to=%2Fscrapy%2Fscrapy)

* [Code](/scrapy/scrapy)
* [Issues
  426](/scrapy/scrapy/issues)
* [Pull requests
  182](/scrapy/scrapy/pulls)
* [Discussions](/scrapy/scrapy/discussions)
* [Actions](/scrapy/scrapy/actions)
* [Projects
  0](/scrapy/scrapy/projects)
* [Wiki](/scrapy/scrapy/wiki)
* [Security](/scrapy/scrapy/security)
* [Insights](/scrapy/scrapy/pulse)

Additional navigation options

* [Code](/scrapy/scrapy)
* [Issues](/scrapy/scrapy/issues)
* [Pull requests](/scrapy/scrapy/pulls)
* [Discussions](/scrapy/scrapy/discussions)
* [Actions](/scrapy/scrapy/actions)
* [Projects](/scrapy/scrapy/projects)
* [Wiki](/scrapy/scrapy/wiki)
* [Security](/scrapy/scrapy/security)
* [Insights](/scrapy/scrapy/pulse)

# S3FilesStore can use a lot of memory #482

[New issue](/login?return_to=)[Jump to bottom](#comment-composer-heading)Copy link[New issue](/login?return_to=)[Jump to bottom](#comment-composer-heading)Copy linkOpenOpen[S3FilesStore can use a lot of memory](#top)#482Copy linkLabels[S3](https://github.com/scrapy/scrapy/issues?q=state%3Aopen%20label%3A%22S3%22)[bug](https://github.com/scrapy/scrapy/issues?q=state%3Aopen%20label%3A%22bug%22)[media pipelines](https://github.com/scrapy/scrapy/issues?q=state%3Aopen%20label%3A%22media%20pipelines%22)[security](https://github.com/scrapy/scrapy/issues?q=state%3Aopen%20label%3A%22security%22)Milestone[Scrapy 2.13](https://github.com/scrapy/scrapy/milestone/46)[![@kmike](https://avatars.githubusercontent.com/u/107893?v=4&size=80)](/kmike)
## Description

[![@kmike](https://avatars.githubusercontent.com/u/107893?v=4&size=48)](/kmike)[kmike](https://github.com/kmike)opened [on Dec 5, 2013](https://github.com/scrapy/scrapy/issues/482#issue-23779747)

Hi,

[@nramirezuy](https://github.com/nramirezuy) and me were debugging memory issue with one of the spiders some time ago, and it seems to be caused by ImagesPipeline + [S3FilesStore](https://github.com/scrapy/scrapy/blob/master/scrapy/contrib/pipeline/files.py#L66). I haven't confirmed that it was the cause of memory issue, this ticket is based solely on reading the source code.

FilesPipeline reads the whole file to memory and then defers the uploading to thread (via `S3FilesStore.persist_file`, passing file contents as bytes). So there could be many files loaded to memory at the same time, and as soon as files are downloaded faster than they are are uploaded to s3, memory usage will grow. This is not unlikely IMHO because s3 is not super-fast. For ImagesPipeline it is worse because it uploads not only the image itself, but also the generated thumbnails.

I think S3FilesStore should persist files to temporary location before uploading them to S3 (at least optionally). This would allow streaming files without storing them in memory.

## Metadata

### Assignees

No one assigned

### Labels

[S3](https://github.com/scrapy/scrapy/issues?q=state%3Aopen%20label%3A%22S3%22)[bug](https://github.com/scrapy/scrapy/issues?q=state%3Aopen%20label%3A%22bug%22)[media pipelines](https://github.com/scrapy/scrapy/issues?q=state%3Aopen%20label%3A%22media%20pipelines%22)[security](https://github.com/scrapy/scrapy/issues?q=state%3Aopen%20label%3A%22security%22)
### Type

No type
### Projects

No projects
### Milestone

* [Scrapy 2.13No due date](https://github.com/scrapy/scrapy/milestone/46)
### Relationships

None yet
### Development

No branches or pull requests
## Issue actions

## Footer

© 2025 GitHub, Inc.

### Footer navigation

* [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)
* [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)
* [Security](https://github.com/security)
* [Status](https://www.githubstatus.com/)
* [Docs](https://docs.github.com/)
* [Contact](https://support.github.com?tags=dotcom-footer)
* Manage cookies
* Do not share my personal information

You can’t perform that action at this time.



=== Content from blog.csdn.net_2877c52a_20250125_052125.html ===


# (原创)scrapy的MemoryError(后续)

最新推荐文章于 2024-08-15 08:00:00 发布
![](https://csdnimg.cn/release/blogv2/dist/pc/img/original.png)
[橘猫且engi](https://blog.csdn.net/wangtua "橘猫且engi")
![](https://csdnimg.cn/release/blogv2/dist/pc/img/newCurrentTime2.png)
最新推荐文章于 2024-08-15 08:00:00 发布
![](https://csdnimg.cn/release/blogv2/dist/pc/img/articleReadEyes2.png)
阅读量7.7k
![](https://csdnimg.cn/release/blogv2/dist/pc/img/tobarCollect2.png)
![](https://csdnimg.cn/release/blogv2/dist/pc/img/tobarCollectionActive2.png)
收藏
1
![](https://csdnimg.cn/release/blogv2/dist/pc/img/newHeart2023Active.png)
![](https://csdnimg.cn/release/blogv2/dist/pc/img/newHeart2023Black.png)
点赞数

分类专栏：
[python&amp;爬虫](https://blog.csdn.net/wangtua/category_6834237.html)
文章标签：
[python](https://so.csdn.net/so/search/s.do?q=python&t=all&o=vip&s=&l=&f=&viparticle=&from_tracking_code=tag_word&from_code=app_blog_art)
[scrapy-爬虫](https://so.csdn.net/so/search/s.do?q=scrapy-%E7%88%AC%E8%99%AB&t=all&o=vip&s=&l=&f=&viparticle=&from_tracking_code=tag_word&from_code=app_blog_art)

版权声明：本文为博主原创文章，遵循 [CC 4.0 BY-SA](http://creativecommons.org/licenses/by-sa/4.0/) 版权协议，转载请附上原文出处链接和本声明。

本文链接：<https://blog.csdn.net/wangtua/article/details/75228728>

版权

[![](https://i-blog.csdnimg.cn/columns/default/20201014180756754.png?x-oss-process=image/resize,m_fixed,h_224,w_224)
python&爬虫
专栏收录该内容](https://blog.csdn.net/wangtua/category_6834237.html "python&爬虫")

7 篇文章
0 订阅

订阅专栏

* [scrapy的MemoryError后续](#scrapy的memoryerror后续)
  + [问题](#问题)
  + [分析](#分析)

## scrapy的MemoryError（后续）

---

### 问题

之前写过一篇关于scrapy的MemoryError的博客，主要是介绍了MemoryError这个异常出现的原因和解决方案，但是对于其原因的探讨似乎还是不太明了，这次我们来深入探讨一下这个问题。
 “`

### 分析

这个问题真正的原因是因为scrapy在大文件下载时出现的一个bug所致，深入分析其源代码可以发现，其在进行大文件下载的时候，是把所有的数据全部保存在内存中，之后再一次性的写入文件。
 ![twisted的数据处理回调函数](https://img-blog.csdn.net/20170726111320878?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd2FuZ3R1YQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

![这里写图片描述](https://img-blog.csdn.net/20170726111729430?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd2FuZ3R1YQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

[这是一个会在2G内存，ubuntu12的机器上重现该问题的脚本](http://download.csdn.net/download/wangtua/9911046)
 有一个“治标不治本”的办法就是降低爬虫的并发度，让其每次只进行一个大文件的下载，但是这样的话如果大文件本身大于机器内存大小的话就会出现问题。
 更加根本的办法是修改这一机制，可以让大文件下载的过程中下载一部分就写入一部分。
 （关于这点，本人已经fork了scrapy的代码库，如果有开发进展会第一时间报告给大家）

![](https://csdnimg.cn/release/blogv2/dist/pc/img/vip-limited-close-newWhite.png)
确定要放弃本次机会？

福利倒计时

*:*

*:*

![](https://csdnimg.cn/release/blogv2/dist/pc/img/vip-limited-close-roup.png)
立减 ¥

普通VIP年卡可用

[立即使用](https://mall.csdn.net/vip)

[![](https://profile-avatar.csdnimg.cn/768ec68ebf4642b09b46ad7c285b6a45_wangtua.jpg!1)
橘猫且engi](https://blog.csdn.net/wangtua)

关注
关注

* ![](https://csdnimg.cn/release/blogv2/dist/pc/img/tobarThumbUpactive.png)
  ![](https://csdnimg.cn/release/blogv2/dist/pc/img/toolbar/like-active.png)
  ![](https://csdnimg.cn/release/blogv2/dist/pc/img/toolbar/like.png)
  0
  点赞
* ![](https://csdnimg.cn/release/blogv2/dist/pc/img/toolbar/unlike-active.png)
  ![](https://csdnimg.cn/release/blogv2/dist/pc/img/toolbar/unlike.png)
  踩
* ![](https://csdnimg.cn/release/blogv2/dist/pc/img/toolbar/collect-active.png)
  ![](https://csdnimg.cn/release/blogv2/dist/pc/img/toolbar/collect.png)
  ![](https://csdnimg.cn/release/blogv2/dist/pc/img/newCollectActive.png)
  1
  收藏

  觉得还不错?
  一键收藏

  ![](https://csdnimg.cn/release/blogv2/dist/pc/img/collectionCloseWhite.png)
* ![](https://csdnimg.cn/release/blogv2/dist/pc/img/guideRedReward01.png)
  知道了

  [![](https://csdnimg.cn/release/blogv2/dist/pc/img/toolbar/comment.png)
  0](#commentBox)
  评论
* ![](https://csdnimg.cn/release/blogv2/dist/pc/img/toolbar/share.png)
  分享
  复制链接

  分享到 QQ

  分享到新浪微博

  ![](https://csdnimg.cn/release/blogv2/dist/pc/img/share/icon-wechat.png)扫一扫
* ![](https://csdnimg.cn/release/blogv2/dist/pc/img/toolbar/more.png)
  ![](https://csdnimg.cn/release/blogv2/dist/pc/img/toolbar/report.png)
  举报

  ![](https://csdnimg.cn/release/blogv2/dist/pc/img/toolbar/report.png)
  举报

专栏目录

[AttributeError: module ‘*scrapy*‘ has no attribute ‘Filed‘](https://catcoder.blog.csdn.net/article/details/134936648)

[weixin\_43178406的博客](https://blog.csdn.net/weixin_43178406)

12-12
![](https://csdnimg.cn/release/blogv2/dist/pc/img/readCountWhite.png)
5万+

[本文主要介绍了AttributeError: module ‘*scrapy*’ has no attribute 'Filed’解决方案，希望能对使用*scrapy*同学们有所帮助。
文章目录
1. 问题描述
2. 解决方案](https://catcoder.blog.csdn.net/article/details/134936648)

[*python* 问题自动匹配解决方案\_详解解决*Python* memory error的问题（四种解决方案）...](https://blog.csdn.net/weixin_39728221/article/details/110324651)

[weixin\_39728221的博客](https://blog.csdn.net/weixin_39728221)

11-28
![](https://csdnimg.cn/release/blogv2/dist/pc/img/readCountWhite.png)
3516

[昨天在用用Pycharm读取一个200+M的CSV的过程中，竟然出现了Memory Error！简直让我怀疑自己买了个假电脑，毕竟是8G内存i7处理器，一度怀疑自己装了假的内存条。。。。下面说一下几个解题步骤。。。。一般就是用下面这些方法了，按顺序试试。一、逐行读取如果你用pd.read\_csv来读文件，会一次性把数据都读到内存里来，导致内存爆掉，那么一个想法就是一行一行地读它，代码如下：data...](https://blog.csdn.net/weixin_39728221/article/details/110324651)

参与评论
您还未登录，请先
登录
后发表或查看评论

[*memoryerror*!!!!](https://blog.csdn.net/qq_39682365/article/details/102544617)

[qq\_39682365的博客](https://blog.csdn.net/qq_39682365)

10-14
![](https://csdnimg.cn/release/blogv2/dist/pc/img/readCountWhite.png)
826

[最近加载图像训练，笔记本控制台输出*memoryerror*!
我在想导致这个的原因真的是数据太大，GPU太小溢出吗？？
1、数据过大，数据重复加载
2、电脑打开的程序串口多，关机重启
3、换台电脑再试一下程序
方法总比困难多。。
...](https://blog.csdn.net/qq_39682365/article/details/102544617)

[【*Python*报错】*MemoryError*
热门推荐](https://devpress.csdn.net/v1/article/detail/103086411)

[mie haha](https://blog.csdn.net/qq_40315080)

11-15
![](https://csdnimg.cn/release/blogv2/dist/pc/img/readCountWhite.png)
2万+

[报错：
*(*上图是在虚拟机中运行服务器代码的时候出现的报错*)*
*MemoryError*：涉及到了读取、保存、写入，内存不够用了
解决方法：
检查安装64位*python*，64位*python*内的pandas和Numpy都是64位，内存更大。
如果在虚拟机中运行：设置虚拟机内存大小即可（我使用的这种方法）
再次运行，不再报错：
...](https://devpress.csdn.net/v1/article/detail/103086411)

[pickle错误“*MemoryError*”](https://blog.csdn.net/xulingqiang/article/details/52440689)

[xulingqiang的专栏](https://blog.csdn.net/xulingqiang)

09-05
![](https://csdnimg.cn/release/blogv2/dist/pc/img/readCountWhite.png)
2314

[问题描述：32位 *Python* 使用pickle 加载较大的数据时，发生了“*MemoryError*” 错误。
解决办法：换成64位的就可以了](https://blog.csdn.net/xulingqiang/article/details/52440689)

[*(**原创**)**scrapy*的*MemoryError*](https://blog.csdn.net/wangtua/article/details/75235220)

[wangtua的博客](https://blog.csdn.net/wangtua)

07-17
![](https://csdnimg.cn/release/blogv2/dist/pc/img/readCountWhite.png)
1640

[*(**原创**)**scrapy*的*MemoryError**原创**scrapy*的*MemoryError*
背景知识
*MemoryError*
*scrapy**爬虫*的内存检查
问题代码
使用telnet进行检查
代码修改背景知识*MemoryError**MemoryError*是*python*的常见异常之一，可以通过查看*python*的在线文档来进行了解：exception *MemoryError*
Raised when an oper](https://blog.csdn.net/wangtua/article/details/75235220)

[wangtua的博客附件（*scrapy*的*memoryerror**后续*）](https://download.csdn.net/download/wangtua/9911046)

07-26

[【标题】"wangtua的博客附件（*scrapy*的*memoryerror**后续*）"涉及的主要知识点是*Scrapy*框架在处理大数据或复杂*爬虫*项目时遇到的内存错误*(**MemoryError**)*及其解决方案。*Scrapy*是一个强大的*Python**爬虫*框架，用于高效地抓取...](https://download.csdn.net/download/wangtua/9911046)

[重装系统后*python*环境问题*后续*——*scrapy*生成spider报错：Fatal error in launcher: Unable to create process using](https://download.csdn.net/download/weixin_38599412/13739804)

12-20

[重装系统后*python*环境问题*后续*：*scrapy*报错：Fatal error in launcher: Unable to create process using'”c:\users\administrator\appdata\local\programs\*python*\*python*37\*python*.exe” “D:\Users\Administrator\...](https://download.csdn.net/download/weixin_38599412/13739804)

[初学者*scrapy*框架爬取数据不成功，出现ERROR: Spider error processing错误问题的解决方法](https://download.csdn.net/download/weixin_54586360/89296777)

05-11

[当运行*Scrapy**爬虫*项目时，如果出现“ERROR: Spider error processing”错误提示，则意味着在处理某个页面的过程中发生了异常，导致*爬虫*无法正常提取所需数据。 \*\*原因分析：\*\* 1. \*\*请求头设置不当\*\*：某些网站为了...](https://download.csdn.net/download/weixin_54586360/89296777)

[*scrapy-**scrapy*
最新发布](https://download.csdn.net/download/lly202406/90157149)

12-20

[*scrapy* *scrapy* *scrapy* *scrapy* *scrapy*](https://download.csdn.net/download/lly202406/90157149)

[read file *memoryerror*](https://blog.csdn.net/dlhlSC/article/details/83743140)

[dlhlSC的博客](https://blog.csdn.net/dlhlSC)

11-05
![](https://csdnimg.cn/release/blogv2/dist/pc/img/readCountWhite.png)
478

[读取文件代码：
lines=file*(*"myfile.txt"*)*.read*(**)*.split*(*"\n"*)*
分析：文件过大，不能一次性读入内存
解决方法：
with open*(*"myfile.txt"*)* as myfile:
for line in myfile:
do\_something*(*line.rstrip*(*"\n"*)**)*
...](https://blog.csdn.net/dlhlSC/article/details/83743140)

[Memory Error问题的解决方法](https://blog.csdn.net/weixin_42547085/article/details/108695553)

[weixin\_42547085的博客](https://blog.csdn.net/weixin_42547085)

09-20
![](https://csdnimg.cn/release/blogv2/dist/pc/img/readCountWhite.png)
1万+

[Memory Error
问题
在用*Python*读取数据的时候，遇到Memory Error 问题，数据大小约为4G
解决方法
扩大虚拟内存，操作方法为
1、打开 控制面板；
2、打开 系统；
3、打开 高级系统设置；
4、点击 性能----设置；
5、点击 高级----虚拟内存----更改；
6、取消选中“自动管理所有驱动器的分页文件大小”，选择D盘（其他盘应该也可以）；
7、选择 系统管理的大小；
8、点击 设置----确定----重启电脑。
...](https://blog.csdn.net/weixin_42547085/article/details/108695553)

[*python*程序memory error\_如何处理“ *MemoryError*”？用*Python*代码](https://blog.csdn.net/weixin_39608300/article/details/110751456)

[weixin\_39608300的博客](https://blog.csdn.net/weixin_39608300)

12-05
![](https://csdnimg.cn/release/blogv2/dist/pc/img/readCountWhite.png)
288

[I have some piece of *python* code which generates a *MemoryError* after a while. I know that it consumes a lot of memory.So, I decided to put the code within a try/except block so that the skeleton looks...](https://blog.csdn.net/weixin_39608300/article/details/110751456)

[*python*报错*MemoryError*](https://blog.csdn.net/BinChasing/article/details/53158889)

[BinChasing的博客](https://blog.csdn.net/BinChasing)

11-14
![](https://csdnimg.cn/release/blogv2/dist/pc/img/readCountWhite.png)
1万+

[*python*报错*MemoryError**python* 32bit 最大只能使用 2G 内存，坑爹之处，超过 2G 报错*MemoryError*。而 64bit *python*则无此限制，所以建议使用 64bit *python*。
可能存在的问题：以前 numpy、scipy 官方的库只支持 32bit *python*，现在应该发布了 64bit 对应版本。](https://blog.csdn.net/BinChasing/article/details/53158889)

[*MemoryError*\*\*：内存不足的完美解决方法](https://yanwc.blog.csdn.net/article/details/141091515)

[沉淀、分享、成长，让自己和他人都能有所收获！](https://blog.csdn.net/qq_42055933)

08-15
![](https://csdnimg.cn/release/blogv2/dist/pc/img/readCountWhite.png)
1284

[大家好，我是默语！在*Python*开发中，*MemoryError*是一种常见的错误，通常发生在程序试图分配超过可用内存的资源时。这种错误在处理大数据集、进行复杂计算或操作大型文件时尤其容易出现。今天，我将详细讲解如何有效地解决和预防内存不足的问题，并分享一些最佳实践，以确保你的*Python*程序能够高效稳定地运行。是在*Python*程序尝试分配的内存量超过了系统可用的内存时引发的异常。以下是一个典型的# 尝试创建一个超大的列表，可能会导致*MemoryError*。](https://yanwc.blog.csdn.net/article/details/141091515)

[解决*Python*中出现的Memory Error的问题](https://blog.csdn.net/weixin_36976645/article/details/102564976)

[weixin\_36976645的博客](https://blog.csdn.net/weixin_36976645)

10-15
![](https://csdnimg.cn/release/blogv2/dist/pc/img/readCountWhite.png)
1万+

[最近刚刚学习*Python*，用的软件是PyCharm。在手打了一个基于物品的推荐算法，并且跑一个20000+的数据集时，出现了Memory Error错误。一开始以为是代码问题，后面才发现自己的内存已经到达了80%+了。查阅相关资料，这个大哥写的很不错，大家可以去看看。
l解决*Python* memory error的问题（四种解决方案）.
这是我自己出现的问题
我分析了一下出现这个错误的原因：
1....](https://blog.csdn.net/weixin_36976645/article/details/102564976)

[*MemoryError*异常的正确解决方法，亲测有效，嘿嘿嘿，已解决](https://blog.csdn.net/PythonAigc/article/details/138659323)

[PythonAigc的博客](https://blog.csdn.net/PythonAigc)

05-10
![](https://csdnimg.cn/release/blogv2/dist/pc/img/readCountWhite.png)
3635

[`*MemoryError*` 异常通常发生在 *Python* 程序试图分配的内存超过可用内存时。这可能是由于处理大型数据集、创建大型数据结构、递归调用过深导致栈溢出，或者是因为内存泄漏（即程序未能释放不再需要的内存）。](https://blog.csdn.net/PythonAigc/article/details/138659323)

[*MemoryError*的处理方式](https://blog.csdn.net/Hanx09/article/details/107456037)

[Explore](https://blog.csdn.net/Hanx09)

07-21
![](https://csdnimg.cn/release/blogv2/dist/pc/img/readCountWhite.png)
1万+

[*MemoryError*几种处理方式：
1. 低精度保存数据；
2. 更新*Python*为64位；
3. 修改PyCharm运行内存；
4. 扩充虚拟内存；
5. 优化数据读取方式；
6. 手动回收变量。](https://blog.csdn.net/Hanx09/article/details/107456037)

[*Python* *MemoryError* Problem](https://blog.csdn.net/Lyrassongs/article/details/83053505)

[Eureka](https://blog.csdn.net/Lyrassongs)

10-15
![](https://csdnimg.cn/release/blogv2/dist/pc/img/readCountWhite.png)
2125

[*Python*的内存管理相对于C++来说相当低效已经是老生之谈了。今天遇到了一个内存问题，大概是我想拿30000×51230000\times{512}30000×512的一个矩阵来和它自己的转秩做类似内积的运算。在*Python*中查看一个变量占多少内存的指令如下：
import numpy as np
from sys import getsizeof
a = [0] \* 1024
b = np.a...](https://blog.csdn.net/Lyrassongs/article/details/83053505)

[使用*scrapy*框架时keyerror](https://wenku.csdn.net/answer/2ee8id1wda)

09-29

[在*Scrapy*框架中，KeyError通常发生在试图访问字典中不存在的键时。*Scrapy*是一个用于网络*爬虫*的*Python*库，当你从某个数据源（比如JSON、HTML等）解析数据，并尝试通过特定的键名获取值时，如果这个键不存在，就会抛出KeyError。
例如，假设你在`response.json*(**)*`结果中查找一个键值对，如果该JSON对象没有这个键，如：
```*python*
data = response.json*(**)*
value = data['non\_existent\_key']
```
在这个情况下，如果`non\_existent\_key`不存在于JSON中，*Scrapy*会抛出`KeyError: 'non\_existent\_key'`。
处理KeyError的一般做法包括：
1. \*\*检查键是否存在\*\*：使用`.get*(*key, default\_value*)*`方法可以避免直接访问，如果键不存在，会返回默认值，而不是引发异常。
```*python*
value = data.get*(*'non\_existent\_key', ''*)*
```
2. \*\*异常处理\*\*：你可以使用try-except块捕获并处理KeyError。
```*python*
try:
value = data['non\_existent\_key']
except KeyError:
value = None
```
3. \*\*使用列表推导式\*\* 或 `dict.get*(**)*` 结合 `list*(**)*`：当不确定键是否存在时，可以用这种方式先将可能存在也可能不存在的键值对转化为列表。
```*python*
values = [data.get*(*k, ''*)* for k in ['key1', 'key2', 'non\_existent\_key']]
```](https://wenku.csdn.net/answer/2ee8id1wda)

[![](https://profile-avatar.csdnimg.cn/768ec68ebf4642b09b46ad7c285b6a45_wangtua.jpg!1)](https://blog.csdn.net/wangtua)

[橘猫且engi](https://blog.csdn.net/wangtua "橘猫且engi")

CSDN认证博客专家

CSDN认证企业博客

码龄10年
[![](https://csdnimg.cn/identity/nocErtification.png)
暂无认证](https://i.csdn.net/#/uc/profile?utm_source=14998968 "暂无认证")

[13
原创](https://blog.csdn.net/wangtua)

[32万+
周排名](https://blog.csdn.net/rank/list/weekly)

[142万+
总排名](https://blog.csdn.net/rank/list/total)

7万+
访问

[![](https://csdnimg.cn/identity/blog3.png)](https://blog.csdn.net/blogdevteam/article/details/103478461)

等级

797
积分

19
粉丝

16
获赞

13
评论

65
收藏

![创作能手](https://csdnimg.cn/medal/qixiebiaobing4@240.png)

[私信](https://im.csdn.net/chat/wangtua)

关注

![]()
[![写文章](https://img-home.csdnimg.cn/images/20250107060517.png)](https://www.csdn.net/blogstar2024?utm_source=1333680121)

![]()

### 热门文章

* [(原创)python zipfile实现压缩整个目录和子目录
  ![](https://csdnimg.cn/release/blogv2/dist/pc/img/readCountWhite.png)
  20816](https://blog.csdn.net/wangtua/article/details/68943231)
* [（原创）clang的python接口（一）
  ![](https://csdnimg.cn/release/blogv2/dist/pc/img/readCountWhite.png)
  10216](https://blog.csdn.net/wangtua/article/details/77426883)
* [（原创）clang的python接口教程（二）
  ![](https://csdnimg.cn/release/blogv2/dist/pc/img/readCountWhite.png)
  8173](https://blog.csdn.net/wangtua/article/details/78612331)
* [(原创)scrapy的MemoryError(后续)
  ![](https://csdnimg.cn/release/blogv2/dist/pc/img/readCountWhite.png)
  7730](https://blog.csdn.net/wangtua/article/details/75228728)
* [（原创）cmake的编译命令提取过程
  ![](https://csdnimg.cn/release/blogv2/dist/pc/img/readCountWhite.png)
  5626](https://blog.csdn.net/wangtua/article/details/77712236)

### 分类专栏

* [![](https://i-blog.csdnimg.cn/columns/default/20201014180756754.png?x-oss-process=image/resize,m_fixed,h_64,w_64)
  python&爬虫](https://blog.csdn.net/wangtua/category_6834237.html)
  7篇
* [![](https://i-blog.csdnimg.cn/columns/default/20201014180756927.png?x-oss-process=image/resize,m_fixed,h_64,w_64)
  面试题](https://blog.csdn.net/wangtua/category_6834247.html)
  1篇
* [![](https://i-blog.csdnimg.cn/columns/default/20201014180756913.png?x-oss-process=image/resize,m_fixed,h_64,w_64)
  CTF题目](https://blog.csdn.net/wangtua/category_6840954.html)
* [![](https://i-blog.csdnimg.cn/columns/default/20201014180756754.png?x-oss-process=image/resize,m_fixed,h_64,w_64)
  web杂谈（开发，安全，chrome扩展等）](https://blog.csdn.net/wangtua/category_6976093.html)
  2篇
* [![](https://i-blog.csdnimg.cn/columns/default/20201014180756922.png?x-oss-process=image/resize,m_fixed,h_64,w_64)
  程序分析与软件安全](https://blog.csdn.net/wangtua/category_7114112.html)
  3篇
* [![](https://i-blog.csdnimg.cn/columns/default/20201014180756724.png?x-oss-process=image/resize,m_fixed,h_64,w_64)
  数据库技术](https://blog.csdn.net/wangtua/category_7252095.html)
  1篇
* [![](https://i-blog.csdnimg.cn/columns/default/20201014180756922.png?x-oss-process=image/resize,m_fixed,h_64,w_64)
  python&amp;爬虫](https://blog.csdn.net/wangtua/category_8865088.html)
* [![](https://i-blog.csdnimg.cn/columns/default/20201014180756918.png?x-oss-process=image/resize,m_fixed,h_64,w_64)
  python&amp;amp;爬虫](https://blog.csdn.net/wangtua/category_8865090.html)
* [![](https://i-blog.csdnimg.cn/columns/default/20201014180756923.png?x-oss-process=image/resize,m_fixed,h_64,w_64)
  python&amp;amp;amp;爬虫](https://blog.csdn.net/wangtua/category_8865092.html)
* [![](https://i-blog.csdnimg.cn/columns/default/20201014180756724.png?x-oss-process=image/resize,m_fixed,h_64,w_64)
  python&amp;amp;amp;amp;爬虫](https://blog.csdn.net/wangtua/category_8871983.html)
  1篇

![](https://csdnimg.cn/release/blogv2/dist/pc/img/arrowDownWhite.png)

### 最新评论

* [(原创)python zipfile实现压缩整个目录和子目录](https://blog.csdn.net/wangtua/article/details/68943231#comments_16148012)

  [幸运 lucky:](https://yetingyun.blog.csdn.net)
  给你点赞
* [（原创）反爬虫策略对抗实战（一）——绕过网页请求认证](https://blog.csdn.net/wangtua/article/details/89187793#comments_12701818)

  [ruc\_liangyiyao:](https://blog.csdn.net/ruc_liangyiyao)
  请问怎么确定var tkn = "LayATQ0pWwvmRv2OY84jDRjPB3a9"字段对应Authorization: Bearer 9FVz6wKBXM0lr1CHYCvyUm2NjW98字段的。明显两个字段不同啊
* [（原创）反爬虫策略对抗实战（一）——绕过网页请求认证](https://blog.csdn.net/wangtua/article/details/89187793#comments_11844747)

  [最差的一届:](https://blog.csdn.net/weixin_43159092)
  哦，有点懂了，第一次请求的html页面，获取字段再请求的js生成的页面
* [（原创）反爬虫策略对抗实战（一）——绕过网页请求认证](https://blog.csdn.net/wangtua/article/details/89187793#comments_11844693)

  [最差的一届:](https://blog.csdn.net/weixin_43159092)
  请问，第一次请求获取两个字段提供给第二次爬取，那么第一次请求为什么没有被阻止？
* [（原创）clang的python接口教程（二）](https://blog.csdn.net/wangtua/article/details/78612331#comments_10861140)

  [橘猫且engi:](https://blog.csdn.net/wangtua)
  说明一下，本人已经从原公司离职，也不再做这个方向。希望大家可以理解，不要再私信我问相关问题了。这个账号还会更新，不过会以其他技术为主，请大家不要取关。再次感谢大家的理解！

### 最新文章

* [（原创）反爬虫策略对抗实战（一）——绕过网页请求认证](https://blog.csdn.net/wangtua/article/details/89187793)
* [（原创）clang的python接口教程（二）](https://blog.csdn.net/wangtua/article/details/78612331)
* [欢迎使用CSDN-markdown编辑器（留档）](https://blog.csdn.net/wangtua/article/details/78612117)

[2019年1篇](https://blog.csdn.net/wangtua?type=blog&year=2019&month=04)
[2017年14篇](https://blog.csdn.net/wangtua?type=blog&year=2017&month=11)

![]()

### 目录

![]()

### 目录

![]()

### 分类专栏

* [![](https://i-blog.csdnimg.cn/columns/default/20201014180756754.png?x-oss-process=image/resize,m_fixed,h_64,w_64)
  python&爬虫](https://blog.csdn.net/wangtua/category_6834237.html)
  7篇
* [![](https://i-blog.csdnimg.cn/columns/default/20201014180756927.png?x-oss-process=image/resize,m_fixed,h_64,w_64)
  面试题](https://blog.csdn.net/wangtua/category_6834247.html)
  1篇
* [![](https://i-blog.csdnimg.cn/columns/default/20201014180756913.png?x-oss-process=image/resize,m_fixed,h_64,w_64)
  CTF题目](https://blog.csdn.net/wangtua/category_6840954.html)
* [![](https://i-blog.csdnimg.cn/columns/default/20201014180756754.png?x-oss-process=image/resize,m_fixed,h_64,w_64)
  web杂谈（开发，安全，chrome扩展等）](https://blog.csdn.net/wangtua/category_6976093.html)
  2篇
* [![](https://i-blog.csdnimg.cn/columns/default/20201014180756922.png?x-oss-process=image/resize,m_fixed,h_64,w_64)
  程序分析与软件安全](https://blog.csdn.net/wangtua/category_7114112.html)
  3篇
* [![](https://i-blog.csdnimg.cn/columns/default/20201014180756724.png?x-oss-process=image/resize,m_fixed,h_64,w_64)
  数据库技术](https://blog.csdn.net/wangtua/category_7252095.html)
  1篇
* [![](https://i-blog.csdnimg.cn/columns/default/20201014180756922.png?x-oss-process=image/resize,m_fixed,h_64,w_64)
  python&amp;爬虫](https://blog.csdn.net/wangtua/category_8865088.html)
* [![](https://i-blog.csdnimg.cn/columns/default/20201014180756918.png?x-oss-process=image/resize,m_fixed,h_64,w_64)
  python&amp;amp;爬虫](https://blog.csdn.net/wangtua/category_8865090.html)
* [![](https://i-blog.csdnimg.cn/columns/default/20201014180756923.png?x-oss-process=image/resize,m_fixed,h_64,w_64)
  python&amp;amp;amp;爬虫](https://blog.csdn.net/wangtua/category_8865092.html)
* [![](https://i-blog.csdnimg.cn/columns/default/20201014180756724.png?x-oss-process=image/resize,m_fixed,h_64,w_64)
  python&amp;amp;amp;amp;爬虫](https://blog.csdn.net/wangtua/category_8871983.html)
  1篇

### 目录

评论
![](https://csdnimg.cn/release/blogv2/dist/pc/img/closeBt.png)

![](https://csdnimg.cn/release/blogv2/dist/pc/img/commentArrowLeftWhite.png)被折叠的  条评论
[为什么被折叠?](https://blogdev.blog.csdn.net/article/details/122245662)
[![](https://csdnimg.cn/release/blogv2/dist/pc/img/iconPark.png)到【灌水乐园】发言](https://bbs.csdn.net/forums/FreeZone)

查看更多评论![](https://csdnimg.cn/release/blogv2/dist/pc/img/commentArrowDownWhite.png)

添加红包

祝福语

请填写红包祝福语或标题

红包数量

个

红包个数最小为10个

红包总金额

元

红包金额最低5元

余额支付
当前余额3.43元
[前往充值 >](https://i.csdn.net/#/wallet/balance/recharge)

需支付：10.00元

取消
确定

![](https://csdnimg.cn/release/blogv2/dist/pc/img/guideRedReward02.png)
下一步

![](https://csdnimg.cn/release/blogv2/dist/pc/img/guideRedReward03.png)
知道了

![]()

领取后你会自动成为博主和红包主的粉丝
规则

![](https://profile-avatar.csdnimg.cn/default.jpg!2)
hope\_wisdom 发出的红包

实付元
使用余额支付
![](https://csdnimg.cn/release/blogv2/dist/pc/img/pay-time-out.png)
点击重新获取

![](https://csdnimg.cn/release/blogv2/dist/pc/img/weixin.png)![](https://csdnimg.cn/release/blogv2/dist/pc/img/zhifubao.png)![](https://csdnimg.cn/release/blogv2/dist/pc/img/jingdong.png)扫码支付

钱包余额
0
![](https://csdnimg.cn/release/blogv2/dist/pc/img/pay-help.png)

抵扣说明：

1.余额是钱包充值的虚拟货币，按照1:1的比例进行支付金额的抵扣。
 2.余额无法直接购买下载，可以购买VIP、付费专栏及课程。

[![](https://csdnimg.cn/release/blogv2/dist/pc/img/recharge.png)余额充值](https://i.csdn.net/#/wallet/balance/recharge)

![]()


