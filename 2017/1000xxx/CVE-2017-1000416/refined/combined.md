=== Content from www.youtube.com_24c95393_20250125_092513.html ===
[00:10] so hello I'm Cu from Purdue University
[00:13] it is my honor to be given the chance to
[00:16] come here to talk about latest work
[00:18] called sim set on finding semantic
[00:21] problems in software code that does
[00:23] x.509 certificate validation so this is
[00:26] a joint work with Omar from the U of
[00:28] Iowa Christina who also happened to be
[00:31] the section chief from not Easton and
[00:33] then do Hongki as well as my two
[00:35] advisers Anika turning away from Purdue
[00:38] to begin with Xbox Online has long been
[00:42] used in TRS as a way of doing
[00:45] authentication and key diversion it is
[00:47] worth noting that many times the
[00:50] confidentiality and integrity guarantee
[00:52] that years provides are kind of build on
[00:54] top of the ability of doing those to the
[00:58] x.509 PGI realizes the concept of
[01:01] so-called chain of trust what it means
[01:03] is that given a chain of certificates
[01:05] from a server a client is expected to
[01:08] verify the chain of search and see
[01:11] whether he can extend the trust from a
[01:13] trusted root CA down to a intermediary
[01:17] cert and then all the way down to the
[01:19] one of the server sounds easy enough I
[01:22] guess but from yesterday's talk
[01:25] yesterday yes min told us in her talk on
[01:27] cryptographic API I think none of the
[01:30] participant in her user study managed to
[01:33] implement certificate validation
[01:35] correctly and also we talked to some
[01:38] developers of TRS libraries and they
[01:40] some of them seem to have somewhat
[01:42] negative feelings towards the task of
[01:44] implementing support for x.509 so it
[01:48] turns out that what makes it difficult
[01:49] is that apart from the cryptographic
[01:51] signatures on certificates you have just
[01:54] so many other different things that you
[01:55] have to check so for example you'll be
[01:57] given different names that are made of
[02:00] multiple attributes and you are expected
[02:02] to match them up and then you also have
[02:04] to do some daytime checking which turns
[02:06] out to be also surprisingly difficult
[02:08] and perhaps more importantly you are
[02:10] given some optional but possibly
[02:12] critical extensions that you have to get
[02:14] processed correctly now if an
[02:18] implementation is deviating from the
[02:21] specification it can
[02:23] come in two different flavors so for
[02:26] example with an implementation turns out
[02:28] to be overly permissive then you might
[02:30] be susceptible to different kinds of
[02:32] attacks and if the implementation is
[02:35] overly restrictive then you might lose
[02:36] surface we noticed that in recent years
[02:40] there's a huge demand for internet
[02:43] connected and better systems and because
[02:46] they have generally very limited amount
[02:48] of resources as far as processing power
[02:51] developers have been making new TRS
[02:53] libraries for these platforms generally
[02:55] these libraries would be written in C
[02:57] and they are heavily optimized to kind
[02:59] of make them run better on resource
[03:02] restricted platforms and we notice that
[03:04] there are multiple such libraries being
[03:06] released under the open source license
[03:08] and perhaps because of their impeccable
[03:11] performance advantages over the
[03:13] traditional and perhaps heavier TLS
[03:16] libraries we are seeing a new trend of
[03:18] app developers trying to take advantage
[03:20] of these smaller green libraries in
[03:22] their day to day app development even on
[03:24] desktop platforms hence the focus of our
[03:28] research is to expose non-compliance in
[03:31] xo9 certificate verification in small
[03:34] footprint iOS libraries and for that
[03:37] we'll take our C 5280 as our main
[03:39] reference the motivation for this paper
[03:42] is simple given small footprint small
[03:45] codebase it should not be the case that
[03:47] the library is kind of compromising and
[03:49] sacrificing too much of security and
[03:53] because GRS is such a critical part of
[03:55] our modern day security it has been
[03:58] studied a lot and when it comes to
[04:02] testing software implementation I think
[04:03] black box fighting has been kind of the
[04:05] prominent the most prominent approach
[04:07] where researchers try to look into the
[04:10] problem of you know protocol say machine
[04:12] translation kind of problems as well as
[04:14] a highly relevant paper that also got
[04:16] published at SNP three years ago which
[04:18] also looked into the problem of xf9
[04:20] certificate validation so the
[04:24] observation that we use into developing
[04:26] our approach is that given a universe of
[04:29] all the possible input certifications
[04:31] a library essentially does wanted it
[04:34] splits the universe into two possible
[04:36] smaller universes one that it accepts
[04:39] and one that it rejects with that in
[04:42] mind our white box approach goes like
[04:44] this so for each of the library we fit
[04:47] them input that we refer to as same set
[04:49] and symbolically execute the portion of
[04:52] the code that is in charge of validating
[04:55] certificates the purpose of that is to
[04:58] approximate the two universes and have
[05:01] them represented in logical formulas
[05:05] given the availability of multiple
[05:08] different open source implementations we
[05:10] can do a differential testing by doing a
[05:13] pairwise cross-validation so different
[05:16] given two different implementations if
[05:18] you can find such a certificate that one
[05:21] library would accept and one would
[05:23] reject then you have a discrepancy
[05:25] design exercise problem and because
[05:28] these approximations are in logic
[05:30] formers with the help of SMT software
[05:33] you can do all these kind of
[05:34] automatically now you might be thinking
[05:37] that this sounds too good to be true
[05:38] because civil execution is simply
[05:41] unscalable now indeed symbolic location
[05:44] is cursed by the problem of path
[05:46] explosion where first of all you
[05:49] essentially each conditional statement
[05:51] contributes to a multiplicative factor
[05:52] and things get particularly bad when you
[05:56] have loops that are depending on
[05:58] symbolic values so to make symbolic
[06:01] execution practical for our particular
[06:03] research problem we make instead of
[06:06] making a whole certificate chain
[06:08] symbolic we make concrete values with
[06:11] symbolic values think of the thing about
[06:14] x.509 certificates in BR encoding
[06:17] essentially a encoded cert would be a
[06:20] tree of tag length and value triplets
[06:23] where value can be further recursively
[06:26] defined by more TFE triplets so the key
[06:29] idea here is to keep T and L to be
[06:32] concrete and then for the leaf node the
[06:37] leaf V notes that correspond to a
[06:39] certificate fields and extensions you
[06:41] can make them symbolic we also try to
[06:45] limit our scope for our analysis and we
[06:48] also draw some more can
[06:50] domain-specific observations so first of
[06:52] all we don't try to comment on whether
[06:55] the amendment Asians are doing correctly
[06:57] in terms of cryptography and we also try
[07:00] to simplify the string matching which I
[07:03] think the next talk in the session would
[07:04] give us a bit more insight on that
[07:07] and finally we break the original
[07:08] problem into smaller independence a
[07:10] problem where we essentially make
[07:12] different things
[07:13] symbolic at different times with all
[07:16] that said I think now would be a good
[07:19] time to show you some of our findings we
[07:21] have a lot of findings so I cannot go
[07:23] two of them I will encourage you to read
[07:26] our paper if you have the time so we
[07:29] applied our approach into testing nine
[07:31] implementations from four different
[07:33] families of the earth libraries that are
[07:36] actively being used and deployed on real
[07:39] IOT products as well as on desktop app
[07:42] development and we found the total
[07:44] number of 48 instances of non-compliance
[07:47] and you might have noticed that I
[07:50] switched the template midway through I
[07:53] am hoping that the red color would wake
[07:55] you up because have something
[07:56] interesting to show you to summarize our
[07:59] findings we found numerous certificate
[08:02] fields and extensions not being checked
[08:04] and some libraries with a mishandle and
[08:07] recognized critical extensions and we
[08:11] also found in completing implementation
[08:13] in some of the libraries it looks like
[08:15] the developer wanted to add certain
[08:17] features but perhaps due to release that
[08:20] line they didn't get to finish it
[08:21] completely and then we also found
[08:24] incorrect interpretations of some
[08:27] semantic meanings of certain certificate
[08:30] fields and expansions as well as the
[08:32] outsi itself so the first piece of
[08:36] interesting new finding is that we found
[08:38] in multiple libraries they are
[08:41] misinterpreting the 2-byte year of UTC
[08:44] time so the r3 says for UTC time the
[08:47] expected range of the year should be
[08:49] between 1950 to 2049 unfortunately for
[08:54] matrix s facility put the boundary at 95
[08:56] effectively implementing a range between
[08:59] 1996 to 2095 this has a
[09:03] severe implication because certificates
[09:06] that got expired in back in 1995 would
[09:09] be considered to expire in 2095 which
[09:11] reminds me of a certain famous sci-fi
[09:14] movie a similar issue exists in tropical
[09:19] system but the mistake is actually
[09:21] different because they put a boundary at
[09:23] 89 and we also found that ax ers has a
[09:28] similar problem but it looks like the
[09:30] developer perhaps which read the RFC
[09:33] because it's only off by one however the
[09:36] the problem of having some boundary
[09:39] cases got shifted by 100 years would
[09:42] still be the same a second highlight of
[09:49] our findings would be how sometimes
[09:50] libraries would opt for a lags
[09:53] comparison logic take this as an example
[09:56] so on certificates you can have this
[09:58] extended key usage extension which
[10:01] mandates the purposes of a key that can
[10:03] be useful and purposes are uniquely
[10:06] identified by object identifier for
[10:10] example server authentication would have
[10:11] this particular ID
[10:15] unfortunately in both Matrix SSL and
[10:18] move SSL instead of trying to match the
[10:21] bite exactly they just take a summation
[10:24] of the encoded bytes and then try to
[10:27] imagine leda something so in this scheme
[10:28] they take 71 to represent server
[10:33] authentication now this is problematic
[10:35] because ou IDs are meant to be unique
[10:39] hierarchically where whereas the
[10:42] summation of bytes is not and this is an
[10:45] example of implementations being overly
[10:48] permissive because some garbage IDs
[10:51] would be considered to be meaningful and
[10:53] it also presents a compatibility issue
[10:55] because some legitimate custom or IDs
[10:59] would be miss recognized if their sums
[11:01] happen to collide and then we also found
[11:06] that in a x-ers given subject names and
[11:09] issuer names with multiple attributes
[11:10] they do not check country state and
[11:13] locality meaning that if you have a see
[11:15] in the
[11:16] u.s. and hypothetically you have a CA of
[11:19] the same name in Australia the two would
[11:21] be considered to match and then we also
[11:26] found an ineffective sanity check in
[11:28] matrix SSL so the function that I'm
[11:30] showing you here on the screen its
[11:32] purpose is to perform some basic sanity
[11:35] check on input eight strings however
[11:39] because of the if conditions are pretty
[11:41] much all of them unsatisfiable it
[11:44] actually never rejects any strings due
[11:47] to a incorrect usage of the logical
[11:50] operator for a scientific curiosity we
[11:54] also revisited some of the libraries
[11:56] that the previous passing paper tested
[11:59] specifically we have some new findings
[12:01] regarding these two libraries first in
[12:05] science SL we notice that there is a
[12:07] hidden coding mistake in the parsing
[12:11] code that parses certificate expansions
[12:13] it turns out that this piece of code is
[12:16] a lot like a poor grad student both of
[12:18] them are missing a break this is bad
[12:23] because the parser is incorrectly
[12:25] parting certificates tensions with which
[12:27] means in the end you have some
[12:29] extensions mishandled our speculation
[12:32] here is that because this bug given
[12:34] concrete test cases doesn't seem to
[12:37] trigger a observable crash perhaps it
[12:39] would have been hard for pausing to kind
[12:42] of catch this bug also inside the cell
[12:45] we found the overly restrictive time
[12:47] check recorded on certificates the
[12:49] validity periods are meant to be
[12:52] inclusive but we found a hidden coding
[12:55] mystic in the functions that does
[12:57] daytime checking so if you look at the
[12:59] function names they're showing correct
[13:02] intention but if you look closer the
[13:05] negation of greater than is less than or
[13:07] equal to not less than and because of
[13:09] that sighs SSL turns out to be overly
[13:11] restrictive since they would be
[13:13] incorrectly rejecting some valid suits
[13:15] because this is a specific boundary case
[13:18] among a large number of possible values
[13:20] perhaps it would be difficult for
[13:23] unguided passing to perhaps hit this
[13:25] part one
[13:28] for discussion that we have is regarding
[13:30] the puddling constraint in size of zero
[13:32] and polar so putting's constraint is a
[13:35] part of the basic constraint extension
[13:38] it is used by upper-level issuing CAS to
[13:41] restrict the length of a chain of
[13:43] certificate that you can construct from
[13:45] here on now in the previous work it has
[13:48] been reported that both size SL and
[13:51] polarization are incorrectly rejecting
[13:53] such regarding some corner cases of
[13:55] checking puzzling coloring however our
[13:59] findings suggest that both is SL and
[14:02] polar system along with wool versus oh
[14:04] and crop against or so they do not check
[14:06] path length constraint at all so it
[14:09] cannot be the case that they are
[14:10] incorrectly rejecting suit due to this
[14:13] reason our speculation here would be
[14:16] perhaps the first testing cases that
[14:19] they generated and used to serve as
[14:22] evidence for this particular claim also
[14:24] happen to contain multiple other
[14:26] problems that triggered a rejection in
[14:28] the libraries hence resulting in a small
[14:30] inaccuracy in the claim to summarize
[14:35] x.509 certificate validation
[14:38] unfortunately in all the tested
[14:40] libraries they awesome what DVA from
[14:42] actually 50 to 80 and app developers
[14:46] might not know all these intricacies all
[14:48] these details so we propose a white box
[14:52] approach for exposing these non
[14:54] compliance and semantic level kind of
[14:56] problems and our experiments turn out to
[14:59] be quite fruitful we found numerous
[15:01] problems have them reported and many of
[15:04] them got fixed as a final takeaway my
[15:08] advice to app developers would be to
[15:11] upgrade try to upgrade to newer versions
[15:13] of the TS libraries that you are using
[15:15] and I'm pretty sure I'm not the first
[15:18] one to say this many different people
[15:20] have been talking about this from
[15:22] different perspective so looking from
[15:25] the context of certificate validation
[15:29] I think newer versions of TS libraries
[15:31] generally they have more bug fixes
[15:33] better capability of handling complex
[15:36] inputs such as extensions and generally
[15:39] they also have a more rigorous
[15:41] certificate validation logic and with
[15:44] that I'll end my talk and I'll be more
[15:46] than happy to take questions from the
[15:47] audience thank you hello I'm Thomas
[16:02] Costa from Qualcomm thank you for the
[16:03] talk I have a question about the the
[16:07] part where you compare the equivalence
[16:10] of constraints I was wondering if I
[16:14] understand correctly you would execute
[16:16] multiple implementations of certain
[16:19] library of you know certain
[16:22] functionality using symbolic execution
[16:25] right but then to compare the
[16:28] equivalence you would need to know sort
[16:30] of like which paths you compare in each
[16:32] of these libraries right right so I
[16:35] think so first of all for each libraries
[16:38] you essentially extracted path
[16:40] constraints that are kind of describing
[16:43] the two universes you know accepting and
[16:46] rejecting and as long as you keep the
[16:49] symbolic variables kind of of the same
[16:52] name then you can rather easily compare
[16:54] the two of them so are you asking the
[16:59] problem of translating one path
[17:01] constraint from one library to another
[17:02] like yes exactly right so that's the
[17:05] kind of also the beauty of doing this at
[17:07] the symbolic level so as long as you
[17:09] have the symbolic variables of the same
[17:11] name then they are the same variable to
[17:13] a say SMT solver so it's you don't
[17:16] really have this translation problem or
[17:19] ontological problem between different
[17:21] implementations because the variables
[17:23] that we have we gave the name to them we
[17:26] are not looking at the code variables we
[17:29] have a variable that we named which
[17:31] correspond to some certificate fields
[17:34] and extensions that we know of and the
[17:37] two of them would match up thank you
[17:40] very much thank you yeah I Philip your
[17:44] honor with a PFL thank you for your talk
[17:46] can you elaborate a little bit more on
[17:48] how you came to how you selected the
[17:52] libraries for your analysis
[17:54] as you did and for example I missed like
[17:57] libraries like OpenSSL and so on right
[18:00] so that's a very good question so we
[18:03] started looking at small implementations
[18:07] because they are useful and they also
[18:09] easier to analyze to be honest and then
[18:12] due to obviously their lines
[18:15] whee-hee we got our story and we kind of
[18:19] decided to do the small between
[18:22] libraries and surprisingly there are
[18:24] actually new libraries that are still
[18:26] being made so I think one week before
[18:28] the Oakland deadline we found a new you
[18:30] know small foreign libraries that's
[18:32] being made and people actually using
[18:34] them so it turns out to be quite ok for
[18:37] us to analyze this and and I think it's
[18:39] useful and speaking of the baker
[18:41] libraries we are actually actively
[18:44] working on those open SSL NSS and canoe
[18:48] TRS and also some other possible
[18:50] implementations a.x organizer is also
[18:53] being using perhaps IPSec i ke those
[18:56] kind of scenarios we're also looking
[18:57] into that so we are kind of working on
[19:00] follow-up work ok thank you thank you
[19:04] hey you SML ski university Baku ma I
[19:08] would like to ask so what would need to
[19:10] be done to use this approach to verify
[19:14] TLS implementations not only certificate
[19:16] our think I think here we're looking at
[19:23] a different problem so we are focusing
[19:26] on kind of the semantics less of
[19:29] verifying or validating certificate it
[19:33] doesn't really have a like a back and
[19:34] forth different messages exchanged and
[19:36] it's also not like in ers you have a
[19:39] somewhat or rather complicated actually
[19:42] say machine and you don't also have a
[19:45] lot of kind of cipher suites and
[19:47] handshake messages to choose from
[19:49] that's so I think in here steps one of
[19:51] the most difficult things but here we
[19:53] don't really have that so it's actually
[19:55] given the input what kind of scrutiny
[19:57] the library imposes on before deciding
[20:00] to accept it so I think these two are
[20:02] quite different
[20:04] thank you
[20:05] and I still can I steal five seconds I
[20:09] think I will also like to share with you
[20:11] the name of our paper so I talked to
[20:13] some other researchers the other day and
[20:15] they were saying that oh because you are
[20:17] mixing concrete values with symbolic
[20:19] values isn't it Khan colic and my answer
[20:21] to them were simple was simple actually
[20:23] because if I named my paper concert it
[20:26] would be rather confusing and weird so
[20:28] we kept the name same set thank
[20:32] [Applause]

=== Content from www.ieee-security.org_9a3ac0a1_20250125_092512.html ===
SymCerts: Practical Symbolic Execution For
Exposing Noncompliance in X.509 Certiﬁcate
Validation Implementations

Sze Yiu Chau∗ Omar Chowdhury† Endadul Hoque∗ Huangyi Ge∗ Aniket Kate∗ Cristina Nita-Rotaru‡ Ninghui Li∗

schau,mhoque,geh,aniket,ninghui
{

}

@purdue.edu, Purdue University, West Lafayette, Indiana, USA∗

omar-chowdhury@uiowa.edu, The University of Iowa, Iowa City, Iowa, USA†
c.nitarotaru@neu.edu, Northeastern University, Boston, Massachusetts, USA‡

Abstract—The X.509 Public-Key Infrastructure has long been
used in the SSL/TLS protocol to achieve authentication. A recent
trend of Internet-of-Things (IoT) systems employing small foot-
print SSL/TLS libraries for secure communication has further
propelled its prominence. The security guarantees provided by
X.509 hinge on the assumption that the underlying implementa-
tion rigorously scrutinizes X.509 certiﬁcate chains, and accepts
only the valid ones. Noncompliant implementations of X.509
can potentially lead to attacks and/or interoperability issues. In
the literature, black-box fuzzing has been used to ﬁnd ﬂaws
in X.509 validation implementations; fuzzing, however, cannot
guarantee coverage and thus severe ﬂaws may remain undetected.
To thoroughly analyze X.509 implementations in small footprint
SSL/TLS libraries, this paper takes the complementary approach
of using symbolic execution.

We observe that symbolic execution, a technique proven to
be effective in ﬁnding software implementation ﬂaws, can also
be leveraged to expose noncompliance in X.509 implementations.
Directly applying an off-the-shelf symbolic execution engine on
SSL/TLS libraries is, however, not practical due to the problem
of path explosion. To this end, we propose the use of SymCerts,
which are X.509 certiﬁcate chains carefully constructed with
a mixture of symbolic and concrete values. Utilizing SymCerts
and some domain-speciﬁc optimizations, we symbolically execute
the certiﬁcate chain validation code of each library and extract
path constraints describing its accepting and rejecting certiﬁcate
universes. These path constraints help us identify missing checks
in different libraries. For exposing subtle but intricate noncom-
pliance with X.509 standard, we cross-validate the constraints
extracted from different libraries to ﬁnd further implementation
ﬂaws. Our analysis of 9 small footprint X.509 implementations
has uncovered 48 instances of noncompliance. Findings and
suggestions provided by us have already been incorporated by
developers into newer versions of their libraries.

I. INTRODUCTION

The X.509 Public-Key Infrastructure (PKI) standard [1], [2]
has long been used in SSL/TLS as a means to distribute keys
and provide authentication. The security assurance expected
from SSL/TLS handshake critically hinges on the premise
that communication peers, particularly the clients, correctly
perform the prescribed validation of the server-provided X.509
certiﬁcate chain. Put differently, correctly validating X.509
certiﬁcate chains is imperative to achieving security. Flaws
in implementations of the certiﬁcate chain validation logic
(CCVL) could potentially lead to two pitfalls: (1) Overly

restrictive CCVL (i.e., incorrectly rejecting valid certiﬁcate
chains) may result in interoperability issues and potential
loss of service; (2) Overly permissive CCVL (i.e., incorrectly
accepting invalid certiﬁcate chains) may allow attackers to
conduct impersonation attacks. We call an X.509 CCVL
implementation noncompliant with the X.509 speciﬁcation
if it suffers from over-permissiveness, over-restrictiveness, or
both. The X.509 standard [1] is deﬁned in a generic way to
accommodate different usage scenarios (e.g. for code signing,
encipherment, authentication, etc.). In this work, we concen-
trate on X.509’s use in the context of Internet communication
(i.e., clients performing server authentication during SSL/TLS
negotiation) and focus on the RFC 5280 speciﬁcation [2].

Although the SSL/TLS protocol implementations have un-
dergone extensive scrutiny [3]–[8], similar rigorous investi-
gation is absent for checking compliance of X.509 CCVL
implementations. For instance, researchers have developed a
formally veriﬁed reference implementation for the SSL/TLS
protocol [7] but it does not include a formally veriﬁed CCVL.
The portion of code in SSL/TLS libraries responsible for
performing the X.509 chain validation are often plagued with
severe bugs [9]–[21].

Implementing a compliant X.509 CCVL is not easy, primar-
ily due to the complexity of its requirements. For example,
through our analysis, we have seen how a supposedly simple
boundary check on date and time can lead to various instances
of noncompliance in different libraries due to mishandling
time zones and misinterpreting the speciﬁcation. The following
comment from an SSL/TLS library developer that we con-
tacted regarding a bug report concisely capture the intricacy
of the task: “In general, X.509 validation is one of the most
error prone, code bloating, and compatibility nightmares in
TLS implementation.”

There are two possible directions for addressing X.509
CCVL’s noncompliance problem: (1) Formally proving com-
pliance of a (possibly reference) CCVL implementation with
respect to the speciﬁcation and having every library use it;
(2) Devising approaches for ﬁnding noncompliance in CCVL
implementations. The difﬁculty of automatically proving com-
pliance of an X.509 CCVL implementation, in addition to
the problem being undecidable in general [22], stems from

Fig. 1. Our noncompliance ﬁnding approach for X.509 CCVL implementations. Symbolic execution engine takes as input a CCVL implementation and
extracts the approximated accepted and rejecting certiﬁcate universe whereas extraction validator validates it through concrete execution. Missing ﬁeld check
detector ﬁnds unscrutinized certiﬁcate ﬁelds from the universes. Cross validation engine performs cross validation among two implementations universes.

the fact
that standard formal veriﬁcation techniques [23]–
[42] often do not support all the idiosyncrasies of a system
level programming language like C. The direction of ﬁnding
noncompliance was adopted by Brubaker et al. [43] and they
uncovered a number of bugs in the CCVL implementations
using black-box fuzzing, which raised awareness on both
the existence and severity of the problem. Our approach is
also geared towards ﬁnding noncompliance in real CCVL
implementations.

Although black-box fuzzing is an effective technique for
ﬁnding implementation ﬂaws, especially when the source code
is not available, it suffers from the following well known
limitation: given a vast input space, black box fuzzing fails
to concentrate on relevant portions of the source code without
explicit guidance (i.e. lack of code coverage).

Symbolic execution [44] has been found to address the
above limitation [45]–[47]. Symbolic execution is also known
to be effective in ﬁnding bugs buried deep in the execution.
It is, however, cursed by the problem of path explosion [48],
which severely hinders its scalability and practicality, espe-
cially when the input is recursively structured and complex as
in the case of X.509 certiﬁcates.

In this paper, we take the ﬁrst step in making symbolic
execution practical for ﬁnding noncompliance in real X.509
implementations. To this end, we solve symbolic execution’s
path explosion problem in the following manner: (1) Focusing
our analysis on open source SSL/TLS libraries that have a
small footprint and code base; (2) Applying a combination of
domain-speciﬁc insights, abstractions, and compartmentaliza-
tion techniques to the symbolic execution environment.

Small footprint SSL/TLS libraries are typically tailor-made
for resource constrained platforms, and often prioritize efﬁ-
ciency over robustness. With the emergence of Internet-of-
Things (IoT), these libraries are actively deployed on com-
modity devices to satisfy the needs for secure communica-
tion in the IoT ecosystem [49]–[52]. Furthermore, following
the discovery of several high-proﬁle vulnerabilities due to
implementation ﬂaws in recent years [53]–[55], traditional
SSL/TLS libraries like OpenSSL has been criticized to have
an unnecessarily large and messy code base that is both slow
and infested with bugs [56]. A call for diverse alternative
implementations with better maintainability and a desire for

performance have sparked interests in adopting small footprint
SSL/TLS libraries for building applications on even conven-
tional PC platforms [57]–[61]. Hence it
is of interest for
us to evaluate these implementations of X.509 validation for
robustness and compliance to speciﬁcation.

To make symbolic execution practical and feasible, we
develop the concept of SymCerts, which are syntactically
well-formed symbolic X.509 certiﬁcate chains, such that each
certiﬁcate contains a mix of concrete and symbolic values. To
further reduce path explosion, we decompose the problem of
noncompliance ﬁnding into smaller independent sub-problems
based on the domain-speciﬁc observation that some certiﬁcate
ﬁelds are logically independent in their semantic meanings.
Fields in the same sub-problem are made symbolic at the
same time, while the other unrelated ﬁelds are kept concrete.
The use of SymCerts, along with the observation of semantic
independence of ﬁelds, address the path explosion problem of
symbolic execution that stem from the recursive and complex
nature of the input certiﬁcate chain representation.

Approach: An X.509 CCVL partitions the certiﬁcate chain
input universe into accepting (chains deemed valid) and re-
jecting (chains deemed invalid) certiﬁcate universes. We use
symbolic execution to automatically extract the approximation
of the certiﬁcate accepting and rejecting universes (See Figure
1), and symbolically represent these sets as path constraints
(quantiﬁer-free ﬁrst order logic formulas), where the symbolic
variables correspond to ﬁelds and extensions of certiﬁcates.

In the case where an X.509 CCVL implementation is
noncompliant due to the lack of certain checks, a simple
search (e.g. with grep) of the path constraints will uncover
such noncompliance, as the corresponding symbolic variables
will not appear in the extracted path constraints. For catching
deeper noncompliance, we leverage the principal of differential
testing [62], [63], by carrying out a cross validation of different
implementations. Given two implementations I1, I2, and their
corresponding accepting and rejecting certiﬁcate universes
A1,
R2, we can automatically determine whether
R1,
discrepancies exist between I1 and I2 (i.e., one implementation
accepts a certiﬁcate chain whereas the other rejects it) by
checking whether the sets
A2 ∩R1 are nonempty.
A1 ∩R2 and
Representing these sets symbolically enables us to implement
intersection operator by leveraging a Satisﬁability
the set

A2, and

2

SymbolicExecutionX.509CCVLLibrary1ExtractionValidatorA1R1SymbolicExecutionX.509CCVLLibrary2ExtractionValidatorA2R2.........AiRiSymbolicExecutionX.509CCVLLibrarynExtractionValidatorAnRnAccepted&RejectedX.509CertiﬁcateUniversesMissingFieldCheckDetectorAjRjMissingFieldCheckReportA1R1A2R2CrossValidationEngineDetectedInconsistenciesModulo Theory (SMT) solver [64], [65].

Evaluation and Findings: We analyzed 9 implementations
from 4 families of code base (axTLS, wolfSSL, mbedTLS,
MatrixSSL) and uncovered 48 instances of noncompliance.

Notably, we have detected the erroneous

logic em-
braced by wolfSSL 3.6.6 and MatrixSSL 3.7.2 for matching
ExtKeyUsage object identiﬁers (OID); such OID matching
is used to assert the proper use of the key according to its
intended purposes (e.g., for code signing). Although standard
usage purposes are identiﬁed with pre-deﬁned values (e.g.,
1.3.6.1.5.5.7.3.1 means server authentication), other values
are allowed for deﬁning custom purposes. Both wolfSSL
3.6.6 and MatrixSSL 3.7.2 take a summation of the en-
coded bytes of an OID, and uses only the sum for match-
ing against known standard key usage purposes. In their
scheme, OID 1.3.6.1.5.5.7.3.1 (ASN.1 DER-encoded bytes:
0x2B 0x06 0x01 0x05 0x05 0x07 0x03 0x01) will be identi-
ﬁed as decimal 71. Despite OIDs being unique hierarchically,
the summation of their encoded bytes may not be. An adver-
sary may request a certiﬁcate authority to issue an innocuous-
looking certiﬁcate with a custom key usage purpose OID value
that adds up to 71, and would then be able to use it for server
authentication in these libraries. We have reported this bug to
the library developers. They acknowledged the problem and
have it ﬁxed in new releases.

∈

[0, 49] is treated as the year 20YY whereas YY

Another notable ﬁnding is the misinterpretation of the year
ﬁeld of UTCTime by MatrixSSL 3.7.2. In UTCTime format,
the RFC prescribes two bytes YY to denote years such that
YY
[50, 99]
∈
is treated as 19YY, allowing years to be in range 1950
2049.
However, MatrixSSL 3.7.2 misinterprets the YY ﬁeld and
hence miscalculates some certiﬁcate expiration by 100 years
(e.g., certiﬁcates expired in 1995 are considered to expire in
2095). Developers of MatrixSSL acknowledged this bug after
receiving our report and implemented a ﬁx in a newer version.
Other ﬁndings are reported in Section VI.

−

Contributions: In summary, this paper makes the following

contributions:

1) We take the ﬁrst step towards developing a more princi-
pled approach to systematically analyze real implemen-
tations of X.509 validation.

2) Though scalability issue exists, we show that symbolic
execution could be made practical by limiting the scope
of analysis and using domain speciﬁc optimization, and
it is very effective in exposing implementation ﬂaws.
3) We revisit three speciﬁc implementations that have been
studied before in the literature [43]. With new ﬁndings
to ﬁnd with an unguided
that are otherwise difﬁcult
fuzzing approach, we show that previous work based
on fuzz testing indeed suffers from false negatives, and
some of their claims are inaccurate due to a possible
misinterpretation of those false negatives.

4) For the other and more recent implementations that had
not been studied before, we found multiple instances of
noncompliance and have them reported to the developers.

3

II. RELATED WORK

Given their prominence and importance, the research com-
munity has put implementations of the SSL/TLS protocols
under close scrutiny in recent years. Here we give a brief
overview of previous research efforts and account for how our
work is different from them.

A. Forged certiﬁcates, attacks, and patching

Huang et al. [66] designed a client-side applet to moni-
tor and report the certiﬁcates that were actually presented
to clients. Their study discovered about 6 thousand forged
certiﬁcates in over 3 million connections, and showed that
not just malware but surveillance devices as well as anti-virus
software are also forging certiﬁcates to tamper with SSL/TLS
connections. A new attack on TLS known as the KCI attack
has been found in [67]. This attack is possible due to the
use of certain weak non-ephemeral cipher suites, plus the fact
that installing end-entity (in contrast to CA) certiﬁcates do
not trigger any warnings, and many implementations are not
correctly handling the key usage extensions. This also high-
lights the importance of correctly handling extensions when
verifying a chain of certiﬁcates. Bates et al. [68] proposed
to use dynamically linked objects and binary instrumentation
to implement a defense layer, so that vulnerabilities can be
patched in a prompt manner, proper extension handling can
be enforced, and insecure options can be overridden.

B. Incorrect and insecure usage of TLS library APIs

Georgiev et al. [69] crafted a handful of attack certiﬁcates to
attempt MITM attacks against various SSL/TLS library-using
applications, and showed that application developers often
misunderstand and misuse APIs, resulting in vulnerabilities.
Further discussions on false beliefs of developers, exploits on
TLS-using applications and correct usage of TLS can be found
in [70]. He et al. [71] showed how to use static analysis to
vet and identify vulnerable API usage in applications. Yun et
al. [72] propose a fully automated system called APISAN that
can infer correct API usage from other usages to that API, and
use this information to ﬁnd inconsistent API usages.

The major difference between this line of research and our
work is the different scope of focus. These research efforts
focus on how application developers are making mistakes in
terms of API calls to the libraries, while our work is focused
on how the underlying SSL/TLS libraries providing those APIs
are implementing the certiﬁcate validation logic. Problems
in the libraries would affect applications even if application
developers made no mistakes in using the APIs.

C. Fuzz testing of TLS implementations

Fuzzing has been a prominent approach in testing SSL/TLS
implementations, where test cases are typically synthesized
by applying mutation heuristics on known valid input (e.g.
message sequences and certiﬁcates). Beurdouche et al. [3]
looked at the problem of libraries mishandling unexpected
sequences of messages when implementing support for various
ciphersuites, authentication modes and protocol extensions.

Brubaker et al. [43] used black-box fuzzing to test client-side
validation of X.509 certiﬁcates in SSL/TLS implementations.
De Ruiter et al. [4] showed that the implemented state machine
of SSL/TLS can be inferred by applying a fuzzing-based
technique, which can then be veriﬁed manually to discover
errors. A recent work by Somorovsky [5] presents a framework
that allows developers to evaluate the behavior of TLS servers
in a ﬂexible manner, with the ability to create arbitrary
protocol ﬂows and dynamically modiﬁed messages.

Unguided black-box fuzzing, as used in previous work [43],
makes a good ﬁrst attempt to reveal the existence of problems
in X.509 implementations of SSL/TLS libraries, especially
when the source code is not available. However, there are
limitations of such approach: 1) given a particular test case
that indicates an error, it is not easy to account for the root
causes; 2) it yields no guarantees on coverage of the code
being tested; 3) each generated test case could contain multiple
problems that might mask each other, making results difﬁcult
to interpret. Our work attempts to take advantage of the fact
that when the underlying source code is available, one can
infer useful information out of the code, and perform testing
with better code coverage.

D. TLS state machine and high-conﬁdence implementations

Attempts were made on building high-conﬁdence TLS im-
plementations with a focus on correct state transitions and
cryptographic primitives, using re-engineered protocol speci-
ﬁcation and modular code base [6], as well as veriﬁed code
along with security proofs [7]. Beurdouche et al. [8] designed
a tool that uses a veriﬁed implementation as a reference to test
the state machine of other SSL/TLS implementations.

State machine bugs are about how state transitions are being
performed in response to a sequence of messages of the sub-
protocols, whereas we investigate how the validation logic of
X.509 certiﬁcates are being done, which is one crucial step
in typical SSL/TLS handshakes, and have a broader scope of
implications outside the context of SSL/TLS.

At the time of writing, previous work on reference SSL/TLS
implementations do not
include a formally veriﬁed X.509
CCVL. Possible future efforts made along this direction on
building a high-conﬁdence implementation of X.509 validation
can be used as references to put verdicts on which behaviors
are incorrect and noncompliant, given the discrepancies in
libraries found by previous work [43] and this work.

E. Symbolic Execution

Symbolic execution has been shown to be effective for
detecting low-level (memory) errors (e.g., null dereferencing)
[45]–[47], [73]–[79]. It has also been used for checking the
equivalence of C functions [80], [81], for checking server–
client interoperability of network protocols based on the set
of packets accepted by them [82], for checking controllers in
software-deﬁned networks [83], [84], and for cross-checking
different ﬁle system implementations to ﬁnd semantic bugs
[85]. The input arguments/messages considered in these work
are structurally simpler than an X.509 certiﬁcate.

F. Symbolic Finite Automata

Symbolic ﬁnite automata (SFA) [86] extend ﬁnite automata
by supporting symbolic inputs, that is, typical ﬁnite automata
require the alphabet to be of ﬁnite size whereas a symbolic
ﬁnite automata can support an inﬁnite alphabet set. Recently,
Argyros et al. [87], [88] proposed a black-box automata
learning algorithm (i.e., querying the program in a black box
fashion) for extracting the SFA of a given program (i.e., regular
expression ﬁlters and string sanitizers), and use it to perform
differential testing and ﬁnd bugs in various implementations
(e.g., TCP, Web Application Firewalls). Our collected path
constraints can be viewed as an unwinding of their learned
SFA. The SFA learning algorithm requires sample transitions
for each state to be explicitly given as an input. It is, however,
not obvious how one would obtain such sample transitions for
an X.509 implementation so that it can be given as input to
the learning algorithm. Also, due to the white box nature of
our analysis, our approach is likely to yield a more precise
characterization of the implementation’s internal logic.

III. BACKGROUND AND PROBLEM DEFINITION

In this section, we ﬁrst present a brief introduction on X.509
certiﬁcates and their validation logic. We then present the
noncompliance ﬁnding problem and the associated high level
challenges.

A. Preliminary on X.509 Certiﬁcate Validation

The X.509 PKI standard is described in ITU-T Recom-
mendation X.509 [1]. The certiﬁcate format itself, at the time
of writing, has 3 versions. Version 2 and 3 were introduced
to add support for certiﬁcate revocation lists (CRLs) and
certiﬁcate extensions, respectively. X.509 certiﬁcates can be
used in various environments for different purposes. A variety
of standard certiﬁcate extensions are deﬁned in the standard
documents [1] and ANSI X9.55. RFC 5280 [2] proﬁles how
version 3 certiﬁcates, extensions and CRLs are meant to be
used speciﬁcally for the Internet. Since we focus on this
particular prominent use case of X.509, in the rest of this
section, we provide a simpliﬁed overview of what makes a
certiﬁcate and how validation should happen in general, taking
the viewpoint of an Internet client and using RFC 5280 as the
main reference.

1) Contents of an X.509 certiﬁcate: At a very high level, a
X.509 certiﬁcate is made of 3 parts: the TBS (To-Be-Signed)
part, which includes most of the semantic content of the
certiﬁcate; a signature algorithm identiﬁer, which denotes the
algorithm the issuer used to sign the certiﬁcate; and ﬁnally the
actual signature value. The TBS part generally includes the
following ﬁelds: version (version number), serialNumber (that
can uniquely identify a certiﬁcate), signature (the signature
algorithm identiﬁer), issuer (name of the entity who signed
the certiﬁcate), validity (a time period of which the certiﬁcate
can be considered as valid), subject (name of the subject of
the certiﬁcate), subjectPublicKeyInfo (the public key of the
subject of the certiﬁcate).

4

is only issued for signing software in a SSL/TLS handshake
for authentication would not be allowed).

There are other standard extensions which we do not present
here. For a complete list of extensions deemed useful for the
Internet, and the details on how to handle them, we refer the
readers to RFC 5280 [2].

3) Sources of noncompliance: The intricacies of imple-
menting a compliant X.509 CCVL stem from the rich set of
ﬁelds in certiﬁcates, which are further complicated by their
wide range of possible values, as well as the numerous optional
but possibly critical extensions. Noncompliance can occur due
to the following two reasons:

a) Certain ﬁelds and/or extensions that must be checked are
not involved in the decision making procedure of a CCVL
implementation. This can be further divided into:
i) The ﬁelds and/or extensions are not being parsed into
an internal data structure. This is mostly due to a lack
of intention to support a thorough and robust check,
possibly due to concerns on resource usage.

ii) The ﬁelds and/or extensions are being parsed into an
internal data structure but checks did not happen. This
is mostly due to an intention to perform the checks but
the implementation is not complete.

b) The ﬁelds and/or extensions are involved into deciding
whether to accept or reject the chain, but due to coding
and/or logical errors in the parsing code and/or validation
code, the checks are not performed correctly.

B. Goal and Challenges

In this paper, our goal is to check whether a given X.509
CCVL implementation is compliant with the X.509 speciﬁ-
cation. There are two ways to go about checking compliance
of an implementation, namely, (1) proving the compliance
of the implementation with respect to the speciﬁcation and
(2) trying to ﬁnd noncompliance in the implementation. Our
approach is geared towards ﬁnding noncompliance.

1) Why Not Prove Compliance: To prove compliance of
a given CCVL implementation, we have to formally specify
the valid sets of X.509 certiﬁcate chains that a CCVL imple-
mentation should accept. The X.509 speciﬁcation is, however,
described in natural languages and coming up with a complete
formal speciﬁcation is cumbersome and error-prone. Further-
more, even if we have such a formal speciﬁcation Ψ at our dis-
posal, proving that Ψ is satisﬁed by the CCVL implementation
I (i.e., I
= Ψ) using standard formal veriﬁcation techniques
|
[23]–[42] is infeasible as the problem is undecidable in general
[22]. Also, formal veriﬁcation techniques often do not scale
and support real implementations. For this reason, we resort
to noncompliance ﬁnding in the implementation.

2) Challenges: We now discuss the inherent challenges of

the noncompliance ﬁnding.

Natural Languages Speciﬁcation: The X.509 speciﬁca-
tion is written in English and it is inherently prone to under-
speciﬁcation, ambiguities, inconsistencies, and misinterpreta-
tions. To validate a noncompliant instance it is often required

Fig. 2. A simpliﬁed structural view of an X.509 version 3 certiﬁcate (inspired
by a similar ﬁgure in [89]).

Towards the end of the TBS of a X.509 version 3 certiﬁcate
there are three optional constructs: the issuerUniqueID and
subjectUniqueID, which are respectively unique identiﬁers of
the issuer and subject of the certiﬁcate, followed by extensions,
which is a sequence of X.509 version 3 extensions. See Figure
2 for a simpliﬁed visualization of the structure of a typical
X.509 version 3 certiﬁcate.

2) X.509 certiﬁcate validation: The X.509 PKI is based on
the idea of “chain of trust”. The main objective of certiﬁcate
validation is to show that given a trust anchor, C0, the trust can
be extended through a chain of certiﬁcates, all the way down
to the communication peer (e.g. a speciﬁc server). Hence the
basic check requires that for each certiﬁcate of a chain, the
issuer name of a certiﬁcate Ci must equal to the subject name
of the previous certiﬁcate Ci−1, and the signature on Ci can
be correctly veriﬁed using the algorithm, the public key and
other parameters derived from Ci−1.

In addition, each certiﬁcate involved in forming the chain
of trust must be currently valid, in the sense that the current
system time should be within the range (inclusively) prescribed
by the notBefore and notAfter attributes of the Validity ﬁeld.
Other checks in X.509 certiﬁcate validation are related to
the handling of version 3 extensions. Extensions give CAs a
means to impose additional restrictions on certiﬁcates issued
by them, to avoid abuse of certiﬁcates.

Extensions can be marked as critical or non-critical. For the
standard set of extensions, RFC 5280 [2] mandates some de-
fault criticality that a conforming CAs should follow. However,
from the point of view of a certiﬁcate-using system, extensions
should be processed regardless of their criticality if the system
is able to, and in case it cannot process any of the critical
extensions then the certiﬁcate should be rejected.

On a valid certiﬁcate chain, each of the certiﬁcates needs
to be a CA certiﬁcate, except for the leaf one (both CA and
non-CA are allowed). In X.509 version 3, this is achieved by
checking the basicConstraints extension, which contains an
isCA boolean ﬁeld indicating whether the certiﬁcate is a CA
certiﬁcate or not, and an optional integer pathLenConstraint
that limits the number of non-self-issued intermediate CA cer-
tiﬁcates that can follow on the chain, not counting the leaf one.
Before version 3, X.509 certiﬁcates do not have extensions.
In such cases, clients can choose to either consider those to
be non-CA certiﬁcates, or use an out-of-band mechanism to
verify if those are CA certiﬁcates or not.

The KeyUsage and ExtKeyUsage are two useful exten-
sions that describe the intended purposes of a certiﬁcate.
With issuing CAs imposing these on certiﬁcates, and clients
faithfully checking the intended purposes, some certiﬁcate
abuse scenarios can be stopped (e.g. using a certiﬁcate that

5

VersionSerialNumberSignatureIssuerValiditySubjectSubjectPublicKeyInfoIssuerUniqueIDSubjectUniqueIDExtensionsDigitalSignatureTo-Be-SignedbyCertiﬁcateAuthority(Issuer)VersionoftheCertiﬁcate(e.g.,3)UniqueIntegerIdentiﬁeroftheCertiﬁcateAlgorithmIDusedtosigntheCertiﬁcateNameoftheCertiﬁcateissuerNotBeforeandNotAfter(TimePeriod)NameoftheCertiﬁcateownerPublickey(andAlgorithmID)oftheCertiﬁcateownerUniqueIDoftheissuingCA(Optional)UniqueIDoftheSubject(Optional)Extensions(PossiblyCritical)(Optional)AlgorithmIDandDigitalSignatureto consult the speciﬁcation when we do not have a formal
speciﬁcation at our disposal. We resort to manual effort to
address this challenge.

Scalability: The complex format of X.509 certiﬁcates
and also the intricacies in certiﬁcate chain validation make
it difﬁcult to develop a scalable noncompliance checker. Also,
it is difﬁcult to develop a scalable noncompliance checker for
real libraries written in system level languages such as C.

Cryptographic Libraries: A X.509 CCVL relies on cryp-
tographic functions to perform operations such as digital signa-
ture veriﬁcation. Cryptographic functions are well recognized
to be difﬁcult to automatically analyze for correctness.

IV. OUR NONCOMPLIANCE FINDING APPROACH

In this section, we ﬁrst brieﬂy describe symbolic execution
and then present how we leverage it for noncompliance
detection. Finally, we discuss several technical challenges of
applying symbolic execution and how we overcome them.

A. Preliminary on Symbolic Execution

Symbolic execution [44] has been shown to be an effective
way of inferring test cases that yield high code coverage [45]–
[47], [73]–[79]. It achieves this objective by running a program
with symbolic values for input variables. During execution,
when it encounters a branch instruction (e.g., if-else) with a
branching condition on symbolic values, it consults a Satis-
ﬁability Modulo Theory (SMT) solver [64], [65] to check
whether any of the two branches (i.e., the if and else branches)
are possible according to their branching conditions. If any
of the branches are feasible (i.e., the branching conditions are
satisﬁable for some concrete values for the input variables), the
execution explores the corresponding paths. It keeps collecting
all the feasible branching conditions on the input (symbolic)
variables, also known as path constraints, until the program
terminates or reaches a point of interest (e.g., an error loca-
tion). It then consults an SMT solver to obtain concrete values
for the input that will induce the path in question.

B. Approximating Universes with Symbolic Execution

For noncompliance detection, our approach critically relies
on extracting the universes of accepted and rejected certiﬁcate
chains induced by a given X.509 CCVL implementation.

Suppose we denote the universe of all possible X.509

C

C

=

=

A

R

A ∩ R

, a given X.509 CCVL partitions
(the set of accepting certiﬁcate chains) and

C
R
A ∪ R
. To detect noncompliance in a given X.509
∅

certiﬁcate chains with
into two sets
(the set of rejecting certiﬁcate chains) such that
and
CCVL implementation, we automatically extract the sets
A
. Due to the large number of possible certiﬁcate chains,
and
is not
explicitly enumerating elements of the sets
feasible. We represent the sets
symbolically by a
set of quantiﬁer-free ﬁrst order logic (QFFOL) formulas [64]
where each QFFOL formula fi represents a
f1, f2, . . . , fn}
{
set of concrete certiﬁcate chains. We choose QFFOL as it is
sufﬁciently expressive and also decidable for certain theories
(e.g., bitvector, array)—one can leverage an SMT solver to

and

and

R

R

A

A

R

detect noncompliance—whereas the full ﬁrst order logic (FOL)
is undecidable. We use the theory of bitvectors and array.
For a given X.509 implementation, we extract the sets

A
and
by symbolically executing the CCVL of that given
implementation with respect to a symbolic certiﬁcate chain.
Symbolically executing the CCVL can capture the validation
logic for that given implementation through path constraints
and their associated return values of the CCVL function.
The path constraint in question here contains input variables
coming from the input certiﬁcate chain that has ﬁelds and
extensions we marked to have symbolic values. Given a
collected path constraint f and its associated boolean value
b returned by the CCVL function, if b = true (resp., false),
it signiﬁes that any concrete certiﬁcate chain c that satisﬁes
the constraint f (i.e., c
= f ) is accepted (resp., rejected) by
|
the given CCVL. Precisely, after symbolic execution of the
CCVL, we have
where
,
{(cid:104)
fi is a path constraint (i.e., QFFOL formula) we obtained
true, false
during symbolic execution of the CCVL and bi ∈ {
}
is the return value of the CCVL function for the path constraint
in the following way:
fi. From
, we construct
R
C
fi, true
=
fj | (cid:104)
fi | (cid:104)
{
{
induced by a given X.509 CCVL imple-
and
The sets
mentation are the core asset of our noncompliance detection
approach. Given the sets of
test induced by a
CCVL implementation under test Itest and the sets
standard
and
standard induced by the X.509 standard speciﬁcation
(e.g., RFC), Itest is noncompliant if one of the following (or,
standard.
both) hold: (1)
A
For a given Itest, we can use its
test to expose
noncompliance in several ways, possibly by leveraging an
SMT solver [65], [90]. We discuss them presently.

standard and (2)

fn, bn(cid:105)}
(cid:104)

(cid:105) ∈ C}
R

R
test and

.
(cid:105) ∈ C}

f2, b2(cid:105)

f1, b1(cid:105)

and
=

fj, false

test and

A
and

, . . . ,

R

R

R

R

R

test

test

A

A

A

A

A

A

=

=

=

C

(cid:104)

C. Approaches for Exposing Noncompliance

We now discuss three approaches where we leverage sym-
to ﬁnd noncompliance

and

bolic execution and the sets
in X.509 CCVL implementations.

A

R

1) Noncompliance during Symbolic Execution: During
symbolic execution of the X.509 CCVL function of a given
implementation, the symbolic execution engine can discover
certain low level memory errors (e.g., array out of bounds).
We have discovered an erroneous extension processing bug
using this approach. We present the details in Section VI.

A ∪ R

2) Simple Searching of the Path Constraints: By inspecting
all the path constraints in the set
for a particular
CCVL implementation, one can easily notice missing checks
of certain certiﬁcate ﬁelds. Let us assume that we assigned
the subject name ﬁeld of a certiﬁcate to have the symbolic
value sym sub name. We can then perform a search with
the string sym sub name (i.e., often a simple grep will
sufﬁce) among all the path constraints in
. If the search
turns up empty, one can conclude with high conﬁdence that
the implementation does not check the subject name ﬁeld.
This approach enables exposure of noncompliance due to
an implementation’s inability to take certiﬁcate ﬁelds into

A ∪ R

6

(cid:54)
(cid:54)
A

A

R

R

standard and

standard and

standard and

consideration during the CCVL decision making process. We
have discovered several serious noncompliances using grep.
3) Cross Validation: To expose deeper noncompliant
instances—the ones due to an implementation’s inability to
impose proper validity checks on a certiﬁcate ﬁeld even after
recognizing it—ideally we want the sets
standard
induced by the X.509 standard speciﬁcation. We have, how-
ever, neither a formally veriﬁed CCVL implementation we
can extract the sets
standard from, nor a formal
speciﬁcation for X.509 CCVL at our disposal. We compensate
standard by utilizing
for the lack of the sets
the existence of a large number of open source SSL/TLS
library implementations. We can perform a cross validation
(or, differential
testing [62], [63]) by pitting the different
implementations against each other. If two implementations
come to different conclusions about whether a given certiﬁcate
chain is valid, even though it is not clear which implementation
is noncompliant, we can conclude that one of the libraries is
noncompliant. Precisely, for any two implementations I1 and
R2, any
I2 and their corresponding sets
R1,
∈ R2 or (2) c
c
c
∈ R1
represents an instance of noncompliance.
One can utilize the path constraints from two different im-
plementations to ﬁnd inconsistent conclusions in the following
two ways. In our analysis, we follow approach 2.

A2, and
∈ A2 ∧

such that (1) c

∈ A1 ∧

A1,

∈ C

R

A

c

Let us assume for any two given implementations Ip and

Iq, we have the following sets:
2 , . . . , a p
Ap =
n}
2 , . . . , r p
Rp =
m}
2 , . . . , a q
Aq =
s }
2 , . . . , r q
Rq =
t }
Approach 1: To detect

1 , a p
a p
{
r p
1 , r p
{
a q
1 , a q
{
1 , r q
r q
{

(accepting certiﬁcate universe of Ip)
(rejecting certiﬁcate universe of Ip)
(accepting certiﬁcate universe of Iq)
(rejecting certiﬁcate universe of Iq)

(cid:87)

(cid:87)

↔

i ↔

((cid:87)
¬
(1≤j≤t) r q

inconsistencies between Ip and
Iq, one can check to see whether either of the following
(1≤i≤n) a p
(1≤j≤s) a q
formulas is satisﬁable:
j ) and
i ↔
((cid:87)
(1≤i≤m) r p
stands for logical equiva-
j ) (
¬
lence). The ﬁrst (resp., second) formula asserts that the accept-
ing (resp., rejecting) paths of Ip and Iq are not equivalent. Any
model of either of the formulas will signify a noncompliant
instance. We, however, do not utilize this approach to detect
noncompliance for the following three reasons: (1) For each
satisﬁability query the SMT solver will present one model (i.e.,
one noncompliant instance) even in the presence of multiple
noncompliant instances (We desire as many noncompliant
instances instead of just one at a time); (2) The resulting
formulas are large and it may put heavy burden on the SMT
solver; (3) Due to the incompleteness caused by techniques
may
used to relieve path explosion, the extracted sets
not be exhaustive (i.e., complete), yielding false positives.

and

R

A

Ap and each rejecting path r q
j
≤

Approach 2: In this approach, we ﬁrst take each accepting
path a p
i from
Rq where
t, and check to see whether the formula
n, 1
i
1
≤
a p
is satisﬁable by consulting an SMT solver. If the
i ∧
formula is satisﬁable, it signiﬁes that there is at least one
certiﬁcate chain that Ip accepts but Iq rejects. The model

j from

≤
r q
j

≤

7

obtained for the formula from the SMT solver, can be used to
construct a concrete certiﬁcate chain signifying an evidence
of inconsistency. We can then repeat the same process by
taking each accepting path from Iq and each rejecting path
from Ip. Note that, multiple pairs may induce inconsistencies
due to the same noncompliant behavior and sometimes best-
effort manual analysis of the source code is needed to detect
the root cause.

D. Scalability Challenges of Applying Symbolic Execution

A

R

and

The application of symbolic execution in a straightforward
way to extract the sets
, considering all certiﬁcates
in the chains and other arguments to the CCVL function to
have symbolic values, will not yield a scalable noncompliance
detection approach. Our feasibility evaluation have veriﬁed
this observation. We have also tried only one of the certiﬁcates
in the chain to have symbolic values and even then the
symbolic execution did not ﬁnish due to resource exhaustion.
The scalability problem is predominantly due to symbolic
value dependent loops—loops whose terminating conditions
depend on symbolic values—in the certiﬁcate parsing im-
plementation. One way to get around this challenge is to
assume the correctness of the parsing code and just focus on
the core CCVL logic. Ignoring the parsing logic, however,
is not sufﬁcient to capture the majority of the CCVL logic
as some of the sanity checks on the certiﬁcate ﬁelds are
done during parsing. In addition, capturing only the CCVL
logic would require one to manually modify the internal data
structure where the certiﬁcate ﬁelds are stored after parsing.
This approach requires signiﬁcant manual efforts (i.e., code
comprehension) and is also error-prone.

E. Our Solution—SymCerts and Problem Decomposition

and

For addressing the scalability challenge we rely on carefully
crafting symbolic certiﬁcates and also on our domain speciﬁc
and
observations. Rather than extracting the complete sets
, we use domain-speciﬁc observations and specially crafted
R
symbolic certiﬁcate chains to extract an approximation of the
sets
approx. Our approximation has
R
both under- and over-approximation. To overcome path explo-
sion, we create a chain of SymCerts where some portions
of each certiﬁcate have concrete values whereas the others
have symbolic values. SymCerts along with the following
observation aid in achieving scalability during the extractions
of the sets

approx from an X.509 CCVL.

approx and

approx and

, i.e.,

R

A

A

A

One domain speciﬁc observation we use is the logical
independence between certiﬁcate ﬁelds. For instance, the
logic of checking whether a certiﬁcate is expired according
to its notAfter ﬁeld is independent of the logic of checking
whether a certiﬁcate’s issuer name matches with the subject
name of the predecessor certiﬁcate in the chain. In this
case, we can try to capture the logic of checking certiﬁcate
expiration independently of the checking of issuer and subject
names. Based on the notion of independence, we group the
certiﬁcate ﬁelds into equivalence classes where the logic of
ﬁelds in the same equivalence class should be extracted at the

R

A

same time, that is, ﬁelds in the same equivalence class should
be marked to have symbolic values at the same time. We
leverage this observation by generating a SymCert chain for
each equivalence class where each element of the equivalence
class has symbolic values whereas the rest of the ﬁelds have
concrete values. Note that we certainly do not claim that the
checking logic of all certiﬁcate ﬁelds are independent; there
are obviously certiﬁcate ﬁelds whose value inﬂuences one
another. For instance, the value of the isCA ﬁeld of an X.509
certiﬁcate (i.e., whether the certiﬁcate is a CA certiﬁcate)
prescribes certain corresponding key usage purposes (i.e.,
affecting the KeyUsage extension). In this case, the isCA ﬁeld
needs to be in the same equivalence class as KeyUsage.

In our analysis, we conservatively partition the certiﬁcate
ﬁelds into 2 equivalence classes. We refer to these two
equivalence classes as EqC1 and EqC2, respectively. EqC1 has
all the relevant certiﬁcate ﬁelds symbolic, except the Validity
date time period ﬁelds which are symbolic only in EqC2.

V. IMPLEMENTATION

In this section, we discuss additional challenges of applying
symbolic execution to CCVL code, and our approach to
addressing these challenges. We also discuss other aspects of
implementing our noncompliance ﬁnding approach.

(cid:105)

(cid:104)

t, (cid:96), v

Challenge 1 (Complex Structure of X.509 Certiﬁcates):
X.509 certiﬁcates are represented in the Abstract Syntax No-
tation One (ASN.1) [91], [92] notation. X.509 certiﬁcates are
typically transmitted in byte streams encoded following the
DER (Distinguished Encoding Rules), which are binary in
nature. Under the DER format, an X.509 certiﬁcate has the
form
where t denotes a type, (cid:96) denotes the length of
the values in bytes, and ﬁnally v represents the value. t can
represent complex types such as a sequence where the value v
can be recursively made of other
triplets. Such nesting
of

(cid:104)
triplets inside a v ﬁeld can be arbitrarily deep.

t, (cid:96), v
(cid:104)
The problem of marking the whole certiﬁcate byte-stream
as symbolic is that, during certiﬁcate parsing, the symbolic ex-
ecution engine will try different values for (cid:96) as it is symbolic,
and the parsing code will keep reading bytes without knowing
when to stop. This will cause memory exhaustion.

t, (cid:96), v

(cid:105)

(cid:105)

Approach—SymCerts (Certiﬁcates With Symbolic and
Concrete Values): To avoid the scalability problem, instead
of using a fully symbolic certiﬁcate chain, we develop a
certiﬁcate chain in which each certiﬁcate byte-stream contains
some of concrete values and some symbolic values. We call
each such certiﬁcate a SymCert.

(cid:105)

(cid:105)

We construct a SymCert in the following way: For each
tuple (i.e., v contains a value instead of another
leaf
t, (cid:96), v
(cid:104)
tuple) in a certiﬁcate byte-stream, we ensure that the
t, (cid:96), v
(cid:104)
ﬁelds t and (cid:96) have concrete values whereas only the v ﬁeld
is symbolic. Concrete values of t can be obtained from actual
certiﬁcates and we use them as the backbone for generating
SymCerts. For the l ﬁeld, we consult the RFC document to
select appropriate concrete values. For instance, when marking
the OIDs used in the ExtKeyUsage extension symbolic, we

give it a concrete length of 8, as most of the standard key
usage purposes deﬁned in RFC 5280 [2] are 8-byte long.

(cid:105)

(cid:105)

t, (cid:96), v
(cid:104)

Due to the complexity of DER byte-streams, it is difﬁcult
for a user to directly manipulate and construct SymCerts from
scratch. In addition, due to nesting, changing the length ﬁeld
(i.e., (cid:96)) of a child
triplet may require adjustment on
t, (cid:96), v
(cid:104)
the length ﬁeld (i.e., (cid:96)) of the parent
triplet. For this,
we developed a Graphical User Interface (GUI), by extending
the ASN.1 JavaScript decoder [93]. Our GUI allows a user to
see and click on different certiﬁcate ﬁelds, so that they can
be replaced with a desired number of symbolic bytes, and
the new length will be correctly adjusted. The GUI will then
automatically generate code that can be used for symbolic
execution. We use OpenSSL to generate concrete certiﬁcate
chains as the input to our GUI, which constitute the basis of
our SymCerts. The philosophy here is that all major ﬁelds (e.g.
optional extensions, criticality booleans) of a certiﬁcate need
to be explicitly available on the base input certiﬁcate, as it is
difﬁcult to mark nonexistent ﬁelds symbolic.

Challenge 2 (System Time Handling): Given that our
symbolic execution of the implementations would happen at
different times, if we simply allow the implementations to use
the local system time, then the constraints we have extracted
would not be comparable, as the system time elapses.

Approach—Constant Static Time: We consider a ﬁxed
concrete time value for the system time. We use the same
concrete value for these inputs during the analysis of all
implementations. Using a symbolic variable is also possible,
but using concrete values has the advantage of reducing
the complexity of the path constraints which consequently
improves scalability.

Challenge 3 (Cryptographic Functions): The cryptographic
functions (e.g., for verifying digital signatures) called by the
CCVL contain loops dependent on symbolic data, which
severely impact the scalability of symbolic execution.

Approach—Cryptographic Stub Functions: We abstract
away the cryptographic functions with stub functions. For
instance, the function that matches the digital signature of a
certiﬁcate is abstracted away by a stub function that returns
True indicating the match was successful. In this work, we
consider cryptographic correctness beyond our scope. Instead,
we are interested in ﬁnding out what ﬁelds are checked and
what restrictions are imposed on these ﬁelds.

Challenge 4 (Complex String Operations): As part of the
CCVL, implementations are sometimes required to perform
complex string operations (e.g., wild card matching, null
checking) on certiﬁcate ﬁelds such as subject name and issuer
name. Faithfully capturing the string operations with QF BVA
logic (i.e., QFFOL formulas with equality, bit vector, and
array theories)—which is the underlying logic of the symbolic
execution engine we use—does not scale well.

Approach—Single Byte Strings: We consider names and
other string-based certiﬁcate ﬁelds to have a single byte
symbolic value, which signiﬁcantly improves the scalability.
However, because of this, our analysis misses out on ﬁnding
noncompliance due to erroneous string operations.

8

Challenge 5 (Hashing for Checking Multi-Field Equal-
ity): When checking the equality of two name ﬁelds of
certiﬁcates—name ﬁelds are compound ﬁelds containing the
following sub-ﬁelds such as street address, city, state/province,
locality, organizational name & unit, country, common name—
some implementations take a hash of the concatenation of all
the sub-ﬁelds and match the hash values, instead of checking
the equality of each sub-ﬁeld. Trying to solve the constraints
from such a match would be similar to attacking the hash
collision problem, which is not scalable to analyze with
symbolic execution due to symbolic data-dependent loops.

Approach—Hash Stub: The hash function in question
(i.e., SHA-1) returns a 20-byte hash value. We replace it
with a SHA-1 stub which returns a 20-byte value where the
(symbolic) name sub-ﬁelds are packed together. Because of
the single byte approach we introduced to simplify string
operations described in the previous challenge, 20-byte is more
than enough to pack all name sub-ﬁelds of interests.

Challenge 6 (Certiﬁcate Chain Length): While symboli-
cally executing the CCVL of a given implementation, one
natural question that arises is: “How many certiﬁcates in the
symbolic certiﬁcate chain should we consider?” An X.509
CCVL implementation often parses the input X.509 certiﬁcate
chain ﬁrst and then checks the validity of different ﬁelds in the
certiﬁcates of the parsed chain. During symbolic execution,
if the execution detects a loop whose terminating condition
relies on a symbolic value, it faces the dilemma of how many
times to unroll the loop. Such loops in the implementation
often cause path explosion in symbolic execution, resulting
in incompleteness and scalability challenges. If we consider
the certiﬁcate chain length to be symbolic, then the symbolic
execution, especially during parsing, would try all possible
values for the chain length, causing memory exhaustion.

Approach—Concrete Chain Length: For majority of our
analysis, we consider a certiﬁcate chain of length 3 such that
one of the certiﬁcates is the root CA certiﬁcate, the other
is an intermediate CA certiﬁcate, and ﬁnally the remaining
certiﬁcate is the certiﬁcate of the server currently being
authenticated. While analyzing the logic of checking the path
length constraint of the basic constraint extension, we also
consider certiﬁcates with chain length 4 where we have two
intermediate CA certiﬁcates.

t, (cid:96), v
(cid:104)

Challenge 7 (Other aspects of Path Explosion): After the
simpliﬁcations described above, the symbolic execution engine
still generates a large number of paths. We especially observed
that making all the v values of
-tuples that represent
(cid:105)
certiﬁcate ﬁelds and extensions symbolic yields a lot of paths.
Approach—Early Rejection and Grouping Fields: We
observed that implementations sometimes do not return early
even in the case one of the certiﬁcates cannot be parsed or one
of the ﬁelds validity checks failed. This contributes to a multi-
plicative factor to the number of paths. We judiciously applied
early rejection when parsing or validation check fail. Finally,
we applied the logical independence between certiﬁcate ﬁelds
based on their semantics to decompose the noncompliance
ﬁnding ﬁelds. We generated two equivalence classes, one

consists of time ﬁelds related to the certiﬁcate Validity period
checking, whereas the other contains all the remaining ﬁelds.
One could possibly employ a more aggressive grouping of
ﬁelds that need to be check together. We, however, make a
conservative choice because if developer incorrectly introduces
artiﬁcial dependencies in the implementation, we would like
to capture them as well.

Challenge 8 (Time Field Comparison): An X.509 certiﬁcate
contains two time ﬁelds (i.e., notBefore and notAfter) which
are compared to the current system time. A time ﬁeld can
be represented in two formats (i.e., GeneralizedTime and
UTCTime). In GeneralizedTime, the time ﬁeld contains a 15-
byte ASCII string where day, month, hour, minute, second
contribute 2 bytes each; year contributes 4 bytes, and 1 byte
is used to represent the time zone. For UTCTime, the only
difference is that year contributes 2 bytes instead of 4. Sanity
checks are often performed to ensure the ﬁelds are well-
formed (e.g., for minute, the most signiﬁcant digit cannot
be larger than 6). Marking the format symbolic and let the
symbolic execution engine choose the length of the ASCII
string contributes to poor scalability.

Approach—Decomposing Time Fields:

In addition to
checking noncompliance in time ﬁelds handling independently
from other ﬁelds, we further decompose the analysis by ana-
lyzing the two time formats separately. We use two different
SymCerts during symbolic execution, one with UTCTime and
the other with GeneralizedTime, using the concrete length of
the date time ASCII string according to the format.

Challenge 9 (Redundant Pair of Paths in Cross-Validation):
When cross-validating two implementations Ip and Iq, the
upper bound of discrepancies is
.
|Aq| × |Rp|
|Ap| × |Rq|
Based on the number of paths in accepting (e.g.,
Ap and
Aq) and rejecting (e.g.,
Rq) universes, the maximum
Rp and
number of noncompliance instances can be fairly large which
creates a challenge for manual inspection to identify the root
cause of the noncompliance.

+

Approach—Iterative Pruning: We observe that many
pairwise discrepancies are due to the same root cause. Suppose
implementation Ip does not check a particular ﬁeld that Iq
checks. In this case, the missing check in Ip’s accepting path
will likely be enumerated through many rejecting paths of
Iq, resulting in a large number of redundant noncompliance
instances. To make it easier to analyze the results of cross-
validation, once we have identiﬁed such a case, we can
concretize the value of that speciﬁc ﬁeld, repeat the extraction
step and continue cross-validation with a pruned search space.
Challenge 10 (False Positives): Due to different domain
speciﬁc simpliﬁcations and the fact that we are abstracting
away cryptographic functions, our approach can yield false
positives, predominantly due to the path constraint extrac-
tion might not be capturing the real execution faithfully. In
addition, the speciﬁcation (i.e., RFC document) states some
ﬁelds should be checked by a certiﬁcate using system, without
imposing whether the library or application (the two of them
constitute the system) should perform each check. Conse-
quently, SSL/TLS libraries have different API designs due to

9

such unclear separation of responsibility. Some libraries might
enforce all the checks during certiﬁcate chain validation, while
some might not and instead provide optional function calls
for application developers desiring such checks, and the other
libraries might completely delegate the task of implementing
such checks to the application developers. As a clear boundary
cannot be drawn easily, false positives can arise if some
optional but provided checks are missed out during extraction.
Approach—Concrete Replay: To avoid false positives, we
use a real client-server setup to help us verify our ﬁndings. We
capitalize on the fact that a minimalistic sample client code is
often made available in the source tree by library developers
to demonstrate how the library should be used in application
development and use such clients to draw the baseline. To
gain conﬁdence that our extracted path constraints adequately
capture the real execution, for each accepted (resp., rejected)
path constraint, we consult the SMT solver to obtain a concrete
certiﬁcate chain and feed it to a real client-server setup to
see whether the client would actually accept (resp., reject) the
chain. This helps us to see whether the real execution concurs
with our extraction. Similarly, during cross validation between
implementations Ip and Iq, for the discrepancies we found (in
the form of a model provided by the SMT solver), we construct
a concrete certiﬁcate chain out of the model and use the client-
server setup to verify it is indeed the case that Ip would accept
and Iq would reject the chain.

VI. EVALUATION AND RESULTS

We applied our approach in testing 9 open-source imple-
mentations from 4 major families of SSL/TLS library source
trees, as shown in Table I. Implementations that have been
tested in previous study by Brubaker et al. [43] are preﬁxed
with an asterisk. These libraries have seen active deployments
in embedded systems and IoT products to satisfy the security
needs for connecting to the Internet (e.g. axTLS in Arduino
[51] and MicroPython [52] for ESP8266, mbedTLS, tropicSSL
and MatrixSSL on Particle hardware [49], [50], etc.), and are
sometimes used even in building applications and libraries
on conventional desktop platforms [57]–[61], due to their
performance and small footprint advantage. We test multiple
versions of a library from the same family in order to compare
with previous work, and to see if the more recent versions
implement a more complete and robust validation logic.

In this section we ﬁrst show statistics that

justify the
practicality of our approach, and then present noncompliance
ﬁndings grouped by how we uncovered them along the 3
approaches described in Section IV-C, together with other
discrepancies and observations that we made while work-
ing with the libraries. Findings on recent versions of the
implementations, whenever applicable, are reported to the
corresponding developers. Many of our reports had led to ﬁxes
being implemented in newer versions.

A. Implementation Efforts and Practicality

For our analysis, we used the KLEE symbolic execution
engine [45] and the STP SMT solver [65]. We added around

2000 lines of C++ code for implementing the path constraint
extraction and cross validation engines, around 500 lines of
Python for parsing path constraints and automating concrete
test case generation, and around 400 lines of HTML plus less
than 300 lines of JavaScript for the GUI that enables the easy
construction of SymCerts.

In order to implement the various optimizations described
before, a limited amount of new code need to be added to the
libraries that we tested. As shown in Table I, no more than
75 lines of code were added to each of the library. Most of
the new code is used to implement a static system time (see
Section V-Challenge 2) and a stub cryptographic signature
check (Section V-Challenge 3). Additionally, for CyaSSL
2.7.0, wolfSSL 3.6.6, and MatrixSSL 3.7.2, some code was
added to implement the hash stub (see Section V-Challenge
5). PolarSSL 1.2.8 and tropicSSL needed a simpliﬁed version
of sscanf(), and axTLS (both 1.4.3 and 1.5.3) needed
a simpliﬁed version of mktime(), to avoid symbolic-data
dependent loops, both of which are used for reading in and
converting the format of date-time inputs.

Also shown in Table I are the performance statistics regard-
ing path extraction. We ran our experiments on a commodity
laptop equipped with an Intel i5-2520M CPU and 16GB RAM.
Path extraction using EqC1 for most implementations ﬁnished
in minutes, while for some heavier ones it completed in hours.
The total number of paths ranges from hundreds to the level
of ten thousands. For EqC2, we report the upper bound of the
total number of paths, referred to in the table as “Total Paths”,
because the actual number could vary within each library
due to different treatments (and possibly missing checks) for
UTCTime and GeneralizedTime (see Section VI-C and VI-D
for examples). For each library, extraction using EqC2 yielded
paths at the scale of tens, and ﬁnished within a minute.

B. Errors Discovered By Symbolic Execution

The ﬁrst opportunity our approach provides is that, during
symbolic execution, certain low-level coding issues (e.g. mem-
ory access errors, division by zeros, etc.) could be found.

Finding 1 (Incorrect extension parsing in CyaSSL 2.7.0 1):
As shown in Listing 1, due to a missing break statement after
DecodeAltNames(), the execution falls through to the next
case and also invokes DecodeAuthKeyId(). Consequently,
some bytes of the subject alternative name extension, which
we made symbolic, will overwrite the authority key identiﬁer
(a pre-computed hash value) at the time of parsing. The error
manifests later during certiﬁcate chain validation, when the
authority key identiﬁer undergoes some bit shifting operations
and modulo arithmetic, effectively turning it into an array
accessing index, which is then used to fetch a CA certiﬁcate
from a table of trusted CA certiﬁcates. Since some bytes of the
authority key identiﬁer were incorrectly made symbolic during
parsing, the execution engine caught potential memory access
errors in fetching from the table. This was not reported in [43],
which applied fuzzing to test CyaSSL 2.7.0. Our conjecture is

1This bug has been ﬁxed in newer versions of CyaSSL and wolfSSL.

10

TABLE I
PRACTICALITY AND EFFICACY OF APPLYING THE SYMCERT APPROACH IN TESTING VARIOUS SMALL FOOTPRINT SSL/TLS LIBRARIES

Library - version

Released

axTLS - 1.4.3
axTLS - 1.5.3
* CyaSSL - 2.7.0
wolfSSL - 3.6.6

Jul 2011
Apr 2015
Jun 2013
Aug 2015
tropicSSL - (Github) Mar 2013
Jun 2013
* PolarSSL - 1.2.8
Jan 2016
mbedTLS - 2.1.4
Feb 2013
* MatrixSSL - 3.4.2
Apr 2015
MatrixSSL - 3.7.2

Lines of C
code in library

Lines
Added

16,283
16,832
51,786
103,690
13,610
29,470
53,433
18,360
37,879

72
69
33
40
66
66
15
9
30

(cid:3)

Paths
(cid:2)EqC1
276 (419)
276 (419)
32 (504)
256 (31409)
16 (67)
56 (90)
13 (536)
8 (160)
3240 (8786)

(cid:3)

Extraction
Time (cid:2)EqC1
∼ 1 Minute
∼ 1 Minute
∼ 2 Minutes
∼ 1 Hour
∼ 1 Minute
∼ 1 Minute
∼ 1 Minute
∼ 1 Minute
∼ 1 Hour

(cid:3)

Total Paths
(cid:2)EqC2
≤ 52
≤ 52
≤ 26
≤ 26
≤ 30
≤ 81
≤ 41
1
≤ 25

Extraction
Time (cid:2)EqC2
≤ 1 minute
≤ 1 minute
≤ 1 minute
≤ 1 minute
≤ 1 minute
≤ 1 minute
≤ 1 minute
≤ 1 minute
≤ 1 minute

(cid:3)

Found Instances
of Noncompliance

7
6
7
2
10
4
1
6
5

§ The fourth column of the table refers to the lines of code we added to the libraries to make them amenable to our analysis. The ﬁfth and sixth columns
display the number of accepting (rejecting) paths we obtained when we made the ﬁelds in equivalence class EqC1 symbolic, and the time it took to
complete the extraction process, respectively. The seventh and eighth columns show the upper bound of total paths (including both accepting and rejecting)
we observed when the ﬁelds in EqC2 are made symbolic, and the time it took for the path extraction process to complete, respectively.

that it would be difﬁcult for concrete test cases to hit this bug,
as the execution is likely to fall through without triggering any
noticeable crashes.

Listing 1. Extension Processing In CyaSSL 2.7.0

switch (oid) {
...
case AUTH_INFO_OID:
DecodeAuthInfo(&input[idx], length, cert);
break;
case ALT_NAMES_OID:
DecodeAltNames(&input[idx], length, cert);
case AUTH_KEY_OID:
DecodeAuthKeyId(&input[idx], length, cert);
break;
... }

C. Findings From Simple Search of Path Constraints

Fields of certiﬁcates, represented by symbolic variables in
our approach, will appear on path constraints if they are in-
volved in branching decisions either directly or indirectly (e.g.
some other decision variables were calculated based on their
values). Consequently, the second opportunity our approach
offers is that immediately after extracting path constraints
using symbolic execution, missing checks of ﬁelds can be
discovered by performing “grep” on the path constraints.

Finding 2 (pathLenConstraint ignored in CyaSSL 2.7.0,
wolfSSL 3.6.6 2): We noticed that both of the aforementioned
libraries fail to take pathLenConstraint into consideration,
which means any such restrictions imposed by upper level
issuing CAs would be ignored by the libraries.

This was not reported in [43], where fuzzing was ap-
plied to CyaSSL 2.7.0. Interestingly, [43] instead reported
that CyaSSL 2.7.0 incorrectly rejects leaf CA certiﬁcates
given the intermediate CA certiﬁcate has a pathLenConstraint
of 0, and is noncompliant because such certiﬁcates should
be accepted according to the RFC. Our ﬁndings, however,
demonstrate that CyaSSL 2.7.0 could not possibly be re-
jecting certiﬁcates for such a reason because it completely
ignores pathLenConstraint. Testing CyaSSL 2.7.0 with con-
the conclu-
crete certiﬁcates conﬁrmed our ﬁnding. Thus,
sion in [43] that CyaSSL 2.7.0 misinterprets RFC regarding

pathLenConstraint and leaf CA certiﬁcate is incorrect. We
conjecture that this is because the frankencerts used as evi-
dence for such conclusion also happen to contain other errors,
and were thus rejected by CyaSSL 2.7.0. This demonstrates
the difﬁculty of interpreting results obtained from fuzzing.

Finding 3 (pathLenConstraint of intermediate CA certiﬁ-
cates ignored in tropicSSL, PolarSSL 1.2.8 3): Our path con-
straints show that even though both tropicSSL and PolarSSL
1.2.8 recognize the pathLenConstraint variable during parsing
time, they check only the one that is on the trusted root
certiﬁcate during chain validation, and ignores those that are
on intermediate CA certiﬁcates of a given chain.

In addition to the fact that PolarSSL 1.2.8 does not check
pathLenConstraint on intermediate CA certiﬁcates, another
simple search found that PolarSSL 1.2.8 does not check
whether the leaf certiﬁcate is CA or not (which is not a
noncompliant behavior). It was however reported in [43]
that PolarSSL 1.2.8 violates the RFC by always rejecting
leaf CA certiﬁcates if the intermediate CA certiﬁcate has a
pathLenConstraint of 0. This is incorrect because PolarSSL
1.2.8 checks neither pathLenConstraint on intermediate CA
certiﬁcates, nor whether the leaf certiﬁcate is CA or not.

Finding 4 (Certain attribute types of distinguished names
ignored in axTLS 1.4.3 and 1.5.3): Both axTLS 1.4.3 and 1.5.3
ignore the country, state/province and locality attribute types
of the issuer and subject names. In other words, organizations
from different countries and states having the same name
would be considered equivalent during matching. This is a
clear deviation from RFC 5280 (Section 4.1.2.4) [2].

We have this ﬁnding reported to the developer of axTLS,
who acknowledged the existence of the problem and imple-
mented a ﬁx in the new 2.1.1 release.

Finding 5 (Inability to process GeneralizedTime in axTLS
1.4.3, tropicSSL): RFC 5280 (Section 4.1.2.5) [2] states “Con-
forming applications MUST be able to process validity dates
that are encoded in either UTCTime or GeneralizedTime.”
However, given our SymCerts with GeneralizedTime, both

2wolfSSL 3.9.10 has implemented support for pathLenConstraint [94].

3The enforcement of pathLenConstraint from intermediate CA certiﬁcates

has been introduced since PolarSSL 1.2.18 [95].

11

tropicSSL and axTLS 1.4.3 returned only 1 concrete rejecting
path with an empty path constraint, hence we conclude that
the aforementioned libraries cannot handle GeneralizedTime,
which is a non-conformance to the RFC. However, the same
SymCerts managed to yield meaningful path constraints in
axTLS 1.5.3, showing that support for GeneralizedTime has
been added in the newer version of axTLS.

Finding 6 (KeyUsage and ExtKeyUsage being ignored
in MatrixSSL 3.4.2, CyaSSL 2.7.0,
tropicSSL): The three
aforementioned implementations do not check KeyUsage and
ExtKeyUsage extensions. This noncompliance implies that
certiﬁcates issued speciﬁcally for certain intended purposes
(e.g. only for software code signing) can be used to authenti-
cate a server in SSL/TLS handshakes. Honoring such restric-
tions imposed by issuing CAs allows the PKI to implement
different levels of trust, and help avoid certiﬁcate (and CA)
misuse in general.

Finding 7 (notBefore ignored in tropicSSL, PolarSSL 1.2.8;
validity not checked in MatrixSSL 3.4.2): Our SymCerts
revealed that PolarSSL 1.2.8 does not check the notBefore
ﬁeld, and MatrixSSL 3.4.2 does not have an inbuilt validity
check, as there is only 1 path, which is an accepting path with
empty constraints, for each of the aforementioned libraries in
their respective cases. This is coherent with the ﬁndings in
[43]. MatrixSSL 3.4.2 delegates the task of checking certiﬁcate
validity to application developers. tropicSSL has the same
problem as PolarSSL 1.2.8, which is not a surprise considering
the fact that tropicSSL is a fork of PolarSSL.

Finding 8 (hhmmss of UTCTime ignored in tropic-
SSL, axTLS 1.4.3 and 1.5.3; hhmmss of both UTCTime
and GeneralizedTime ignored in MatrixSSL 3.7.2): Given
UTCTime on certiﬁcates, even though axTLS 1.4.3 and 1.5.3
check for both notBefore and notAfter, they do not take the
hour, minute and second into consideration, which means
that there could be a shift for as long as a day in terms
of rejecting future and expired certiﬁcates. This ﬁnding is
particularly interesting for axTLS 1.5.3, as its implementation
of GeneralizedTime support can actually handle hour, minute
and second, but for some reason UTCTime is processed in a
laxer manner. Following our report, the developer of axTLS
has acknowledged the problem and is currently considering a
ﬁx. Our extracted path constraints show and tropicSSL also
suffer from the same problem.

Unlike its older counterpart, MatrixSSL 3.7.2 has im-
plemented validity checks that handle both UTCTime and
GeneralizedTime. However, our extracted path constraints re-
vealed that MatrixSSL 3.7.2 does not attempt to check the time
portion of the validity ﬁelds, regardless of whether the date-
time information is in UTCTime or GeneralizedTime. The
developers of MatrixSSL had explained to us the decision to
ignore the time portion was made due to its embedded origin,
where a local timer might not always be available, and in their
own words “having date set correctly is difﬁcult enough”. They
have also admitted that as the result of such decision, a 24-hour
shift in rejecting future and expired certiﬁcates is inevitable.

Finding 9 (notAfter check applies only to leaf certiﬁcate in
tropicSSL): Not just that future certiﬁcates are not rejected
(e.g. missing check for notBefore as described above) in
tropicSSL, our path constraints show that, given a chain of
certiﬁcates, the check on notAfter only applies to the leaf one.
This could lead to severe problems, for instance, if a retired
private key of an intermediate issuing CA corresponding to an
expired certiﬁcate got leaked, attackers would be able to issue
new certiﬁcates and construct a new chain of certiﬁcate that
will be accepted by tropicSSL.

Finding 10 (Incorrect CA certiﬁcate and version number as-
sumptions in axTLS 1.4.3 and 1.5.3, CyaSSL 2.7.0, MatrixSSL
3.4.2): The aforementioned implementations deviate from the
RFC in how they establish whether certiﬁcates of various
versions are CA certiﬁcates or not. As explained previously
in Section III-A2, in case the certiﬁcate has a version older
than 3, some out-of-band mechanisms would be necessary to
verify whether it is a CA certiﬁcate or not. axTLS 1.4.3 and
1.5.3 assume certiﬁcates to be CA certiﬁcates regardless of the
version number. CyaSSL 2.7.0 also does not check the version
number, though whenever the basicConstraints extension is
present, it will be used to determine whether the certiﬁcate
is a CA certiﬁcate or not. MatrixSSL 3.4.2 does check the
version number, and would check the basicConstraints exten-
sion for version 3 certiﬁcates. However, it would just assume
certiﬁcates older than version 3 to be CA certiﬁcates. The
ﬁndings on CyaSSL 2.7.0 and MatrixSSL 3.4.2 are coherent
with the relevant results reported in [43].

Finding 11 (Unrecognized critical extensions in MatrixSSL
3.4.2, CyaSSL 2.7.0, axTLS 1.4.3 and 1.5.3): Section 4.2 of
RFC 5280 states “A certiﬁcate-using system MUST reject
the certiﬁcate if it encounters a critical extension it does not
recognize or a critical extension that contains information that
it cannot process.” [2]. Not rejecting unknown critical exten-
sions could lead to interoperability issues. For example, certain
entities might deﬁne and issue certiﬁcates with additional non-
standard custom extensions, and rely on the default rejection
behavior as described in RFC 5280 to make sure that only
a speciﬁc group of implementations can handle and process
their certiﬁcates. However, we found that MatrixSSL v3.4.2
and CyaSSL 2.7.0 would accept certiﬁcates with unrecognized
critical extensions, which is consistent to the ﬁndings in [43].
In addition, we found that axTLS 1.4.3 and 1.5.3 would
also accept certiﬁcates with unrecognized critical extensions.
In fact, based on the path constraints we have extracted, they
do not recognize any of the standard extensions that we wanted
to test at all, which deviates from RFC 5280, as Section 4.2
says the minimum requirement for applications conforming
to the document MUST recognize extensions like key usage,
basic constraints, name constraints, and extended key usage,
etc. Similarly for mbedTLS 2.1.4, as we have noticed for
not implementing support for the name constraints extension,
is also noncompliant in that sense. The implication of this
is that restrictions imposed by issuing CAs in the form of
name constraints will not be honored by mbedTLS 2.1.4,
resulting in potential erroneous acceptance of certiﬁcates. At

12

the time of writing, developers of mbedTLS have indicated
that they currently have no plans on implementing support for
this extension, and suggested that application developers can
implement their own if desired.

D. Findings From Cross-Validating Libraries

The ﬁnal opportunity would be to cross-validate libraries,
speciﬁcally, for each accepting path of library A and each
rejecting path of library B, we perform a conjunction and see
if the resulting constraints would be solvable or not. If yes, it
signiﬁes a discrepancy exists between the two libraries.

Finding 12 (ExtKeyUsage OID handling in wolfSSL 3.6.6,
MatrixSSL 3.7.4): Our path constraints also unveiled that
despite being two of the few libraries that support the extended
key usage extension, both wolfSSL 3.6.6 and MatrixSSL 3.7.2
opted for a somewhat lax shortcut in handling the extension:
given the object identiﬁer (OID) of a key usage purpose,
they do a simple summation (referred colloquially as a non-
cryptographic digest function by the developers of MatrixSSL)
over all nodes of the OID, and then try to match only that sum.
For example, under such scheme, the standard usage purpose
“server authentication” (OID 1.3.6.1.5.5.7.3.1, DER-encoded
byte values are 0x2B 0x06 0x01 0x05 0x05 0x07 0x03 0x01)
would be treated as decimal 71.

Notice that the extension itself is not restricted to only hold
standard usage purposes that are deﬁned in the RFC, and
custom key usage purposes are common 4. Since OIDs are
only meant to be unique in a hierarchical manner, the sums
over nodes of OIDs are not necessarily unique. Hypothetically
some enterprises under the private enterprise arc (1.3.6.1.4.1)
could deﬁne OIDs to describe their own key usage purposes,
and if added to the extension, those OIDs might be incorrectly
treated as some of the standard key usage purposes by the two
libraries. This could be problematic for both interoperability
and security, as custom key usage purposes would be misin-
terpreted, and the standard ones could be spoofed.

This ﬁnding is a good example of how our approach can be
used to discover the exact treatments that variables undergo
inside the libraries during execution. It might also be difﬁcult
for unguided fuzzing to hit this particular problem.

We contacted the corresponding developers of the 2 libraries
regarding this, and both acknowledged the problem exists.
wolfSSL has introduced a more rigorous OID bytes checking
since version 3.7.3 5, and MatrixSSL is planning to incorporate
additional checks of the OID bytes in a new release.

Finding 13 (Incorrect interpretation of UTCTime year in
MatrixSSL 3.7.2, axTLS 1.4.3 and 1.5.3, tropicSSL): Since
UTCTime reserves only two bytes for representing the year,
one needs to be cautious when interpreting it. RFC 5280
Section 4.1.2.5.1 [2] says that when the YY of a UTCTime is

4For example, Microsoft deﬁnes its own key usage purposes and the
corresponding OIDs that are deemed meaningful to the Windows ecosystem
[96] (the extension is referred to as “Application Policy” in Microsoft
terminology, and is not to be confused with “Certiﬁcate Policy”).

5https://github.com/wolfSSL/wolfssl/commit/
d248a7660cc441b68dc48728b10256e852928ea3

larger than or equal to 50 then it should be treated as 19YY,
otherwise it should be treated as 20YY. This essentially means
that the represented range of year is 1950 to 2049 inclusively.
During cross-validation, we noticed that in certain libraries,
some legitimate years are being incorrectly rejected (and
accepted). A quick inspection of the path constraints, concrete-
value counterexamples, and ﬁnally the source code, found the
following instances of noncompliance.

Listing 2. UTCTime year adjustment in MatrixSSL 3.7.2
2000 + 10 * (c[0] - ’0’) + (c[1] - ’0’); c += 2;

y =
/* Years from ’96 through ’99 are in the 1900’s */
if (y >= 2096) { y -= 100; }

As shown in Listing 2, MatrixSSL 3.7.2 interprets any YY less
than 96 to be in the twenty ﬁrst century. This means certiﬁcates
that had expired back in 1995 would be considered valid, as
the expiration date is incorrectly interpreted to be in 2095.
On the other hand, long-living certiﬁcates that have a validity
period began in 1995 would be treated as not valid yet. The
developers acknowledged our report on this and have since
implemented a ﬁx in a new release.

Listing 3. UTCTime year adjustment in tropicSSL

to->year += 100 * (to->year < 90);
to->year += 1900;

A similar instance of noncompliance was found in tropicSSL,
as shown in Listing 3. tropicSSL interprets any YY less than
90 to be in the twenty ﬁrst century.

Listing 4. UTCTime year adjustment in axTLS 1.4.3 and 1.5.3

if (tm.tm_year <= 50) {
}
tm.tm_year += 100;

/* 1951-2050 thing */

A similar issue exists in both axTLS 1.4.3 and 1.5.3. As shown
in Listing 4, there is an off-by-one error in the condition used
to decide whether to adjust the year or not. In this case, the
year 1950 would be incorrectly considered to mean 2050.
Based on the inline comment, it seems to be a case where the
developer misinterpreted the RFC. A ﬁx has been implemented
in a new version of axTLS following our report.

Finding 14 (Incorrect timezone adjustment in MatrixSSL
3.7.2): During cross-validation with other libraries, we noticed
that the boundary of date checking in the path constraints of
MatrixSSL 3.7.2 was shifted by one day. A quick inspection of
the date time checking code found that MatrixSSL 3.7.2 uses
the localtime_r() instead of gmtime_r() to convert
the current integer epoch time into a time structure. The shift
was due to the fact that in conventional libc implementa-
tions, localtime_r() would adjust for the local time zone,
which might not necessarily be Zulu, hence deviating from the
RFC requirements.

Assuming the date time on certiﬁcates are in the Zulu
timezone,
the implication of this subtle issue is that for
systems in GMT-minus time-zones, expired certiﬁcates could
be considered still valid because of the shift, and certiﬁcates
just became valid could be considered not yet valid.
that
Similarly, for systems in GMT-plus time-zones, certiﬁcates
that are still valid might be considered expired, and future
certiﬁcates that are not yet valid would be considered valid.

13

We discussed this with the developers of MatrixSSL. They
conjectured the reason for using localtime_r() instead
of gmtime_r() was due to the latter being unavailable on
certain embedded platforms. They have agreed, however, as
MatrixSSL is gaining popularity on non-embedded platforms,
they will start using gmtime_r() on
in a new release,
platforms that support it.

Finding 15 (Overly restrictive notBefore check in CyaSSL
2.7.0 6): RFC 5280 Section 4.1.2.5 says “The validity period
for a certiﬁcate is the period of time from notBefore through
notAfter, inclusive.” However, when cross-validating CyaSSL
2.7.0 with other libraries, from the concrete counterexamples
we noticed that discrepancy exists in how the same notBefore
values would be accepted by other libraries but rejected
by CyaSSL 2.7.0, while such discrepancy was not observed
with notAfter. An inspection of the notBefore checking code
yielded the following instance of noncompliance:

Listing 5. Erroneous “less than” check in CyaSSL 2.7.0
static INLINE int DateLessThan(const struct tm* a, const

struct tm* b)

{ return !DateGreaterThan(a,b); }

Notice that the negation of > is

, not <, which explains
why if the current date time happen to be the same as the one
described in notBefore, the certiﬁcate would be considered
future (not valid yet) and rejected. Hence the notBefore
checking in CyaSSL 2.7.0 turns out to be overly restrictive
than what the RFC mandates.

≤

This is again a new result, comparing to the previous work
[43] that also studied CyaSSL 2.7.0. Our conjecture is that
given a large number of possible values, it might be difﬁcult
for unguided fuzzing to hit boundary cases, hence such a subtle
logical error eluded their analysis.

Finding 16 (KeyUsage and ExtKeyUsage being ignored
in PolarSSL 1.2.8): The fact that PolarSSL 1.2.8 does not
check KeyUsage and ExtKeyUsage, evaded our simple search
approach but was caught during cross-validation, as the im-
plementation actually parses the two extensions, hence some
constraints were added as the result of several basic san-
ity checks happened during parsing. However, during cross-
validation, it became clear that apart from the parsing sanity
checks, PolarSSL 1.2.8 does not do any meaningful checks on
KeyUsage and ExtKeyUsage.

In fact, this resulted in another instance of noncompliance,
as PolarSSL 1.2.8 would not reject certiﬁcates with KeyUsage
or ExtKeyUsage, even if those two extensions were made
critical, and it does not perform any meaningful checks apart
from merely parsing them. This is an example where a library
is intended to handle an extension but was not able to, because
of incomplete implementation.

This is consistent with similar results reported in [43],
although the ﬁnding that PolarSSL 1.2.8 does not check the
KeyUsage extension on intermediate CA certiﬁcates was not
reported in that paper.

Finding 17 (pathLenConstraint of trusted root misinter-
preted in tropicSSL): During cross validation, it became clear
to us that, in tropicSSL: (1) on one hand, some accepting
paths would allow the pathLenConstraint variable to be 0;
(2) on the other hand, some rejecting paths reject because the
pathLenConstraint was deemed to be smaller than an unex-
pectedly large boundary. In both cases, the pathLenConstraint
variable appears to have been misinterpreted by tropicSSL.

We suspect that this might be due to the value 0 in the
internal parsed certiﬁcate data structure is used to capture the
case where the pathLenConstraint variable is absent (i.e. no
limit is imposed). A quick inspection of the parsing code
revealed that our suspicion is indeed correct. In fact,
the
parsing code is supposed to always add 1 to the variable if it
is present on the certiﬁcate, but a coding error 7 of missing a
dereferencing operator (*) in front of an integer pointer means
that the increment was applied to the pointer itself but not the
value, hence the observed behavior described above.

This subtle bug has a severe implication: it completely de-
feats the purpose of imposing such restriction on a certiﬁcate,
as a pathLenConstraint of 0 would be incorrectly treated to
mean that the chain length could be unlimited.

Finding 18 (Not critical means not a CA in tropicSSL):
During cross validation, we also noticed that when the interme-
diate CA certiﬁcate’s basicConstraints extension is set to non-
critical, and the isCA boolean is set to True, tropicSSL would
consider the intermediate CA certiﬁcate not a CA certiﬁcate.
Additionally, in the path constraints, the symbolic variable
representing the criticality of basicConstraints and the one that
represents the isCA boolean are always in conjunction through
a logical AND.

A quick inspection found the following problem in the

parsing code that handles the basicConstraints extension:

Listing 6.

Incorrect adjustment to the isCA boolean in tropicSSL

*ca_istrue = is_critical & is_cacert;

This interpretation of the basicConstraints extension de-
viates from the speciﬁcation, as RFC 5280 says that clients
should process extensions that they can recognize, regardless
of whether the extension is critical or not. The criticality of
basicConstraints should not affect the semantic meaning of
attributes in the extension itself. This is an example of a CCVL
being overly restrictive.

E. Other ﬁndings

Here we present other interesting ﬁndings that are not
explicitly noncompliant behaviors deviating from RFC 5280.
Extra 1 (Ineffective date string sanity check in MatrixSSL
3.7.2): During cross-validation, we noticed that date time byte
values in MatrixSSL 3.7.2 are not bounded for exceedingly
large or unexpectedly small values. However,
in the con-
straints, we see combinations of whether each byte is too
small or not (though not affecting the acceptance decision),
which looked suspiciously like a failed lower boundary check.
A quick inspection of the certiﬁcate parsing code unveiled the

6This has been ﬁxed in newer versions of CyaSSL and WolfSSL.

7This has been ﬁxed in later versions of PolarSSL and mbedTLS.

14

snippet shown in Listing 7 that is meant to vet a given date
string from a certiﬁcate, and reject it with a parser error if the
values are outside of an expected range. Unfortunately, due
to incorrectly using the && operator instead of ||, the if
conditions are never satisﬁable. This is also proven by the fact
that if we symbolically execute the code snippet in Listing 7,
all possible execution paths returns 1. Consequently that code
snippet would actually never reject any given strings, hence
completely defeating the purpose of having a sanity check.

Listing 7. date string sanity check in MatrixSSL 3.7.2

/* 4 character year */

if (utctime != 1) {
if (*c < ’1’ && *c > ’2’) return 0; c++; /* Year */
if (*c < ’0’ && *c > ’9’) return 0; c++;
}
if (*c < ’0’ && *c > ’9’) return 0; c++;
if (*c < ’0’ && *c > ’9’) return 0; c++;
if (*c < ’0’ && *c > ’1’) return 0; c++;
if (*c < ’0’ && *c > ’9’) return 0; c++;
if (*c < ’0’ && *c > ’3’) return 0; c++;
if (*c < ’0’ && *c > ’9’) return 0;
return 1;

/* Day */

/* Month */

Following our report, the developers of MatrixSSL have ac-
knowledged this is indeed a faulty implementation. Along with
other ﬁxes being implemented to make date-time processing
more robust, they have decided that this sanity check will no
longer be used in newer versions of MatrixSSL.

Extra 2 (notBefore and notAfter bytes taken “as is” in
CyaSSL 2.7.0, WolfSSL 3.6.6, axTLS 1.4.3 and 1.5.3): For
the four aforementioned implementations, we noticed during
cross-validation that they do not perform any explicit boundary
checks on the value of the date time value bytes of notBefore
and notAfter, and just assumed that those bytes are going to
be valid ASCII digits (i.e. 0–9). It is hence possible to put
other ASCII characters in the date time bytes and obtain an
exceptionally large (small) values for notAfter (notBefore),
though this does not seem to be an imminent threat, nor
does it violate the RFC, as the RFC did not stipulate what
implementations should do.

Extra 3 (Timezone Handling): Another discrepancy that
we have observed during cross-validating path constraints of
different libraries was how they impose/assume the time zone
of notBefore and notAfter on certiﬁcates. Speciﬁcally, we
notice that mbedTLS 2.1.4 and wolfSSL v2.3.3 would reject
certiﬁcates that do not have the timezone ending with a ‘Z’.
This is possibly due to the fact that RFC 5280 [2] mandates
conforming CAs to express validity in Zulu time (a.k.a GMT
or Zero Meridian Time) when issuing certiﬁcates, regardless
of the type being UTCTime or GeneralizedTime. Other imple-
mentations like MatrixSSL 3.7.2, axTLS 1.5.3 and PolarSSL
1.2.8 ignore the timezone character and simply assume the
Zulu timezone is always being used.

This is arguably an example of under-speciﬁcation, as it is
not clear whether implementations should try to handle (with
proper time zone adjustment) or reject certiﬁcates with a non-
Zulu timezone, since RFC 5280 [2] did not explicitly mandate
an expected behavior.

15

VII. DISCUSSION
A. Takeaway for Application Developers

As a takeaway for application developers that need to use
SSL/TLS libraries for processing X.509 certiﬁcates, a general
rule of thumb is to upgrade to newer versions of the libraries
if possible. As demonstrated by our ﬁndings, newer versions
of implementations, even when originated from the same
source tree as their legacy counterparts, are better equipped
in terms of features and extension handling, as well as in
general having more rigorous checks. Holding on to legacy
code could potentially hurt both security and interoperability.
Unfortunately, regular software patching, particularly for IoT
devices, does not seem to happen widespread enough [97].

We understand that due to the needs to optimize for different
application scenarios (e.g. small footprint for resource con-
strained platforms), certain features might not be implemented
in their entirety as described in the standard speciﬁcations.
In order to help application developers to better understand
the trade-offs and make a more well-informed decision in
choosing which SSL/TLS library to use, we believe that one
possibility would be to have a certiﬁcation program that tests
for implementation conformance and interoperability, similar
to that of the IPv6 Ready Logo Program [98], and the High
Deﬁnition Logos [99]. For example, an “X.509 Gold” for
libraries that implement most required features correctly, and
an “X.509 Ready” for libraries that can only handle the bare
minimum but are missing out on certain features.

B. Limitations

Since our noncompliance detection approach critically relies
on symbolic execution which is known to suffer from path ex-
plosion, especially in the presence of symbolic data-dependent
loops, it is deliberately made to trade away completeness for
practicality (i.e., our approach is not guaranteed to reveal all
possible noncompliances in an implementation and can have
false negatives).

Our current scope of analysis does not include the logic for
checking certiﬁcation revocation status and hostname match-
ing. As noted in [43], for both revocation status checking
and hostname matching, while some libraries provide relevant
facilities, some delegate the task to application developers. In
addition, a typical implementation of a hostname matching
logic uses complex string operations and analyzing these
require a dedicated SMT solver with support for the theory
of strings [100]. We leave that for future work.

Moreover, as we use concrete values in SymCerts, symbolic
execution sway away from rigorously exercising the parsing
logic. Though we have uncovered parsing bugs as reported in
Section VI, our scrutiny on the parsing code is not meant to
be comprehensive. Noticeably, low-level memory errors due to
incorrect buffer management in the parsing code, as reported
in a recent Vulnerability Note [101], can elude our analysis.

C. Threat to Validity

In some cases during certiﬁcate validation, it is not clear
who is required to perform the validity check on a ﬁeld, i.e.,

the underlying library or the application using the library.
The RFC states that some speciﬁc validity check must be
performed without clearly identifying the responsible party.
This unclear separation of responsibilities have resulted in
libraries opting for signiﬁcantly different API designs. We rely
on example usage—often come with the source code in the
form of a sample client—to draw a boundary for extracting the
approximated certiﬁcate accepting (and rejecting) universes.
Optional function calls to extra checking logics, if not demon-
strated in the sample client programs, will be missed by our
analysis. Additionally, if some of the checks performed on
certiﬁcates are being pushed down to a different phase during
SSL/TLS handshake instead of the server certiﬁcate validation
phase, these checks might be missing from our extraction. We
rely on the concrete client-server replay setup to catch them
and iteratively include them in the extraction.

Our optimization often rely on the expectation that the value
of some ﬁelds are handled in the implementation in an uniform
way. For checking validity of ﬁelds that can have variable
lengths, we assume the implementation treats each regular
length (not corner cases) uniformly. In addition, we also
assume that the semantic independence of certain certiﬁcate
ﬁelds are maintained in the implementation. For instance, we
assume that the certiﬁcate validity ﬁelds are not dependent on
any other ﬁelds. Although we have observed that this seems to
be the case and the RFC supports it, hypothetically a developer
can mistakenly create an artiﬁcial dependency.

VIII. CONCLUSION AND FUTURE DIRECTION

In this paper, we present a novel approach that leverages
symbolic execution to ﬁnd noncompliance in X.509 implemen-
tations. In alignment with the general consensus, we observe
that due to the recursive nature of certiﬁcate representation,
an off-the-shelf symbolic execution engine suffers from path
explosion problem. We overcome this inherent challenge in
two ways: (1) Focusing on real implementations with a small
resource footprint; (2) Leveraging domain-speciﬁc insights,
abstractions, and compartmentalization. We use SymCerts—
certiﬁcate chains in which each certiﬁcate has a mix of
symbolic and concrete values—such that symbolic execution
can be made scalable on many X.509 implementations while
meaningful analysis can be conducted.

We applied our noncompliance approach to analyze 9 real
implementations selected from 4 major families of SSL/TLS
source base. Our analysis exposed 48 instances of noncom-
pliance, some of which has severe security implications. We
have responsibly shared our new ﬁndings with the respective
library developers. Most of our reports have generated positive
acknowledgments from the developers, and led to the imple-
mentation of ﬁxes to the said problems in new releases.
We now identify possible ways of extending our work.

The Vision of Fully Automatic Noncompliance Detection:
Our current approach is a signiﬁcant ﬁrst step towards a fully
automatic noncompliance detection approach for X.509 CCVL
implementation. We envision the following two automatic
approaches that can leverage our current work.

(1) Signature-driven Automated Noncompliance Detection: Vul-
nerability signatures capture properties that should not hold
If one can represent a
for any CCVL implementations.
vulnerability signature as a QFFOL formula Ψ, then for a
given implementation I, we can check, using an SMT solver,
whether there exists a certiﬁcate in I’s certiﬁcate accepting
universe that satisﬁes Ψ.

(2) Cross Validation With a Reference Implementation: For this
approach, one would need to develop a formally veriﬁed ref-
erence implementation of X.509 CCVL, Iref . Such a reference
implementation can be highly valuable and is also sought after
by the research community, as indicated by a panel in a NSF
workshop titled “Formal Methods for Security” [102]. For
automatically detecting noncompliance of a given implementa-
tion Itest, one can simply cross validate Itest against Iref using
our automatic cross validation approach.

Analyzing Libraries for Conventional Systems: SSL/TLS
traditional systems (e.g.,
libraries that are developed for
OpenSSL, GnuTLS, Mozilla NSS) are not included in our
current analysis. We leave the analysis of these libraries as a
subject of future work.

ACKNOWLEDGMENT

We thank the various developers of the SSL/TLS libraries
for their efforts in implementing and maintaining all
the
tested open source implementations. In particular, we sincerely
appreciate the developers of WolfSSL, MatrixSSL and axTLS,
for giving us prompt feedbacks and implementing new ﬁxes in
a timely manner. We would also like to thank the anonymous
reviewers for their helpful comments. The work reported here
serves as the basis for the NSF grant CNS-1657124; it was
supported in part by a Purdue Research Foundation award, and
the NSF grants CNS-1314688 and CNS-1421815.

REFERENCES

[1] ITU-T Recommendation X.509 (2005) — ISO/IEC 9594-8:2005, “In-
formation technology - Open Systems Interconnection - The Directory:
Public-key and attribute certiﬁcate frameworks,” International Telecom-
munication Union, 2005.

[2] D. Cooper, S. Santesson, S. Farrell, S. Boeyen, R. Housley,
and W. Polk, “Internet X.509 Public Key Infrastructure Certiﬁcate
and Certiﬁcate Revocation List (CRL) Proﬁle,” Internet Requests
for Comments, Tech. Rep. 5280, May 2008. [Online]. Available:
http://www.ietf.org/rfc/rfc5280.txt

[3] B. Beurdouche, K. Bhargavan, A. Delignat-Lavaud, C. Fournet,
M. Kohlweiss, A. Pironti, P.-Y. Strub, and J. K. Zinzindohoue, “A
messy state of the union: Taming the composite state machines of
TLS,” in IEEE Symposium on Security and Privacy, 2015.

[4] J. De Ruiter and E. Poll, “Protocol state fuzzing of TLS implemen-
tations,” in 24th USENIX Security Symposium (USENIX Security 15),
2015, pp. 193–206.

[5] J. Somorovsky, “Systematic fuzzing and testing of tls libraries,” in
Proceedings of the 2016 ACM SIGSAC Conference on Computer and
Communications Security. ACM, 2016, pp. 1492–1504.

[6] D. Kaloper-Merˇsinjak, H. Mehnert, A. Madhavapeddy, and P. Sewell,
“Not-quite-so-broken TLS: lessons in re-engineering a security protocol
speciﬁcation and implementation,” in 24th USENIX Security Sympo-
sium (USENIX Security 15), 2015, pp. 223–238.

[7] K. Bhargavan, C. Fournet, M. Kohlweiss, A. Pironti, and P. Strub,
“Implementing TLS with veriﬁed cryptographic security,” in Security
and Privacy (SP), 2013 IEEE Symposium on.
IEEE, 2013, pp. 445–
459.

16

[8] B. Beurdouche, A. Delignat-Lavaud, N. Kobeissi, A. Pironti, and
K. Bhargavan, “FLEXTLS: A Tool for Testing TLS Implementations,”
the 9th USENIX Conference on Offensive
in Proceedings of
Technologies,
Berkeley, CA, USA: USENIX
Association, 2015, pp. 1–1. [Online]. Available: http://dl.acm.org/
citation.cfm?id=2831211.2831212

ser. WOOT’15.

[9] “CVE-2016-1115,” https://web.nvd.nist.gov/view/vuln/detail?vulnId=

CVE-2016-1115.

[39] M. J. C. Gordon and T. F. Melham, Eds., Introduction to HOL: A
Theorem Proving Environment for Higher Order Logic. New York,
NY, USA: Cambridge University Press, 1993.

[40] J. Jaffar, V. Murali, J. Navas, and A. Santosa, “Tracer: A symbolic
execution tool for veriﬁcation,” in Computer Aided Veriﬁcation, ser.
Lecture Notes in Computer Science, P. Madhusudan and S. Seshia,
Eds. Springer Berlin Heidelberg, 2012, vol. 7358, pp. 758–766.
[41] E. M. Clarke, S. Jha, and W. Marrero, “Verifying security protocols

[10] “CVE-2016-5669,” https://web.nvd.nist.gov/view/vuln/detail?vulnId=

with brutus,” TOSEM, vol. 9, no. 4, 2000.

CVE-2016-5669.

[42] P. Godefroid, “Model checking for programming languages using

[11] “CVE-2016-5672,” https://web.nvd.nist.gov/view/vuln/detail?vulnId=

verisoft,” in POPL. ACM, 1997, pp. 174–186.

CVE-2016-5672.

[12] “CVE-2016-2180,” https://web.nvd.nist.gov/view/vuln/detail?vulnId=

CVE-2016-2180.

[13] “CVE-2016-5655,” https://web.nvd.nist.gov/view/vuln/detail?vulnId=

CVE-2016-5655.

[43] C. Brubaker, S. Jana, B. Ray, S. Khurshid, and V. Shmatikov, “Using
frankencerts for automated adversarial testing of certiﬁcate validation in
SSL/TLS implementations,” in Security and Privacy (SP), 2014 IEEE
Symposium on.

IEEE, 2014, pp. 114–129.

[44] J. C. King, “Symbolic execution and program testing,” Communications

[14] “CVE-2016-3664,” https://web.nvd.nist.gov/view/vuln/detail?vulnId=

of the ACM, vol. 19, no. 7, pp. 385–394, 1976.

CVE-2016-3664.

[15] “CVE-2016-2113,” https://web.nvd.nist.gov/view/vuln/detail?vulnId=

CVE-2016-2113.

[16] “CVE-2016-1563,” https://web.nvd.nist.gov/view/vuln/detail?vulnId=

CVE-2016-1563.

[17] “CVE-2016-2562,” https://web.nvd.nist.gov/view/vuln/detail?vulnId=

CVE-2016-2562.

[18] “CVE-2016-2047,” https://web.nvd.nist.gov/view/vuln/detail?vulnId=

CVE-2016-2047.

[19] “CVE-2015-5655,” https://web.nvd.nist.gov/view/vuln/detail?vulnId=

CVE-2015-5655.

[20] “CVE-2014-0092,”

https://cve.mitre.org/cgi-bin/cvename.cgi?name=

CVE-2014-0092.

[21] “CVE-2014-1266,”

https://cve.mitre.org/cgi-bin/cvename.cgi?name=

CVE-2014-1266.

[22] A. M. Turing, “On computable numbers, with an application to
the entscheidungsproblem,” Proceedings of the London Mathematical
Society, vol. 2, no. 42, pp. 230–265, 1936.

[23] C. Hawblitzel, J. Howell, M. Kapritsos, J. Lorch, B. Parno, M. Roberts,
S. Setty, and B. Zill, “Ironﬂeet: Proving practical distributed systems
correct,” in SOSP, 2015.

[24] K. L. McMillan, Symbolic model checking. Springer, 1993.
[25] T. Ball, R. Majumdar, T. Millstein, and S. K. Rajamani, “Automatic
predicate abstraction of c programs,” in ACM SIGPLAN Notices,
vol. 36, no. 5. ACM, 2001, pp. 203–213.

[45] C. Cadar, D. Dunbar, and D. R. Engler, “Klee: Unassisted and
automatic generation of high-coverage tests for complex systems
programs.” in OSDI, 2008, pp. 209–224.

[46] C. Cadar, P. Godefroid, S. Khurshid, C. S. P˘as˘areanu, K. Sen, N. Till-
mann, and W. Visser, “Symbolic execution for software testing in
practice: preliminary assessment,” in ICSE, 2011, pp. 1066–1071.
[47] C. Cadar and K. Sen, “Symbolic execution for software testing: three
decades later,” Communications of the ACM, vol. 56, no. 2, pp. 82–90,
2013.

[48] V. Kuznetsov, J. Kinder, S. Bucur, and G. Candea, “Efﬁcient
state merging in symbolic execution,” in Proceedings of
the 33rd
ACM SIGPLAN Conference on Programming Language Design and
Implementation, ser. PLDI
’12. New York, NY, USA: ACM,
2012, pp. 193–204. [Online]. Available: http://doi.acm.org/10.1145/
2254064.2254088

[49] HTTPS client

the Photon!, 2015 (accessed Nov
3, 2016), https://community.particle.io/t/https-client-is-here-for-the-
photon-by-the-glowﬁ-sh-team/15934.

is here for

[50] spark / ﬁrmware / communication / lib, 2016 (accessed Nov 3, 2016),
https://github.com/spark/ﬁrmware/tree/master/communication/lib.
[51] Arduino/libraries/ESP8266WiFi/src/include/ssl.h, 2016 (accessed Feb
2, 2017), https://github.com/esp8266/Arduino/blob/master/libraries/
ESP8266WiFi/src/include/ssl.h.

[52] micropython/extmod, 2017 (accessed Feb 2, 2017), https://github.com/

micropython/micropython/tree/master/extmod.

[26] T. Andrews, S. Qadeer, S. K. Rajamani, J. Rehof, and Y. Xie, “Zing:

[53] “CVE-2016-6303,” https://web.nvd.nist.gov/view/vuln/detail?vulnId=

A model checker for concurrent software,” in CAV, 2004.

CVE-2016-6303.

[27] G. Holzmann and M. Smith, “A practical method for verifying event-

[54] “CVE-2016-7052,” https://web.nvd.nist.gov/view/vuln/detail?vulnId=

driven software,” in ICSE. ACM, 1999.

CVE-2016-7052.

[28] R. Jhala and R. Majumdar, “Software model checking,” ACM Comput-

[55] “CVE-2016-6305,” https://web.nvd.nist.gov/view/vuln/detail?vulnId=

ing Surveys (CSUR), vol. 41, no. 4, p. 21, 2009.

CVE-2016-6305.

[29] T. A. Henzinger, R. Jhala, R. Majumdar, and G. Sutre, “Lazy abstrac-
tion,” in ACM SIGPLAN Notices, vol. 37, no. 1. ACM, 2002, pp.
58–70.

[56] P.-H. Kamp, “Please put openssl out of

its misery,” Queue,
vol. 12, no. 3, pp. 20:20–20:23, Apr. 2014. [Online]. Available:
http://doi.acm.org/10.1145/2602649.2602816

[30] E. Clarke, O. Grumberg, S. Jha, Y. Lu, and H. Veith, “Counterexample-

[57] how to install curl and libcurl, (accessed Nov 3, 2016), https://

guided abstraction reﬁnement,” in CAV, 2000, pp. 154–169.

curl.haxx.se/docs/install.html.

[31] B. S. Gulavani, T. A. Henzinger, Y. Kannan, A. V. Nori, and S. K.
Rajamani, “Synergy: A new algorithm for property checking,” in FSE,
2006.

[32] T. Ball and S. Rajamani, “The slam project: Debugging system software

via static analysis,” SIGPLAN Not., vol. 37, no. 1, 2002.

[33] D. Beyer, T. Henzinger, R. Jhala, and R. Majumdar, “The software
model checker blast: Applications to software engineering,” Int. J.
Softw. Tools Technol. Transf., vol. 9, no. 5, 2007.

[34] S. Chaki, E. M. Clarke, A. Groce, S. Jha, and H. Veith, “Modular
veriﬁcation of software components in C,” IEEE Trans. Software Eng.,
vol. 30, no. 6, pp. 388–402, 2004.

[35] E. Clarke, D. Kroening, N. Sharygina, and K. Yorav, “Satabs: Sat-based

predicate abstraction for ansi-c,” in TACAS. Springer-Verlag, 2005.

[36] J. Esparza, S. Kiefer, and S. Schwoon, “Abstraction reﬁnement with
Craig interpolation and symbolic pushdown systems,” in TACAS.
Springer, 2006.

[37] S. L¨owe, “Cpachecker with explicit-value analysis based on cegar and

interpolation,” in TACAS. Springer-Verlag, 2013.

[38] G. Brat, K. Havelund, S. Park, and W. Visser, “Model checking
programs,” in IEEE International Conference on Automated Software
Engineering (ASE). Citeseer, 2000, pp. 3–12.

[58] ipsvd - internet protocol service daemons - installation, (accessed Feb

2, 2017), http://smarden.org/ipsvd/install.html.

[59] Package: gatling (0.12cvs20120114-4) high performance web server
and ﬁle server, (accessed Feb 2, 2017), https://packages.debian.org/
wheezy/gatling.

[60] A Python library that encapsulates wolfSSL’s wolfCrypt API., (accessed

Feb 2, 2017), https://pypi.python.org/pypi/wolfcrypt/0.2.0.

[61] mbed TLS (PolarSSL) wrapper,

(accessed Feb 2, 2017), https://

pypi.python.org/pypi/python-mbedtls/0.6.

[62] W. M. McKeeman, “Differential testing for software,” DIGITAL TECH-

NICAL JOURNAL, vol. 10, no. 1, pp. 100–107, 1998.

[63] R. B. Evans and A. Savoia, “Differential testing: A new approach to
change detection,” in The 6th Joint Meeting on European Software
Engineering Conference and the ACM SIGSOFT Symposium on the
Foundations of Software Engineering: Companion Papers, ser. ESEC-
FSE companion ’07. New York, NY, USA: ACM, 2007, pp. 549–552.
[Online]. Available: http://doi.acm.org/10.1145/1295014.1295038
[64] C. Barrett, R. Sebastiani, S. Seshia, and C. Tinelli, “Satisﬁability
modulo theories,” in Handbook of Satisﬁability, A. Biere, M. J. H.
Heule, H. van Maaren, and T. Walsh, Eds.
IOS Press, February 2009,
vol. 185, ch. 26, pp. 825–885.

17

[86] L. D’Antoni and M. Veanes, “Minimization of symbolic automata,”
SIGPLAN Not., vol. 49, no. 1, pp. 541–553, Jan. 2014. [Online].
Available: http://doi.acm.org/10.1145/2578855.2535849

[87] G. Argyros, I. Stais, A. Kiayias, and A. D. Keromytis, “Back in black:
towards formal, black box analysis of sanitizers and ﬁlters,” in Security
and Privacy (SP), 2016 IEEE Symposium on.
IEEE, 2016, pp. 91–109.
[88] George Argyros and Ioannis Stais and Suman Jana and Angelos
D. Keromytis and Aggelos Kiayias, “SFADiff: Automated Evasion
Attacks and Fingerprinting Using Black-box Differential Automata
Learning,” in Proceedings of the 23rd ACM Conference on Computer
and Communications Security, Vienna, Austria, Oct 2016.

[89] C. Adams and S. Lloyd, Understanding PKI: Concepts, Standards, and
Deployment Considerations, 2nd ed. Boston, MA, USA: Addison-
Wesley Longman Publishing Co., Inc., 2002.

[90] G. Nelson and D. C. Oppen, “Fast decision procedures based on
congruence closure,” J. ACM, vol. 27, no. 2, pp. 356–364, Apr. 1980.
the LDAP and X.500
[Online].

[91] S. Legg, “ASN.1 Module Deﬁnition for

Component Matching Rules,” RFC 3727, Mar. 2013.
Available: https://rfc-editor.org/rfc/rfc3727.txt

[92] C. Gardiner and C. Wallace, “ASN.1 Translation,” RFC 6025, Oct.

2015. [Online]. Available: https://rfc-editor.org/rfc/rfc6025.txt

[93] “ASN.1 JavaScript decoder,” https://lapo.it/asn1js/.
[94] wolfSSL ChangeLog, 2016 (accessed Oct 22, 2016), https://

www.wolfssl.com/wolfSSL/Docs-wolfssl-changelog.html.

[95] mbed TLS 2.2.0, 2.1.3, 1.3.15 and PolarSSL 1.2.18 released, 2015
(accessed Mar 14, 2017), https://tls.mbed.org/tech-updates/releases/
mbedtls-2.2.0-2.1.3-1.3.15-and-polarssl.1.2.18-released.

[96] Certiﬁcate Template Extensions: Application Policy, (accessed Oct
https://technet.microsoft.com/en-us/library/cc731792(v=

28,
2016),
ws.11).aspx.

[97] C. Young, Flawed MatrixSSL Code Highlights Need

for
IoT Update Practices, 2016 (accessed Feb 6, 2017),

Better
http://www.tripwire.com/state-of-security/security-data-protection/
cyber-security/ﬂawed-matrixssl-code-highlights-need-for-better-iot-
update-practices/.

[98] IPv6 Ready Logo Program, 2016 (accessed Sept 04, 2016), https://

www.ipv6ready.org.

[99] High Deﬁnition Logos, 2016 (accessed Sept 04, 2016), http://

www.digitaleurope.org/Services/High-Deﬁnition-Logos.

[100] T. Liang, A. Reynolds, C. Tinelli, C. Barrett, and M. Deters, A DPLL(T)
Theory Solver for a Theory of Strings and Regular Expressions.
Cham: Springer International Publishing, 2014, pp. 646–662. [Online].
Available: http://dx.doi.org/10.1007/978-3-319-08867-9 43

[101] Vulnerability Note VU#396440 - MatrixSSL contains multiple vulner-
abilities, 2016 (accessed Feb 6, 2017), http://www.kb.cert.org/vuls/id/
396440.

[102] S. Chong, J. Guttman, A. Datta, A. C. Myers, B. Pierce, P. Schaumont,
T. Sherwood, and N. Zeldovich, “Report on the NSF workshop
on formal methods for security,” CoRR, vol. abs/1608.00678, 2016.
[Online]. Available: http://arxiv.org/abs/1608.00678

[65] V. Ganesh and D. L. Dill, “A decision procedure for bit-vectors
the 19th International Conference
Berlin, Heidelberg:

and arrays,” in Proceedings of
on Computer Aided Veriﬁcation, ser. CAV’07.
Springer-Verlag, 2007, pp. 519–531.

[66] L. S. Huang, A. Rice, E. Ellingsen, and C. Jackson, “Analyzing Forged
SSL Certiﬁcates in the Wild,” in Security and Privacy (SP), 2014 IEEE
Symposium on.

IEEE, 2014, pp. 83–97.

[67] C. Hlauschek, M. Gruber, F. Fankhauser, and C. Schanes, “Prying open
Pandora’s box: KCI attacks against TLS,” in 9th USENIX Workshop
on Offensive Technologies (WOOT 15), 2015.

[68] A. Bates, J. Pletcher, T. Nichols, B. Hollembaek, D. Tian, K. R. Butler,
and A. Alkhelaiﬁ, “Securing ssl certiﬁcate veriﬁcation through dynamic
linking,” in Proceedings of the 2014 ACM SIGSAC Conference on
Computer and Communications Security. ACM, 2014, pp. 394–405.
Iyengar, S. Jana, R. Anubhai, D. Boneh, and
V. Shmatikov, “The most dangerous code in the world: validating SSL
certiﬁcates in non-browser software,” in Proceedings of the 2012 ACM
conference on Computer and communications security. ACM, 2012,
pp. 38–49.

[69] M. Georgiev, S.

[70] K. Bhargavan, A. D. Lavaud, C. Fournet, A. Pironti, and P. Y.
Strub, “Triple handshakes and cookie cutters: Breaking and ﬁxing
authentication over TLS,” in Security and Privacy (SP), 2014 IEEE
Symposium on.

IEEE, 2014, pp. 98–113.

[71] B. He, V. Rastogi, Y. Cao, Y. Chen, V. Venkatakrishnan, R. Yang, and
Z. Zhang, “Vetting ssl usage in applications with sslint,” in 2015 IEEE
Symposium on Security and Privacy.

IEEE, 2015, pp. 519–534.

[72] I. Yun, C. Min, X. Si, Y. Jang, T. Kim, and M. Naik, “Apisan: Sanitizing
api usages through semantic cross-checking,” in 25th USENIX Security
Symposium (USENIX Security 16). Austin, TX: USENIX Association,
Aug. 2016, pp. 363–378. [Online]. Available: https://www.usenix.org/
conference/usenixsecurity16/technical-sessions/presentation/yun
[73] C. Cadar, V. Ganesh, P. M. Pawlowski, D. L. Dill, and D. R. Engler,
“Exe: automatically generating inputs of death,” TISSEC, vol. 12, no. 2,
2008.

[74] K. Sen, D. Marinov, and G. Agha, “Cute: A concolic unit testing engine

for c,” in ESEC/FSE. ACM, 2005.

[75] M. Das, S. Lerner, and M. Seigle, “Esp: Path-sensitive program

veriﬁcation in polynomial time,” in PLDI, 2002.

[76] P. Godefroid, M. Y. Levin, and D. Molnar, “Sage: whitebox fuzzing

for security testing,” Queue, vol. 10, no. 1, p. 20, 2012.

[77] V. Chipounov, V. Kuznetsov, and G. Candea, “S2e: a platform for in-
vivo multi-path analysis of software systems,” in ASPLOS, 2012.
[78] P. Godefroid, N. Klarlund, and K. Sen, “DART: Directed automated

random testing,” in PLDI. ACM, 2005.

[79] P. Godefroid, P. de Halleux, A. V. Nori, S. K. Rajamani, W. Schulte,
N. Tillmann, and M. Y. Levin, “Automating software testing using
program analysis,” Software, IEEE, vol. 25, no. 5, pp. 30–37, 2008.

[80] D. A. Ramos and D. R. Engler, “Practical, low-effort equivalence
veriﬁcation of real code,” in Proceedings of the 23rd International
Conference on Computer Aided Veriﬁcation, ser. CAV’11. Berlin,
Heidelberg: Springer-Verlag, 2011, pp. 669–685. [Online]. Available:
http://dl.acm.org/citation.cfm?id=2032305.2032360

[81] D. A. Ramos and D. Engler, “Under-constrained symbolic execution:
Correctness checking for real code,” in 24th USENIX Security Sympo-
sium (USENIX Security 15). Washington, D.C.: USENIX Association,
Aug. 2015, pp. 49–64. [Online]. Available: https://www.usenix.org/
conference/usenixsecurity15/technical-sessions/presentation/ramos
[82] L. Pedrosa, A. Fogel, N. Kothari, R. Govindan, R. Mahajan, and
T. Millstein, “Analyzing protocol implementations for interoperability,”
in NSDI, 2015.

[83] M. Canini, D. Venzano, P. Pereˇs´ıni, D. Kosti´c, and J. Rexford, “A nice

way to test openﬂow applications,” in NSDI, 2012.

[84] M. Kuzniar, P. Peresini, M. Canini, D. Venzano, and D. Kostic,
“A soft way for openﬂow switch interoperability testing,” in
Proceedings of
the 8th International Conference on Emerging
Networking Experiments and Technologies, ser. CoNEXT ’12. New
York, NY, USA: ACM, 2012, pp. 265–276. [Online]. Available:
http://doi.acm.org/10.1145/2413176.2413207

[85] C. Min, S. Kashyap, B. Lee, C. Song, and T. Kim, “Cross-checking
semantic correctness: The case of ﬁnding ﬁle system bugs,” in
Proceedings of the 25th Symposium on Operating Systems Principles,
ser. SOSP ’15. New York, NY, USA: ACM, 2015, pp. 361–377.
[Online]. Available: http://doi.acm.org/10.1145/2815400.2815422

18


