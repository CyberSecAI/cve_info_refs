=== Content from bugs.launchpad.net_39f62ef7_20250114_183602.html ===

[Log in / Register](https://bugs.launchpad.net/nova/%2Bbug/2004555/%2Blogin)

[![](https://launchpadlibrarian.net/324577028/nova64.png)](https://launchpad.net/nova)

## [OpenStack Compute (nova)](https://launchpad.net/nova)

* [Overview](https://launchpad.net/nova)
* [Code](https://code.launchpad.net/nova)
* [Bugs](https://bugs.launchpad.net/nova)
* [Blueprints](https://blueprints.launchpad.net/nova)
* [Translations](https://translations.launchpad.net/nova)
* [Answers](https://answers.launchpad.net/nova)

# [OSSA-2023-003] Unauthorized volume access through deleted volume attachments (CVE-2023-2088)

Bug #2004555 reported by
[Jan Wasilewski](https://launchpad.net/~janwasilewski)
on 2023-02-02

[294](/%2Bhelp-bugs/bug-heat.html)

This bug affects 2 people

| Affects | | Status | Importance | Assigned to | Milestone |
| --- | --- | --- | --- | --- | --- |
|  | [Cinder](https://bugs.launchpad.net/cinder) | Fix Released | Undecided | Unassigned |  |
|  | [OpenStack Compute (nova)](https://bugs.launchpad.net/nova) | Fix Released | Undecided | Unassigned |  |
|  | [Antelope](https://bugs.launchpad.net/nova/antelope) | Fix Released | Undecided | Unassigned |  |
|  | [Wallaby](https://bugs.launchpad.net/nova/wallaby) | Fix Released | Undecided | Unassigned |  |
|  | [Xena](https://bugs.launchpad.net/nova/xena) | Fix Released | Undecided | Unassigned |  |
|  | [Yoga](https://bugs.launchpad.net/nova/yoga) | Fix Released | Undecided | Unassigned |  |
|  | [Zed](https://bugs.launchpad.net/nova/zed) | Fix Released | Undecided | Unassigned |  |
|  | [OpenStack Security Advisory](https://bugs.launchpad.net/ossa) | Fix Released | High | [Jeremy Stanley](https://launchpad.net/~fungi) |  |
|  | [OpenStack Security Notes](https://bugs.launchpad.net/ossn) | Fix Released | High | [Jeremy Stanley](https://launchpad.net/~fungi) |  |
|  | [glance\_store](https://bugs.launchpad.net/glance-store) | Fix Released | Undecided | Unassigned |  |
|  | [kolla-ansible](https://bugs.launchpad.net/kolla-ansible) | Fix Released | Undecided | Unassigned |  |
|  | [Zed](https://bugs.launchpad.net/kolla-ansible/zed) | Fix Released | Undecided | Unassigned |  |
|  | [os-brick](https://bugs.launchpad.net/os-brick) | In Progress | Undecided | Unassigned |  |

### Bug Description

Hello OpenStack Security Team,

I’m writing to you, as we faced a serious security breach in OpenStack functionality(correlated a bit with libvirt, iscsi and huawei driver). I was going through OSSA documents and correlated libvirt notes, but I couldn't find something similar. It is not related to <https://security.openstack.org/ossa/OSSA-2020-006.html>

In short: we observed that newly created cinder volume(1GB size) was attached to compute node instance, but an instance recognized it as a 115GB volume, which(this 115GB volume) in fact was connected to another instance on the same compute node.

[1. Test environment]

Compute node: OpenStack Ussuri configured with Huawei dorado as a storage backend(configuration driver is available here: <https://docs.openstack.org/cinder/rocky/configuration/block-storage/drivers/huawei-storage-driver.html>)

Packages:

v# dpkg -l | grep libvirt

ii libvirt-clients 6.0.0-0ubuntu8.16 amd64 Programs for the libvirt library

ii libvirt-daemon 6.0.0-0ubuntu8.16 amd64 Virtualization daemon

ii libvirt-daemon-driver-qemu 6.0.0-0ubuntu8.16 amd64 Virtualization daemon QEMU connection driver

ii libvirt-daemon-driver-storage-rbd 6.0.0-0ubuntu8.16 amd64 Virtualization daemon RBD storage driver

ii libvirt-daemon-system 6.0.0-0ubuntu8.16 amd64 Libvirt daemon configuration files

ii libvirt-daemon-system-systemd 6.0.0-0ubuntu8.16 amd64 Libvirt daemon configuration files (systemd)

ii libvirt0:amd64 6.0.0-0ubuntu8.16 amd64 library for interfacing with different virtualization systems

ii nova-compute-libvirt 2:21.2.4-0ubuntu1 all OpenStack Compute - compute node libvirt support

ii python3-libvirt 6.1.0-1 amd64 libvirt Python 3 bindings

# dpkg -l | grep qemu

ii ipxe-qemu 1.0.0+git-20190109.133f4c4-0ubuntu3.2 all PXE boot firmware - ROM images for qemu

ii ipxe-qemu-256k-compat-efi-roms 1.0.0+git-20150424.a25a16d-0ubuntu4 all PXE boot firmware - Compat EFI ROM images for qemu

ii libvirt-daemon-driver-qemu 6.0.0-0ubuntu8.16 amd64 Virtualization daemon QEMU connection driver

ii qemu 1:4.2-3ubuntu6.23 amd64 fast processor emulator, dummy package

ii qemu-block-extra:amd64 1:4.2-3ubuntu6.23 amd64 extra block backend modules for qemu-system and qemu-utils

ii qemu-kvm 1:4.2-3ubuntu6.23 amd64 QEMU Full virtualization on x86 hardware

ii qemu-system-common 1:4.2-3ubuntu6.23 amd64 QEMU full system emulation binaries (common files)

ii qemu-system-data 1:4.2-3ubuntu6.23 all QEMU full system emulation (data files)

ii qemu-system-gui:amd64 1:4.2-3ubuntu6.23 amd64 QEMU full system emulation binaries (user interface and audio support)

ii qemu-system-x86 1:4.2-3ubuntu6.23 amd64 QEMU full system emulation binaries (x86)

ii qemu-utils 1:4.2-3ubuntu6.23 amd64 QEMU utilities

# dpkg -l | grep nova

ii nova-common 2:21.2.4-0ubuntu1 all OpenStack Compute - common files

ii nova-compute 2:21.2.4-0ubuntu1 all OpenStack Compute - compute node base

ii nova-compute-kvm 2:21.2.4-0ubuntu1 all OpenStack Compute - compute node (KVM)

ii nova-compute-libvirt 2:21.2.4-0ubuntu1 all OpenStack Compute - compute node libvirt support

ii python3-nova 2:21.2.4-0ubuntu1 all OpenStack Compute Python 3 libraries

ii python3-novaclient 2:17.0.0-0ubuntu1 all client library for OpenStack Compute API - 3.x

# dpkg -l | grep multipath

ii multipath-tools 0.8.3-1ubuntu2 amd64 maintain multipath block device access

# dpkg -l | grep iscsi

ii libiscsi7:amd64 1.18.0-2 amd64 iSCSI client shared library

ii open-iscsi 2.0.874-7.1ubuntu6.2 amd64 iSCSI initiator tools

# cat /etc/lsb-release

DISTRIB\_ID=Ubuntu

DISTRIB\_RELEASE=20.04

DISTRIB\_CODENAME=focal

DISTRIB\_DESCRIPTION="Ubuntu 20.04.4 LTS"

Instance OS: Debian-11-amd64

[2. Test scenario]

Already created instance with two volumes attached. First - 10GB for root system, second - 115GB used as vdb. Recognized by compute node as vda - dm-11, vdb - dm-9:

# virsh domblklist 90fas439-fc0e-4e22-8d0b-6f2a18eee5c1

 Target Source

----------------------

 vda /dev/dm-11

 vdb /dev/dm-9

# multipath -ll

(...)

36e00084100ee7e7ed6ad25d900002f6b dm-9 HUAWEI,XSG1

size=115G features='0' hwhandler='0' wp=rw

`-+- policy='service-time 0' prio=1 status=active

  |- 14:0:0:4 sdm 8:192 active ready running

  |- 15:0:0:4 sdo 8:224 active ready running

  |- 16:0:0:4 sdl 8:176 active ready running

  `- 17:0:0:4 sdn 8:208 active ready running

(...)

36e00084100ee7e7ed6acaa2900002f6a dm-11 HUAWEI,XSG1

size=10G features='0' hwhandler='0' wp=rw

`-+- policy='service-time 0' prio=1 status=active

  |- 14:0:0:3 sdq 65:0 active ready running

  |- 15:0:0:3 sdr 65:16 active ready running

  |- 16:0:0:3 sdp 8:240 active ready running

  `- 17:0:0:3 sds 65:32 active ready running

Creating a new instance, with the same OS guest system and 10GB root volume. After successful deployment, creating a new volume(1GB) size and attaching it to newly created instance. We can see after that:

# multipath -ll

(...)

36e00084100ee7e7ed6ad25d900002f6b dm-9 HUAWEI,XSG1

size=115G features='0' hwhandler='0' wp=rw

`-+- policy='service-time 0' prio=1 status=active

  |- 14:0:0:10 sdao 66:128 failed faulty running

  |- 14:0:0:4 sdm 8:192 active ready running

  |- 15:0:0:10 sdap 66:144 failed faulty running

  |- 15:0:0:4 sdo 8:224 active ready running

  |- 16:0:0:10 sdan 66:112 failed faulty running

  |- 16:0:0:4 sdl 8:176 active ready running

  |- 17:0:0:10 sdaq 66:160 failed faulty running

  `- 17:0:0:4 sdn 8:208 active ready running

This way at instance we were able to see a new drive - not 1GB, but 115GB -> so it seems it was incorrectly attached and this way we were able to destroy some data on that volume.

Additionaly we were able to see many errors like that in compute node logs:

# dmesg -T | grep dm-9

[Fri Jan 27 13:37:42 2023] blk\_update\_request: critical target error, dev dm-9, sector 62918760 op 0x1:(WRITE) flags 0x8800 phys\_seg 2 prio class 0

[Fri Jan 27 13:37:42 2023] blk\_update\_request: critical target error, dev dm-9, sector 33625152 op 0x1:(WRITE) flags 0x8800 phys\_seg 6 prio class 0

[Fri Jan 27 13:37:46 2023] blk\_update\_request: critical target error, dev dm-9, sector 66663000 op 0x1:(WRITE) flags 0x8800 phys\_seg 5 prio class 0

[Fri Jan 27 13:37:46 2023] blk\_update\_request: critical target error, dev dm-9, sector 66598120 op 0x1:(WRITE) flags 0x8800 phys\_seg 5 prio class 0

[Fri Jan 27 13:37:51 2023] blk\_update\_request: critical target error, dev dm-9, sector 66638680 op 0x1:(WRITE) flags 0x8800 phys\_seg 12 prio class 0

[Fri Jan 27 13:37:56 2023] blk\_update\_request: critical target error, dev dm-9, sector 66614344 op 0x1:(WRITE) flags 0x8800 phys\_seg 1 prio class 0

[Fri Jan 27 13:37:56 2023] blk\_update\_request: critical target error, dev dm-9, sector 66469296 op 0x1:(WRITE) flags 0x8800 phys\_seg 24 prio class 0

[Fri Jan 27 13:37:56 2023] blk\_update\_request: critical target error, dev dm-9, sector 66586472 op 0x1:(WRITE) flags 0x8800 phys\_seg 3 prio class 0

(...)

Unfortunately we do not know what is a perfect test-scenario for it as we could face such issue in less than 2% of our tries, but it looks like a serious security breach.

Additionally we observed that linux kernel is not fully clearing a device allocation(from volume detach), so some of drives names are visible in an output, i.e. lsblk command. Then, after new volume attachment we can see such names(i.e. sdao, sdap, sdan and so on) are reusable by that drive and wrongly mapped by multipath/iscsi to another drive and this way we hit an issue.

Our question is why linux kernel of compute node is not removing devices allocation and this way is leading to a scenario like that? Maybe this can be a solution here.

Thanks in advance for your help and understanding. In case when more details is needed, do not hesitate to contact me.

See [original description](comments/0)

Tags:

[in-stable-train](/nova/%2Bbugs?field.tag=in-stable-train)
[in-stable-ussuri](/nova/%2Bbugs?field.tag=in-stable-ussuri)
[in-stable-victoria](/nova/%2Bbugs?field.tag=in-stable-victoria)
[in-stable-wallaby](/nova/%2Bbugs?field.tag=in-stable-wallaby)
[in-stable-xena](/nova/%2Bbugs?field.tag=in-stable-xena)
[in-stable-yoga](/nova/%2Bbugs?field.tag=in-stable-yoga)
[in-stable-zed](/nova/%2Bbugs?field.tag=in-stable-zed)
[in-unmaintained-victoria](/nova/%2Bbugs?field.tag=in-unmaintained-victoria)
[in-unmaintained-wallaby](/nova/%2Bbugs?field.tag=in-unmaintained-wallaby)

## CVE References

* [2023-2088](/bugs/cve/2023-2088 "A flaw was found in OpenStack due to ...")

Revision history for this message
![](/+icing/build/overlay/assets/skins/sam/images/close.gif)

| [Jeremy Stanley (fungi)](https://launchpad.net/~fungi) wrote on 2023-02-02: |  |  | [#1](/nova/%2Bbug/2004555/comments/1) |
| --- | --- | --- | --- |

Since this report concerns a possible security risk, an incomplete

security advisory task has been added while the core security

reviewers for the affected project or projects confirm the bug and

discuss the scope of any vulnerability along with potential

solutions.

Since this report concerns a possible security risk, an incomplete
security advisory task has been added while the core security
reviewers for the affected project or projects confirm the bug and
discuss the scope of any vulnerability along with potential
solutions.

| **description**: | updated |
| --- | --- |
| Changed in ossa: | |
| **status**: | New → Incomplete |

Revision history for this message
![](/+icing/build/overlay/assets/skins/sam/images/close.gif)

| [Dan Smith (danms)](https://launchpad.net/~danms) wrote on 2023-02-02: |  |  | [#2](/nova/%2Bbug/2004555/comments/2) |
| --- | --- | --- | --- |

I feel like this is almost certainly something that will require involvement from the cinder people. Nova's part in the volume attachment is pretty minimal, in that we get stuff from cinder, pass it to brick, and then configure the guest with the block device we're told (AFAIK). Unless we're messing up the last step, I think it's likely this is not just a Nova thing. Should we add cinder or brick as an affected project or just add some cinder people to the bug here?

I feel like this is almost certainly something that will require involvement from the cinder people. Nova's part in the volume attachment is pretty minimal, in that we get stuff from cinder, pass it to brick, and then configure the guest with the block device we're told (AFAIK). Unless we're messing up the last step, I think it's likely this is not just a Nova thing. Should we add cinder or brick as an affected project or just add some cinder people to the bug here?

Revision history for this message
![](/+icing/build/overlay/assets/skins/sam/images/close.gif)

| [Sylvain Bauza (sylvain-bauza)](https://launchpad.net/~sylvain-bauza) wrote on 2023-02-02: |  |  | [#3](/nova/%2Bbug/2004555/comments/3) |
| --- | --- | --- | --- |

> Should we add cinder or brick as an affected project or just add some cinder people to the bug here?

I'd be in favor of adding the cinder project which would pull the cinder coresec team, right?

> Should we add cinder or brick as an affected project or just add some cinder people to the bug here?
I'd be in favor of adding the cinder project which would pull the cinder coresec team, right?

Revision history for this message
![](/+icing/build/overlay/assets/skins/sam/images/close.gif)

| [Sylvain Bauza (sylvain-bauza)](https://launchpad.net/~sylvain-bauza) wrote on 2023-02-02: |  |  | [#4](/nova/%2Bbug/2004555/comments/4) |
| --- | --- | --- | --- |

In the meantime, could you please provide us the block device mapping information that's stored in the DB and ideally the cinder-side attachment information ?

Putting the bug report to Incomplete, please mark its status back to New when you reply.

In the meantime, could you please provide us the block device mapping information that's stored in the DB and ideally the cinder-side attachment information ?
Putting the bug report to Incomplete, please mark its status back to New when you reply.

| Changed in nova: | |
| --- | --- |
| **status**: | New → Incomplete |

Revision history for this message
![](/+icing/build/overlay/assets/skins/sam/images/close.gif)

| [Jan Wasilewski (janwasilewski)](https://launchpad.net/~janwasilewski) wrote on 2023-02-02: |  |  | [#5](/nova/%2Bbug/2004555/comments/5) |
| --- | --- | --- | --- |

[Download full text](https://bugs.launchpad.net/nova/%2Bbug/2004555/comments/5/%2Bdownload) (45.5 KiB)

Hi,

below you can find requested information from OpenStack DB. There is no issue right now, but maybe historical tracking could list to some hint? Anyway, issue was related with /dev/vdb drive for instance: 128f1398-a7c5-48f8-8bbc-a132e3e2d556 -> in DB output you can observe that size of volume is 15GB, when directly from instance it was reported as 115GB(so vdb of second instance presented in this output)

mysql> select \* from block\_device\_mapping where instance\_uuid = '90fda439-fc0e-4e22-8d0b-6f2a18eeb9c1';

+---------------------+---------------------+------------+--------+-------------+-----------------------+--------------------------------------+--------------------------------------+-------------+-----------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------+---------+-------------+------------------+--------------+-------------+----------+------------+----------+------+--------------------------------------+--------------------------------------+-------------+

| created\_at | updated\_at | deleted\_at | id | device\_name | delete\_on\_termination | snapshot\_id | volume\_id | volume\_size | no\_device | connection\_info | instance\_uuid | deleted | source\_type | destination\_type | guest\_format | device\_type | disk\_bus | boot\_index | image\_id | ta...

Hi,
below you can find requested information from OpenStack DB. There is no issue right now, but maybe historical tracking could list to some hint? Anyway, issue was related with /dev/vdb drive for instance: 128f1398-a7c5-48f8-8bbc-a132e3e2d556 -> in DB output you can observe that size of volume is 15GB, when directly from instance it was reported as 115GB(so vdb of second instance presented in this output)
mysql> select \* from block\_device\_mapping where instance\_uuid = '90fda439-fc0e-4e22-8d0b-6f2a18eeb9c1';
+---------------------+---------------------+------------+--------+-------------+-----------------------+--------------------------------------+--------------------------------------+-------------+-----------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------+---------+-------------+------------------+--------------+-------------+----------+------------+----------+------+--------------------------------------+--------------------------------------+-------------+
| created\_at | updated\_at | deleted\_at | id | device\_name | delete\_on\_termination | snapshot\_id | volume\_id | volume\_size | no\_device | connection\_info | instance\_uuid | deleted | source\_type | destination\_type | guest\_format | device\_type | disk\_bus | boot\_index | image\_id | tag | attachment\_id | uuid | volume\_type |
+---------------------+---------------------+------------+--------+-------------+-----------------------+--------------------------------------+--------------------------------------+-------------+-----------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------+---------+-------------+------------------+--------------+-------------+----------+------------+----------+------+--------------------------------------+--------------------------------------+-------------+
| 2022-12-08 11:45:26 | 2023-01-27 16:52:32 | NULL | 409637 | /dev/vda | 0 | f8c435fe-2dbe-46bd-a964-7d5d7f8532f7 | 5f61f246-abc5-4e4b-a22b-1d2594e244eb | 10 | 0 | {"driver\_volume\_type": "iscsi", "data": {"target\_discovered": false, "hostlun\_id": 30, "mappingview\_id": "3542", "lun\_id": "12138", "target\_iqns": ["iqn.2006-08.com.huawei:oceanstor:2100e00084ee7e7e::20000:10.10.16.200", "iqn.2006-08.com.huawei:oceanstor:2100e00084ee7e7e::20001:10.10.16.201", "iqn.2006-08.com.huawei:oceanstor:2100e00084ee7e7e::1020000:10.10.16.202", "iqn.2006-08.com.huawei:oceanstor:2100e00084ee7e7e::1020001:10.10.16.203"], "target\_portals": ["10.10.16.200:3260", "10.10.16.201:3260", "10.10.16.202:3260", "10.10.16.203:3260"], "target\_luns": [30, 30, 30, 30], "discard": true, "qos\_specs": {"IOType": "2", "maxIOPS": "1000"}, "access\_mode": "rw", "encrypted": false}, "status": "attaching", "instance": "90fda439-fc0e-4e22-8d0b-6f2a18eeb9c1", "attached\_at": "2023-01-27T16:52:28.000000", "detached\_at": "", "volume\_id": "5f61f246-abc5-4e4b-a22b-1d2594e244eb", "serial": "5f61f246-abc5-4e4b-a22b-1d2594e244eb"} | 90fda439-fc0e-4e22-8d0b-6f2a18eeb9c1 | 0 | snapshot | volume | NULL | disk | virtio | 0 | NULL | NULL | 09d8b28b-346a-4d3c-9835-1cef0047c702 | 6cbb9ce6-bc11-4de2-97b4-99879a4350f0 | 1000iops |
| 2022-12-08 11:50:12 | 2023-01-27 16:52:31 | NULL | 409640 | /dev/vdb | 0 | NULL | 75b0e159-034c-4f6a-abb2-145a1e314e77 | 115 | NULL | {"driver\_volume\_type": "iscsi", "data": {"target\_discovered": false, "hostlun\_id": 31, "mappingview\_id": "3542", "lun\_id": "12139", "target\_iqns": ["iqn.2006-08.com.huawei:oceanstor:2100e00084ee7e7e::20000:10.10.16.200", "iqn.2006-08.com.huawei:oceanstor:2100e00084ee7e7e::20001:10.10.16.201", "iqn.2006-08.com.huawei:oceanstor:2100e00084ee7e7e::1020000:10.10.16.202", "iqn.2006-08.com.huawei:oceanstor:2100e00084ee7e7e::1020001:10.10.16.203"], "target\_portals": ["10.10.16.200:3260", "10.10.16.201:3260", "10.10.16.202:3260", "10.10.16.203:3260"], "target\_luns": [31, 31, 31, 31], "discard": true, "qos\_specs": {"IOType": "2", "maxIOPS": "1000"}, "access\_mode": "rw", "encrypted": false}, "status": "attaching", "instance": "90fda439-fc0e-4e22-8d0b-6f2a18eeb9c1", "attached\_at": "2023-01-27T16:52:31.000000", "detached\_at": "", "volume\_id": "75b0e159-034c-4f6a-abb2-145a1e314e77", "serial": "75b0e159-034c-4f6a-abb2-145a1e314e77"} | 90fda439-fc0e-4e22-8d0b-6f2a18eeb9c1 | 0 | volume | volume | NULL | disk | virtio | NULL | NULL | NULL | dece5935-ee5c-4d35-a258-1bc19b304f83 | 7eef2b0d-0c1a-4d3a-9388-921250b82ac4 | NULL |
+---------------------+---------------------+------------+--------+-------------+-----------------------+--------------------------------------+--------------------------------------+-------------+-----------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------+---------+-------------+------------------+--------------+-------------+----------+------------+----------+------+--------------------------------------+--------------------------------------+-------------+
mysql> select \* from block\_device\_mapping where instance\_uuid = '128f1398-a7c5-48f8-8bbc-a132e3e2d556';
+---------------------+---------------------+------------+--------+-------------+-----------------------+--------------------------------------+--------------------------------------+-------------+-----------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------+---------+-------------+------------------+--------------+-------------+----------+------------+----------+------+--------------------------------------+--------------------------------------+-------------+
| created\_at | updated\_at | deleted\_at | id | device\_name | delete\_on\_termination | snapshot\_id | volume\_id | volume\_size | no\_device | connection\_info | instance\_uuid | deleted | source\_type | destination\_type | guest\_format | device\_type | disk\_bus | boot\_index | image\_id | tag | attachment\_id | uuid | volume\_type |
+---------------------+---------------------+------------+--------+-------------+-----------------------+--------------------------------------+--------------------------------------+-------------+-----------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------+---------+-------------+------------------+--------------+-------------+----------+------------+----------+------+--------------------------------------+--------------------------------------+-------------+
| 2023-01-27 12:15:29 | 2023-01-27 16:50:46 | NULL | 468725 | /dev/vda | 0 | 5b8ff61b-ac75-4289-b093-5aaf36686de2 | eb2e4515-45b7-4c3b-b31c-8e2fc3b9ef9b | 10 | 0 | {"driver\_volume\_type": "iscsi", "data": {"target\_discovered": false, "hostlun\_id": 28, "mappingview\_id": "3542", "lun\_id": "14481", "target\_iqns": ["iqn.2006-08.com.huawei:oceanstor:2100e00084ee7e7e::20000:10.10.16.200", "iqn.2006-08.com.huawei:oceanstor:2100e00084ee7e7e::20001:10.10.16.201", "iqn.2006-08.com.huawei:oceanstor:2100e00084ee7e7e::1020000:10.10.16.202", "iqn.2006-08.com.huawei:oceanstor:2100e00084ee7e7e::1020001:10.10.16.203"], "target\_portals": ["10.10.16.200:3260", "10.10.16.201:3260", "10.10.16.202:3260", "10.10.16.203:3260"], "target\_luns": [28, 28, 28, 28], "discard": true, "qos\_specs": {"IOType": "2", "maxIOPS": "1000"}, "access\_mode": "rw", "encrypted": false}, "status": "attaching", "instance": "128f1398-a7c5-48f8-8bbc-a132e3e2d556", "attached\_at": "2023-01-27T16:50:42.000000", "detached\_at": "", "volume\_id": "eb2e4515-45b7-4c3b-b31c-8e2fc3b9ef9b", "serial": "eb2e4515-45b7-4c3b-b31c-8e2fc3b9ef9b"} | 128f1398-a7c5-48f8-8bbc-a132e3e2d556 | 0 | snapshot | volume | NULL | disk | virtio | 0 | NULL | NULL | ca24cffd-1de7-4db4-9d83-6cb8d421ad1d | d4f7bd0c-4264-462b-aa09-6485f3558e36 | 1000iops |
| 2023-01-27 12:19:01 | 2023-01-27 16:50:46 | NULL | 468728 | /dev/vdb | 0 | NULL | df2b7131-9599-47cb-9250-49e5899a4f51 | 15 | NULL | {"driver\_volume\_type": "iscsi", "data": {"target\_discovered": false, "hostlun\_id": 29, "mappingview\_id": "3542", "lun\_id": "14482", "target\_iqns": ["iqn.2006-08.com.huawei:oceanstor:2100e00084ee7e7e::20000:10.10.16.200", "iqn.2006-08.com.huawei:oceanstor:2100e00084ee7e7e::20001:10.10.16.201", "iqn.2006-08.com.huawei:oceanstor:2100e00084ee7e7e::1020000:10.10.16.202", "iqn.2006-08.com.huawei:oceanstor:2100e00084ee7e7e::1020001:10.10.16.203"], "target\_portals": ["10.10.16.200:3260", "10.10.16.201:3260", "10.10.16.202:3260", "10.10.16.203:3260"], "target\_luns": [29, 29, 29, 29], "discard": true, "qos\_specs": {"IOType": "2", "maxIOPS": "1000"}, "access\_mode": "rw", "encrypted": false}, "status": "attaching", "instance": "128f1398-a7c5-48f8-8bbc-a132e3e2d556", "attached\_at": "2023-01-27T16:50:46.000000", "detached\_at": "", "volume\_id": "df2b7131-9599-47cb-9250-49e5899a4f51", "serial": "df2b7131-9599-47cb-9250-49e5899a4f51"} | 128f1398-a7c5-48f8-8bbc-a132e3e2d556 | 0 | volume | volume | NULL | NULL | NULL | NULL | NULL | NULL | cf7e988c-791a-49e1-b4d1-d730d1460ed0 | 93f97ffe-9113-4bfd-bfa0-9fceac7e89d6 | NULL |
+---------------------+---------------------+------------+--------+-------------+-----------------------+--------------------------------------+--------------------------------------+-------------+-----------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------+---------+-------------+------------------+--------------+-------------+----------+------------+----------+------+--------------------------------------+--------------------------------------+-------------+
mysql> select \* from volume\_attachment where instance\_uuid = '90fda439-fc0e-4e22-8d0b-6f2a18eeb9c1';
+---------------------+---------------------+---------------------+---------+--------------------------------------+--------------------------------------+---------------+--------------------------------------+------------+---------------------+---------------------+-------------+---------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| created\_at | updated\_at | deleted\_at | deleted | id | volume\_id | attached\_host | instance\_uuid | mountpoint | attach\_time | detach\_time | attach\_mode | attach\_status | connection\_info | connector |
+---------------------+---------------------+---------------------+---------+--------------------------------------+--------------------------------------+---------------+--------------------------------------+------------+---------------------+---------------------+-------------+---------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 2023-01-27 16:52:24 | 2023-01-27 16:52:39 | NULL | 0 | 09d8b28b-346a-4d3c-9835-1cef0047c702 | 5f61f246-abc5-4e4b-a22b-1d2594e244eb | comphc-a01 | 90fda439-fc0e-4e22-8d0b-6f2a18eeb9c1 | /dev/vda | 2023-01-27 16:52:28 | NULL | null | attached | {"target\_discovered": false, "hostlun\_id": 30, "mappingview\_id": "3542", "lun\_id": "12138", "target\_iqns": ["iqn.2006-08.com.huawei:oceanstor:2100e00084ee7e7e::20000:10.10.16.200", "iqn.2006-08.com.huawei:oceanstor:2100e00084ee7e7e::20001:10.10.16.201", "iqn.2006-08.com.huawei:oceanstor:2100e00084ee7e7e::1020000:10.10.16.202", "iqn.2006-08.com.huawei:oceanstor:2100e00084ee7e7e::1020001:10.10.16.203"], "target\_portals": ["10.10.16.200:3260", "10.10.16.201:3260", "10.10.16.202:3260", "10.10.16.203:3260"], "target\_luns": [30, 30, 30, 30], "discard": true, "qos\_specs": {"IOType": "2", "maxIOPS": "1000"}, "access\_mode": "rw", "encrypted": false, "driver\_volume\_type": "iscsi", "attachment\_id": "09d8b28b-346a-4d3c-9835-1cef0047c702"} | {"platform": "x86\_64", "os\_type": "linux", "ip": "10.10.0.234", "host": "comphc-a01", "multipath": true, "initiator": "iqn.1993-08.org.debian:01:57947cb5cc16", "do\_local\_attach": false, "system uuid": "00000000-0000-0000-0000-AC1F6B4E3306", "mountpoint": "/dev/vda", "mode": null} |
| 2022-12-20 10:37:09 | 2022-12-20 10:37:26 | 2023-01-02 14:22:11 | 1 | 36851043-a5d9-4fdc-8103-ed809c131077 | 75b0e159-034c-4f6a-abb2-145a1e314e77 | comphc-a01 | 90fda439-fc0e-4e22-8d0b-6f2a18eeb9c1 | /dev/vdb | 2022-12-20 10:37:12 | 2023-01-02 14:22:11 | null | detached | {"target\_discovered": false, "hostlun\_id": 16, "mappingview\_id": "3542", "lun\_id": "12139", "target\_iqns": ["iqn.2006-08.com.huawei:oceanstor:2100e00084ee7e7e::20000:10.10.16.200", "iqn.2006-08.com.huawei:oceanstor:2100e00084ee7e7e::20001:10.10.16.201", "iqn.2006-08.com.huawei:oceanstor:2100e00084ee7e7e::1020000:10.10.16.202", "iqn.2006-08.com.huawei:oceanstor:2100e00084ee7e7e::1020001:10.10.16.203"], "target\_portals": ["10.10.16.200:3260", "10.10.16.201:3260", "10.10.16.202:3260", "10.10.16.203:3260"], "target\_luns": [16, 16, 16, 16], "discard": true, "qos\_specs": {"IOType": "2", "maxIOPS": "1000"}, "access\_mode": "rw", "encrypted": false, "driver\_volume\_type": "iscsi", "attachment\_id": "36851043-a5d9-4fdc-8103-ed809c131077"} | {"platform": "x86\_64", "os\_type": "linux", "ip": "10.10.0.234", "host": "comphc-a01", "multipath": true, "initiator": "iqn.1993-08.org.debian:01:57947cb5cc16", "do\_local\_attach": false, "system uuid": "00000000-0000-0000-0000-AC1F6B4E3306", "mountpoint": "/dev/vdb", "mode": null} |
| 2022-12-08 11:50:12 | 2022-12-08 11:50:20 | 2022-12-20 10:38:44 | 1 | 490ab84f-f875-4ebf-a0a7-c22c260e7bb0 | 75b0e159-034c-4f6a-abb2-145a1e314e77 | comphc-a02 | 90fda439-fc0e-4e22-8d0b-6f2a18eeb9c1 | /dev/vdb | 2022-12-08 11:50:15 | 2022-12-20 10:38:44 | rw | detached | {"target\_discovered": false, "hostlun\_id": 10, "mappingview\_id": "101", "lun\_id": "12139", "target\_iqns": ["iqn.2006-08.com.huawei:oceanstor:2100e00084ee7e7e::20000:10.10.16.200", "iqn.2006-08.com.huawei:oceanstor:2100e00084ee7e7e::20001:10.10.16.201", "iqn.2006-08.com.huawei:oceanstor:2100e00084ee7e7e::1020000:10.10.16.202", "iqn.2006-08.com.huawei:oceanstor:2100e00084ee7e7e::1020001:10.10.16.203"], "target\_portals": ["10.10.16.200:3260", "10.10.16.201:3260", "10.10.16.202:3260", "10.10.16.203:3260"], "target\_luns": [10, 10, 10, 10], "discard": true, "qos\_specs": {"IOType": "2", "maxIOPS": "1000"}, "access\_mode": "rw", "encrypted": false, "driver\_volume\_type": "iscsi", "attachment\_id": "490ab84f-f875-4ebf-a0a7-c22c260e7bb0"} | {"platform": "x86\_64", "os\_type": "linux", "ip": "10.10.1.114", "host": "comphc-a02", "multipath": true, "initiator": "iqn.1993-08.org.debian:01:e7791652284b", "do\_local\_attach": false, "system uuid": "00000000-0000-0000-0000-3cecef0e241e", "mountpoint": "/dev/vdb"} |
| 2022-12-20 10:37:04 | 2022-12-20 10:37:26 | 2023-01-02 14:22:09 | 1 | 6a44c982-4ed8-4940-b8a2-74a3f729e426 | 5f61f246-abc5-4e4b-a22b-1d2594e244eb | comphc-a01 | 90fda439-fc0e-4e22-8d0b-6f2a18eeb9c1 | /dev/vda | 2022-12-20 10:37:08 | 2023-01-02 14:22:09 | null | detached | {"target\_discovered": false, "hostlun\_id": 15, "mappingview\_id": "3542", "lun\_id": "12138", "target\_iqns": ["iqn.2006-08.com.huawei:oceanstor:2100e00084ee7e7e::20000:10.10.16.200", "iqn.2006-08.com.huawei:oceanstor:2100e00084ee7e7e::20001:10.10.16.201", "iqn.2006-08.com.huawei:oceanstor:2100e00084ee7e7e::1020000:10.10.16.202", "iqn.2006-08.com.huawei:oceanstor:2100e00084ee7e7e::1020001:10.10.16.203"], "target\_portals": ["10.10.16.200:3260", "10.10.16.201:3260", "10.10.16.202:3260", "10.10.16.203:3260"], "target\_luns": [15, 15, 15, 15], "discard": true, "qos\_specs": {"IOType": "2", "maxIOPS": "1000"}, "access\_mode": "rw", "encrypted": false, "driver\_volume\_type": "iscsi", "attachment\_id": "6a44c982-4ed8-4940-b8a2-74a3f729e426"} | {"platform": "x86\_64", "os\_type": "linux", "ip": "10.10.0.234", "host": "comphc-a01", "multipath": true, "initiator": "iqn.1993-08.org.debian:01:57947cb5cc16", "do\_local\_attach": false, "system uuid": "00000000-0000-0000-0000-AC1F6B4E3306", "mountpoint": "/dev/vda", "mode": null} |
| 2023-01-02 14:22:08 | 2023-01-02 14:22:27 | 2023-01-27 09:01:28 | 1 | 6a875c66-f0f2-4c66-b22b-f2b1e934b96d | 5f61f246-abc5-4e4b-a22b-1d2594e244eb | comphc-b02 | 90fda439-fc0e-4e22-8d0b-6f2a18eeb9c1 | /dev/vda | 2023-01-02 14:22:17 | 2023-01-27 09:01:28 | rw | detached | {"target\_discovered": false, "hostlun\_id": 40, "mappingview\_id": "3615", "lun\_id": "12138", "target\_iqns": ["iqn.2006-08.com.huawei:oceanstor:2100e00084ee7e7e::20000:10.10.16.200", "iqn.2006-08.com.huawei:oceanstor:2100e00084ee7e7e::20001:10.10.16.201", "iqn.2006-08.com.huawei:oceanstor:2100e00084ee7e7e::1020000:10.10.16.202", "iqn.2006-08.com.huawei:oceanstor:2100e00084ee7e7e::1020001:10.10.16.203"], "target\_portals": ["10.10.16.200:3260", "10.10.16.201:3260", "10.10.16.202:3260", "10.10.16.203:3260"], "target\_luns": [40, 40, 40, 40], "discard": true, "qos\_specs": {"IOType": "2", "maxIOPS": "1000"}, "access\_mode": "rw", "encrypted": false, "driver\_volume\_type": "iscsi", "attachment\_id": "6a875c66-f0f2-4c66-b22b-f2b1e934b96d"} | {"platform": "x86\_64", "os\_type": "linux", "ip": "10.10.1.115", "host": "comphc-b02", "multipath": true, "initiator": "iqn.1993-08.org.debian:01:ce501c96eca6", "do\_local\_attach": false, "system uuid": "00000000-0000-0000-0000-3CECEF0E2420", "mountpoint": "/dev/vda"} |
| 2023-01-02 14:22:10 | 2023-01-02 14:22:27 | 2023-01-27 09:01:29 | 1 | 721d3929-0fad-4293-a736-93ecbd4bc210 | 75b0e159-034c-4f6a-abb2-145a1e314e77 | comphc-b02 | 90fda439-fc0e-4e22-8d0b-6f2a18eeb9c1 | /dev/vdb | 2023-01-02 14:22:20 | 2023-01-27 09:01:29 | rw | detached | {"target\_discovered": false, "hostlun\_id": 41, "mappingview\_id": "3615", "lun\_id": "12139", "target\_iqns": ["iqn.2006-08.com.huawei:oceanstor:2100e00084ee7e7e::20000:10.10.16.200", "iqn.2006-08.com.huawei:oceanstor:2100e00084ee7e7e::20001:10.10.16.201", "iqn.2006-08.com.huawei:oceanstor:2100e00084ee7e7e::1020000:10.10.16.202", "iqn.2006-08.com.huawei:oceanstor:2100e00084ee7e7e::1020001:10.10.16.203"], "target\_portals": ["10.10.16.200:3260", "10.10.16.201:3260", "10.10.16.202:3260", "10.10.16.203:3260"], "target\_luns": [41, 41, 41, 41], "discard": true, "qos\_specs": {"IOType": "2", "maxIOPS": "1000"}, "access\_mode": "rw", "encrypted": false, "driver\_volume\_type": "iscsi", "attachment\_id": "721d3929-0fad-4293-a736-93ecbd4bc210"} | {"platform": "x86\_64", "os\_type": "linux", "ip": "10.10.1.115", "host": "comphc-b02", "multipath": true, "initiator": "iqn.1993-08.org.debian:01:ce501c96eca6", "do\_local\_attach": false, "system uuid": "00000000-0000-0000-0000-3CECEF0E2420", "mountpoint": "/dev/vdb"} |
| 2022-12-08 11:45:48 | 2022-12-08 11:45:51 | 2022-12-20 10:38:43 | 1 | 7cc17cfa-01eb-4ada-a666-5b27ba677d66 | 5f61f246-abc5-4e4b-a22b-1d2594e244eb | comphc-a02 | 90fda439-fc0e-4e22-8d0b-6f2a18eeb9c1 | /dev/vda | 2022-12-08 11:45:51 | 2022-12-20 10:38:43 | rw | detached | {"target\_discovered": false, "hostlun\_id": 9, "mappingview\_id": "101", "lun\_id": "12138", "target\_iqns": ["iqn.2006-08.com.huawei:oceanstor:2100e00084ee7e7e::20000:10.10.16.200", "iqn.2006-08.com.huawei:oceanstor:2100e00084ee7e7e::20001:10.10.16.201", "iqn.2006-08.com.huawei:oceanstor:2100e00084ee7e7e::1020000:10.10.16.202", "iqn.2006-08.com.huawei:oceanstor:2100e00084ee7e7e::1020001:10.10.16.203"], "target\_portals": ["10.10.16.200:3260", "10.10.16.201:3260", "10.10.16.202:3260", "10.10.16.203:3260"], "target\_luns": [9, 9, 9, 9], "discard": true, "qos\_specs": {"IOType": "2", "maxIOPS": "1000"}, "access\_mode": "rw", "encrypted": false, "driver\_volume\_type": "iscsi", "attachment\_id": "7cc17cfa-01eb-4ada-a666-5b27ba677d66"} | {"platform": "x86\_64", "os\_type": "linux", "ip": "10.10.1.114", "host": "comphc-a02", "multipath": true, "initiator": "iqn.1993-08.org.debian:01:e7791652284b", "do\_local\_attach": false, "system uuid": "00000000-0000-0000-0000-3cecef0e241e", "mountpoint": "/dev/vda"} |
| 2023-01-27 09:01:27 | 2023-01-27 09:01:47 | 2023-01-27 16:53:09 | 1 | a121513d-5e5a-4fdb-acab-704bb8d06792 | 5f61f246-abc5-4e4b-a22b-1d2594e244eb | comphc-a02 | 90fda439-fc0e-4e22-8d0b-6f2a18eeb9c1 | /dev/vda | 2023-01-27 09:01:36 | 2023-01-27 16:53:09 | rw | detached | {"target\_discovered": false, "hostlun\_id": 3, "mappingview\_id": "906", "lun\_id": "12138", "target\_iqns": ["iqn.2006-08.com.huawei:oceanstor:2100e00084ee7e7e::20000:10.10.16.200", "iqn.2006-08.com.huawei:oceanstor:2100e00084ee7e7e::20001:10.10.16.201", "iqn.2006-08.com.huawei:oceanstor:2100e00084ee7e7e::1020000:10.10.16.202", "iqn.2006-08.com.huawei:oceanstor:2100e00084ee7e7e::1020001:10.10.16.203"], "target\_portals": ["10.10.16.200:3260", "10.10.16.201:3260", "10.10.16.202:3260", "10.10.16.203:3260"], "target\_luns": [3, 3, 3, 3], "discard": true, "qos\_specs": {"IOType": "2", "maxIOPS": "1000"}, "access\_mode": "rw", "encrypted": false, "driver\_volume\_type": "iscsi", "attachment\_id": "a121513d-5e5a-4fdb-acab-704bb8d06792"} | {"platform": "x86\_64", "os\_type": "linux", "ip": "10.10.1.114", "host": "comphc-a02", "multipath": true, "initiator": "iqn.1993-08.org.debian:01:e7791652284b", "do\_local\_attach": false, "system uuid": "00000000-0000-0000-0000-3cecef0e241e", "mountpoint": "/dev/vda"} |
| 2023-01-27 09:01:28 | 2023-01-27 09:01:47 | 2023-01-27 16:53:10 | 1 | a9362ba1-155d-4f35-94aa-846fcef6fab6 | 75b0e159-034c-4f6a-abb2-145a1e314e77 | comphc-a02 | 90fda439-fc0e-4e22-8d0b-6f2a18eeb9c1 | /dev/vdb | 2023-01-27 09:01:39 | 2023-01-27 16:53:10 | rw | detached | {"target\_discovered": false, "hostlun\_id": 4, "mappingview\_id": "906", "lun\_id": "12139", "target\_iqns": ["iqn.2006-08.com.huawei:oceanstor:2100e00084ee7e7e::20000:10.10.16.200", "iqn.2006-08.com.huawei:oceanstor:2100e00084ee7e7e::20001:10.10.16.201", "iqn.2006-08.com.huawei:oceanstor:2100e00084ee7e7e::1020000:10.10.16.202", "iqn.2006-08.com.huawei:oceanstor:2100e00084ee7e7e::1020001:10.10.16.203"], "target\_portals": ["10.10.16.200:3260", "10.10.16.201:3260", "10.10.16.202:3260", "10.10.16.203:3260"], "target\_luns": [4, 4, 4, 4], "discard": true, "qos\_specs": {"IOType": "2", "maxIOPS": "1000"}, "access\_mode": "rw", "encrypted": false, "driver\_volume\_type": "iscsi", "attachment\_id": "a9362ba1-155d-4f35-94aa-846fcef6fab6"} | {"platform": "x86\_64", "os\_type": "linux", "ip": "10.10.1.114", "host": "comphc-a02", "multipath": true, "initiator": "iqn.1993-08.org.debian:01:e7791652284b", "do\_local\_attach": false, "system uuid": "00000000-0000-0000-0000-3cecef0e241e", "mountpoint": "/dev/vdb"} |
| 2023-01-27 16:52:28 | 2023-01-27 16:52:39 | NULL | 0 | dece5935-ee5c-4d35-a258-1bc19b304f83 | 75b0e159-034c-4f6a-abb2-145a1e314e77 | comphc-a01 | 90fda439-fc0e-4e22-8d0b-6f2a18eeb9c1 | /dev/vdb | 2023-01-27 16:52:31 | NULL | null | attached | {"target\_discovered": false, "hostlun\_id": 31, "mappingview\_id": "3542", "lun\_id": "12139", "target\_iqns": ["iqn.2006-08.com.huawei:oceanstor:2100e00084ee7e7e::20000:10.10.16.200", "iqn.2006-08.com.huawei:oceanstor:2100e00084ee7e7e::20001:10.10.16.201", "iqn.2006-08.com.huawei:oceanstor:2100e00084ee7e7e::1020000:10.10.16.202", "iqn.2006-08.com.huawei:oceanstor:2100e00084ee7e7e::1020001:10.10.16.203"], "target\_portals": ["10.10.16.200:3260", "10.10.16.201:3260", "10.10.16.202:3260", "10.10.16.203:3260"], "target\_luns": [31, 31, 31, 31], "discard": true, "qos\_specs": {"IOType": "2", "maxIOPS": "1000"}, "access\_mode": "rw", "encrypted": false, "driver\_volume\_type": "iscsi", "attachment\_id": "dece5935-ee5c-4d35-a258-1bc19b304f83"} | {"platform": "x86\_64", "os\_type": "linux", "ip": "10.10.0.234", "host": "comphc-a01", "multipath": true, "initiator": "iqn.1993-08.org.debian:01:57947cb5cc16", "do\_local\_attach": false, "system uuid": "00000000-0000-0000-0000-AC1F6B4E3306", "mountpoint": "/dev/vdb", "mode": null} |
+---------------------+---------------------+---------------------+---------+--------------------------------------+--------------------------------------+---------------+--------------------------------------+------------+---------------------+---------------------+-------------+---------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
mysql> select \* from volume\_attachment where instance\_uuid = '128f1398-a7c5-48f8-8bbc-a132e3e2d556';
+---------------------+---------------------+---------------------+---------+--------------------------------------+--------------------------------------+---------------+--------------------------------------+------------+---------------------+---------------------+-------------+---------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| created\_at | updated\_at | deleted\_at | deleted | id | volume\_id | attached\_host | instance\_uuid | mountpoint | attach\_time | detach\_time | attach\_mode | attach\_status | connection\_info | connector |
+---------------------+---------------------+---------------------+---------+--------------------------------------+--------------------------------------+---------------+--------------------------------------+------------+---------------------+---------------------+-------------+---------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 2023-01-27 12:15:51 | 2023-01-27 12:15:54 | 2023-01-27 16:52:01 | 1 | 1457d22b-c539-4b87-ace4-28c73fe12199 | eb2e4515-45b7-4c3b-b31c-8e2fc3b9ef9b | comphc-a02 | 128f1398-a7c5-48f8-8bbc-a132e3e2d556 | /dev/vda | 2023-01-27 12:15:54 | 2023-01-27 16:52:01 | rw | detached | {"target\_discovered": false, "hostlun\_id": 7, "mappingview\_id": "906", "lun\_id": "14481", "target\_iqns": ["iqn.2006-08.com.huawei:oceanstor:2100e00084ee7e7e::20000:10.10.16.200", "iqn.2006-08.com.huawei:oceanstor:2100e00084ee7e7e::20001:10.10.16.201", "iqn.2006-08.com.huawei:oceanstor:2100e00084ee7e7e::1020000:10.10.16.202", "iqn.2006-08.com.huawei:oceanstor:2100e00084ee7e7e::1020001:10.10.16.203"], "target\_portals": ["10.10.16.200:3260", "10.10.16.201:3260", "10.10.16.202:3260", "10.10.16.203:3260"], "target\_luns": [7, 7, 7, 7], "discard": true, "qos\_specs": {"IOType": "2", "maxIOPS": "1000"}, "access\_mode": "rw", "encrypted": false, "driver\_volume\_type": "iscsi", "attachment\_id": "1457d22b-c539-4b87-ace4-28c73fe12199"} | {"platform": "x86\_64", "os\_type": "linux", "ip": "10.10.1.114", "host": "comphc-a02", "multipath": true, "initiator": "iqn.1993-08.org.debian:01:e7791652284b", "do\_local\_attach": false, "system uuid": "00000000-0000-0000-0000-3cecef0e241e", "mountpoint": "/dev/vda"} |
| 2023-01-27 12:19:01 | 2023-01-27 12:19:10 | 2023-01-27 16:52:02 | 1 | 7960bdb7-b724-4e20-8e68-aa9ba3ccfd86 | df2b7131-9599-47cb-9250-49e5899a4f51 | comphc-a02 | 128f1398-a7c5-48f8-8bbc-a132e3e2d556 | /dev/vdb | 2023-01-27 12:19:05 | 2023-01-27 16:52:02 | rw | detached | {"target\_discovered": false, "hostlun\_id": 8, "mappingview\_id": "906", "lun\_id": "14482", "target\_iqns": ["iqn.2006-08.com.huawei:oceanstor:2100e00084ee7e7e::20000:10.10.16.200", "iqn.2006-08.com.huawei:oceanstor:2100e00084ee7e7e::20001:10.10.16.201", "iqn.2006-08.com.huawei:oceanstor:2100e00084ee7e7e::1020000:10.10.16.202", "iqn.2006-08.com.huawei:oceanstor:2100e00084ee7e7e::1020001:10.10.16.203"], "target\_portals": ["10.10.16.200:3260", "10.10.16.201:3260", "10.10.16.202:3260", "10.10.16.203:3260"], "target\_luns": [8, 8, 8, 8], "discard": true, "qos\_specs": {"IOType": "2", "maxIOPS": "1000"}, "access\_mode": "rw", "encrypted": false, "driver\_volume\_type": "iscsi", "attachment\_id": "7960bdb7-b724-4e20-8e68-aa9ba3ccfd86"} | {"platform": "x86\_64", "os\_type": "linux", "ip": "10.10.1.114", "host": "comphc-a02", "multipath": true, "initiator": "iqn.1993-08.org.debian:01:e7791652284b", "do\_local\_attach": false, "system uuid": "00000000-0000-0000-0000-3cecef0e241e", "mountpoint": "/dev/vdb"} |
| 2023-01-27 16:50:39 | 2023-01-27 16:50:53 | NULL | 0 | ca24cffd-1de7-4db4-9d83-6cb8d421ad1d | eb2e4515-45b7-4c3b-b31c-8e2fc3b9ef9b | comphc-a01 | 128f1398-a7c5-48f8-8bbc-a132e3e2d556 | /dev/vda | 2023-01-27 16:50:42 | NULL | null | attached | {"target\_discovered": false, "hostlun\_id": 28, "mappingview\_id": "3542", "lun\_id": "14481", "target\_iqns": ["iqn.2006-08.com.huawei:oceanstor:2100e00084ee7e7e::20000:10.10.16.200", "iqn.2006-08.com.huawei:oceanstor:2100e00084ee7e7e::20001:10.10.16.201", "iqn.2006-08.com.huawei:oceanstor:2100e00084ee7e7e::1020000:10.10.16.202", "iqn.2006-08.com.huawei:oceanstor:2100e00084ee7e7e::1020001:10.10.16.203"], "target\_portals": ["10.10.16.200:3260", "10.10.16.201:3260", "10.10.16.202:3260", "10.10.16.203:3260"], "target\_luns": [28, 28, 28, 28], "discard": true, "qos\_specs": {"IOType": "2", "maxIOPS": "1000"}, "access\_mode": "rw", "encrypted": false, "driver\_volume\_type": "iscsi", "attachment\_id": "ca24cffd-1de7-4db4-9d83-6cb8d421ad1d"} | {"platform": "x86\_64", "os\_type": "linux", "ip": "10.10.0.234", "host": "comphc-a01", "multipath": true, "initiator": "iqn.1993-08.org.debian:01:57947cb5cc16", "do\_local\_attach": false, "system uuid": "00000000-0000-0000-0000-AC1F6B4E3306", "mountpoint": "/dev/vda", "mode": null} |
| 2023-01-27 16:50:42 | 2023-01-27 16:50:53 | NULL | 0 | cf7e988c-791a-49e1-b4d1-d730d1460ed0 | df2b7131-9599-47cb-9250-49e5899a4f51 | comphc-a01 | 128f1398-a7c5-48f8-8bbc-a132e3e2d556 | /dev/vdb | 2023-01-27 16:50:46 | NULL | null | attached | {"target\_discovered": false, "hostlun\_id": 29, "mappingview\_id": "3542", "lun\_id": "14482", "target\_iqns": ["iqn.2006-08.com.huawei:oceanstor:2100e00084ee7e7e::20000:10.10.16.200", "iqn.2006-08.com.huawei:oceanstor:2100e00084ee7e7e::20001:10.10.16.201", "iqn.2006-08.com.huawei:oceanstor:2100e00084ee7e7e::1020000:10.10.16.202", "iqn.2006-08.com.huawei:oceanstor:2100e00084ee7e7e::1020001:10.10.16.203"], "target\_portals": ["10.10.16.200:3260", "10.10.16.201:3260", "10.10.16.202:3260", "10.10.16.203:3260"], "target\_luns": [29, 29, 29, 29], "discard": true, "qos\_specs": {"IOType": "2", "maxIOPS": "1000"}, "access\_mode": "rw", "encrypted": false, "driver\_volume\_type": "iscsi", "attachment\_id": "cf7e988c-791a-49e1-b4d1-d730d1460ed0"} | {"platform": "x86\_64", "os\_type": "linux", "ip": "10.10.0.234", "host": "comphc-a01", "multipath": true, "initiator": "iqn.1993-08.org.debian:01:57947cb5cc16", "do\_local\_attach": false, "system uuid": "00000000-0000-0000-0000-AC1F6B4E3306", "mountpoint": "/dev/vdb", "mode": null} |
+---------------------+---------------------+---------------------+---------+--------------------------------------+--------------------------------------+---------------+--------------------------------------+------------+---------------------+---------------------+-------------+---------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

| Changed in nova: | |
| --- | --- |
| **status**: | Incomplete → New |

Revision history for this message
![](/+icing/build/overlay/assets/skins/sam/images/close.gif)

| [Jeremy Stanley (fungi)](https://launchpad.net/~fungi) wrote on 2023-02-02: |  |  | [#6](/nova/%2Bbug/2004555/comments/6) |
| --- | --- | --- | --- |

I've added Cinder as an effected project (though maybe it should be os-brick?) and subscribed the Cinder security reviewers for additional input.

I've added Cinder as an effected project (though maybe it should be os-brick?) and subscribed the Cinder security reviewers for additional input.

Revision history for this message
![](/+icing/build/overlay/assets/skins/sam/images/close.gif)

| [Rajat Dhasmana (whoami-rajat)](https://launchpad.net/~whoami-rajat) wrote on 2023-02-03: |  |  | [#7](/nova/%2Bbug/2004555/comments/7) |
| --- | --- | --- | --- |

Hi,

Based on the given information, the strange part is same multipath device is used for the old and new volume 36e00084100ee7e7ed6ad25d900002f6b

36e00084100ee7e7ed6ad25d900002f6b dm-9 HUAWEI,XSG1

size=115G features='0' hwhandler='0' wp=rw

`-+- policy='service-time 0' prio=1 status=active

  |- 14:0:0:4 sdm 8:192 active ready running

  |- 15:0:0:4 sdo 8:224 active ready running

  |- 16:0:0:4 sdl 8:176 active ready running

  `- 17:0:0:4 sdn 8:208 active ready running

36e00084100ee7e7ed6ad25d900002f6b dm-9 HUAWEI,XSG1

size=115G features='0' hwhandler='0' wp=rw

`-+- policy='service-time 0' prio=1 status=active

  |- 14:0:0:10 sdao 66:128 failed faulty running

  |- 14:0:0:4 sdm 8:192 active ready running

  |- 15:0:0:10 sdap 66:144 failed faulty running

  |- 15:0:0:4 sdo 8:224 active ready running

  |- 16:0:0:10 sdan 66:112 failed faulty running

  |- 16:0:0:4 sdl 8:176 active ready running

  |- 17:0:0:10 sdaq 66:160 failed faulty running

  `- 17:0:0:4 sdn 8:208 active ready running

Also it's interesting to note that the paths under the multipath device (sdm, sdo, sdl, sdn) with LUN ID: 4 are also used by the second multipath device whereas it should use LUN 10 paths (which is currently in failed faulty status).

This looks multipath related but it would be helpful if we can get the os-brick logs for this 1GB volume attachment to understand if os-brick is doing something that is resulting in this.

I would also recommend to cleanup the system with any leftover devices of past failed detachments (i.e. flush and remove mpath devices not belonging to any instance) that might be interfering with this. Although I'm not certain if that's the case, it's still to cleanup those devices.

Hi,
Based on the given information, the strange part is same multipath device is used for the old and new volume 36e00084100ee7e7ed6ad25d900002f6b
36e00084100ee7e7ed6ad25d900002f6b dm-9 HUAWEI,XSG1
size=115G features='0' hwhandler='0' wp=rw
`-+- policy='service-time 0' prio=1 status=active
|- 14:0:0:4 sdm 8:192 active ready running
|- 15:0:0:4 sdo 8:224 active ready running
|- 16:0:0:4 sdl 8:176 active ready running
`- 17:0:0:4 sdn 8:208 active ready running
36e00084100ee7e7ed6ad25d900002f6b dm-9 HUAWEI,XSG1
size=115G features='0' hwhandler='0' wp=rw
`-+- policy='service-time 0' prio=1 status=active
|- 14:0:0:10 sdao 66:128 failed faulty running
|- 14:0:0:4 sdm 8:192 active ready running
|- 15:0:0:10 sdap 66:144 failed faulty running
|- 15:0:0:4 sdo 8:224 active ready running
|- 16:0:0:10 sdan 66:112 failed faulty running
|- 16:0:0:4 sdl 8:176 active ready running
|- 17:0:0:10 sdaq 66:160 failed faulty running
`- 17:0:0:4 sdn 8:208 active ready running
Also it's interesting to note that the paths under the multipath device (sdm, sdo, sdl, sdn) with LUN ID: 4 are also used by the second multipath device whereas it should use LUN 10 paths (which is currently in failed faulty status).
This looks multipath related but it would be helpful if we can get the os-brick logs for this 1GB volume attachment to understand if os-brick is doing something that is resulting in this.
I would also recommend to cleanup the system with any leftover devices of past failed detachments (i.e. flush and remove mpath devices not belonging to any instance) that might be interfering with this. Although I'm not certain if that's the case, it's still to cleanup those devices.

Revision history for this message
![](/+icing/build/overlay/assets/skins/sam/images/close.gif)

| [Gorka Eguileor (gorka)](https://launchpad.net/~gorka) wrote on 2023-02-03: |  |  | [#8](/nova/%2Bbug/2004555/comments/8) |
| --- | --- | --- | --- |

[Download full text](https://bugs.launchpad.net/nova/%2Bbug/2004555/comments/8/%2Bdownload) (3.2 KiB)

Hi,

I think I know what happened, but there are some things that don't match unless

somebody has manually changed some things in the host (like cleaning up

multipaths).

Bit of context:

- SCSI volumes (iSCSI and FC) on Linux are NEVER removed automatically by the

  kernel and must always be removed explicitly. This means that they will

  remain in the system even if the remote connection is severed, unless

  something in OpenStack removes it.

- The os-brick library has a strong policy of not removing devices from the

  system if flushing fails during detach, to prevent data loss.

The `disconnect\_volume` method in the os-brick library has an additional

  parameter called `force` to allow callers to ignore flushing errors and

  ensure that the devices are being removed. This is useful when after failing

  the detach the volume is either going to be deleted or into error status.

I don't have the logs, but from what you said my guess is that this is what has

happened:

- Volume with SCSI ID 36e00084100ee7e7ed6ad25d900002f6b was attached to that

  host on LUN 10 at some point since the last reboot (sdao, sdap, sdan, sdaq).

- When detaching the volume from the host using os-brick the operation failed

  and it wasn't removed, yet Nova still called Cinder to unexport and unmap the

  volume. At this point LUN 10 is free on the Huawei array and the volume is

  no longer attacheable, but /dev/sda[o-q] are still present, and their SCSI\_ID

  are still known to multipathd.

- Nova asked Cinder to attach the volume again, and the volume is mapped to LUN

  4 (which must have been available as well) and it successfully attaches (sdm,

  sdo, sdl, sdn), appears as a multipath, and is used by the VM.

- Nova asks Cinder to export and map the new 1GB volume, and Huawai maps it to

  LUN 10, at this point iSCSI detects that the remote LUNs are back and

  reconnects to them, which makes the multipathd path checker detect sdao,

  sdap, sdan, sdaq are alive on the compute host and they are added to the

  existing multipath device mapper using their known SCSI ID.

You should find out why the detach actually failed, but I think I see multiple

issues:

- Nova:

- Should not call Cinder to unmap a volume if the os-brick to disconnect the

    volume has failed, as we know this will leave leftover devices that can

    cause issues like this.

- If it's not already doing it, Nova should call disconnect\_volume method

    from os-brick passing force=True when the volume is going to be deleted.

- os-brick:

- Should try to detect when the newly added devices are being added to a

    multipath device mapper that has live paths to other LUNs and fail if that

    is the case.

- As an improvement over the previous check, os-brick could forcefully remove

    those devices that are in the wrong device mapper, force a refresh of their

    SCSI IDs and add them back to multipathd to form a new device mapper.

    Though personally this is a non trivial and maybe potentially problematic

    feature.

In other words, the source of the problem is probably Nova, but os-brick should

try to prevent these possible data leaks.

Cheers,

Gorka.

[1]: <https://github.com/opens>...

[Read more...](/nova/%2Bbug/2004555/comments/8)

Hi,
I think I know what happened, but there are some things that don't match unless
somebody has manually changed some things in the host (like cleaning up
multipaths).
Bit of context:
- SCSI volumes (iSCSI and FC) on Linux are NEVER removed automatically by the
kernel and must always be removed explicitly. This means that they will
remain in the system even if the remote connection is severed, unless
something in OpenStack removes it.
- The os-brick library has a strong policy of not removing devices from the
system if flushing fails during detach, to prevent data loss.
The `disconnect\_volume` method in the os-brick library has an additional
parameter called `force` to allow callers to ignore flushing errors and
ensure that the devices are being removed. This is useful when after failing
the detach the volume is either going to be deleted or into error status.
I don't have the logs, but from what you said my guess is that this is what has
happened:
- Volume with SCSI ID 36e00084100ee7e7ed6ad25d900002f6b was attached to that
host on LUN 10 at some point since the last reboot (sdao, sdap, sdan, sdaq).
- When detaching the volume from the host using os-brick the operation failed
and it wasn't removed, yet Nova still called Cinder to unexport and unmap the
volume. At this point LUN 10 is free on the Huawei array and the volume is
no longer attacheable, but /dev/sda[o-q] are still present, and their SCSI\_ID
are still known to multipathd.
- Nova asked Cinder to attach the volume again, and the volume is mapped to LUN
4 (which must have been available as well) and it successfully attaches (sdm,
sdo, sdl, sdn), appears as a multipath, and is used by the VM.
- Nova asks Cinder to export and map the new 1GB volume, and Huawai maps it to
LUN 10, at this point iSCSI detects that the remote LUNs are back and
reconnects to them, which makes the multipathd path checker detect sdao,
sdap, sdan, sdaq are alive on the compute host and they are added to the
existing multipath device mapper using their known SCSI ID.
You should find out why the detach actually failed, but I think I see multiple
issues:
- Nova:
- Should not call Cinder to unmap a volume if the os-brick to disconnect the
volume has failed, as we know this will leave leftover devices that can
cause issues like this.
- If it's not already doing it, Nova should call disconnect\_volume method
from os-brick passing force=True when the volume is going to be deleted.
- os-brick:
- Should try to detect when the newly added devices are being added to a
multipath device mapper that has live paths to other LUNs and fail if that
is the case.
- As an improvement over the previous check, os-brick could forcefully remove
those devices that are in the wrong device mapper, force a refresh of their
SCSI IDs and add them back to multipathd to form a new device mapper.
Though personally this is a non trivial and maybe potentially problematic
feature.
In other words, the source of the problem is probably Nova, but os-brick should
try to prevent these possible data leaks.
Cheers,
Gorka.
[1]: https://github.com/openstack/os-brick/blob/655fcc41b33d3f6afc8f85005868d0111077bdb5/os\_brick/initiator/connectors/iscsi.py#L858

Revision history for this message
![](/+icing/build/overlay/assets/skins/sam/images/close.gif)

| [Dan Smith (danms)](https://launchpad.net/~danms) wrote on 2023-02-03: |  |  | [#9](/nova/%2Bbug/2004555/comments/9) |
| --- | --- | --- | --- |

I don't see in the test scenario description that any instances had to be deleted or volumes disconnected for this to happen. Maybe the reporter can confirm with logs if this is the case?

I'm still chasing down the nova calls, but we don't ignore anything in the actual disconnect other than "volume not found". I need to follow that up to where we call cinder to see if we're ignoring a failure.

When you say "nova should call disconnect\_volume with force=true if the volume is going to be deleted... I'm not sure what you mean by this. Do you mean if we're disconnecting because of \*instance\* delete and are sure that we don't want to let a failure hold us up? I would think this would be dangerous because just deleting an instance doesn't mean you don't care about the data in the volume.

It seems to me that if brick \*has\* the information available to it to avoid connecting a volume to the wrong location, that it's the thing that needs to guard against this. Nova has no knowledge of the things underneath brick, so we don't know that wires are going to get crossed. Obviously if we can do stuff to avoid even getting there, then we should.

I don't see in the test scenario description that any instances had to be deleted or volumes disconnected for this to happen. Maybe the reporter can confirm with logs if this is the case?
I'm still chasing down the nova calls, but we don't ignore anything in the actual disconnect other than "volume not found". I need to follow that up to where we call cinder to see if we're ignoring a failure.
When you say "nova should call disconnect\_volume with force=true if the volume is going to be deleted... I'm not sure what you mean by this. Do you mean if we're disconnecting because of \*instance\* delete and are sure that we don't want to let a failure hold us up? I would think this would be dangerous because just deleting an instance doesn't mean you don't care about the data in the volume.
It seems to me that if brick \*has\* the information available to it to avoid connecting a volume to the wrong location, that it's the thing that needs to guard against this. Nova has no knowledge of the things underneath brick, so we don't know that wires are going to get crossed. Obviously if we can do stuff to avoid even getting there, then we should.

Revision history for this message
![](/+icing/build/overlay/assets/skins/sam/images/close.gif)

| [Jan Wasilewski (janwasilewski)](https://launchpad.net/~janwasilewski) wrote on 2023-02-03: |  |  | [#10](/nova/%2Bbug/2004555/comments/10) |
| --- | --- | --- | --- |

Hi,

I'm just wondering if there is a chance for me to try to reproduce an issue again with all debug flags set to on. Should I turn on this flag on controllers(cinder, nova) or compute node logs(with debug flags set to on) should be enough to further troubleshoot this issue? If yes, please let me know which flags are needed here, just to speed up further troubleshooting. As I said - this case is not easy to reproduce, I can't even say what is a trigger here, but we faced it 3 or 4 times already.

Thanks in advance for reply and your helps so far.

Best regards,

Jan

Hi,
I'm just wondering if there is a chance for me to try to reproduce an issue again with all debug flags set to on. Should I turn on this flag on controllers(cinder, nova) or compute node logs(with debug flags set to on) should be enough to further troubleshoot this issue? If yes, please let me know which flags are needed here, just to speed up further troubleshooting. As I said - this case is not easy to reproduce, I can't even say what is a trigger here, but we faced it 3 or 4 times already.
Thanks in advance for reply and your helps so far.
Best regards,
Jan

Revision history for this message
![](/+icing/build/overlay/assets/skins/sam/images/close.gif)

| [Gorka Eguileor (gorka)](https://launchpad.net/~gorka) wrote on 2023-02-03: |  |  | [#11](/nova/%2Bbug/2004555/comments/11) |
| --- | --- | --- | --- |

Apologies if I wasn't clear enough.

The disconnect call I say it's probably being ignored/swallowed is the one to os-brick, not Cinder. In other words, Nova first calls os-brick to disconnect the volume from the compute host and then always considers this as successful (at least in some scenarios, probably instance destruction). Since it always considers in those scenarios that local disconnect was successful it calls Cinder to unmap/unexport the volume.

The force=True parameter to os-brick's disconnect\_volume should only be added when the BDM for the volume has the delete on disconnect flag thingy.

OS-Brick has the information, the problem is that multipathd is the one that is adding the leftover devices that have been reused to the multipath device mapper.

Apologies if I wasn't clear enough.
The disconnect call I say it's probably being ignored/swallowed is the one to os-brick, not Cinder. In other words, Nova first calls os-brick to disconnect the volume from the compute host and then always considers this as successful (at least in some scenarios, probably instance destruction). Since it always considers in those scenarios that local disconnect was successful it calls Cinder to unmap/unexport the volume.
The force=True parameter to os-brick's disconnect\_volume should only be added when the BDM for the volume has the delete on disconnect flag thingy.
OS-Brick has the information, the problem is that multipathd is the one that is adding the leftover devices that have been reused to the multipath device mapper.

Revision history for this message
![](/+icing/build/overlay/assets/skins/sam/images/close.gif)

| [Gorka Eguileor (gorka)](https://launchpad.net/~gorka) wrote on 2023-02-03: |  |  | [#12](/nova/%2Bbug/2004555/comments/12) |
| --- | --- | --- | --- |

A solution/workaround would be to change /etc/multipath.conf and set "recheck\_wwid" to yes.

I haven't actually tested it myself, but the documentation explicitly calls out that it's used to solve this specific issue: "If set to yes, when a failed path is restored, the multipathd daemon rechecks the path WWID. If there is a change in the WWID, the path is removed from the current multipath device, and added again as a new path. The multipathd daemon also checks the path WWID again if it is manually re-added."

I believe this is probably something that is best fixed at the deployment tool level. For example extending the multipathing THT template code [1] to support "recheck\_wwid" and defaulting it to yes instead to no like multipath.conf does.

[1]: <https://opendev.org/openstack/tripleo-heat-templates/commit/906d03ea19a4446ed198c321f68791b7fa6e0c47>

A solution/workaround would be to change /etc/multipath.conf and set "recheck\_wwid" to yes.
I haven't actually tested it myself, but the documentation explicitly calls out that it's used to solve this specific issue: "If set to yes, when a failed path is restored, the multipathd daemon rechecks the path WWID. If there is a change in the WWID, the path is removed from the current multipath device, and added again as a new path. The multipathd daemon also checks the path WWID again if it is manually re-added."
I believe this is probably something that is best fixed at the deployment tool level. For example extending the multipathing THT template code [1] to support "recheck\_wwid" and defaulting it to yes instead to no like multipath.conf does.
[1]: https://opendev.org/openstack/tripleo-heat-templates/commit/906d03ea19a4446ed198c321f68791b7fa6e0c47

Revision history for this message
![](/+icing/build/overlay/assets/skins/sam/images/close.gif)

| [Dan Smith (danms)](https://launchpad.net/~danms) wrote on 2023-02-03: |  |  | [#13](/nova/%2Bbug/2004555/comments/13) |
| --- | --- | --- | --- |

Okay, thanks for the clarification.

Yeah, recheck\_wwid seems like it should \*always\* be on to prevent potentially reconnecting to the wrong thing!

Okay, thanks for the clarification.
Yeah, recheck\_wwid seems like it should \*always\* be on to prevent potentially reconnecting to the wrong thing!

Revision history for this message
![](/+icing/build/overlay/assets/skins/sam/images/close.gif)

| [Jeremy Stanley (fungi)](https://launchpad.net/~fungi) wrote on 2023-02-03: |  |  | [#14](/nova/%2Bbug/2004555/comments/14) |
| --- | --- | --- | --- |

If that configuration ends up being the recommended solution, we might want to consider drafting a brief security note with guidance for deployers and maintainers of deployment tooling.

Unless I misunderstand the conditions necessary, it sounds like it would be challenging for a malicious user to force this problem to occur. Is that the current thinking? If so, we could probably safely work on the actual text of the note in public.

If that configuration ends up being the recommended solution, we might want to consider drafting a brief security note with guidance for deployers and maintainers of deployment tooling.
Unless I misunderstand the conditions necessary, it sounds like it would be challenging for a malicious user to force this problem to occur. Is that the current thinking? If so, we could probably safely work on the actual text of the note in public.

| 1 comments hidden Loading more comments | [view all 288 comments](?comments=all) |
| --- | --- |

Revision history for this message
![](/+icing/build/overlay/assets/skins/sam/images/close.gif)

| [melanie witt (melwitt)](https://launchpad.net/~melwitt) wrote on 2023-02-04: |  |  | [#16](/nova/%2Bbug/2004555/comments/16) |
| --- | --- | --- | --- |

> The disconnect call I say it's probably being ignored/swallowed is the one to os-brick, not Cinder. In other words, Nova first calls os-brick to disconnect the volume from the compute host and then always considers this as successful (at least in some scenarios, probably instance destruction). Since it always considers in those scenarios that local disconnect was successful it calls Cinder to unmap/unexport the volume.

I just checked and indeed Nova will ignore a volume disconnect error in the case of an instance being deleted [1]:

try:

        self.\_disconnect\_volume(context, connection\_info, instance)

    except Exception as exc:

        with excutils.save\_and\_reraise\_exception() as ctxt:

            if cleanup\_instance\_disks:

                # Don't block on Volume errors if we're trying to

                # delete the instance as we may be partially created

                # or deleted

                ctxt.reraise = False

                LOG.warning(

                    "Ignoring Volume Error on vol %(vol\_id)s "

                    "during delete %(exc)s",

                    {'vol\_id': vol.get('volume\_id'),

                     'exc': encodeutils.exception\_to\_unicode(exc)},

                    instance=instance)

In all other scenarios, Nova will not proceed further if the disconnect was not successful.

If Nova does proceed past \_disconnect\_volume(), it will later call Cinder API to delete the attachment [2]. I assume that is what does the unmap/unexport.

[1] <https://github.com/openstack/nova/blob/1bf98f128710c374a0141720a7ccc21f5d1afae0/nova/virt/libvirt/driver.py#L1445-L1459> (ussuri)

[2] <https://github.com/openstack/nova/blob/1bf98f128710c374a0141720a7ccc21f5d1afae0/nova/compute/manager.py#L2922> (ussuri)

> The disconnect call I say it's probably being ignored/swallowed is the one to os-brick, not Cinder. In other words, Nova first calls os-brick to disconnect the volume from the compute host and then always considers this as successful (at least in some scenarios, probably instance destruction). Since it always considers in those scenarios that local disconnect was successful it calls Cinder to unmap/unexport the volume.
I just checked and indeed Nova will ignore a volume disconnect error in the case of an instance being deleted [1]:
try:
self.\_disconnect\_volume(context, connection\_info, instance)
except Exception as exc:
with excutils.save\_and\_reraise\_exception() as ctxt:
if cleanup\_instance\_disks:
# Don't block on Volume errors if we're trying to
# delete the instance as we may be partially created
# or deleted
ctxt.reraise = False
LOG.warning(
"Ignoring Volume Error on vol %(vol\_id)s "
"during delete %(exc)s",
{'vol\_id': vol.get('volume\_id'),
'exc': encodeutils.exception\_to\_unicode(exc)},
instance=instance)
In all other scenarios, Nova will not proceed further if the disconnect was not successful.
If Nova does proceed past \_disconnect\_volume(), it will later call Cinder API to delete the attachment [2]. I assume that is what does the unmap/unexport.
[1] https://github.com/openstack/nova/blob/1bf98f128710c374a0141720a7ccc21f5d1afae0/nova/virt/libvirt/driver.py#L1445-L1459 (ussuri)
[2] https://github.com/openstack/nova/blob/1bf98f128710c374a0141720a7ccc21f5d1afae0/nova/compute/manager.py#L2922 (ussuri)

Revision history for this message
![](/+icing/build/overlay/assets/skins/sam/images/close.gif)

| [Jan Wasilewski (janwasilewski)](https://launchpad.net/~janwasilewski) wrote on 2023-02-06: |  |  | [#17](/nova/%2Bbug/2004555/comments/17) |
| --- | --- | --- | --- |

I believe it can be a bit challenging for ubuntu users to introduce recheck\_wwid parameter. What I checked already, this parameter is available for multipath-tools, but the package which provides it is on-board with ubuntu 22.04LTS. Older ubuntu releases do not have this possibility and gives an error:

/etc/multipath.conf line XX, invalid keyword: recheck\_wwid

I made such assumption based on release documentation:

- for ubuntu 20.04: <https://manpages.ubuntu.com/manpages/focal/en/man5/multipath.conf.5.html>

- for ubuntu 22.04: <https://manpages.ubuntu.com/manpages/jammy/en/man5/multipath.conf.5.html>

So it seems that partially Yoga, but fully Zed OS release can take such parameter directly, but older releases should manage such change differently.

I know that OpenStack code is independent of linux distros, but just wanted to add this info here, as worth to consider.

I believe it can be a bit challenging for ubuntu users to introduce recheck\_wwid parameter. What I checked already, this parameter is available for multipath-tools, but the package which provides it is on-board with ubuntu 22.04LTS. Older ubuntu releases do not have this possibility and gives an error:
/etc/multipath.conf line XX, invalid keyword: recheck\_wwid
I made such assumption based on release documentation:
- for ubuntu 20.04: https://manpages.ubuntu.com/manpages/focal/en/man5/multipath.conf.5.html
- for ubuntu 22.04: https://manpages.ubuntu.com/manpages/jammy/en/man5/multipath.conf.5.html
So it seems that partially Yoga, but fully Zed OS release can take such parameter directly, but older releases should manage such change differently.
I know that OpenStack code is independent of linux distros, but just wanted to add this info here, as worth to consider.

Revision history for this message
![](/+icing/build/overlay/assets/skins/sam/images/close.gif)

| [Gorka Eguileor (gorka)](https://launchpad.net/~gorka) wrote on 2023-02-08: |  |  | [#18](/nova/%2Bbug/2004555/comments/18) |
| --- | --- | --- | --- |

I don't know if my assumption is correct or not, because I can't reproduce the multipath device mapper situation from the report (some failed some active) no matter how much I force things to break in different ways.

Since each iSCSI storage backend behaves differently I don't know if I can't reproduce it because the difference in behavior or because the way I'm trying to reproduce it is different. It may even be that multipathd is different in my system.

Unfortunately I don't know if the host where that happened had leftover devices before the leak happened, or what the SCSI IDs of the 2 volumes involved really are.

From os-brick's connect\_volume perspective what it did is the right thing, because when it looked at the multipath device containing the newly connected devices it was dm-9, so that's the one that it should return.

How multipath ended up with 2 different volumes in the same device mapper, I don't know.

I don't think "recheck\_wwid" would solve the issue because os-brick would be too fast in finding the multipath and it wouldn't give enough time for multipathd to activate the paths and form a new device mapper.

In any case I strongly believe that nova should never proceed to delete the cinder attachment if detaching with os-brick fails because that usually implies data loss.

The exception would be when the cinder volume is going to be delete after disconnecting it, and in that case the disconnect call to os-brick should be always forced, since data loss is irrelevant.

That would ensure that compute nodes are not left with leftover devices that could cause problems.

I'll see if I can find a reasonable improvement in os-brick that would detect this issues and fail the connection, although it's probably going to be a bit of a mess.

I don't know if my assumption is correct or not, because I can't reproduce the multipath device mapper situation from the report (some failed some active) no matter how much I force things to break in different ways.
Since each iSCSI storage backend behaves differently I don't know if I can't reproduce it because the difference in behavior or because the way I'm trying to reproduce it is different. It may even be that multipathd is different in my system.
Unfortunately I don't know if the host where that happened had leftover devices before the leak happened, or what the SCSI IDs of the 2 volumes involved really are.
From os-brick's connect\_volume perspective what it did is the right thing, because when it looked at the multipath device containing the newly connected devices it was dm-9, so that's the one that it should return.
How multipath ended up with 2 different volumes in the same device mapper, I don't know.
I don't think "recheck\_wwid" would solve the issue because os-brick would be too fast in finding the multipath and it wouldn't give enough time for multipathd to activate the paths and form a new device mapper.
In any case I strongly believe that nova should never proceed to delete the cinder attachment if detaching with os-brick fails because that usually implies data loss.
The exception would be when the cinder volume is going to be delete after disconnecting it, and in that case the disconnect call to os-brick should be always forced, since data loss is irrelevant.
That would ensure that compute nodes are not left with leftover devices that could cause problems.
I'll see if I can find a reasonable improvement in os-brick that would detect this issues and fail the connection, although it's probably going to be a bit of a mess.

Revision history for this message
![](/+icing/build/overlay/assets/skins/sam/images/close.gif)

| [Jan Wasilewski (janwasilewski)](https://launchpad.net/~janwasilewski) wrote on 2023-02-09: |  |  | [#19](/nova/%2Bbug/2004555/comments/19) |
| --- | --- | --- | --- |

[Download full text](https://bugs.launchpad.net/nova/%2Bbug/2004555/comments/19/%2Bdownload) (6.3 KiB)

@Gorka Eguileor: I can try to reproduce this case with recheck\_wwid option set to true when a valid package of multipath-tools will be available for ubuntu 20.04.

What I can add is that it happened only for one compute node, but I've seen similar warnings in other compute nodes in dmesg -T output, which looks dangerously, but so far I haven't faced similar issue there:

[Thu Feb 9 14:28:16 2023] scsi\_io\_completion: 42 callbacks suppressed

[Thu Feb 9 14:28:16 2023] sd 15:0:0:98: [sdgr] tag#2 FAILED Result: hostbyte=DID\_OK driverbyte=DRIVER\_SENSE

[Thu Feb 9 14:28:16 2023] sd 15:0:0:98: [sdgr] tag#2 Sense Key : Illegal Request [current]

[Thu Feb 9 14:28:16 2023] sd 15:0:0:98: [sdgr] tag#2 Add. Sense: Logical unit not supported

[Thu Feb 9 14:28:16 2023] sd 15:0:0:98: [sdgr] tag#2 CDB: Read(10) 28 00 03 bf ff 00 00 00 08 00

[Thu Feb 9 14:28:16 2023] print\_req\_error: 42 callbacks suppressed

[Thu Feb 9 14:28:16 2023] print\_req\_error: I/O error, dev sdgr, sector 62914304

[Thu Feb 9 14:28:16 2023] sd 15:0:0:98: [sdgr] tag#2 FAILED Result: hostbyte=DID\_OK driverbyte=DRIVER\_SENSE

[Thu Feb 9 14:28:16 2023] sd 15:0:0:98: [sdgr] tag#2 Sense Key : Illegal Request [current]

[Thu Feb 9 14:28:16 2023] sd 15:0:0:98: [sdgr] tag#2 Add. Sense: Logical unit not supported

[Thu Feb 9 14:28:16 2023] sd 15:0:0:98: [sdgr] tag#2 CDB: Read(10) 28 00 03 bf ff 00 00 00 01 00

[Thu Feb 9 14:28:16 2023] print\_req\_error: I/O error, dev sdgr, sector 62914304

[Thu Feb 9 14:28:16 2023] buffer\_io\_error: 30 callbacks suppressed

[Thu Feb 9 14:28:16 2023] Buffer I/O error on dev sdgr1, logical block 62686976, async page read

[Thu Feb 9 14:28:16 2023] sd 15:0:0:98: [sdgr] tag#3 FAILED Result: hostbyte=DID\_OK driverbyte=DRIVER\_SENSE

[Thu Feb 9 14:28:16 2023] sd 15:0:0:98: [sdgr] tag#3 Sense Key : Illegal Request [current]

[Thu Feb 9 14:28:16 2023] sd 15:0:0:98: [sdgr] tag#3 Add. Sense: Logical unit not supported

[Thu Feb 9 14:28:16 2023] sd 15:0:0:98: [sdgr] tag#3 CDB: Read(10) 28 00 03 bf ff 01 00 00 01 00

[Thu Feb 9 14:28:16 2023] print\_req\_error: I/O error, dev sdgr, sector 62914305

[Thu Feb 9 14:28:16 2023] Buffer I/O error on dev sdgr1, logical block 62686977, async page read

[Thu Feb 9 14:28:16 2023] sd 15:0:0:98: [sdgr] tag#4 FAILED Result: hostbyte=DID\_OK driverbyte=DRIVER\_SENSE

[Thu Feb 9 14:28:16 2023] sd 15:0:0:98: [sdgr] tag#4 Sense Key : Illegal Request [current]

[Thu Feb 9 14:28:16 2023] sd 15:0:0:98: [sdgr] tag#4 Add. Sense: Logical unit not supported

[Thu Feb 9 14:28:16 2023] sd 15:0:0:98: [sdgr] tag#4 CDB: Read(10) 28 00 03 bf ff 02 00 00 01 00

[Thu Feb 9 14:28:16 2023] print\_req\_error: I/O error, dev sdgr, sector 62914306

[Thu Feb 9 14:28:16 2023] Buffer I/O error on dev sdgr1, logical block 62686978, async page read

[Thu Feb 9 14:28:16 2023] sd 15:0:0:98: [sdgr] tag#5 FAILED Result: hostbyte=DID\_OK driverbyte=DRIVER\_SENSE

[Thu Feb 9 14:28:16 2023] sd 15:0:0:98: [sdgr] tag#5 Sense Key : Illegal Request [current]

[Thu Feb 9 14:28:16 2023] sd 15:0:0:98: [sdgr] tag#5 Add. Sense: Logical unit not supported

[Thu Feb 9 14:28:16 2023] sd 15:0:0:98: [sdgr] tag#5 CDB: Read(10) 28 00 03 bf ff 03 00 00 01 00

[Thu Feb 9 14:28:16 2023] print\_req...

[Read more...](/nova/%2Bbug/2004555/comments/19)

@Gorka Eguileor: I can try to reproduce this case with recheck\_wwid option set to true when a valid package of multipath-tools will be available for ubuntu 20.04.
What I can add is that it happened only for one compute node, but I've seen similar warnings in other compute nodes in dmesg -T output, which looks dangerously, but so far I haven't faced similar issue there:
[Thu Feb 9 14:28:16 2023] scsi\_io\_completion: 42 callbacks suppressed
[Thu Feb 9 14:28:16 2023] sd 15:0:0:98: [sdgr] tag#2 FAILED Result: hostbyte=DID\_OK driverbyte=DRIVER\_SENSE
[Thu Feb 9 14:28:16 2023] sd 15:0:0:98: [sdgr] tag#2 Sense Key : Illegal Request [current]
[Thu Feb 9 14:28:16 2023] sd 15:0:0:98: [sdgr] tag#2 Add. Sense: Logical unit not supported
[Thu Feb 9 14:28:16 2023] sd 15:0:0:98: [sdgr] tag#2 CDB: Read(10) 28 00 03 bf ff 00 00 00 08 00
[Thu Feb 9 14:28:16 2023] print\_req\_error: 42 callbacks suppressed
[Thu Feb 9 14:28:16 2023] print\_req\_error: I/O error, dev sdgr, sector 62914304
[Thu Feb 9 14:28:16 2023] sd 15:0:0:98: [sdgr] tag#2 FAILED Result: hostbyte=DID\_OK driverbyte=DRIVER\_SENSE
[Thu Feb 9 14:28:16 2023] sd 15:0:0:98: [sdgr] tag#2 Sense Key : Illegal Request [current]
[Thu Feb 9 14:28:16 2023] sd 15:0:0:98: [sdgr] tag#2 Add. Sense: Logical unit not supported
[Thu Feb 9 14:28:16 2023] sd 15:0:0:98: [sdgr] tag#2 CDB: Read(10) 28 00 03 bf ff 00 00 00 01 00
[Thu Feb 9 14:28:16 2023] print\_req\_error: I/O error, dev sdgr, sector 62914304
[Thu Feb 9 14:28:16 2023] buffer\_io\_error: 30 callbacks suppressed
[Thu Feb 9 14:28:16 2023] Buffer I/O error on dev sdgr1, logical block 62686976, async page read
[Thu Feb 9 14:28:16 2023] sd 15:0:0:98: [sdgr] tag#3 FAILED Result: hostbyte=DID\_OK driverbyte=DRIVER\_SENSE
[Thu Feb 9 14:28:16 2023] sd 15:0:0:98: [sdgr] tag#3 Sense Key : Illegal Request [current]
[Thu Feb 9 14:28:16 2023] sd 15:0:0:98: [sdgr] tag#3 Add. Sense: Logical unit not supported
[Thu Feb 9 14:28:16 2023] sd 15:0:0:98: [sdgr] tag#3 CDB: Read(10) 28 00 03 bf ff 01 00 00 01 00
[Thu Feb 9 14:28:16 2023] print\_req\_error: I/O error, dev sdgr, sector 62914305
[Thu Feb 9 14:28:16 2023] Buffer I/O error on dev sdgr1, logical block 62686977, async page read
[Thu Feb 9 14:28:16 2023] sd 15:0:0:98: [sdgr] tag#4 FAILED Result: hostbyte=DID\_OK driverbyte=DRIVER\_SENSE
[Thu Feb 9 14:28:16 2023] sd 15:0:0:98: [sdgr] tag#4 Sense Key : Illegal Request [current]
[Thu Feb 9 14:28:16 2023] sd 15:0:0:98: [sdgr] tag#4 Add. Sense: Logical unit not supported
[Thu Feb 9 14:28:16 2023] sd 15:0:0:98: [sdgr] tag#4 CDB: Read(10) 28 00 03 bf ff 02 00 00 01 00
[Thu Feb 9 14:28:16 2023] print\_req\_error: I/O error, dev sdgr, sector 62914306
[Thu Feb 9 14:28:16 2023] Buffer I/O error on dev sdgr1, logical block 62686978, async page read
[Thu Feb 9 14:28:16 2023] sd 15:0:0:98: [sdgr] tag#5 FAILED Result: hostbyte=DID\_OK driverbyte=DRIVER\_SENSE
[Thu Feb 9 14:28:16 2023] sd 15:0:0:98: [sdgr] tag#5 Sense Key : Illegal Request [current]
[Thu Feb 9 14:28:16 2023] sd 15:0:0:98: [sdgr] tag#5 Add. Sense: Logical unit not supported
[Thu Feb 9 14:28:16 2023] sd 15:0:0:98: [sdgr] tag#5 CDB: Read(10) 28 00 03 bf ff 03 00 00 01 00
[Thu Feb 9 14:28:16 2023] print\_req\_error: I/O error, dev sdgr, sector 62914307
[Thu Feb 9 14:28:16 2023] Buffer I/O error on dev sdgr1, logical block 62686979, async page read
[Thu Feb 9 14:28:16 2023] sd 15:0:0:98: [sdgr] tag#6 FAILED Result: hostbyte=DID\_OK driverbyte=DRIVER\_SENSE
[Thu Feb 9 14:28:16 2023] sd 15:0:0:98: [sdgr] tag#6 Sense Key : Illegal Request [current]
[Thu Feb 9 14:28:16 2023] sd 15:0:0:98: [sdgr] tag#6 Add. Sense: Logical unit not supported
[Thu Feb 9 14:28:16 2023] sd 15:0:0:98: [sdgr] tag#6 CDB: Read(10) 28 00 03 bf ff 04 00 00 01 00
[Thu Feb 9 14:28:16 2023] print\_req\_error: I/O error, dev sdgr, sector 62914308
[Thu Feb 9 14:28:16 2023] Buffer I/O error on dev sdgr1, logical block 62686980, async page read
[Thu Feb 9 14:28:16 2023] sd 15:0:0:98: [sdgr] tag#7 FAILED Result: hostbyte=DID\_OK driverbyte=DRIVER\_SENSE
[Thu Feb 9 14:28:16 2023] sd 15:0:0:98: [sdgr] tag#7 Sense Key : Illegal Request [current]
[Thu Feb 9 14:28:16 2023] sd 15:0:0:98: [sdgr] tag#7 Add. Sense: Logical unit not supported
[Thu Feb 9 14:28:16 2023] sd 15:0:0:98: [sdgr] tag#7 CDB: Read(10) 28 00 03 bf ff 05 00 00 01 00
[Thu Feb 9 14:28:16 2023] print\_req\_error: I/O error, dev sdgr, sector 62914309
[Thu Feb 9 14:28:16 2023] Buffer I/O error on dev sdgr1, logical block 62686981, async page read
[Thu Feb 9 14:28:16 2023] sd 15:0:0:98: [sdgr] tag#8 FAILED Result: hostbyte=DID\_OK driverbyte=DRIVER\_SENSE
[Thu Feb 9 14:28:16 2023] sd 15:0:0:98: [sdgr] tag#8 Sense Key : Illegal Request [current]
[Thu Feb 9 14:28:16 2023] sd 15:0:0:98: [sdgr] tag#8 Add. Sense: Logical unit not supported
[Thu Feb 9 14:28:16 2023] sd 15:0:0:98: [sdgr] tag#8 CDB: Read(10) 28 00 03 bf ff 06 00 00 01 00
[Thu Feb 9 14:28:16 2023] print\_req\_error: I/O error, dev sdgr, sector 62914310
[Thu Feb 9 14:28:16 2023] Buffer I/O error on dev sdgr1, logical block 62686982, async page read
[Thu Feb 9 14:28:16 2023] sd 15:0:0:98: [sdgr] tag#9 FAILED Result: hostbyte=DID\_OK driverbyte=DRIVER\_SENSE
[Thu Feb 9 14:28:16 2023] sd 15:0:0:98: [sdgr] tag#9 Sense Key : Illegal Request [current]
[Thu Feb 9 14:28:16 2023] sd 15:0:0:98: [sdgr] tag#9 Add. Sense: Logical unit not supported
[Thu Feb 9 14:28:16 2023] sd 15:0:0:98: [sdgr] tag#9 CDB: Read(10) 28 00 03 bf ff 07 00 00 01 00
[Thu Feb 9 14:28:16 2023] print\_req\_error: I/O error, dev sdgr, sector 62914311
[Thu Feb 9 14:28:16 2023] Buffer I/O error on dev sdgr1, logical block 62686983, async page read
[Thu Feb 9 14:28:16 2023] sd 15:0:0:98: [sdgr] tag#3 FAILED Result: hostbyte=DID\_OK driverbyte=DRIVER\_SENSE
[Thu Feb 9 14:28:16 2023] sd 15:0:0:98: [sdgr] tag#3 Sense Key : Illegal Request [current]
[Thu Feb 9 14:28:16 2023] sd 15:0:0:98: [sdgr] tag#3 Add. Sense: Logical unit not supported
[Thu Feb 9 14:28:16 2023] sd 15:0:0:98: [sdgr] tag#3 CDB: Read(10) 28 00 00 00 27 80 00 00 08 00
[Thu Feb 9 14:28:16 2023] print\_req\_error: I/O error, dev sdgr, sector 10112
[Thu Feb 9 14:28:16 2023] Buffer I/O error on dev sdgr14, logical block 1008, async page read
[Thu Feb 9 14:28:16 2023] Buffer I/O error on dev sdgr15, logical block 27120, async page read
That repeats every ~15 minutes.
Anyway, I would like to add such prints from dmesg, maybe this way it can show you something which I can't see from my pov.

Revision history for this message
![](/+icing/build/overlay/assets/skins/sam/images/close.gif)

| [Gorka Eguileor (gorka)](https://launchpad.net/~gorka) wrote on 2023-02-09: |  |  | [#20](/nova/%2Bbug/2004555/comments/20) |
| --- | --- | --- | --- |

Don't bother trying with recheck\_wwid, as it won't work due to the speed of os-brick.

Don't bother trying with recheck\_wwid, as it won't work due to the speed of os-brick.

Revision history for this message
![](/+icing/build/overlay/assets/skins/sam/images/close.gif)

| [Gorka Eguileor (gorka)](https://launchpad.net/~gorka) wrote on 2023-02-09: |  |  | [#21](/nova/%2Bbug/2004555/comments/21) |
| --- | --- | --- | --- |

I have finally been able to reproduce the issue.

So far I have been able to identify 3 different ways to create similar situations to the reported one, and it was what I thought, leftover devices from a 'nova delete' call.

Took me longer to figure it out because it requires an iSCSI Cinder driver that uses shared targets, and the one I use doesn't.

After I locally modified the cinder driver code to do target sharing and then force a disconnect error on specific Nova calls to os-brick I was able to work it out.

I have a local patch that detects these issues and fixes them the best it can, but I wouldn't like to backport that, because the fixing is a bit scary as a backport.

So I'll split the code into 2 patches:

- The backportable patch that detects and prevents the connection if a potential leak is detected. To fix this manual intervention will be necessary.

- Another patch that extends the previous code to try to fix things when possible.

I have finally been able to reproduce the issue.
So far I have been able to identify 3 different ways to create similar situations to the reported one, and it was what I thought, leftover devices from a 'nova delete' call.
Took me longer to figure it out because it requires an iSCSI Cinder driver that uses shared targets, and the one I use doesn't.
After I locally modified the cinder driver code to do target sharing and then force a disconnect error on specific Nova calls to os-brick I was able to work it out.
I have a local patch that detects these issues and fixes them the best it can, but I wouldn't like to backport that, because the fixing is a bit scary as a backport.
So I'll split the code into 2 patches:
- The backportable patch that detects and prevents the connection if a potential leak is detected. To fix this manual intervention will be necessary.
- Another patch that extends the previous code to try to fix things when possible.

Revision history for this message
![](/+icing/build/overlay/assets/skins/sam/images/close.gif)

| [melanie witt (melwitt)](https://launchpad.net/~melwitt) wrote on 2023-02-09: |  |  | [#22](/nova/%2Bbug/2004555/comments/22) |
| --- | --- | --- | --- |

> In any case I strongly believe that nova should never proceed to delete the cinder attachment if detaching with os-brick fails because that usually implies data loss.

> The exception would be when the cinder volume is going to be delete after disconnecting it, and in that case the disconnect call to os-brick should be always forced, since data loss is irrelevant.

> That would ensure that compute nodes are not left with leftover devices that could cause problems.

Understood. I guess that must mean that the reported bug scenario is a volume that is \*not\* delete\_on\_termination=True attached to an instance that is being deleted.

I think we could probably propose a patch in nova to not delete the attachment if it's instance delete + not delete\_on\_termination.

> In any case I strongly believe that nova should never proceed to delete the cinder attachment if detaching with os-brick fails because that usually implies data loss.
> The exception would be when the cinder volume is going to be delete after disconnecting it, and in that case the disconnect call to os-brick should be always forced, since data loss is irrelevant.
> That would ensure that compute nodes are not left with leftover devices that could cause problems.
Understood. I guess that must mean that the reported bug scenario is a volume that is \*not\* delete\_on\_termination=True attached to an instance that is being deleted.
I think we could probably propose a patch in nova to not delete the attachment if it's instance delete + not delete\_on\_termination.

Revision history for this message
![](/+icing/build/overlay/assets/skins/sam/images/close.gif)

| [Gorka Eguileor (gorka)](https://launchpad.net/~gorka) wrote on 2023-02-10: |  |  | [#23](/nova/%2Bbug/2004555/comments/23) |
| --- | --- | --- | --- |

Hi Melanie,

In my opinion there should be 2 code changes to prevent leaving devices behind:

- Instance deletion operation should fail like the normal volume-detach call when the disconnect\_volume call fails, even if the instance is left in a "weird" state, manual intervention is usually necessary to fix things.

  This manual intervention does not necessarily mean doing something to the volume, it can be fixing the network.

- Any Cinder volume with delete\_on\_termination=True should have the os-brick call to disconnect\_volume with "force=True, ignore\_errors=True" parameters.

  The tricky part here is that not all os-brick connectors support the force parameter, so when the call fails we have to decide whether to halt the operation and wait for human intervention, or just log it and continue as we are doing today.

  We could make an effort in os-brick to increase coverage of the force parameter.

Thanks,

Gorka.

Hi Melanie,
In my opinion there should be 2 code changes to prevent leaving devices behind:
- Instance deletion operation should fail like the normal volume-detach call when the disconnect\_volume call fails, even if the instance is left in a "weird" state, manual intervention is usually necessary to fix things.
This manual intervention does not necessarily mean doing something to the volume, it can be fixing the network.
- Any Cinder volume with delete\_on\_termination=True should have the os-brick call to disconnect\_volume with "force=True, ignore\_errors=True" parameters.
The tricky part here is that not all os-brick connectors support the force parameter, so when the call fails we have to decide whether to halt the operation and wait for human intervention, or just log it and continue as we are doing today.
We could make an effort in os-brick to increase coverage of the force parameter.
Thanks,
Gorka.

Revision history for this message
![](/+icing/build/overlay/assets/skins/sam/images/close.gif)

| [Dan Smith (danms)](https://launchpad.net/~danms) wrote on 2023-02-10: |  |  | [#24](/nova/%2Bbug/2004555/comments/24) |
| --- | --- | --- | --- |

Our policy is that instance delete should never fail, and I think that's the experience the users expect. Perhaps we need to still mark the instance deleted immediately and continue retrying the volume detach in a periodic until it succeeds, but that's the only thing I can see working.

Our policy is that instance delete should never fail, and I think that's the experience the users expect. Perhaps we need to still mark the instance deleted immediately and continue retrying the volume detach in a periodic until it succeeds, but that's the only thing I can see working.

Revision history for this message
![](/+icing/build/overlay/assets/skins/sam/images/close.gif)

| [Sylvain Bauza (sylvain-bauza)](https://launchpad.net/~sylvain-bauza) wrote on 2023-02-10: |  |  | [#25](/nova/%2Bbug/2004555/comments/25) |
| --- | --- | --- | --- |

Agree with Dan, we shouldn't raise an exception on instance delete but rather possibly make some status available for knowing whether the volume was eventually detached.

For example, we accept to delete an instance if the compute goes down (as the user may not know that the underlying compute is in a bad state) and we only delete the instance when the compute is back.

That being said, I don't really see how we can easily fix this in a patch as we should discuss this correctly. Would a LOG statement adverting that the volume connection is still present would help ?

Agree with Dan, we shouldn't raise an exception on instance delete but rather possibly make some status available for knowing whether the volume was eventually detached.
For example, we accept to delete an instance if the compute goes down (as the user may not know that the underlying compute is in a bad state) and we only delete the instance when the compute is back.
That being said, I don't really see how we can easily fix this in a patch as we should discuss this correctly. Would a LOG statement adverting that the volume connection is still present would help ?

| 1 comments hidden Loading more comments | [view all 288 comments](?comments=all) |
| --- | --- |

Revision history for this message
![](/+icing/build/overlay/assets/skins/sam/images/close.gif)

| [melanie witt (melwitt)](https://launchpad.net/~melwitt) wrote on 2023-02-10: |  |  | [#27](/nova/%2Bbug/2004555/comments/27) |
| --- | --- | --- | --- |

[Download full text](https://bugs.launchpad.net/nova/%2Bbug/2004555/comments/27/%2Bdownload) (3.2 KiB)

We definitely should not allow a delete to fail from a user's perspective.

My suggestion of a patch to not delete an attachment when detach fails during instance delete if delete\_on\_termination=False is intended to be better than what we have today, not necessarily to be perfect.

We could consider doing a periodic like Dan mentions. We already do similar with our "cleanup running deleted instances" periodic. The volume attachment cleanup could be hooked into that if it doesn't already do it.

From what I can tell, our periodic is already capable of taking care of it, but it's not enabled [1][2]:

elif action == 'reap':

        LOG.info("Destroying instance with name label "

                 "'%s' which is marked as "

                 "DELETED but still present on host.",

                 instance.name, instance=instance)

        bdms = objects.BlockDeviceMappingList.get\_by\_instance\_uuid(

            context, instance.uuid, use\_slave=True)

        self.instance\_events.clear\_events\_for\_instance(instance)

        try:

            self.\_shutdown\_instance(context, instance, bdms,

                                    notify=False)

            self.\_cleanup\_volumes(context, instance, bdms,

                                  detach=False)

def \_cleanup\_volumes(self, context, instance, bdms, raise\_exc=True,

                         detach=True):

        original\_exception = None

        for bdm in bdms:

            if detach and bdm.volume\_id:

                try:

                    LOG.debug("Detaching volume: %s", bdm.volume\_id,

                              instance\_uuid=instance.uuid)

                    destroy = bdm.delete\_on\_termination

                    self.\_detach\_volume(context, bdm, instance,

                                        destroy\_bdm=destroy)

                except Exception as exc:

                    original\_exception = exc

                    LOG.warning('Failed to detach volume: %(volume\_id)s '

                                'due to %(exc)s',

                                {'volume\_id': bdm.volume\_id, 'exc': exc})

if bdm.volume\_id and bdm.delete\_on\_termination:

                try:

                    LOG.debug("Deleting volume: %s", bdm.volume\_id,

                              instance\_uuid=instance.uuid)

                    self.volume\_api.delete(context, bdm.volume\_id)

                except Exception as exc:

                    original\_exception = exc

                    LOG.warning('Failed to delete volume: %(volume\_id)s '

                                'due to %(exc)s',

                                {'volume\_id': bdm.volume\_id, 'exc': exc})

        if original\_exception is not None and raise\_exc:

            raise original\_exception

Currently we're calling \_cleanup\_volumes with detach=False. Not sure what the reason for that is but if we determine there should be no problems with it, we can change it to detach=True in combination with not deleting the attachment on instance delete if delete\_on\_termination=False.

[1] <https://github.com/openstack/nova/blob/a2964417822bd1a4a83fa5c27282d2be1e18868a/nova/compute/manager.py#L10579>

[2] <https://github.com/openstack/nova/blob/a2964417822bd1a4a83f>...

[Read more...](/nova/%2Bbug/2004555/comments/27)

We definitely should not allow a delete to fail from a user's perspective.
My suggestion of a patch to not delete an attachment when detach fails during instance delete if delete\_on\_termination=False is intended to be better than what we have today, not necessarily to be perfect.
We could consider doing a periodic like Dan mentions. We already do similar with our "cleanup running deleted instances" periodic. The volume attachment cleanup could be hooked into that if it doesn't already do it.
From what I can tell, our periodic is already capable of taking care of it, but it's not enabled [1][2]:
elif action == 'reap':
LOG.info("Destroying instance with name label "
"'%s' which is marked as "
"DELETED but still present on host.",
instance.name, instance=instance)
bdms = objects.BlockDeviceMappingList.get\_by\_instance\_uuid(
context, instance.uuid, use\_slave=True)
self.instance\_events.clear\_events\_for\_instance(instance)
try:
self.\_shutdown\_instance(context, instance, bdms,
notify=False)
self.\_cleanup\_volumes(context, instance, bdms,
detach=False)
def \_cleanup\_volumes(self, context, instance, bdms, raise\_exc=True,
detach=True):
original\_exception = None
for bdm in bdms:
if detach and bdm.volume\_id:
try:
LOG.debug("Detaching volume: %s", bdm.volume\_id,
instance\_uuid=instance.uuid)
destroy = bdm.delete\_on\_termination
self.\_detach\_volume(context, bdm, instance,
destroy\_bdm=destroy)
except Exception as exc:
original\_exception = exc
LOG.warning('Failed to detach volume: %(volume\_id)s '
'due to %(exc)s',
{'volume\_id': bdm.volume\_id, 'exc': exc})
if bdm.volume\_id and bdm.delete\_on\_termination:
try:
LOG.debug("Deleting volume: %s", bdm.volume\_id,
instance\_uuid=instance.uuid)
self.volume\_api.delete(context, bdm.volume\_id)
except Exception as exc:
original\_exception = exc
LOG.warning('Failed to delete volume: %(volume\_id)s '
'due to %(exc)s',
{'volume\_id': bdm.volume\_id, 'exc': exc})
if original\_exception is not None and raise\_exc:
raise original\_exception
Currently we're calling \_cleanup\_volumes with detach=False. Not sure what the reason for that is but if we determine there should be no problems with it, we can change it to detach=True in combination with not deleting the attachment on instance delete if delete\_on\_termination=False.
[1] https://github.com/openstack/nova/blob/a2964417822bd1a4a83fa5c27282d2be1e18868a/nova/compute/manager.py#L10579
[2] https://github.com/openstack/nova/blob/a2964417822bd1a4a83fa5c27282d2be1e18868a/nova/compute/manager.py#L3183-L3211

Revision history for this message
![](/+icing/build/overlay/assets/skins/sam/images/close.gif)

| [Gorka Eguileor (gorka)](https://launchpad.net/~gorka) wrote on 2023-02-13: |  |  | [#28](/nova/%2Bbug/2004555/comments/28) |
| --- | --- | --- | --- |

What is the reason why Nova has the policy that deleting the instance should never fail?

I'm talking about the instance record, not the VM itself, because I agree that the VM should always be deleted to free resources.

From my perspective deleting the instance record would result in a very weird user experience and in users manually creating the same situation we are trying to avoid.

- User requests instance deletion

- Calls to disconnect\_volume fails

- Nova removes everything it can and at the end even the instance record, while it keeps trying to disconnect the device in the background.

- User wants to use the volume again but sees that it's in-use in Cinder

- Looks for the instance in Nova thinking that something may have gone wrong, but not seeing it there thinks it's a problem between cinder and nova.

- Runs the `cinder delete-attachment` command to return the volume to available state.

We end up in the same situation as we were before, with leftover devices.

What is the reason why Nova has the policy that deleting the instance should never fail?
I'm talking about the instance record, not the VM itself, because I agree that the VM should always be deleted to free resources.
From my perspective deleting the instance record would result in a very weird user experience and in users manually creating the same situation we are trying to avoid.
- User requests instance deletion
- Calls to disconnect\_volume fails
- Nova removes everything it can and at the end even the instance record, while it keeps trying to disconnect the device in the background.
- User wants to use the volume again but sees that it's in-use in Cinder
- Looks for the instance in Nova thinking that something may have gone wrong, but not seeing it there thinks it's a problem between cinder and nova.
- Runs the `cinder delete-attachment` command to return the volume to available state.
We end up in the same situation as we were before, with leftover devices.

Revision history for this message
![](/+icing/build/overlay/assets/skins/sam/images/close.gif)

| [Dan Smith (danms)](https://launchpad.net/~danms) wrote on 2023-02-13: |  |  | [#29](/nova/%2Bbug/2004555/comments/29) |
| --- | --- | --- | --- |

Because the user wants to delete a thing in our supposed "elastic infrastructure". They want their quota back, they want to stop being billed for it, they want the IP for use somewhere else, or whatever. They don't care that we can't delete it because of some backend failure - that's not their problem. That's why we have the ability to queue the delete even if the compute is down - that's how important it is.

It's also not at all about deleting the VM, it's about the instance going away from the perspective of the user (i.e. marking the instance record as deleted). The instance record is what determines if they're billed for it, if their quota is used, etc. We "charge" the user the same whether the VM is running or not. Further, even if we have stopped the VM, we cannot re-assign the resources committed to that VM until the deletion completes in the backend. Another scenario that infuriates operators is "I've deleted a thing, the compute node should be clear, but the scheduler tells me I can't boot something else there."

Your example workflow is exactly why I feel like the solution to this problem can't (entirely) be one of preventing a delete if we fail to detach. Because the admins will just force-delete/detach/reset-state/whatever until things free up (as I would expect to do myself). Especially if the user is demanding that they get their quota back, stop being billed, and/or attach the volume somewhere else.

It seems to me that there \*must\* be some way to ensure that we never attach a volume to the wrong place. Regardless of how we get there, there must be some positive affirmation that we're handing precious volume data to the right person.

Because the user wants to delete a thing in our supposed "elastic infrastructure". They want their quota back, they want to stop being billed for it, they want the IP for use somewhere else, or whatever. They don't care that we can't delete it because of some backend failure - that's not their problem. That's why we have the ability to queue the delete even if the compute is down - that's how important it is.
It's also not at all about deleting the VM, it's about the instance going away from the perspective of the user (i.e. marking the instance record as deleted). The instance record is what determines if they're billed for it, if their quota is used, etc. We "charge" the user the same whether the VM is running or not. Further, even if we have stopped the VM, we cannot re-assign the resources committed to that VM until the deletion completes in the backend. Another scenario that infuriates operators is "I've deleted a thing, the compute node should be clear, but the scheduler tells me I can't boot something else there."
Your example workflow is exactly why I feel like the solution to this problem can't (entirely) be one of preventing a delete if we fail to detach. Because the admins will just force-delete/detach/reset-state/whatever until things free up (as I would expect to do myself). Especially if the user is demanding that they get their quota back, stop being billed, and/or attach the volume somewhere else.
It seems to me that there \*must\* be some way to ensure that we never attach a volume to the wrong place. Regardless of how we get there, there must be some positive affirmation that we're handing precious volume data to the right person.

Revision history for this message
![](/+icing/build/overlay/assets/skins/sam/images/close.gif)

| [Gorka Eguileor (gorka)](https://launchpad.net/~gorka) wrote on 2023-02-14: |  |  | [#30](/nova/%2Bbug/2004555/comments/30) |
| --- | --- | --- | --- |

The quota/billing issue is a matter of Nova code. In cinder we resolve it by having a flag for resources (volume and snapshots) to reflect whether they consume quota or not.

The same thing could be done in Nova to reflect what resources are actually consumed by the instance (IPs, VMs, GPUs, etc) and therefore billable.

Users not caring about backend errors would be, in my opinion, naive thinking on their part, since they DO CARE about their persistent data being properly written and they want to avoid data loss, data corruption, and data leakage above all else.

I assume users would also want to have a consistent view of their resources, so if a volume says it's attached to an instance the instance should still exist, otherwise there is an invalid reference.

Data leak/corruption may be prevented in some cases with the code I'm working on for os-brick (although some drivers are missing the feature required), but that won't prevent data loss. For that Nova would need to do the sensible thing.

I'm going to do some additional testings today, because this report is about something that happens accidentally, but I believe there is a way to actually exploit this to gain access to other users data. Though fixing that would require yet another bunch of code.

In other words, there are 3 different to fix here:

- Nova doing the right thing to prevent data corruption/leak/loss.

- os-brick detection of the right volume to prevent data leak.

- Prevent intentional data leak.

The quota/billing issue is a matter of Nova code. In cinder we resolve it by having a flag for resources (volume and snapshots) to reflect whether they consume quota or not.
The same thing could be done in Nova to reflect what resources are actually consumed by the instance (IPs, VMs, GPUs, etc) and therefore billable.
Users not caring about backend errors would be, in my opinion, naive thinking on their part, since they DO CARE about their persistent data being properly written and they want to avoid data loss, data corruption, and data leakage above all else.
I assume users would also want to have a consistent view of their resources, so if a volume says it's attached to an instance the instance should still exist, otherwise there is an invalid reference.
Data leak/corruption may be prevented in some cases with the code I'm working on for os-brick (although some drivers are missing the feature required), but that won't prevent data loss. For that Nova would need to do the sensible thing.
I'm going to do some additional testings today, because this report is about something that happens accidentally, but I believe there is a way to actually exploit this to gain access to other users data. Though fixing that would require yet another bunch of code.
In other words, there are 3 different to fix here:
- Nova doing the right thing to prevent data corruption/leak/loss.
- os-brick detection of the right volume to prevent data leak.
- Prevent intentional data leak.

Revision history for this message
![](/+icing/build/overlay/assets/skins/sam/images/close.gif)

| [Jeremy Stanley (fungi)](https://launchpad.net/~fungi) wrote on 2023-02-14: |  |  | [#31](/nova/%2Bbug/2004555/comments/31) |
| --- | --- | --- | --- |

If there is indeed a way for a normal user (not an operator) of the environment to cause this information leak to happen and then take advantage of it, we should find a way to prevent at least that aspect before making this report public.

If it's not a condition that a normal user can intentionally cause to happen, then it's probably fine to fix this in public instead.

If there is indeed a way for a normal user (not an operator) of the environment to cause this information leak to happen and then take advantage of it, we should find a way to prevent at least that aspect before making this report public.
If it's not a condition that a normal user can intentionally cause to happen, then it's probably fine to fix this in public instead.

Revision history for this message
![](/+icing/build/overlay/assets/skins/sam/images/close.gif)

| [Sylvain Bauza (sylvain-bauza)](https://launchpad.net/~sylvain-bauza) wrote on 2023-02-14: |  |  | [#32](/nova/%2Bbug/2004555/comments/32) |
| --- | --- | --- | --- |

Gorka, Nova even doesn't really know about the Cinder backends, it just uses os-brick.

So, when Nova asks to attach a volume, only os-brick knows whether it's the right volume. That's why I think it's important to have brick to be able to say 'no'.

Gorka, Nova even doesn't really know about the Cinder backends, it just uses os-brick.
So, when Nova asks to attach a volume, only os-brick knows whether it's the right volume. That's why I think it's important to have brick to be able to say 'no'.

Revision history for this message
![](/+icing/build/overlay/assets/skins/sam/images/close.gif)

| [Dan Smith (danms)](https://launchpad.net/~danms) wrote on 2023-02-14: |  |  | [#33](/nova/%2Bbug/2004555/comments/33) |
| --- | --- | --- | --- |

Right, we have to trust os-brick to give us a block device that is actually the thing we're supposed to attach to the guest.

I'm really concerned about what sounds like a very loose association between what we pass to brick from cinder and what we get back from brick in terms of a block device. Isn't there some way for brick to walk the multipath device and the backing iSCSI/FC devices to check WWNs or something to ensure that it's consistent and points to what we expect?

Right, we have to trust os-brick to give us a block device that is actually the thing we're supposed to attach to the guest.
I'm really concerned about what sounds like a very loose association between what we pass to brick from cinder and what we get back from brick in terms of a block device. Isn't there some way for brick to walk the multipath device and the backing iSCSI/FC devices to check WWNs or something to ensure that it's consistent and points to what we expect?

Revision history for this message
![](/+icing/build/overlay/assets/skins/sam/images/close.gif)

| [Sylvain Bauza (sylvain-bauza)](https://launchpad.net/~sylvain-bauza) wrote on 2023-02-14: |  |  | [#34](/nova/%2Bbug/2004555/comments/34) |
| --- | --- | --- | --- |

> If there is indeed a way for a normal user (not an operator) of the environment to cause this information leak to happen and then take advantage of it, we should find a way to prevent at least t hat aspect before making this report public.

Well, I'm trying hard to find a possible attack vector from a malicious user and I don't see any.

I don't disagree with the bug report as it can potentially leak data to any instance, but I don't know how someone could take benefit of this information.

Here, I'm just one voice and I leave others to chime in, but I'm in favor of making this report public so we can discuss the potential solutions with the stakeholders and any operator having concerns about it.

> If there is indeed a way for a normal user (not an operator) of the environment to cause this information leak to happen and then take advantage of it, we should find a way to prevent at least t hat aspect before making this report public.
Well, I'm trying hard to find a possible attack vector from a malicious user and I don't see any.
I don't disagree with the bug report as it can potentially leak data to any instance, but I don't know how someone could take benefit of this information.
Here, I'm just one voice and I leave others to chime in, but I'm in favor of making this report public so we can discuss the potential solutions with the stakeholders and any operator having concerns about it.

Revision history for this message
![](/+icing/build/overlay/assets/skins/sam/images/close.gif)

| [Gorka Eguileor (gorka)](https://launchpad.net/~gorka) wrote on 2023-02-14: |  |  | [#35](/nova/%2Bbug/2004555/comments/35) |
| --- | --- | --- | --- |

Let me summarize things:

1. The source of the problem reported in this bug is that Nova has been doing something wrong since forever. I've been bringing this up for the past 7 years, and every single time we end up in the same place, nova giving priority to instance deletion over everything else.

2. There are some things that os-brick can do to try to detect when Nova doesn't do its job right, but this is equivalent to a taxi driver asking passengers to learn to fall because the car is not going to stop when they want to get off. It's a lot harder to do and it doesn't sound all that reasonable.

3. There is an attack vector that can be exploited and it's pretty easy to do (I've done it locally) but it's separate from the issue reported here and it hasn't existed for as long as the that one. I would resolve this in a different way than the workaround mentioned in #2.

Seeing as we are back to the same conversation of the past 7 years, we'll probably end up in the same place, so I'll just do my best to resolve the attack vector and also introduce code to resolve Nova's mistakes.

Let me summarize things:
1. The source of the problem reported in this bug is that Nova has been doing something wrong since forever. I've been bringing this up for the past 7 years, and every single time we end up in the same place, nova giving priority to instance deletion over everything else.
2. There are some things that os-brick can do to try to detect when Nova doesn't do its job right, but this is equivalent to a taxi driver asking passengers to learn to fall because the car is not going to stop when they want to get off. It's a lot harder to do and it doesn't sound all that reasonable.
3. There is an attack vector that can be exploited and it's pretty easy to do (I've done it locally) but it's separate from the issue reported here and it hasn't existed for as long as the that one. I would resolve this in a different way than the workaround mentioned in #2.
Seeing as we are back to the same conversation of the past 7 years, we'll probably end up in the same place, so I'll just do my best to resolve the attack vector and also introduce code to resolve Nova's mistakes.

Revision history for this message
![](/+icing/build/overlay/assets/skins/sam/images/close.gif)

| [Gorka Eguileor (gorka)](https://launchpad.net/~gorka) wrote on 2023-02-14: |  |  | [#36](/nova/%2Bbug/2004555/comments/36) |
| --- | --- | --- | --- |

Oh, I failed to clarify something. The user exploit case can be made secure (as far as I can tell), but for the scenario in this bug's description, the only secure solution is fixing nova, the os-brick code I'm working on will only reduce the window were the data is leaked or can be corrupted.

Oh, I failed to clarify something. The user exploit case can be made secure (as far as I can tell), but for the scenario in this bug's description, the only secure solution is fixing nova, the os-brick code I'm working on will only reduce the window were the data is leaked or can be corrupted.

Revision history for this message
![](/+icing/build/overlay/assets/skins/sam/images/close.gif)

| [Sylvain Bauza (sylvain-bauza)](https://launchpad.net/~sylvain-bauza) wrote on 2023-02-14: |  |  | [#37](/nova/%2Bbug/2004555/comments/37) |
| --- | --- | --- | --- |

Gorka, I don't want to debate on projects's responsibility, but I'd rather focus on the data leakage, which is the subject of this security report.

The fact that a volume detach can leave residue if a flush error occurs is certainly not ideal, but this isn't a security problem \*UNTIL\* the remaining devices are reused.

To me, it appears that the data leal occurs on the attach and not on the detach and I'd rather prefer to see os-brick avoiding this situation.

That being said, I think Melanie, Dan and I agreed on trying to find a way to asynchronously clean up the devices (see comments #24 #25 and #27) and that can be discussed publicly, but again, this won't help with the data leakage that occurs on the attach command.

Gorka, I don't want to debate on projects's responsibility, but I'd rather focus on the data leakage, which is the subject of this security report.
The fact that a volume detach can leave residue if a flush error occurs is certainly not ideal, but this isn't a security problem \*UNTIL\* the remaining devices are reused.
To me, it appears that the data leal occurs on the attach and not on the detach and I'd rather prefer to see os-brick avoiding this situation.
That being said, I think Melanie, Dan and I agreed on trying to find a way to asynchronously clean up the devices (see comments #24 #25 and #27) and that can be discussed publicly, but again, this won't help with the data leakage that occurs on the attach command.

Revision history for this message
![](/+icing/build/overlay/assets/skins/sam/images/close.gif)

| [Dan Smith (danms)](https://launchpad.net/~danms) wrote on 2023-02-14: |  |  | [#38](/nova/%2Bbug/2004555/comments/38) |
| --- | --- | --- | --- |

Okay Gorka and I just had a nice long chat about things and I think we made some progress on understanding the (several) ways we can get into this situation and came up with some action items. I'll try to summarize here and I'll look for Gorka to correct me if I get anything wrong.

I think that we're now on the same page that delete of a running instance is much more of a forceful act than some might think, and that we expect to try to be graceful with that, but with a limited amount of patience before we kill it with fire. That maps to us actually always calling force=True when we do the detachment. Even with force=True, brick \*tries\* to flush and disconnect gracefully, but if it can't, will cut things off at the knees. Thus, if we did force=True now, we wouldn't get into the situation the bug describes because we would \*definitely\* have cleaned up at that point.

It sounds like there are some robustification steps that can be made in brick to do more validation of the full chain from instance->multipathd->iscsi->volume when we're doing attachments to try to avoid getting into the situation described by this bug, so Gorka is going to work on that.

Gorka also described another way to get into this situation, which is much more exploitable by the user, and I'll let him describe it in more detail. But the short story is that cinder should not let users delete attachments for instances that nova says are running (i.e. not deleted).

Multipathd, while well-intentioned, also has some behavior that is counterproductive when recovering from various situations where paths to a device get disconnected. Enabling the recheck\_wwid thing in multipathd should be a recommended flag to have enabled to reduce the likelihood of that happening. Especially in the case where nova has allowed a blind delete due to a downed compute node, we need multipathd to not "help" by reattaching things without extra checks.

So, the action items roughly are:

1. Nova should start passing force=True in our call to brick detach for instance delete

2. Recommend the recheck\_wwid flag for multipathd, and get deployment tools to enable it

3. Robustification of brick's attach workflow to do some extra sanity checks

4. Cinder should refuse to allow users to delete an attachment for an active volume

Based on the cinder user-exploitable attack vector, it sounds to me like we should keep this bug private on that basis until we have at least the cinder/nova validation step in place. We could create another one for just that scenario, but publicizing the accidental scenario and discussion we have in this bug now might be enough of a suggestion that more people would figure out the user-oriented attack.

Okay Gorka and I just had a nice long chat about things and I think we made some progress on understanding the (several) ways we can get into this situation and came up with some action items. I'll try to summarize here and I'll look for Gorka to correct me if I get anything wrong.
I think that we're now on the same page that delete of a running instance is much more of a forceful act than some might think, and that we expect to try to be graceful with that, but with a limited amount of patience before we kill it with fire. That maps to us actually always calling force=True when we do the detachment. Even with force=True, brick \*tries\* to flush and disconnect gracefully, but if it can't, will cut things off at the knees. Thus, if we did force=True now, we wouldn't get into the situation the bug describes because we would \*definitely\* have cleaned up at that point.
It sounds like there are some robustification steps that can be made in brick to do more validation of the full chain from instance->multipathd->iscsi->volume when we're doing attachments to try to avoid getting into the situation described by this bug, so Gorka is going to work on that.
Gorka also described another way to get into this situation, which is much more exploitable by the user, and I'll let him describe it in more detail. But the short story is that cinder should not let users delete attachments for instances that nova says are running (i.e. not deleted).
Multipathd, while well-intentioned, also has some behavior that is counterproductive when recovering from various situations where paths to a device get disconnected. Enabling the recheck\_wwid thing in multipathd should be a recommended flag to have enabled to reduce the likelihood of that happening. Especially in the case where nova has allowed a blind delete due to a downed compute node, we need multipathd to not "help" by reattaching things without extra checks.
So, the action items roughly are:
1. Nova should start passing force=True in our call to brick detach for instance delete
2. Recommend the recheck\_wwid flag for multipathd, and get deployment tools to enable it
3. Robustification of brick's attach workflow to do some extra sanity checks
4. Cinder should refuse to allow users to delete an attachment for an active volume
Based on the cinder user-exploitable attack vector, it sounds to me like we should keep this bug private on that basis until we have at least the cinder/nova validation step in place. We could create another one for just that scenario, but publicizing the accidental scenario and discussion we have in this bug now might be enough of a suggestion that more people would figure out the user-oriented attack.

Revision history for this message
![](/+icing/build/overlay/assets/skins/sam/images/close.gif)

| [Gorka Eguileor (gorka)](https://launchpad.net/~gorka) wrote on 2023-02-15: |  |  | [#39](/nova/%2Bbug/2004555/comments/39) |
| --- | --- | --- | --- |

Sylvain, the data leak/corruption presented in this bug report is caused by the detach on the nova side.

It may happen when we do the attach, but it is 100% caused by the detach problem, so just focusing on the attach part is not right considering the RCA is the leftover devices from the detach.

Sylvain, the data leak/corruption presented in this bug report is caused by the detach on the nova side.
It may happen when we do the attach, but it is 100% caused by the detach problem, so just focusing on the attach part is not right considering the RCA is the leftover devices from the detach.

Revision history for this message
![](/+icing/build/overlay/assets/skins/sam/images/close.gif)

| [Sylvain Bauza (sylvain-bauza)](https://launchpad.net/~sylvain-bauza) wrote on 2023-02-15: |  |  | [#40](/nova/%2Bbug/2004555/comments/40) |
| --- | --- | --- | --- |

Gorka, I eventually understood all the problems we have and what Dan wrote at comment #38 look good to me as action items.

Yeah, we need to keep this bug private for a bit until we figure out a solid plan for fixing those 4 items and yeah, we need to both force-delete the attachment while we also try to solidify the attachment calls.

Gorka, I eventually understood all the problems we have and what Dan wrote at comment #38 look good to me as action items.
Yeah, we need to keep this bug private for a bit until we figure out a solid plan for fixing those 4 items and yeah, we need to both force-delete the attachment while we also try to solidify the attachment calls.

[Jeremy Stanley (fungi)](https://launchpad.net/~fungi)
on 2023-05-02

| **description**: | updated |
| --- | --- |
| **summary**: | - [ussuri] Wrong volume attachment - volumes overlapping when connected- through iscsi on host+ Unauthorized volume access through deleted volume attachments+ (CVE-2023-2088) |
| Changed in ossa: | |
| **status**: | Incomplete → In Progress |
| **importance**: | Undecided → High |
| **assignee**: | nobody → Jeremy Stanley (fungi) |

[Jeremy Stanley (fungi)](https://launchpad.net/~fungi)
on 2023-05-10

| **description**: | updated |
| --- | --- |
| **information type**: | Private Security → Public Security |
| Changed in ossn: | |
| **assignee**: | nobody → Jeremy Stanley (fungi) |
| **importance**: | Undecided → High |
| **status**: | New → In Progress |

[OpenStack Infra (hudson-openstack)](https://launchpad.net/~hudson-openstack)
on 2023-05-10

| Changed in glance-store: | |
| --- | --- |
| **status**: | New → In Progress |
| Changed in cinder: | |
| **status**: | New → In Progress |

[Jeremy Stanley (fungi)](https://launchpad.net/~fungi)
on 2023-05-10

| **summary**: | - Unauthorized volume access through deleted volume attachments- (CVE-2023-2088)+ [OSSA-2023-003] Unauthorized volume access through deleted volume+ attachments (CVE-2023-2088) |
| --- | --- |

[OpenStack Infra (hudson-openstack)](https://launchpad.net/~hudson-openstack)
on 2023-05-10

| Changed in os-brick: | |
| --- | --- |
| **status**: | New → In Progress |
| Changed in nova: | |
| **status**: | New → In Progress |

[OpenStack Infra (hudson-openstack)](https://launchpad.net/~hudson-openstack)
on 2023-05-10

| Changed in ossa: | |
| --- | --- |
| **status**: | In Progress → Fix Released |

[Jeremy Stanley (fungi)](https://launchpad.net/~fungi)
on 2023-05-10

| Changed in ossn: | |
| --- | --- |
| **status**: | In Progress → Fix Released |

[OpenStack Infra (hudson-openstack)](https://launchpad.net/~hudson-openstack)
on 2023-05-10

| Changed in glance-store: | |
| --- | --- |
| **status**: | In Progress → Fix Released |

[OpenStack Infra (hudson-openstack)](https://launchpad.net/~hudson-openstack)
on 2023-05-10

| **tags**: | added: in-stable-yoga |
| --- | --- |

[OpenStack Infra (hudson-openstack)](https://launchpad.net/~hudson-openstack)
on 2023-05-10

| Changed in nova: | |
| --- | --- |
| **status**: | In Progress → Fix Released |

[OpenStack Infra (hudson-openstack)](https://launchpad.net/~hudson-openstack)
on 2023-05-10

| **tags**: | added: in-stable-zed |
| --- | --- |

[OpenStack Infra (hudson-openstack)](https://launchpad.net/~hudson-openstack)
on 2023-05-11

| Changed in cinder: | |
| --- | --- |
| **status**: | In Progress → Fix Released |

[Maksim Malchuk (mmalchuk)](https://launchpad.net/~mmalchuk)
on 2023-05-11

| Changed in kolla-ansible: | |
| --- | --- |
| **status**: | New → In Progress |

[OpenStack Infra (hudson-openstack)](https://launchpad.net/~hudson-openstack)
on 2023-05-15

| **tags**: | added: in-stable-wallaby |
| --- | --- |

[OpenStack Infra (hudson-openstack)](https://launchpad.net/~hudson-openstack)
on 2023-05-15

| **tags**: | added: in-stable-xena |
| --- | --- |

| 208 comments hidden Loading more comments | [view all 288 comments](?comments=all) |
| --- | --- |

Revision history for this message
![](/+icing/build/overlay/assets/skins/sam/images/close.gif)

| [OpenStack Infra (hudson-openstack)](https://launchpad.net/~hudson-openstack) wrote on 2023-05-23:  [**Related fix proposed to os-brick (master)**](/nova/%2Bbug/2004555/comments/249) |  |  | [#249](/nova/%2Bbug/2004555/comments/249) |
| --- | --- | --- | --- |

Related fix proposed to branch: master

Review: [https://review.opendev.org/c/openstack/os-brick/+/883951](https://review.opendev.org/c/openstack/os-brick/%2B/883951)

Related fix proposed to branch: master
Review: https://review.opendev.org/c/openstack/os-brick/+/883951

Revision history for this message
![](/+icing/build/overlay/assets/skins/sam/images/close.gif)

| [OpenStack Infra (hudson-openstack)](https://launchpad.net/~hudson-openstack) wrote on 2023-05-24:  [**Fix merged to cinder (stable/xena)**](/nova/%2Bbug/2004555/comments/250) |  |  | [#250](/nova/%2Bbug/2004555/comments/250) |
| --- | --- | --- | --- |

[Download full text](https://bugs.launchpad.net/nova/%2Bbug/2004555/comments/250/%2Bdownload) (3.2 KiB)

Reviewed: [https://review.opendev.org/c/openstack/cinder/+/882839](https://review.opendev.org/c/openstack/cinder/%2B/882839)

Committed: <https://opendev.org/openstack/cinder/commit/68fdc323369943f494541a3510e71290b091359f>

Submitter: "Zuul (22348)"

Branch: stable/xena

commit 68fdc323369943f494541a3510e71290b091359f

Author: Gorka Eguileor <email address hidden>

Date: Thu Feb 16 15:57:15 2023 +0100

Reject unsafe delete attachment calls

Due to how the Linux SCSI kernel driver works there are some storage

    systems, such as iSCSI with shared targets, where a normal user can

    access other projects' volume data connected to the same compute host

    using the attachments REST API.

This affects both single and multi-pathed connections.

To prevent users from doing this, unintentionally or maliciously,

    cinder-api will now reject some delete attachment requests that are

    deemed unsafe.

Cinder will process the delete attachment request normally in the

    following cases:

- The request comes from an OpenStack service that is sending the

      service token that has one of the roles in `service\_token\_roles`.

    - Attachment doesn't have an instance\_uuid value

    - The instance for the attachment doesn't exist in Nova

    - According to Nova the volume is not connected to the instance

    - Nova is not using this attachment record

There are 3 operations in the actions REST API endpoint that can be used

    for an attack:

- `os-terminate\_connection`: Terminate volume attachment

    - `os-detach`: Detach a volume

    - `os-force\_detach`: Force detach a volume

In this endpoint we just won't allow most requests not coming from a

    service. The rules we apply are the same as for attachment delete

    explained earlier, but in this case we may not have the attachment id

    and be more restrictive. This should not be a problem for normal

    operations because:

- Cinder backup doesn't use the REST API but RPC calls via RabbitMQ

    - Glance doesn't use this interface

Checking whether it's a service or not is done at the cinder-api level

    by checking that the service user that made the call has at least one of

    the roles in the `service\_token\_roles` configuration. These roles are

    retrieved from keystone by the keystone middleware using the value of

    the "X-Service-Token" header.

If Cinder is configured with `service\_token\_roles\_required = true` and

    an attacker provides non-service valid credentials the service will

    return a 401 error, otherwise it'll return 409 as if a normal user had

    made the call without the service token.

Closes-Bug: #2004555

    Change-Id: I612905a1bf4a1706cce913c0d8a6df7a240d599a

    (cherry picked from commit 6df1839bdf288107c600b3e53dff7593a6d4c161)

    Conflicts:

            cinder/exception.py

    (cherry picked from commit dd6010a9f7bf8cbe0189992f0848515321781747)

    (cherry picked from commit cb4682fb836912225c5da1536108a0d05fd5c46e)

    Conflicts:

            cinder/exception.py

    (cherry picked from commit a66f4afa22fc5a0a85d5224a6b63dd766fef47b1)

    Conflicts:

            cinder/compute/nova.py

            cinder/tests/unit/attach...

[Read more...](/nova/%2Bbug/2004555/comments/250)

Reviewed: https://review.opendev.org/c/openstack/cinder/+/882839
Committed: https://opendev.org/openstack/cinder/commit/68fdc323369943f494541a3510e71290b091359f
Submitter: "Zuul (22348)"
Branch: stable/xena
commit 68fdc323369943f494541a3510e71290b091359f
Author: Gorka Eguileor <geguileo@redhat.com>
Date: Thu Feb 16 15:57:15 2023 +0100
Reject unsafe delete attachment calls
Due to how the Linux SCSI kernel driver works there are some storage
systems, such as iSCSI with shared targets, where a normal user can
access other projects' volume data connected to the same compute host
using the attachments REST API.
This affects both single and multi-pathed connections.
To prevent users from doing this, unintentionally or maliciously,
cinder-api will now reject some delete attachment requests that are
deemed unsafe.
Cinder will process the delete attachment request normally in the
following cases:
- The request comes from an OpenStack service that is sending the
service token that has one of the roles in `service\_token\_roles`.
- Attachment doesn't have an instance\_uuid value
- The instance for the attachment doesn't exist in Nova
- According to Nova the volume is not connected to the instance
- Nova is not using this attachment record
There are 3 operations in the actions REST API endpoint that can be used
for an attack:
- `os-terminate\_connection`: Terminate volume attachment
- `os-detach`: Detach a volume
- `os-force\_detach`: Force detach a volume
In this endpoint we just won't allow most requests not coming from a
service. The rules we apply are the same as for attachment delete
explained earlier, but in this case we may not have the attachment id
and be more restrictive. This should not be a problem for normal
operations because:
- Cinder backup doesn't use the REST API but RPC calls via RabbitMQ
- Glance doesn't use this interface
Checking whether it's a service or not is done at the cinder-api level
by checking that the service user that made the call has at least one of
the roles in the `service\_token\_roles` configuration. These roles are
retrieved from keystone by the keystone middleware using the value of
the "X-Service-Token" header.
If Cinder is configured with `service\_token\_roles\_required = true` and
an attacker provides non-service valid credentials the service will
return a 401 error, otherwise it'll return 409 as if a normal user had
made the call without the service token.
Closes-Bug: #2004555
Change-Id: I612905a1bf4a1706cce913c0d8a6df7a240d599a
(cherry picked from commit 6df1839bdf288107c600b3e53dff7593a6d4c161)
Conflicts:
cinder/exception.py
(cherry picked from commit dd6010a9f7bf8cbe0189992f0848515321781747)
(cherry picked from commit cb4682fb836912225c5da1536108a0d05fd5c46e)
Conflicts:
cinder/exception.py
(cherry picked from commit a66f4afa22fc5a0a85d5224a6b63dd766fef47b1)
Conflicts:
cinder/compute/nova.py
cinder/tests/unit/attachments/test\_attachments\_api.py
cinder/volume/api.py

Revision history for this message
![](/+icing/build/overlay/assets/skins/sam/images/close.gif)

| [OpenStack Infra (hudson-openstack)](https://launchpad.net/~hudson-openstack) wrote on 2023-05-24:  [**Related fix merged to cinder (master)**](/nova/%2Bbug/2004555/comments/251) |  |  | [#251](/nova/%2Bbug/2004555/comments/251) |
| --- | --- | --- | --- |

Reviewed: [https://review.opendev.org/c/openstack/cinder/+/883360](https://review.opendev.org/c/openstack/cinder/%2B/883360)

Committed: <https://opendev.org/openstack/cinder/commit/1101402b8fda7423b41b2f2e078f8f5a1d2bb4bd>

Submitter: "Zuul (22348)"

Branch: master

commit 1101402b8fda7423b41b2f2e078f8f5a1d2bb4bd

Author: Gorka Eguileor <email address hidden>

Date: Wed May 17 13:42:41 2023 +0200

Doc: Improve service token

This patch extends a bit the documentation for the service token

    configuration, since there have been complains about its clarity and

    completeness.

Related-Bug: #2004555

    Change-Id: Id89497d068c1644e4615fc0fb85c4d1a139ecc19

Reviewed: https://review.opendev.org/c/openstack/cinder/+/883360
Committed: https://opendev.org/openstack/cinder/commit/1101402b8fda7423b41b2f2e078f8f5a1d2bb4bd
Submitter: "Zuul (22348)"
Branch: master
commit 1101402b8fda7423b41b2f2e078f8f5a1d2bb4bd
Author: Gorka Eguileor <geguileo@redhat.com>
Date: Wed May 17 13:42:41 2023 +0200
Doc: Improve service token
This patch extends a bit the documentation for the service token
configuration, since there have been complains about its clarity and
completeness.
Related-Bug: #2004555
Change-Id: Id89497d068c1644e4615fc0fb85c4d1a139ecc19

Revision history for this message
![](/+icing/build/overlay/assets/skins/sam/images/close.gif)

| [OpenStack Infra (hudson-openstack)](https://launchpad.net/~hudson-openstack) wrote on 2023-05-28:  [**Fix proposed to nova (stable/victoria)**](/nova/%2Bbug/2004555/comments/252) |  |  | [#252](/nova/%2Bbug/2004555/comments/252) |
| --- | --- | --- | --- |

Fix proposed to branch: stable/victoria

Review: [https://review.opendev.org/c/openstack/nova/+/884571](https://review.opendev.org/c/openstack/nova/%2B/884571)

Fix proposed to branch: stable/victoria
Review: https://review.opendev.org/c/openstack/nova/+/884571

Revision history for this message
![](/+icing/build/overlay/assets/skins/sam/images/close.gif)

| [OpenStack Infra (hudson-openstack)](https://launchpad.net/~hudson-openstack) wrote on 2023-06-07:  [**Related fix merged to os-brick (stable/xena)**](/nova/%2Bbug/2004555/comments/253) |  |  | [#253](/nova/%2Bbug/2004555/comments/253) |
| --- | --- | --- | --- |

Reviewed: [https://review.opendev.org/c/openstack/os-brick/+/882848](https://review.opendev.org/c/openstack/os-brick/%2B/882848)

Committed: <https://opendev.org/openstack/os-brick/commit/70493735d2f99523c4a23ecbeed15969b2e81f6b>

Submitter: "Zuul (22348)"

Branch: stable/xena

commit 70493735d2f99523c4a23ecbeed15969b2e81f6b

Author: Gorka Eguileor <email address hidden>

Date: Wed Mar 1 13:08:16 2023 +0100

Support force disconnect for FC

This patch adds support for the force and ignore\_errors on the

    disconnect\_volume of the FC connector like we have in the iSCSI

    connector.

Related-Bug: #2004555

    Change-Id: Ia74ecfba03ba23de9d30eb33706245a7f85e1d66

    (cherry picked from commit 570df49db9de3030e658619138588b836c007f8c)

    Conflicts:

            os\_brick/initiator/connectors/fibre\_channel.py

    (cherry picked from commit 111b3931a2db1d5be4ebe704bf26c34fa9408483)

Reviewed: https://review.opendev.org/c/openstack/os-brick/+/882848
Committed: https://opendev.org/openstack/os-brick/commit/70493735d2f99523c4a23ecbeed15969b2e81f6b
Submitter: "Zuul (22348)"
Branch: stable/xena
commit 70493735d2f99523c4a23ecbeed15969b2e81f6b
Author: Gorka Eguileor <geguileo@redhat.com>
Date: Wed Mar 1 13:08:16 2023 +0100
Support force disconnect for FC
This patch adds support for the force and ignore\_errors on the
disconnect\_volume of the FC connector like we have in the iSCSI
connector.
Related-Bug: #2004555
Change-Id: Ia74ecfba03ba23de9d30eb33706245a7f85e1d66
(cherry picked from commit 570df49db9de3030e658619138588b836c007f8c)
Conflicts:
os\_brick/initiator/connectors/fibre\_channel.py
(cherry picked from commit 111b3931a2db1d5be4ebe704bf26c34fa9408483)

Revision history for this message
![](/+icing/build/overlay/assets/skins/sam/images/close.gif)

| [OpenStack Infra (hudson-openstack)](https://launchpad.net/~hudson-openstack) wrote on 2023-06-07:  [**Related fix proposed to cinder (stable/wallaby)**](/nova/%2Bbug/2004555/comments/254) |  |  | [#254](/nova/%2Bbug/2004555/comments/254) |
| --- | --- | --- | --- |

Related fix proposed to branch: stable/wallaby

Review: [https://review.opendev.org/c/openstack/cinder/+/885553](https://review.opendev.org/c/openstack/cinder/%2B/885553)

Related fix proposed to branch: stable/wallaby
Review: https://review.opendev.org/c/openstack/cinder/+/885553

Revision history for this message
![](/+icing/build/overlay/assets/skins/sam/images/close.gif)

| [OpenStack Infra (hudson-openstack)](https://launchpad.net/~hudson-openstack) wrote on 2023-06-07:  [**Related fix proposed to cinder (stable/victoria)**](/nova/%2Bbug/2004555/comments/255) |  |  | [#255](/nova/%2Bbug/2004555/comments/255) |
| --- | --- | --- | --- |

Related fix proposed to branch: stable/victoria

Review: [https://review.opendev.org/c/openstack/cinder/+/885554](https://review.opendev.org/c/openstack/cinder/%2B/885554)

Related fix proposed to branch: stable/victoria
Review: https://review.opendev.org/c/openstack/cinder/+/885554

Revision history for this message
![](/+icing/build/overlay/assets/skins/sam/images/close.gif)

| [OpenStack Infra (hudson-openstack)](https://launchpad.net/~hudson-openstack) wrote on 2023-06-07:  [**Related fix proposed to cinder (stable/ussuri)**](/nova/%2Bbug/2004555/comments/256) |  |  | [#256](/nova/%2Bbug/2004555/comments/256) |
| --- | --- | --- | --- |

Related fix proposed to branch: stable/ussuri

Review: [https://review.opendev.org/c/openstack/cinder/+/885555](https://review.opendev.org/c/openstack/cinder/%2B/885555)

Related fix proposed to branch: stable/ussuri
Review: https://review.opendev.org/c/openstack/cinder/+/885555

Revision history for this message
![](/+icing/build/overlay/assets/skins/sam/images/close.gif)

| [OpenStack Infra (hudson-openstack)](https://launchpad.net/~hudson-openstack) wrote on 2023-06-07:  [**Related fix proposed to cinder (stable/train)**](/nova/%2Bbug/2004555/comments/257) |  |  | [#257](/nova/%2Bbug/2004555/comments/257) |
| --- | --- | --- | --- |

Related fix proposed to branch: stable/train

Review: [https://review.opendev.org/c/openstack/cinder/+/885556](https://review.opendev.org/c/openstack/cinder/%2B/885556)

Related fix proposed to branch: stable/train
Review: https://review.opendev.org/c/openstack/cinder/+/885556

Revision history for this message
![](/+icing/build/overlay/assets/skins/sam/images/close.gif)

| [OpenStack Infra (hudson-openstack)](https://launchpad.net/~hudson-openstack) wrote on 2023-06-07:  [**Related fix proposed to os-brick (stable/wallaby)**](/nova/%2Bbug/2004555/comments/258) |  |  | [#258](/nova/%2Bbug/2004555/comments/258) |
| --- | --- | --- | --- |

Related fix proposed to branch: stable/wallaby

Review: [https://review.opendev.org/c/openstack/os-brick/+/885558](https://review.opendev.org/c/openstack/os-brick/%2B/885558)

Related fix proposed to branch: stable/wallaby
Review: https://review.opendev.org/c/openstack/os-brick/+/885558

Revision history for this message
![](/+icing/build/overlay/assets/skins/sam/images/close.gif)

| [OpenStack Infra (hudson-openstack)](https://launchpad.net/~hudson-openstack) wrote on 2023-06-07:  [**Related fix proposed to os-brick (stable/victoria)**](/nova/%2Bbug/2004555/comments/259) |  |  | [#259](/nova/%2Bbug/2004555/comments/259) |
| --- | --- | --- | --- |

Related fix proposed to branch: stable/victoria

Review: [https://review.opendev.org/c/openstack/os-brick/+/885559](https://review.opendev.org/c/openstack/os-brick/%2B/885559)

Related fix proposed to branch: stable/victoria
Review: https://review.opendev.org/c/openstack/os-brick/+/885559

Revision history for this message
![](/+icing/build/overlay/assets/skins/sam/images/close.gif)

| [OpenStack Infra (hudson-openstack)](https://launchpad.net/~hudson-openstack) wrote on 2023-06-07:  [**Related fix proposed to os-brick (stable/ussuri)**](/nova/%2Bbug/2004555/comments/260) |  |  | [#260](/nova/%2Bbug/2004555/comments/260) |
| --- | --- | --- | --- |

Related fix proposed to branch: stable/ussuri

Review: [https://review.opendev.org/c/openstack/os-brick/+/885560](https://review.opendev.org/c/openstack/os-brick/%2B/885560)

Related fix proposed to branch: stable/ussuri
Review: https://review.opendev.org/c/openstack/os-brick/+/885560

Revision history for this message
![](/+icing/build/overlay/assets/skins/sam/images/close.gif)

| [OpenStack Infra (hudson-openstack)](https://launchpad.net/~hudson-openstack) wrote on 2023-06-07:  [**Related fix proposed to os-brick (stable/train)**](/nova/%2Bbug/2004555/comments/261) |  |  | [#261](/nova/%2Bbug/2004555/comments/261) |
| --- | --- | --- | --- |

Related fix proposed to branch: stable/train

Review: [https://review.opendev.org/c/openstack/os-brick/+/885561](https://review.opendev.org/c/openstack/os-brick/%2B/885561)

Related fix proposed to branch: stable/train
Review: https://review.opendev.org/c/openstack/os-brick/+/885561

Revision history for this message
![](/+icing/build/overlay/assets/skins/sam/images/close.gif)

| [OpenStack Infra (hudson-openstack)](https://launchpad.net/~hudson-openstack) wrote on 2023-06-08:  [**Related fix merged to os-brick (stable/wallaby)**](/nova/%2Bbug/2004555/comments/262) |  |  | [#262](/nova/%2Bbug/2004555/comments/262) |
| --- | --- | --- | --- |

Reviewed: [https://review.opendev.org/c/openstack/os-brick/+/885558](https://review.opendev.org/c/openstack/os-brick/%2B/885558)

Committed: <https://opendev.org/openstack/os-brick/commit/5dcda6b961fa765c817f94a782a6fff48295c89a>

Submitter: "Zuul (22348)"

Branch: stable/wallaby

commit 5dcda6b961fa765c817f94a782a6fff48295c89a

Author: Brian Rosmaita <email address hidden>

Date: Wed Jun 7 18:29:20 2023 -0400

[stable-em-only] Add CVE-2023-2088 warning

The Cinder project team does not intend to backport a fix for

    CVE-2023-2088 to stable/wallaby, so add a warning to the README

    so that consumers are aware of the vulnerability of this branch

    of the os-brick code.

Change-Id: I6345a5a3a7c08c88233b47806c28284fa2dd87d3

    Related-bug: #2004555

Reviewed: https://review.opendev.org/c/openstack/os-brick/+/885558
Committed: https://opendev.org/openstack/os-brick/commit/5dcda6b961fa765c817f94a782a6fff48295c89a
Submitter: "Zuul (22348)"
Branch: stable/wallaby
commit 5dcda6b961fa765c817f94a782a6fff48295c89a
Author: Brian Rosmaita <rosmaita.fossdev@gmail.com>
Date: Wed Jun 7 18:29:20 2023 -0400
[stable-em-only] Add CVE-2023-2088 warning
The Cinder project team does not intend to backport a fix for
CVE-2023-2088 to stable/wallaby, so add a warning to the README
so that consumers are aware of the vulnerability of this branch
of the os-brick code.
Change-Id: I6345a5a3a7c08c88233b47806c28284fa2dd87d3
Related-bug: #2004555

| **tags**: | added: in-stable-ussuri |
| --- | --- |

Revision history for this message
![](/+icing/build/overlay/assets/skins/sam/images/close.gif)

| [OpenStack Infra (hudson-openstack)](https://launchpad.net/~hudson-openstack) wrote on 2023-06-08:  [**Related fix merged to os-brick (stable/ussuri)**](/nova/%2Bbug/2004555/comments/263) |  |  | [#263](/nova/%2Bbug/2004555/comments/263) |
| --- | --- | --- | --- |

Reviewed: [https://review.opendev.org/c/openstack/os-brick/+/885560](https://review.opendev.org/c/openstack/os-brick/%2B/885560)

Committed: <https://opendev.org/openstack/os-brick/commit/2845871c87fc4e6384bd16d81832cc71e2fb0d61>

Submitter: "Zuul (22348)"

Branch: stable/ussuri

commit 2845871c87fc4e6384bd16d81832cc71e2fb0d61

Author: Brian Rosmaita <email address hidden>

Date: Wed Jun 7 18:29:20 2023 -0400

[stable-em-only] Add CVE-2023-2088 warning

The Cinder project team does not intend to backport a fix for

    CVE-2023-2088 to stable/ussuri, so add a warning to the README

    so that consumers are aware of the vulnerability of this branch

    of the os-brick code.

Change-Id: Ie54cfc6697b4e54d37fd66dbad2ff20971399c00

    Related-bug: #2004555

Reviewed: https://review.opendev.org/c/openstack/os-brick/+/885560
Committed: https://opendev.org/openstack/os-brick/commit/2845871c87fc4e6384bd16d81832cc71e2fb0d61
Submitter: "Zuul (22348)"
Branch: stable/ussuri
commit 2845871c87fc4e6384bd16d81832cc71e2fb0d61
Author: Brian Rosmaita <rosmaita.fossdev@gmail.com>
Date: Wed Jun 7 18:29:20 2023 -0400
[stable-em-only] Add CVE-2023-2088 warning
The Cinder project team does not intend to backport a fix for
CVE-2023-2088 to stable/ussuri, so add a warning to the README
so that consumers are aware of the vulnerability of this branch
of the os-brick code.
Change-Id: Ie54cfc6697b4e54d37fd66dbad2ff20971399c00
Related-bug: #2004555

| **tags**: | added: in-stable-victoria |
| --- | --- |

Revision history for this message
![](/+icing/build/overlay/assets/skins/sam/images/close.gif)

| [OpenStack Infra (hudson-openstack)](https://launchpad.net/~hudson-openstack) wrote on 2023-06-08:  [**Related fix merged to os-brick (stable/victoria)**](/nova/%2Bbug/2004555/comments/264) |  |  | [#264](/nova/%2Bbug/2004555/comments/264) |
| --- | --- | --- | --- |

Reviewed: [https://review.opendev.org/c/openstack/os-brick/+/885559](https://review.opendev.org/c/openstack/os-brick/%2B/885559)

Committed: <https://opendev.org/openstack/os-brick/commit/78a0ea24a586139343c98821f9914901f1b5ec5b>

Submitter: "Zuul (22348)"

Branch: stable/victoria

commit 78a0ea24a586139343c98821f9914901f1b5ec5b

Author: Brian Rosmaita <email address hidden>

Date: Wed Jun 7 18:29:20 2023 -0400

[stable-em-only] Add CVE-2023-2088 warning

The Cinder project team does not intend to backport a fix for

    CVE-2023-2088 to stable/victoria, so add a warning to the README

    so that consumers are aware of the vulnerability of this branch

    of the os-brick code.

Change-Id: I37da3be26c7099307b46ae6b6320a3de7658e106

    Related-bug: #2004555

Reviewed: https://review.opendev.org/c/openstack/os-brick/+/885559
Committed: https://opendev.org/openstack/os-brick/commit/78a0ea24a586139343c98821f9914901f1b5ec5b
Submitter: "Zuul (22348)"
Branch: stable/victoria
commit 78a0ea24a586139343c98821f9914901f1b5ec5b
Author: Brian Rosmaita <rosmaita.fossdev@gmail.com>
Date: Wed Jun 7 18:29:20 2023 -0400
[stable-em-only] Add CVE-2023-2088 warning
The Cinder project team does not intend to backport a fix for
CVE-2023-2088 to stable/victoria, so add a warning to the README
so that consumers are aware of the vulnerability of this branch
of the os-brick code.
Change-Id: I37da3be26c7099307b46ae6b6320a3de7658e106
Related-bug: #2004555

| **tags**: | added: in-stable-train |
| --- | --- |

Revision history for this message
![](/+icing/build/overlay/assets/skins/sam/images/close.gif)

| [OpenStack Infra (hudson-openstack)](https://launchpad.net/~hudson-openstack) wrote on 2023-06-08:  [**Related fix merged to os-brick (stable/train)**](/nova/%2Bbug/2004555/comments/265) |  |  | [#265](/nova/%2Bbug/2004555/comments/265) |
| --- | --- | --- | --- |

Reviewed: [https://review.opendev.org/c/openstack/os-brick/+/885561](https://review.opendev.org/c/openstack/os-brick/%2B/885561)

Committed: <https://opendev.org/openstack/os-brick/commit/0cc7019eec2b58f507905d52370a74eb80613b99>

Submitter: "Zuul (22348)"

Branch: stable/train

commit 0cc7019eec2b58f507905d52370a74eb80613b99

Author: Brian Rosmaita <email address hidden>

Date: Wed Jun 7 18:29:20 2023 -0400

[stable-em-only] Add CVE-2023-2088 warning

The Cinder project team does not intend to backport a fix for

    CVE-2023-2088 to stable/train, so add a warning to the README

    so that consumers are aware of the vulnerability of this branch

    of the os-brick code.

Change-Id: I6d04c164521b72538665f53ab62250b14b2710fe

    Related-bug: #2004555

Reviewed: https://review.opendev.org/c/openstack/os-brick/+/885561
Committed: https://opendev.org/openstack/os-brick/commit/0cc7019eec2b58f507905d52370a74eb80613b99
Submitter: "Zuul (22348)"
Branch: stable/train
commit 0cc7019eec2b58f507905d52370a74eb80613b99
Author: Brian Rosmaita <rosmaita.fossdev@gmail.com>
Date: Wed Jun 7 18:29:20 2023 -0400
[stable-em-only] Add CVE-2023-2088 warning
The Cinder project team does not intend to backport a fix for
CVE-2023-2088 to stable/train, so add a warning to the README
so that consumers are aware of the vulnerability of this branch
of the os-brick code.
Change-Id: I6d04c164521b72538665f53ab62250b14b2710fe
Related-bug: #2004555

Revision history for this message
![](/+icing/build/overlay/assets/skins/sam/images/close.gif)

| [OpenStack Infra (hudson-openstack)](https://launchpad.net/~hudson-openstack) wrote on 2023-06-08:  [**Related fix merged to cinder (stable/train)**](/nova/%2Bbug/2004555/comments/266) |  |  | [#266](/nova/%2Bbug/2004555/comments/266) |
| --- | --- | --- | --- |

Reviewed: [https://review.opendev.org/c/openstack/cinder/+/885556](https://review.opendev.org/c/openstack/cinder/%2B/885556)

Committed: <https://opendev.org/openstack/cinder/commit/299553a4fe281cde9b14da34a470dcdb3ed17cc0>

Submitter: "Zuul (22348)"

Branch: stable/train

commit 299553a4fe281cde9b14da34a470dcdb3ed17cc0

Author: Brian Rosmaita <email address hidden>

Date: Wed Jun 7 18:01:12 2023 -0400

[stable-em-only] Add CVE-2023-2088 warning

The Cinder project team does not intend to backport a fix for

    CVE-2023-2088 to stable/train, so add a warning to the README

    so that consumers are aware of the vulnerability of this branch

    of the cinder code.

Change-Id: I1621e3d3d9272a7a25b2d9d9e6710efb6b637a89

    Related-bug: #2004555

Reviewed: https://review.opendev.org/c/openstack/cinder/+/885556
Committed: https://opendev.org/openstack/cinder/commit/299553a4fe281cde9b14da34a470dcdb3ed17cc0
Submitter: "Zuul (22348)"
Branch: stable/train
commit 299553a4fe281cde9b14da34a470dcdb3ed17cc0
Author: Brian Rosmaita <rosmaita.fossdev@gmail.com>
Date: Wed Jun 7 18:01:12 2023 -0400
[stable-em-only] Add CVE-2023-2088 warning
The Cinder project team does not intend to backport a fix for
CVE-2023-2088 to stable/train, so add a warning to the README
so that consumers are aware of the vulnerability of this branch
of the cinder code.
Change-Id: I1621e3d3d9272a7a25b2d9d9e6710efb6b637a89
Related-bug: #2004555

Revision history for this message
![](/+icing/build/overlay/assets/skins/sam/images/close.gif)

| [OpenStack Infra (hudson-openstack)](https://launchpad.net/~hudson-openstack) wrote on 2023-06-08:  [**Related fix merged to cinder (stable/victoria)**](/nova/%2Bbug/2004555/comments/267) |  |  | [#267](/nova/%2Bbug/2004555/comments/267) |
| --- | --- | --- | --- |

Reviewed: [https://review.opendev.org/c/openstack/cinder/+/885554](https://review.opendev.org/c/openstack/cinder/%2B/885554)

Committed: <https://opendev.org/openstack/cinder/commit/63d7848a9548180d283a833beb7c5718e0ad0bdb>

Submitter: "Zuul (22348)"

Branch: stable/victoria

commit 63d7848a9548180d283a833beb7c5718e0ad0bdb

Author: Brian Rosmaita <email address hidden>

Date: Wed Jun 7 18:01:12 2023 -0400

[stable-em-only] Add CVE-2023-2088 warning

The Cinder project team does not intend to backport a fix for

    CVE-2023-2088 to stable/victoria, so add a warning to the README

    so that consumers are aware of the vulnerability of this branch

    of the cinder code.

Change-Id: I2866b0ca1511a53b096b73bbe51a74588cdd8947

    Related-bug: #2004555

Reviewed: https://review.opendev.org/c/openstack/cinder/+/885554
Committed: https://opendev.org/openstack/cinder/commit/63d7848a9548180d283a833beb7c5718e0ad0bdb
Submitter: "Zuul (22348)"
Branch: stable/victoria
commit 63d7848a9548180d283a833beb7c5718e0ad0bdb
Author: Brian Rosmaita <rosmaita.fossdev@gmail.com>
Date: Wed Jun 7 18:01:12 2023 -0400
[stable-em-only] Add CVE-2023-2088 warning
The Cinder project team does not intend to backport a fix for
CVE-2023-2088 to stable/victoria, so add a warning to the README
so that consumers are aware of the vulnerability of this branch
of the cinder code.
Change-Id: I2866b0ca1511a53b096b73bbe51a74588cdd8947
Related-bug: #2004555

Revision history for this message
![](/+icing/build/overlay/assets/skins/sam/images/close.gif)

| [OpenStack Infra (hudson-openstack)](https://launchpad.net/~hudson-openstack) wrote on 2023-06-08:  [**Related fix merged to cinder (stable/ussuri)**](/nova/%2Bbug/2004555/comments/268) |  |  | [#268](/nova/%2Bbug/2004555/comments/268) |
| --- | --- | --- | --- |

Reviewed: [https://review.opendev.org/c/openstack/cinder/+/885555](https://review.opendev.org/c/openstack/cinder/%2B/885555)

Committed: <https://opendev.org/openstack/cinder/commit/60f705d722fc6b7c434194a9f3b11595294d6aa0>

Submitter: "Zuul (22348)"

Branch: stable/ussuri

commit 60f705d722fc6b7c434194a9f3b11595294d6aa0

Author: Brian Rosmaita <email address hidden>

Date: Wed Jun 7 18:01:12 2023 -0400

[stable-em-only] Add CVE-2023-2088 warning

The Cinder project team does not intend to backport a fix for

    CVE-2023-2088 to stable/ussuri, so add a warning to the README

    so that consumers are aware of the vulnerability of this branch

    of the cinder code.

Change-Id: I5c55ab7ca6c85d23c5ab7d2d383a18226735aaf2

    Related-bug: #2004555

Reviewed: https://review.opendev.org/c/openstack/cinder/+/885555
Committed: https://opendev.org/openstack/cinder/commit/60f705d722fc6b7c434194a9f3b11595294d6aa0
Submitter: "Zuul (22348)"
Branch: stable/ussuri
commit 60f705d722fc6b7c434194a9f3b11595294d6aa0
Author: Brian Rosmaita <rosmaita.fossdev@gmail.com>
Date: Wed Jun 7 18:01:12 2023 -0400
[stable-em-only] Add CVE-2023-2088 warning
The Cinder project team does not intend to backport a fix for
CVE-2023-2088 to stable/ussuri, so add a warning to the README
so that consumers are aware of the vulnerability of this branch
of the cinder code.
Change-Id: I5c55ab7ca6c85d23c5ab7d2d383a18226735aaf2
Related-bug: #2004555

Revision history for this message
![](/+icing/build/overlay/assets/skins/sam/images/close.gif)

| [OpenStack Infra (hudson-openstack)](https://launchpad.net/~hudson-openstack) wrote on 2023-06-08:  [**Related fix merged to cinder (stable/wallaby)**](/nova/%2Bbug/2004555/comments/269) |  |  | [#269](/nova/%2Bbug/2004555/comments/269) |
| --- | --- | --- | --- |

Reviewed: [https://review.opendev.org/c/openstack/cinder/+/885553](https://review.opendev.org/c/openstack/cinder/%2B/885553)

Committed: <https://opendev.org/openstack/cinder/commit/2fef6c41fa8c5ea772cde227a119dcf22ce7a07d>

Submitter: "Zuul (22348)"

Branch: stable/wallaby

commit 2fef6c41fa8c5ea772cde227a119dcf22ce7a07d

Author: Brian Rosmaita <email address hidden>

Date: Wed Jun 7 18:01:12 2023 -0400

[stable-em-only] Add CVE-2023-2088 warning

The Cinder project team does not intend to backport a fix for

    CVE-2023-2088 to stable/wallaby, so add a warning to the README

    so that consumers are aware of the vulnerability of this branch

    of the cinder code.

Change-Id: I83b5232076250553650b8b97409cbf72e90c15b9

    Related-bug: #2004555

Reviewed: https://review.opendev.org/c/openstack/cinder/+/885553
Committed: https://opendev.org/openstack/cinder/commit/2fef6c41fa8c5ea772cde227a119dcf22ce7a07d
Submitter: "Zuul (22348)"
Branch: stable/wallaby
commit 2fef6c41fa8c5ea772cde227a119dcf22ce7a07d
Author: Brian Rosmaita <rosmaita.fossdev@gmail.com>
Date: Wed Jun 7 18:01:12 2023 -0400
[stable-em-only] Add CVE-2023-2088 warning
The Cinder project team does not intend to backport a fix for
CVE-2023-2088 to stable/wallaby, so add a warning to the README
so that consumers are aware of the vulnerability of this branch
of the cinder code.
Change-Id: I83b5232076250553650b8b97409cbf72e90c15b9
Related-bug: #2004555

Revision history for this message
![](/+icing/build/overlay/assets/skins/sam/images/close.gif)

| [OpenStack Infra (hudson-openstack)](https://launchpad.net/~hudson-openstack) wrote on 2023-09-14:  [**Fix included in openstack/cinder 23.0.0.0rc1**](/nova/%2Bbug/2004555/comments/270) |  |  | [#270](/nova/%2Bbug/2004555/comments/270) |
| --- | --- | --- | --- |

This issue was fixed in the openstack/cinder 23.0.0.0rc1 release candidate.

This issue was fixed in the openstack/cinder 23.0.0.0rc1 release candidate.

Revision history for this message
![](/+icing/build/overlay/assets/skins/sam/images/close.gif)

| [OpenStack Infra (hudson-openstack)](https://launchpad.net/~hudson-openstack) wrote on 2023-09-21:  [**Fix included in openstack/nova 28.0.0.0rc1**](/nova/%2Bbug/2004555/comments/271) |  |  | [#271](/nova/%2Bbug/2004555/comments/271) |
| --- | --- | --- | --- |

This issue was fixed in the openstack/nova 28.0.0.0rc1 release candidate.

This issue was fixed in the openstack/nova 28.0.0.0rc1 release candidate.

[Michal Nasiadka (mnasiadka)](https://launchpad.net/~mnasiadka)
on 2023-11-07

| Changed in kolla-ansible: | |
| --- | --- |
| **status**: | In Progress → Fix Released |

Revision history for this message
![](/+icing/build/overlay/assets/skins/sam/images/close.gif)

| [OpenStack Infra (hudson-openstack)](https://launchpad.net/~hudson-openstack) wrote on 2024-03-05:  [**Fix included in openstack/nova wallaby-eom**](/nova/%2Bbug/2004555/comments/272) |  |  | [#272](/nova/%2Bbug/2004555/comments/272) |
| --- | --- | --- | --- |

This issue was fixed in the openstack/nova wallaby-eom release.

This issue was fixed in the openstack/nova wallaby-eom release.

Revision history for this message
![](/+icing/build/overlay/assets/skins/sam/images/close.gif)

| [OpenStack Infra (hudson-openstack)](https://launchpad.net/~hudson-openstack) wrote on 2024-03-05:  [**Fix included in openstack/nova xena-eom**](/nova/%2Bbug/2004555/comments/273) |  |  | [#273](/nova/%2Bbug/2004555/comments/273) |
| --- | --- | --- | --- |

This issue was fixed in the openstack/nova xena-eom release.

This issue was fixed in the openstack/nova xena-eom release.

Revision history for this message
![](/+icing/build/overlay/assets/skins/sam/images/close.gif)

| [OpenStack Infra (hudson-openstack)](https://launchpad.net/~hudson-openstack) wrote on 2024-03-05:  [**Change abandoned on nova (stable/victoria)**](/nova/%2Bbug/2004555/comments/274) |  |  | [#274](/nova/%2Bbug/2004555/comments/274) |
| --- | --- | --- | --- |

Change abandoned by "Elod Illes <email address hidden>" on branch: stable/victoria

Review: [https://review.opendev.org/c/openstack/nova/+/884571](https://review.opendev.org/c/openstack/nova/%2B/884571)

Reason: stable/victoria branch of openstack/nova is about to be deleted. To be able to do that, all open patches need to be abandoned. Please cherry pick the patch to unmaintained/victoria if you want to further work on this patch.

Change abandoned by "Elod Illes <elod.illes@est.tech>" on branch: stable/victoria
Review: https://review.opendev.org/c/openstack/nova/+/884571
Reason: stable/victoria branch of openstack/nova is about to be deleted. To be able to do that, all open patches need to be abandoned. Please cherry pick the patch to unmaintained/victoria if you want to further work on this patch.

Revision history for this message
![](/+icing/build/overlay/assets/skins/sam/images/close.gif)

| [OpenStack Infra (hudson-openstack)](https://launchpad.net/~hudson-openstack) wrote on 2024-03-05:  [**Fix included in openstack/cinder xena-eom**](/nova/%2Bbug/2004555/comments/275) |  |  | [#275](/nova/%2Bbug/2004555/comments/275) |
| --- | --- | --- | --- |

This issue was fixed in the openstack/cinder xena-eom release.

This issue was fixed in the openstack/cinder xena-eom release.

Revision history for this message
![](/+icing/build/overlay/assets/skins/sam/images/close.gif)

| [OpenStack Infra (hudson-openstack)](https://launchpad.net/~hudson-openstack) wrote on 2024-03-07:  [**Fix included in openstack/glance\_store xena-eom**](/nova/%2Bbug/2004555/comments/276) |  |  | [#276](/nova/%2Bbug/2004555/comments/276) |
| --- | --- | --- | --- |

This issue was fixed in the openstack/glance\_store xena-eom release.

This issue was fixed in the openstack/glance\_store xena-eom release.

Revision history for this message
![](/+icing/build/overlay/assets/skins/sam/images/close.gif)

| [OpenStack Infra (hudson-openstack)](https://launchpad.net/~hudson-openstack) wrote on 2024-08-14:  [**Fix proposed to nova (unmaintained/victoria)**](/nova/%2Bbug/2004555/comments/277) |  |  | [#277](/nova/%2Bbug/2004555/comments/277) |
| --- | --- | --- | --- |

Fix proposed to branch: unmaintained/victoria

Review: [https://review.opendev.org/c/openstack/nova/+/926257](https://review.opendev.org/c/openstack/nova/%2B/926257)

Fix proposed to branch: unmaintained/victoria
Review: https://review.opendev.org/c/openstack/nova/+/926257

Revision history for this message
![](/+icing/build/overlay/assets/skins/sam/images/close.gif)

| [OpenStack Infra (hudson-openstack)](https://launchpad.net/~hudson-openstack) wrote on 2024-08-14:  [**Related fix proposed to nova (unmaintained/victoria)**](/nova/%2Bbug/2004555/comments/278) |  |  | [#278](/nova/%2Bbug/2004555/comments/278) |
| --- | --- | --- | --- |

Related fix proposed to branch: unmaintained/victoria

Review: [https://review.opendev.org/c/openstack/nova/+/926258](https://review.opendev.org/c/openstack/nova/%2B/926258)

Related fix proposed to branch: unmaintained/victoria
Review: https://review.opendev.org/c/openstack/nova/+/926258

Revision history for this message
![](/+icing/build/overlay/assets/skins/sam/images/close.gif)

| [OpenStack Infra (hudson-openstack)](https://launchpad.net/~hudson-openstack) wrote on 2024-08-14:  [**Related fix proposed to os-brick (unmaintained/wallaby)**](/nova/%2Bbug/2004555/comments/279) |  |  | [#279](/nova/%2Bbug/2004555/comments/279) |
| --- | --- | --- | --- |

Related fix proposed to branch: unmaintained/wallaby

Review: [https://review.opendev.org/c/openstack/os-brick/+/926259](https://review.opendev.org/c/openstack/os-brick/%2B/926259)

Related fix proposed to branch: unmaintained/wallaby
Review: https://review.opendev.org/c/openstack/os-brick/+/926259

Revision history for this message
![](/+icing/build/overlay/assets/skins/sam/images/close.gif)

| [OpenStack Infra (hudson-openstack)](https://launchpad.net/~hudson-openstack) wrote on 2024-08-14:  [**Related fix proposed to os-brick (unmaintained/victoria)**](/nova/%2Bbug/2004555/comments/280) |  |  | [#280](/nova/%2Bbug/2004555/comments/280) |
| --- | --- | --- | --- |

Related fix proposed to branch: unmaintained/victoria

Review: [https://review.opendev.org/c/openstack/os-brick/+/926262](https://review.opendev.org/c/openstack/os-brick/%2B/926262)

Related fix proposed to branch: unmaintained/victoria
Review: https://review.opendev.org/c/openstack/os-brick/+/926262

Revision history for this message
![](/+icing/build/overlay/assets/skins/sam/images/close.gif)

| [OpenStack Infra (hudson-openstack)](https://launchpad.net/~hudson-openstack) wrote on 2024-08-14:  [**Fix proposed to glance\_store (unmaintained/wallaby)**](/nova/%2Bbug/2004555/comments/281) |  |  | [#281](/nova/%2Bbug/2004555/comments/281) |
| --- | --- | --- | --- |

Fix proposed to branch: unmaintained/wallaby

Review: [https://review.opendev.org/c/openstack/glance\_store/+/926277](https://review.opendev.org/c/openstack/glance_store/%2B/926277)

Fix proposed to branch: unmaintained/wallaby
Review: https://review.opendev.org/c/openstack/glance\_store/+/926277

Revision history for this message
![](/+icing/build/overlay/assets/skins/sam/images/close.gif)

| [OpenStack Infra (hudson-openstack)](https://launchpad.net/~hudson-openstack) wrote on 2024-08-14:  [**Fix proposed to cinder (unmaintained/wallaby)**](/nova/%2Bbug/2004555/comments/282) |  |  | [#282](/nova/%2Bbug/2004555/comments/282) |
| --- | --- | --- | --- |

Fix proposed to branch: unmaintained/wallaby

Review: [https://review.opendev.org/c/openstack/cinder/+/926279](https://review.opendev.org/c/openstack/cinder/%2B/926279)

Fix proposed to branch: unmaintained/wallaby
Review: https://review.opendev.org/c/openstack/cinder/+/926279

Revision history for this message
![](/+icing/build/overlay/assets/skins/sam/images/close.gif)

| [OpenStack Infra (hudson-openstack)](https://launchpad.net/~hudson-openstack) wrote on 2024-08-14:  [**Fix proposed to glance\_store (unmaintained/victoria)**](/nova/%2Bbug/2004555/comments/283) |  |  | [#283](/nova/%2Bbug/2004555/comments/283) |
| --- | --- | --- | --- |

Fix proposed to branch: unmaintained/victoria

Review: [https://review.opendev.org/c/openstack/glance\_store/+/926286](https://review.opendev.org/c/openstack/glance_store/%2B/926286)

Fix proposed to branch: unmaintained/victoria
Review: https://review.opendev.org/c/openstack/glance\_store/+/926286

Revision history for this message
![](/+icing/build/overlay/assets/skins/sam/images/close.gif)

| [OpenStack Infra (hudson-openstack)](https://launchpad.net/~hudson-openstack) wrote on 2024-08-27:  [**Related fix merged to os-brick (unmaintained/wallaby)**](/nova/%2Bbug/2004555/comments/284) |  |  | [#284](/nova/%2Bbug/2004555/comments/284) |
| --- | --- | --- | --- |

Reviewed: [https://review.opendev.org/c/openstack/os-brick/+/926259](https://review.opendev.org/c/openstack/os-brick/%2B/926259)

Committed: <https://opendev.org/openstack/os-brick/commit/c00bd3fe2ac537ca1bafb8ed634ce353c49684db>

Submitter: "Zuul (22348)"

Branch: unmaintained/wallaby

commit c00bd3fe2ac537ca1bafb8ed634ce353c49684db

Author: Gorka Eguileor <email address hidden>

Date: Wed Mar 1 13:08:16 2023 +0100

Support force disconnect for FC

This patch adds support for the force and ignore\_errors on the

    disconnect\_volume of the FC connector like we have in the iSCSI

    connector.

Related-Bug: #2004555

    Change-Id: Ia74ecfba03ba23de9d30eb33706245a7f85e1d66

    (cherry picked from commit 570df49db9de3030e658619138588b836c007f8c)

    Conflicts:

            os\_brick/initiator/connectors/fibre\_channel.py

    (cherry picked from commit 111b3931a2db1d5be4ebe704bf26c34fa9408483)

    (cherry picked from commit 70493735d2f99523c4a23ecbeed15969b2e81f6b)

Reviewed: https://review.opendev.org/c/openstack/os-brick/+/926259
Committed: https://opendev.org/openstack/os-brick/commit/c00bd3fe2ac537ca1bafb8ed634ce353c49684db
Submitter: "Zuul (22348)"
Branch: unmaintained/wallaby
commit c00bd3fe2ac537ca1bafb8ed634ce353c49684db
Author: Gorka Eguileor <geguileo@redhat.com>
Date: Wed Mar 1 13:08:16 2023 +0100
Support force disconnect for FC
This patch adds support for the force and ignore\_errors on the
disconnect\_volume of the FC connector like we have in the iSCSI
connector.
Related-Bug: #2004555
Change-Id: Ia74ecfba03ba23de9d30eb33706245a7f85e1d66
(cherry picked from commit 570df49db9de3030e658619138588b836c007f8c)
Conflicts:
os\_brick/initiator/connectors/fibre\_channel.py
(cherry picked from commit 111b3931a2db1d5be4ebe704bf26c34fa9408483)
(cherry picked from commit 70493735d2f99523c4a23ecbeed15969b2e81f6b)

| **tags**: | added: in-unmaintained-wallaby |
| --- | --- |

Revision history for this message
![](/+icing/build/overlay/assets/skins/sam/images/close.gif)

| [OpenStack Infra (hudson-openstack)](https://launchpad.net/~hudson-openstack) wrote on 2024-08-27:  [**Fix merged to glance\_store (unmaintained/wallaby)**](/nova/%2Bbug/2004555/comments/285) |  |  | [#285](/nova/%2Bbug/2004555/comments/285) |
| --- | --- | --- | --- |

Reviewed: [https://review.opendev.org/c/openstack/glance\_store/+/926277](https://review.opendev.org/c/openstack/glance_store/%2B/926277)

Committed: <https://opendev.org/openstack/glance_store/commit/e7060befe487d31d2bd40d232f736b83859318b9>

Submitter: "Zuul (22348)"

Branch: unmaintained/wallaby

commit e7060befe487d31d2bd40d232f736b83859318b9

Author: Brian Rosmaita <email address hidden>

Date: Tue Apr 18 11:22:27 2023 -0400

Add force to os-brick disconnect

In order to be sure that devices are being removed from the host,

    we should be using the 'force' parameter with os-brick's

    disconnect\_volume() method.

Closes-bug: #2004555

    Change-Id: I63d09ad9ef465bc154c85a9ea125449c039d1b90

    (cherry picked from commit 1d8033e54e009bbc4408f6e16aec4f6c01687c91

    (cherry picked from commit a7eed0263e436f841a3c277e051bdc6d6e07447d

    Conflicts: glance\_store/\_drivers/cinder/base.py

            glance\_store/tests/unit/cinder/test\_base.py

    (cherry picked from commit e9d2509926445fd95c9bba9e1cacacb85a5e58af)

    Conflicts: glance\_store/tests/unit/test\_cinder\_base.py

    (cherry picked from commit 28301829777d4b1d2d7bca59fda108158d2ad6ca)

    (cherry picked from commit 1f447bc184500e070cbfcada76b0ea51104919b1)

    Conflicts:

            glance\_store/\_drivers/cinder.py

            glance\_store/common/attachment\_state\_manager.py

            glance\_store/tests/unit/common/test\_attachment\_state\_manager.py

            glance\_store/tests/unit/test\_cinder\_store.py

            glance\_store/tests/unit/test\_multistore\_cinder.py

Reviewed: https://review.opendev.org/c/openstack/glance\_store/+/926277
Committed: https://opendev.org/openstack/glance\_store/commit/e7060befe487d31d2bd40d232f736b83859318b9
Submitter: "Zuul (22348)"
Branch: unmaintained/wallaby
commit e7060befe487d31d2bd40d232f736b83859318b9
Author: Brian Rosmaita <rosmaita.fossdev@gmail.com>
Date: Tue Apr 18 11:22:27 2023 -0400
Add force to os-brick disconnect
In order to be sure that devices are being removed from the host,
we should be using the 'force' parameter with os-brick's
disconnect\_volume() method.
Closes-bug: #2004555
Change-Id: I63d09ad9ef465bc154c85a9ea125449c039d1b90
(cherry picked from commit 1d8033e54e009bbc4408f6e16aec4f6c01687c91
(cherry picked from commit a7eed0263e436f841a3c277e051bdc6d6e07447d
Conflicts: glance\_store/\_drivers/cinder/base.py
glance\_store/tests/unit/cinder/test\_base.py
(cherry picked from commit e9d2509926445fd95c9bba9e1cacacb85a5e58af)
Conflicts: glance\_store/tests/unit/test\_cinder\_base.py
(cherry picked from commit 28301829777d4b1d2d7bca59fda108158d2ad6ca)
(cherry picked from commit 1f447bc184500e070cbfcada76b0ea51104919b1)
Conflicts:
glance\_store/\_drivers/cinder.py
glance\_store/common/attachment\_state\_manager.py
glance\_store/tests/unit/common/test\_attachment\_state\_manager.py
glance\_store/tests/unit/test\_cinder\_store.py
glance\_store/tests/unit/test\_multistore\_cinder.py

Revision history for this message
![](/+icing/build/overlay/assets/skins/sam/images/close.gif)

| [OpenStack Infra (hudson-openstack)](https://launchpad.net/~hudson-openstack) wrote on 2024-08-29:  [**Fix merged to glance\_store (unmaintained/victoria)**](/nova/%2Bbug/2004555/comments/286) |  |  | [#286](/nova/%2Bbug/2004555/comments/286) |
| --- | --- | --- | --- |

Reviewed: [https://review.opendev.org/c/openstack/glance\_store/+/926286](https://review.opendev.org/c/openstack/glance_store/%2B/926286)

Committed: <https://opendev.org/openstack/glance_store/commit/e75a7d5cbb4c6993259c346ebdaf16ed8899f96c>

Submitter: "Zuul (22348)"

Branch: unmaintained/victoria

commit e75a7d5cbb4c6993259c346ebdaf16ed8899f96c

Author: Brian Rosmaita <email address hidden>

Date: Tue Apr 18 11:22:27 2023 -0400

Add force to os-brick disconnect

In order to be sure that devices are being removed from the host,

    we should be using the 'force' parameter with os-brick's

    disconnect\_volume() method.

Closes-bug: #2004555

    Change-Id: I63d09ad9ef465bc154c85a9ea125449c039d1b90

    (cherry picked from commit 1d8033e54e009bbc4408f6e16aec4f6c01687c91

    (cherry picked from commit a7eed0263e436f841a3c277e051bdc6d6e07447d

    Conflicts: glance\_store/\_drivers/cinder/base.py

            glance\_store/tests/unit/cinder/test\_base.py

    (cherry picked from commit e9d2509926445fd95c9bba9e1cacacb85a5e58af)

    Conflicts: glance\_store/tests/unit/test\_cinder\_base.py

    (cherry picked from commit 28301829777d4b1d2d7bca59fda108158d2ad6ca)

    (cherry picked from commit 1f447bc184500e070cbfcada76b0ea51104919b1)

    Conflicts:

            glance\_store/\_drivers/cinder.py

            glance\_store/common/attachment\_state\_manager.py

            glance\_store/tests/unit/common/test\_attachment\_state\_manager.py

            glance\_store/tests/unit/test\_cinder\_store.py

            glance\_store/tests/unit/test\_multistore\_cinder.py

    (cherry picked from commit e7060befe487d31d2bd40d232f736b83859318b9)

Reviewed: https://review.opendev.org/c/openstack/glance\_store/+/926286
Committed: https://opendev.org/openstack/glance\_store/commit/e75a7d5cbb4c6993259c346ebdaf16ed8899f96c
Submitter: "Zuul (22348)"
Branch: unmaintained/victoria
commit e75a7d5cbb4c6993259c346ebdaf16ed8899f96c
Author: Brian Rosmaita <rosmaita.fossdev@gmail.com>
Date: Tue Apr 18 11:22:27 2023 -0400
Add force to os-brick disconnect
In order to be sure that devices are being removed from the host,
we should be using the 'force' parameter with os-brick's
disconnect\_volume() method.
Closes-bug: #2004555
Change-Id: I63d09ad9ef465bc154c85a9ea125449c039d1b90
(cherry picked from commit 1d8033e54e009bbc4408f6e16aec4f6c01687c91
(cherry picked from commit a7eed0263e436f841a3c277e051bdc6d6e07447d
Conflicts: glance\_store/\_drivers/cinder/base.py
glance\_store/tests/unit/cinder/test\_base.py
(cherry picked from commit e9d2509926445fd95c9bba9e1cacacb85a5e58af)
Conflicts: glance\_store/tests/unit/test\_cinder\_base.py
(cherry picked from commit 28301829777d4b1d2d7bca59fda108158d2ad6ca)
(cherry picked from commit 1f447bc184500e070cbfcada76b0ea51104919b1)
Conflicts:
glance\_store/\_drivers/cinder.py
glance\_store/common/attachment\_state\_manager.py
glance\_store/tests/unit/common/test\_attachment\_state\_manager.py
glance\_store/tests/unit/test\_cinder\_store.py
glance\_store/tests/unit/test\_multistore\_cinder.py
(cherry picked from commit e7060befe487d31d2bd40d232f736b83859318b9)

| **tags**: | added: in-unmaintained-victoria |
| --- | --- |

Revision history for this message
![](/+icing/build/overlay/assets/skins/sam/images/close.gif)

| [OpenStack Infra (hudson-openstack)](https://launchpad.net/~hudson-openstack) wrote on 2024-08-30:  [**Related fix merged to os-brick (unmaintained/victoria)**](/nova/%2Bbug/2004555/comments/287) |  |  | [#287](/nova/%2Bbug/2004555/comments/287) |
| --- | --- | --- | --- |

Reviewed: [https://review.opendev.org/c/openstack/os-brick/+/926262](https://review.opendev.org/c/openstack/os-brick/%2B/926262)

Committed: <https://opendev.org/openstack/os-brick/commit/c9a017082663ed882c90bdc5ab5765cf236476fb>

Submitter: "Zuul (22348)"

Branch: unmaintained/victoria

commit c9a017082663ed882c90bdc5ab5765cf236476fb

Author: Gorka Eguileor <email address hidden>

Date: Wed Mar 1 13:08:16 2023 +0100

Support force disconnect for FC

This patch adds support for the force and ignore\_errors on the

    disconnect\_volume of the FC connector like we have in the iSCSI

    connector.

Related-Bug: #2004555

    Change-Id: Ia74ecfba03ba23de9d30eb33706245a7f85e1d66

    (cherry picked from commit 570df49db9de3030e658619138588b836c007f8c)

    Conflicts:

            os\_brick/initiator/connectors/fibre\_channel.py

    (cherry picked from commit 111b3931a2db1d5be4ebe704bf26c34fa9408483)

    (cherry picked from commit 70493735d2f99523c4a23ecbeed15969b2e81f6b)

    (cherry picked from commit c00bd3fe2ac537ca1bafb8ed634ce353c49684db)

Reviewed: https://review.opendev.org/c/openstack/os-brick/+/926262
Committed: https://opendev.org/openstack/os-brick/commit/c9a017082663ed882c90bdc5ab5765cf236476fb
Submitter: "Zuul (22348)"
Branch: unmaintained/victoria
commit c9a017082663ed882c90bdc5ab5765cf236476fb
Author: Gorka Eguileor <geguileo@redhat.com>
Date: Wed Mar 1 13:08:16 2023 +0100
Support force disconnect for FC
This patch adds support for the force and ignore\_errors on the
disconnect\_volume of the FC connector like we have in the iSCSI
connector.
Related-Bug: #2004555
Change-Id: Ia74ecfba03ba23de9d30eb33706245a7f85e1d66
(cherry picked from commit 570df49db9de3030e658619138588b836c007f8c)
Conflicts:
os\_brick/initiator/connectors/fibre\_channel.py
(cherry picked from commit 111b3931a2db1d5be4ebe704bf26c34fa9408483)
(cherry picked from commit 70493735d2f99523c4a23ecbeed15969b2e81f6b)
(cherry picked from commit c00bd3fe2ac537ca1bafb8ed634ce353c49684db)

Revision history for this message
![](/+icing/build/overlay/assets/skins/sam/images/close.gif)

| [OpenStack Infra (hudson-openstack)](https://launchpad.net/~hudson-openstack) wrote on 2024-08-30:  [**Fix merged to nova (unmaintained/victoria)**](/nova/%2Bbug/2004555/comments/288) |  |  | [#288](/nova/%2Bbug/2004555/comments/288) |
| --- | --- | --- | --- |

Reviewed: [https://review.opendev.org/c/openstack/nova/+/926257](https://review.opendev.org/c/openstack/nova/%2B/926257)

Committed: <https://opendev.org/openstack/nova/commit/287612b01479df7a2c1d691478352c2d01a15eaa>

Submitter: "Zuul (22348)"

Branch: unmaintained/victoria

commit 287612b01479df7a2c1d691478352c2d01a15eaa

Author: melanie witt <email address hidden>

Date: Wed Feb 15 22:37:40 2023 +0000

Use force=True for os-brick disconnect during delete

The 'force' parameter of os-brick's disconnect\_volume() method allows

    callers to ignore flushing errors and ensure that devices are being

    removed from the host.

We should use force=True when we are going to delete an instance to

    avoid leaving leftover devices connected to the compute host which

    could then potentially be reused to map to volumes to an instance that

    should not have access to those volumes.

We can use force=True even when disconnecting a volume that will not be

    deleted on termination because os-brick will always attempt to flush

    and disconnect gracefully before forcefully removing devices.

Conflicts:

      nova/cmd/status.py

      nova/tests/unit/cmd/test\_status.py

      nova/tests/unit/virt/libvirt/test\_driver.py

NOTE(elod.illes): the conflicts in status.py and test\_status.py come

    from patch Ic3ae48c57e61c4e45883fbae1328a448be025953, which was merged

    in stable/wallaby when it was master.

    The conflicts in test\_driver.py is caused by missing backport of

    I7f2b6330decb92e2838aa7cee47fb228f00f47da.

Closes-Bug: #2004555

Change-Id: I3629b84d3255a8fe9d8a7cea8c6131d7c40899e8

    (cherry picked from commit db455548a12beac1153ce04eca5e728d7b773901)

    (cherry picked from commit efb01985db88d6333897018174649b425feaa1b4)

    (cherry picked from commit 8b4b99149a35663fc11d7d163082747b1b210b4d)

    (cherry picked from commit 4d8efa2d196f72fdde33136a0b50c4ee8da3c941)

    (cherry picked from commit b574901500d936488cdedf9fda90c4d36eeddd97)

    (cherry picked from commit 5b4cb92aa8adab2bd3d7905e0b76eceab680ab28)

Reviewed: https://review.opendev.org/c/openstack/nova/+/926257
Committed: https://opendev.org/openstack/nova/commit/287612b01479df7a2c1d691478352c2d01a15eaa
Submitter: "Zuul (22348)"
Branch: unmaintained/victoria
commit 287612b01479df7a2c1d691478352c2d01a15eaa
Author: melanie witt <melwittt@gmail.com>
Date: Wed Feb 15 22:37:40 2023 +0000
Use force=True for os-brick disconnect during delete
The 'force' parameter of os-brick's disconnect\_volume() method allows
callers to ignore flushing errors and ensure that devices are being
removed from the host.
We should use force=True when we are going to delete an instance to
avoid leaving leftover devices connected to the compute host which
could then potentially be reused to map to volumes to an instance that
should not have access to those volumes.
We can use force=True even when disconnecting a volume that will not be
deleted on termination because os-brick will always attempt to flush
and disconnect gracefully before forcefully removing devices.
Conflicts:
nova/cmd/status.py
nova/tests/unit/cmd/test\_status.py
nova/tests/unit/virt/libvirt/test\_driver.py
NOTE(elod.illes): the conflicts in status.py and test\_status.py come
from patch Ic3ae48c57e61c4e45883fbae1328a448be025953, which was merged
in stable/wallaby when it was master.
The conflicts in test\_driver.py is caused by missing backport of
I7f2b6330decb92e2838aa7cee47fb228f00f47da.
Closes-Bug: #2004555
Change-Id: I3629b84d3255a8fe9d8a7cea8c6131d7c40899e8
(cherry picked from commit db455548a12beac1153ce04eca5e728d7b773901)
(cherry picked from commit efb01985db88d6333897018174649b425feaa1b4)
(cherry picked from commit 8b4b99149a35663fc11d7d163082747b1b210b4d)
(cherry picked from commit 4d8efa2d196f72fdde33136a0b50c4ee8da3c941)
(cherry picked from commit b574901500d936488cdedf9fda90c4d36eeddd97)
(cherry picked from commit 5b4cb92aa8adab2bd3d7905e0b76eceab680ab28)

[See full activity log](https://bugs.launchpad.net/nova/%2Bbug/2004555/%2Bactivity)

Displaying first 40
and last 40
comments.
[View all 288
comments](/nova/%2Bbug/2004555?comments=all) or [add a comment](/nova/%2Bbug/2004555?comments=all).

* [Report a bug](/nova/%2Bfilebug)

This report contains
**Public Security**
information

Everyone can see this security related information.

You are
[not directly subscribed to this bug's notifications.](/nova/%2Bbug/2004555/%2Bsubscribe)

Subscribing...

* [Edit bug mail](https://bugs.launchpad.net/nova/%2Bbug/2004555/%2Bsubscriptions "View and change your subscriptions to this bug")

## Other bug subscribers

[Subscribe someone else](https://bugs.launchpad.net/nova/%2Bbug/2004555/%2Baddsubscriber "Launchpad will email that person whenever this bugs changes")

## Patches

* [glance\_store-2004555-master\_and\_2023.1.patch](https://bugs.launchpad.net/nova/%2Bbug/2004555/%2Battachment/5665264/%2Bfiles/glance_store-2004555.patch)
  [Edit](/nova/%2Bbug/2004555/%2Battachment/5665264 "Change patch details")
* [tempest-2004555.patch](https://bugs.launchpad.net/nova/%2Bbug/2004555/%2Battachment/5665897/%2Bfiles/tempest-2004555.patch)
  [Edit](/nova/%2Bbug/2004555/%2Battachment/5665897 "Change patch details")
* [nova-2004555-master\_to\_yoga.patch](https://bugs.launchpad.net/nova/%2Bbug/2004555/%2Battachment/5665961/%2Bfiles/nova-2004555.patch)
  [Edit](/nova/%2Bbug/2004555/%2Battachment/5665961 "Change patch details")
* [cinder-2004555-master.patch](https://bugs.launchpad.net/nova/%2Bbug/2004555/%2Battachment/5668187/%2Bfiles/cinder-2004555-master.patch)
  [Edit](/nova/%2Bbug/2004555/%2Battachment/5668187 "Change patch details")
* [cinder-2004555-2023.1\_and\_zed.patch](https://bugs.launchpad.net/nova/%2Bbug/2004555/%2Battachment/5668188/%2Bfiles/cinder-2004555-2023.1_and_zed.patch)
  [Edit](/nova/%2Bbug/2004555/%2Battachment/5668188 "Change patch details")
* [cinder-2004555-yoga.patch](https://bugs.launchpad.net/nova/%2Bbug/2004555/%2Battachment/5668189/%2Bfiles/cinder-2004555-yoga.patch)
  [Edit](/nova/%2Bbug/2004555/%2Battachment/5668189 "Change patch details")
* [cinder-2004555-xena.patch](https://bugs.launchpad.net/nova/%2Bbug/2004555/%2Battachment/5668190/%2Bfiles/cinder-2004555-xena.patch)
  [Edit](/nova/%2Bbug/2004555/%2Battachment/5668190 "Change patch details")
* [osbrick-fc-2004555-yoga\_and\_xena.patch](https://bugs.launchpad.net/nova/%2Bbug/2004555/%2Battachment/5668192/%2Bfiles/osbrick-fc-2004555-yoga_and_xena.patch)
  [Edit](/nova/%2Bbug/2004555/%2Battachment/5668192 "Change patch details")
* [osbrick-leak-2004555-master.patch](https://bugs.launchpad.net/nova/%2Bbug/2004555/%2Battachment/5668212/%2Bfiles/osbrick-leak-2004555-master.patch)
  [Edit](/nova/%2Bbug/2004555/%2Battachment/5668212 "Change patch details")
* [glance\_store-2004555-zed.patch](https://bugs.launchpad.net/nova/%2Bbug/2004555/%2Battachment/5668492/%2Bfiles/glance_store-2004555-zed.patch)
  [Edit](/nova/%2Bbug/2004555/%2Battachment/5668492 "Change patch details")
* [glance\_store-2004555-yoga\_and\_xena.patch](https://bugs.launchpad.net/nova/%2Bbug/2004555/%2Battachment/5668493/%2Bfiles/glance_store-2004555-yoga_and_xena.patch)
  [Edit](/nova/%2Bbug/2004555/%2Battachment/5668493 "Change patch details")
* [osbrick-fc-2004555-master\_to\_zed.patch](https://bugs.launchpad.net/nova/%2Bbug/2004555/%2Battachment/5668644/%2Bfiles/osbrick-fc-2004555-master_to_zed.patch)
  [Edit](/nova/%2Bbug/2004555/%2Battachment/5668644 "Change patch details")
* [nova-2004555-xena.patch](https://bugs.launchpad.net/nova/%2Bbug/2004555/%2Battachment/5669261/%2Bfiles/nova-2004555-xena_and_wallaby.patch)
  [Edit](/nova/%2Bbug/2004555/%2Battachment/5669261 "Change patch details")

* [Add patch](/nova/%2Bbug/2004555/%2Baddcomment?field.patch=on)

## Remote bug watches

Bug watches keep track of this bug in other bug trackers.

[![Launchpad](/@@/launchpad-footer-logo.svg)](https://launchpad.net/)
 •
[Take the tour](https://launchpad.net/%2Btour)
 •
[Read the guide](https://help.launchpad.net/)

© 2004
[Canonical Ltd.](http://canonical.com/)
 •
[Terms of use](https://launchpad.net/legal)
 •
[Data privacy](https://www.ubuntu.com/legal/dataprivacy)
 •
[Contact Launchpad Support](/feedback)
 •
[Blog](http://blog.launchpad.net/)
 •
[Careers](https://canonical.com/careers)
 •
[System status](https://ubuntu.social/%40launchpadstatus)
 •
6394e03
([Get the code!](https://dev.launchpad.net/))



=== Content from security.openstack.org_ce9d6533_20250114_183603.html ===


Toggle navigation

 Search

* [Software](https://www.openstack.org/software/)
  + [Overview](https://www.openstack.org/software/)
  + [OpenStack Components](https://www.openstack.org/software/project-navigator/openstack-components)
  + [SDKs](https://www.openstack.org/software/project-navigator/sdks)
  + [Deployment Tools](https://www.openstack.org/software/project-navigator/deployment-tools)
  + [OpenStack Map](https://www.openstack.org/assets/software/projectmap/openstack-map.pdf)
  + [Sample Configs](https://www.openstack.org/software/sample-configs/)
* [Use Cases](https://www.openstack.org/use-cases/)
  + [Users in Production](https://www.openstack.org/use-cases/)
  + [Ironic Bare Metal](https://www.openstack.org/use-cases/bare-metal/)
  + [Edge Computing](https://www.openstack.org/use-cases/edge-computing/)
  + [Telecom & NFV](https://www.openstack.org/use-cases/telecoms-and-nfv/)
  + [Science and HPC](https://www.openstack.org/use-cases/science/)
  + [Containers](https://www.openstack.org/use-cases/containers/)
  + [Enterprise](https://www.openstack.org/use-cases/enterprise/)
  + [User Survey](https://www.openstack.org/surveys/landing)
* [Events](https://openinfra.dev/summit)
  + [OpenInfra Summit](https://openinfra.dev/summit)
  + [Project Teams Gathering](https://www.openstack.org/ptg/)
  + [OpenDev](https://www.openstack.org/events/opendev-2020/)
  + [Community Events](https://www.openstack.org/events/community-events/)
  + [OpenStack & OpenInfra Days](https://www.openstack.org/events/openstackdays)
  + [Summit Videos](https://www.openstack.org/videos/)
* [Community](https://www.openstack.org/community/)
  + [Welcome! Start Here](https://www.openstack.org/community/)
  + [OpenStack Technical Committee](https://www.openstack.org/community/tech-committee)
  + [Speakers Bureau](https://www.openstack.org/community/speakers/)
  + [OpenStack Wiki](http://wiki.openstack.org)
  + [Get Certified (COA)](https://www.openstack.org/coa/)
  + [Jobs](https://www.openstack.org/community/jobs/)
  + [Marketing Resources](https://www.openstack.org/marketing/)
  + [Community News](https://www.openstack.org/news/)
  + [Superuser Magazine](http://superuser.openstack.org)
  + [OpenInfra Foundation Supporting Organizations](https://www.openstack.org/community/supporting-organizations/)
  + [OpenInfra Foundation](https://openinfra.dev)
* [Marketplace](https://www.openstack.org/marketplace/)
  + [Training](https://www.openstack.org/marketplace/training/)
  + [Distros & Appliances](https://www.openstack.org/marketplace/distros/)
  + [Public Clouds](https://www.openstack.org/marketplace/public-clouds/)
  + [Hosted Private Clouds](https://www.openstack.org/marketplace/hosted-private-clouds/)
  + [Remotely Managed Private Clouds](https://www.openstack.org/marketplace/remotely-managed-private-clouds/)
  + [Consulting & Integrators](https://www.openstack.org/marketplace/consulting/)
  + [Drivers](https://www.openstack.org/marketplace/drivers/)
* [Blog](https://www.openstack.org/blog/)
* [Docs](http://docs.openstack.org/)
* [Join](https://openinfra.dev/join/)
  + [Sign up for Foundation Membership](https://openinfra.dev/join/)
  + [Sponsor the Foundation](https://openinfra.dev/join/)
  + [More about the Foundation](https://openinfra.dev)
* [Log In](https://www.openstack.org/Security/login/?BackURL=/home/)

# OSSA-2023-003: Unauthorized volume access through deleted volume attachments

# OSSA-2023-003: Unauthorized volume access through deleted volume attachments[Â¶](#ossa-2023-003-unauthorized-volume-access-through-deleted-volume-attachments "Link to this heading")

Date:

May 10, 2023

CVE:

CVE-2023-2088

## Affects[Â¶](#affects "Link to this heading")

* Cinder: <20.2.1, >=21.0.0 <21.2.1, ==22.0.0
* Glance\_store: <3.0.1, >=4.0.0 <4.1.1, >=4.2.0 <4.3.1
* Nova: <25.1.2, >=26.0.0 <26.1.2, ==27.0.0
* Os-brick: <5.2.3, >=6.0.0 <6.1.1, >=6.2.0 <6.2.2

## Description[Â¶](#description "Link to this heading")

An unauthorized access to a volume could occur when an iSCSI or FC
connection from a host is severed due to a volume being unmapped on
the storage system and the device is later reused for another volume
on the same host.

**Scope:** Only deployments with iSCSI or FC volumes are affected.
However, the fix for this issue includes a configuration change in
Nova and Cinder that may impact you on your next upgrade regardless
of what backend storage technology you use. See the *Configuration
change* section below, and item 4(B) in the *Patches and Associated
Deployment Changes* for details.

This data leak can be triggered by two different situations.

**Accidental case:** If there is a problem with network connectivity
during a normal detach operation, OpenStack may fail to clean the
situation up properly. Instead of force-detaching the compute node
device, Nova ignores the error, assuming the instance has already
been deleted. Due to this incomplete operation OpenStack may end up
selecting the wrong multipath device when connecting another volume
to an instance.

**Intentional case:** A regular user can create an instance with a
volume, and then delete the volume attachment directly in Cinder,
which neglects to notify Nova. The compute node SCSI plumbing (over
iSCSI/FC) will continue trying to connect to the original
host/port/LUN, not knowing the attachment has been deleted. If a
subsequent volume attachment re-uses the host/port/LUN for a
different instance and volume, the original instance will gain
access to it once the SCSI plumbing reconnects.

### Configuration Change[Â¶](#configuration-change "Link to this heading")

To prevent the intentional case, the Block Storage API provided by
Cinder must only accept attachment delete requests from Nova for
instance-attached volumes. A complicating factor is that Nova
deletes an attachment by making a call to the Block Storage API on
behalf of the user (that is, by passing the userâs token), which
makes the request indistinguishable from the user making this
request directly. The solution is to have Nova include a service
token along with the userâs token so that Cinder can determine that
the detach request is coming from Nova. The ability for Nova to pass
a service token has been supported since Ocata, but has not been
required until now. Thus, deployments that are not currently sending
service user credentials from Nova will need to apply the relevant
code changes and also make configuration changes to solve the
problem.

### Patches and Associated Deployment Changes[Â¶](#patches-and-associated-deployment-changes "Link to this heading")

Given the above analysis, a thorough fix must include the following
elements:

1. The os-brick library must implement the `force` option for
   fibre channel, which which has only been available for iSCSI
   until now (covered by the linked patches).
2. Nova must call os-brick with the `force` option when
   disconnecting volumes from deleted instances (covered by the
   linked patches).
3. In deployments where Glance uses the cinder glance\_store driver,
   glance must call os-brick with the `force` option when
   disconnecting volumes (covered by the linked patches).
4. Cinder must distinguish between safe and unsafe attachment delete
   requests and reject the unsafe ones. This part of the fix has two
   components:

   1. The Block Storage API will return a 409 (Conflict) for a
      request to delete an attachment if there is an instance
      currently using the attachment, **unless** the request is
      being made by a service (for example, Nova) on behalf of a
      user (covered by the linked patches).
   2. In order to recognize that a request is being made by a
      service on behalf of a user, Nova must be configured to send a
      service token along with the user token. If this configuration
      change is not made, the cinder change will reject **any**
      request to delete an attachment associated with a volume that
      is attached to an instance. Nova must be configured to send a
      service token to Cinder, and Cinder must be configured to
      accept service tokens. This is described in the following
      document and **IS NOT AUTOMATICALLY APPLIED BY THE LINKED
      PATCHES:** (Using service tokens to prevent long-running job
      failures)
      <https://docs.openstack.org/cinder/latest/configuration/block-storage/service-token.html>
      The Nova patch mentioned in step 2 includes a similar document
      more focused on Nova:
      doc/source/admin/configuration/service-user-token.rst
5. The cinder glance\_store driver does not attach volumes to
   instances; instead, it attaches volumes directly to the Glance
   node. Thus, the Cinder change in step 4 will recognize an
   attachment-delete request coming from Glance as safe and allow
   it. (Of course, we expect that you will have applied the patches
   in steps 1 and 3 to your Glance nodes.)

## Errata[Â¶](#errata "Link to this heading")

An additional nova patch is required to fix a minor regression in periodic tasks and some nova-manage actions (errata 1). Also a patch to tempest is needed to account for behavior changes with fixes in place (errata 2). The stable/wallaby branch fix for nova introduced a regression which was addressed through subsequent adjustment of its patches after the advisory was published (errata 3).

## Patches[Â¶](#patches "Link to this heading")

* <https://review.opendev.org/882836> (2023.1/antelope cinder)
* <https://review.opendev.org/882851> (2023.1/antelope glance\_store)
* <https://review.opendev.org/882858> (2023.1/antelope nova)
* <https://review.opendev.org/882859> (2023.1/antelope nova errata 1)
* <https://review.opendev.org/882843> (2023.1/antelope os-brick)
* <https://review.opendev.org/882835> (2023.2/bobcat cinder)
* <https://review.opendev.org/882834> (2023.2/bobcat glance\_store)
* <https://review.opendev.org/882847> (2023.2/bobcat nova)
* <https://review.opendev.org/882852> (2023.2/bobcat nova errata 1)
* <https://review.opendev.org/882840> (2023.2/bobcat os-brick)
* <https://review.opendev.org/882876> (2023.2/bobcat tempest errata 2)
* <https://review.opendev.org/882869> (Wallaby nova)
* <https://review.opendev.org/882870> (Wallaby nova errata 1)
* <https://review.opendev.org/882839> (Xena cinder)
* <https://review.opendev.org/882855> (Xena glance\_store)
* <https://review.opendev.org/882867> (Xena nova)
* <https://review.opendev.org/882868> (Xena nova errata 1)
* <https://review.opendev.org/882848> (Xena os-brick)
* <https://review.opendev.org/882838> (Yoga cinder)
* <https://review.opendev.org/882854> (Yoga glance\_store)
* <https://review.opendev.org/882863> (Yoga nova)
* <https://review.opendev.org/882864> (Yoga nova errata 1)
* <https://review.opendev.org/882846> (Yoga os-brick)
* <https://review.opendev.org/882837> (Zed cinder)
* <https://review.opendev.org/882853> (Zed glance\_store)
* <https://review.opendev.org/882860> (Zed nova)
* <https://review.opendev.org/882861> (Zed nova errata 1)
* <https://review.opendev.org/882844> (Zed os-brick)

## Credits[Â¶](#credits "Link to this heading")

* Jan Wasilewski from Atman (CVE-2023-2088)
* Gorka Eguileor from Red Hat (CVE-2023-2088)

## References[Â¶](#references "Link to this heading")

* <https://launchpad.net/bugs/2004555>
* <http://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2023-2088>

## Notes[Â¶](#notes "Link to this heading")

* Limited Protection Against Accidentsâ¦ If you are only concerned with
  protecting against the accidental case described earlier in this
  document, steps 1-3 above should be sufficient. Note, however, that
  only applying steps 1-3 leaves your cloud wide open to the intentional
  exploitation of this vulnerability. Therefore, we recommend that the
  full fix be applied to all deployments.
* Using Configuration as a Short-Term Mitigationâ¦ An alternative
  approach to mitigation can be found in OSSN-0092
  <https://wiki.openstack.org/wiki/OSSN/OSSN-0092>
* The stable/xena and stable/wallaby branches are under extended
  maintenance and will receive no new point releases, but patches for
  them are provided as a courtesy where available.

## OSSA History[Â¶](#ossa-history "Link to this heading")

* 2023-05-15 - Errata 3
* 2023-05-10 - Errata 2
* 2023-05-10 - Errata 1
* 2023-05-10 - Original Version

this page last updated: 2025-01-07 18:17:35

[![Creative Commons Attribution 3.0 License](../_static/images/docs/license.png)](https://creativecommons.org/licenses/by/3.0/)

Except where otherwise noted, this document is licensed under
[Creative Commons
Attribution 3.0 License](https://creativecommons.org/licenses/by/3.0/). See all [OpenStack Legal Documents](https://www.openstack.org/legal).

found an error? report a bug

OpenStack Documentation

* Guides
* [Install Guides](https://docs.openstack.org/index.html#install-guides)
* [User Guides](https://docs.openstack.org/index.html#user-guides)
* [Configuration Guides](https://docs.openstack.org/index.html#configuration-guides)
* [Operations and Administration Guides](https://docs.openstack.org/index.html#ops-and-admin-guides)
* [API Guides](https://docs.openstack.org/index.html#api-guides)
* [Contributor Guides](https://docs.openstack.org/index.html#contributor-guides)
* Languages
* [Deutsch (German)](https://docs.openstack.org/de/)
* [FranÃ§ais (French)](https://docs.openstack.org/fr/)
* [Bahasa Indonesia (Indonesian)](https://docs.openstack.org/id/)
* [Italiano (Italian)](https://docs.openstack.org/it/)
* [æ¥æ¬èª (Japanese)](https://docs.openstack.org/ja/)
* [íêµ­ì´ (Korean)](https://docs.openstack.org/ko_KR/)
* [PortuguÃªs (Portuguese)](https://docs.openstack.org/pt_BR/)
* [TÃ¼rkÃ§e (TÃ¼rkiye)](https://docs.openstack.org/tr_TR/)
* [ç®ä½ä¸­æ (Simplified Chinese)](https://docs.openstack.org/zh_CN/)

#### Contents

* OSSA-2023-003: Unauthorized volume access through deleted volume attachments
  + [Affects](#affects)
  + [Description](#description)
    - [Configuration Change](#configuration-change)
    - [Patches and Associated Deployment Changes](#patches-and-associated-deployment-changes)
  + [Errata](#errata)
  + [Patches](#patches)
  + [Credits](#credits)
  + [References](#references)
  + [Notes](#notes)
  + [OSSA History](#ossa-history)

### OpenStack

* [Projects](https://www.openstack.org/software/project-navigator/)
* [OpenStack Security](https://security.openstack.org/)
* [Blog](https://openstack.org/blog/)
* [News](https://openstack.org/news/)

### Community

* [User Groups](https://www.meetup.com/pro/openinfradev/)
* [Events](https://openstack.org/community/events/)
* [Jobs](https://openstack.org/community/jobs/)
* [Companies](https://openinfra.dev/members/)
* [Contribute](https://docs.openstack.org/contributors)

### Documentation

* [OpenStack Manuals](https://docs.openstack.org)
* [Getting Started](https://openstack.org/software/start/)
* [API Documentation](https://developer.openstack.org)
* [Wiki](https://wiki.openstack.org)

### Branding & Legal

* [Legal Docs](https://openinfra.dev/legal)
* [Logos & Guidelines](https://openstack.org/brand/)
* [Trademark Policy](https://openinfra.dev/legal/trademark-policy)
* [Privacy Policy](https://openinfra.dev/privacy-policy)
* [OpenInfra CLA](https://docs.openstack.org/contributors/common/setup-gerrit.html#individual-contributor-license-agreement)

### Stay In Touch

The OpenStack project is provided under the
[Apache 2.0 license](https://www.apache.org/licenses/LICENSE-2.0). Docs.openstack.org is powered by
[Rackspace Cloud Computing](https://rackspace.com).


