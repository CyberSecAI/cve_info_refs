=== Content from git.kernel.org_e8707933_20250114_200233.html ===


| [cgit logo](/) | [index](/) : [kernel/git/torvalds/linux.git](/pub/scm/linux/kernel/git/torvalds/linux.git/) | for-next master vsnprintf |
| --- | --- | --- |
| Linux kernel source tree | Linus Torvalds |

| [about](/pub/scm/linux/kernel/git/torvalds/linux.git/about/)[summary](/pub/scm/linux/kernel/git/torvalds/linux.git/)[refs](/pub/scm/linux/kernel/git/torvalds/linux.git/refs/?id=b5dba59e0cf7e2cc4d3b3b1ac5fe81ddf21959eb)[log](/pub/scm/linux/kernel/git/torvalds/linux.git/log/)[tree](/pub/scm/linux/kernel/git/torvalds/linux.git/tree/?id=b5dba59e0cf7e2cc4d3b3b1ac5fe81ddf21959eb)[commit](/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=b5dba59e0cf7e2cc4d3b3b1ac5fe81ddf21959eb)[diff](/pub/scm/linux/kernel/git/torvalds/linux.git/diff/?id=b5dba59e0cf7e2cc4d3b3b1ac5fe81ddf21959eb)[stats](/pub/scm/linux/kernel/git/torvalds/linux.git/stats/) | log msg author committer range |
| --- | --- |

**diff options**

|  | |
| --- | --- |
| context: | 12345678910152025303540 |
| space: | includeignore |
| mode: | unifiedssdiffstat only |
|  |  |

| author | Jens Axboe <axboe@kernel.dk> | 2019-12-11 14:02:38 -0700 |
| --- | --- | --- |
| committer | Jens Axboe <axboe@kernel.dk> | 2020-01-20 17:01:53 -0700 |
| commit | [b5dba59e0cf7e2cc4d3b3b1ac5fe81ddf21959eb](/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=b5dba59e0cf7e2cc4d3b3b1ac5fe81ddf21959eb) ([patch](/pub/scm/linux/kernel/git/torvalds/linux.git/patch/?id=b5dba59e0cf7e2cc4d3b3b1ac5fe81ddf21959eb)) | |
| tree | [3e08555974f383d3da5d064dbbc75c6c833091a6](/pub/scm/linux/kernel/git/torvalds/linux.git/tree/?id=b5dba59e0cf7e2cc4d3b3b1ac5fe81ddf21959eb) | |
| parent | [0c9d5ccd26a004f59333c06fbbb98f9cb1eed93d](/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=0c9d5ccd26a004f59333c06fbbb98f9cb1eed93d) ([diff](/pub/scm/linux/kernel/git/torvalds/linux.git/diff/?id=b5dba59e0cf7e2cc4d3b3b1ac5fe81ddf21959eb&id2=0c9d5ccd26a004f59333c06fbbb98f9cb1eed93d)) | |
| download | [linux-b5dba59e0cf7e2cc4d3b3b1ac5fe81ddf21959eb.tar.gz](/pub/scm/linux/kernel/git/torvalds/linux.git/snapshot/linux-b5dba59e0cf7e2cc4d3b3b1ac5fe81ddf21959eb.tar.gz) | |

io\_uring: add support for IORING\_OP\_CLOSEThis works just like close(2), unsurprisingly. We remove the file
descriptor and post the completion inline, then offload the actual
(potential) last file put to async context.
Mark the async part of this work as uncancellable, as we really must
guarantee that the latter part of the close is run.
Signed-off-by: Jens Axboe <axboe@kernel.dk>
[Diffstat](/pub/scm/linux/kernel/git/torvalds/linux.git/diff/?id=b5dba59e0cf7e2cc4d3b3b1ac5fe81ddf21959eb)

| -rw-r--r-- | [fs/io\_uring.c](/pub/scm/linux/kernel/git/torvalds/linux.git/diff/fs/io_uring.c?id=b5dba59e0cf7e2cc4d3b3b1ac5fe81ddf21959eb) | 109 | |  |  |  | | --- | --- | --- | |
| --- | --- | --- | --- | --- | --- | --- |
| -rw-r--r-- | [include/uapi/linux/io\_uring.h](/pub/scm/linux/kernel/git/torvalds/linux.git/diff/include/uapi/linux/io_uring.h?id=b5dba59e0cf7e2cc4d3b3b1ac5fe81ddf21959eb) | 1 | |  |  |  | | --- | --- | --- | |

2 files changed, 110 insertions, 0 deletions

| diff --git a/fs/io\_uring.c b/fs/io\_uring.cindex fe227650efd66c..6aaff7bfe8b5c2 100644--- a/[fs/io\_uring.c](/pub/scm/linux/kernel/git/torvalds/linux.git/tree/fs/io_uring.c?id=0c9d5ccd26a004f59333c06fbbb98f9cb1eed93d)+++ b/[fs/io\_uring.c](/pub/scm/linux/kernel/git/torvalds/linux.git/tree/fs/io_uring.c?id=b5dba59e0cf7e2cc4d3b3b1ac5fe81ddf21959eb)@@ -301,6 +301,12 @@ struct io\_poll\_iocb { struct wait\_queue\_entry wait; }; +struct io\_close {+ struct file \*file;+ struct file \*put\_file;+ int fd;+};+ struct io\_timeout\_data { struct io\_kiocb \*req; struct hrtimer timer;@@ -414,6 +420,7 @@ struct io\_kiocb { struct io\_connect connect; struct io\_sr\_msg sr\_msg; struct io\_open open;+ struct io\_close close; };  struct io\_async\_ctx \*io;@@ -2228,6 +2235,94 @@ err: return 0; } +static int io\_close\_prep(struct io\_kiocb \*req, const struct io\_uring\_sqe \*sqe)+{+ /\*+ \* If we queue this for async, it must not be cancellable. That would+ \* leave the 'file' in an undeterminate state.+ \*/+ req->work.flags |= IO\_WQ\_WORK\_NO\_CANCEL;++ if (sqe->ioprio || sqe->off || sqe->addr || sqe->len ||+ sqe->rw\_flags || sqe->buf\_index)+ return -EINVAL;+ if (sqe->flags & IOSQE\_FIXED\_FILE)+ return -EINVAL;++ req->close.fd = READ\_ONCE(sqe->fd);+ if (req->file->f\_op == &io\_uring\_fops ||+ req->close.fd == req->ring\_fd)+ return -EBADF;++ return 0;+}++static void io\_close\_finish(struct io\_wq\_work \*\*workptr)+{+ struct io\_kiocb \*req = container\_of(\*workptr, struct io\_kiocb, work);+ struct io\_kiocb \*nxt = NULL;++ /\* Invoked with files, we need to do the close \*/+ if (req->work.files) {+ int ret;++ ret = filp\_close(req->close.put\_file, req->work.files);+ if (ret < 0) {+ req\_set\_fail\_links(req);+ }+ io\_cqring\_add\_event(req, ret);+ }++ fput(req->close.put\_file);++ /\* we bypassed the re-issue, drop the submission reference \*/+ io\_put\_req(req);+ io\_put\_req\_find\_next(req, &nxt);+ if (nxt)+ io\_wq\_assign\_next(workptr, nxt);+}++static int io\_close(struct io\_kiocb \*req, struct io\_kiocb \*\*nxt,+ bool force\_nonblock)+{+ int ret;++ req->close.put\_file = NULL;+ ret = \_\_close\_fd\_get\_file(req->close.fd, &req->close.put\_file);+ if (ret < 0)+ return ret;++ /\* if the file has a flush method, be safe and punt to async \*/+ if (req->close.put\_file->f\_op->flush && !io\_wq\_current\_is\_worker()) {+ req->work.flags |= IO\_WQ\_WORK\_NEEDS\_FILES;+ goto eagain;+ }++ /\*+ \* No ->flush(), safely close from here and just punt the+ \* fput() to async context.+ \*/+ ret = filp\_close(req->close.put\_file, current->files);++ if (ret < 0)+ req\_set\_fail\_links(req);+ io\_cqring\_add\_event(req, ret);++ if (io\_wq\_current\_is\_worker()) {+ struct io\_wq\_work \*old\_work, \*work;++ old\_work = work = &req->work;+ io\_close\_finish(&work);+ if (work && work != old\_work)+ \*nxt = container\_of(work, struct io\_kiocb, work);+ return 0;+ }++eagain:+ req->work.func = io\_close\_finish;+ return -EAGAIN;+}+ static int io\_prep\_sfr(struct io\_kiocb \*req, const struct io\_uring\_sqe \*sqe) { struct io\_ring\_ctx \*ctx = req->ctx;@@ -3256,6 +3351,9 @@ static int io\_req\_defer\_prep(struct io\_kiocb \*req, case IORING\_OP\_OPENAT: ret = io\_openat\_prep(req, sqe); break;+ case IORING\_OP\_CLOSE:+ ret = io\_close\_prep(req, sqe);+ break; default: printk\_once(KERN\_WARNING "io\_uring: unhandled opcode %d\n", req->opcode);@@ -3426,6 +3524,14 @@ static int io\_issue\_sqe(struct io\_kiocb \*req, const struct io\_uring\_sqe \*sqe, } ret = io\_openat(req, nxt, force\_nonblock); break;+ case IORING\_OP\_CLOSE:+ if (sqe) {+ ret = io\_close\_prep(req, sqe);+ if (ret)+ break;+ }+ ret = io\_close(req, nxt, force\_nonblock);+ break; default: ret = -EINVAL; break;@@ -3572,6 +3678,9 @@ static int io\_grab\_files(struct io\_kiocb \*req) int ret = -EBADF; struct io\_ring\_ctx \*ctx = req->ctx; + if (!req->ring\_file)+ return -EBADF;+ rcu\_read\_lock(); spin\_lock\_irq(&ctx->inflight\_lock); /\*diff --git a/include/uapi/linux/io\_uring.h b/include/uapi/linux/io\_uring.hindex c1a7c1c65eaf2b..084dea85b83804 100644--- a/[include/uapi/linux/io\_uring.h](/pub/scm/linux/kernel/git/torvalds/linux.git/tree/include/uapi/linux/io_uring.h?id=0c9d5ccd26a004f59333c06fbbb98f9cb1eed93d)+++ b/[include/uapi/linux/io\_uring.h](/pub/scm/linux/kernel/git/torvalds/linux.git/tree/include/uapi/linux/io_uring.h?id=b5dba59e0cf7e2cc4d3b3b1ac5fe81ddf21959eb)@@ -79,6 +79,7 @@ enum { IORING\_OP\_CONNECT, IORING\_OP\_FALLOCATE, IORING\_OP\_OPENAT,+ IORING\_OP\_CLOSE,  /\* this goes last, obviously \*/ IORING\_OP\_LAST, |
| --- |

generated by [cgit 1.2.3-korg](https://git.zx2c4.com/cgit/about/) ([git 2.43.0](https://git-scm.com/)) at 2025-01-14 20:01:10 +0000



=== Content from git.kernel.org_2bd5d4b8_20250114_200233.html ===


| [cgit logo](/) | [index](/) : [kernel/git/torvalds/linux.git](/pub/scm/linux/kernel/git/torvalds/linux.git/) | for-next master vsnprintf |
| --- | --- | --- |
| Linux kernel source tree | Linus Torvalds |

| [about](/pub/scm/linux/kernel/git/torvalds/linux.git/about/)[summary](/pub/scm/linux/kernel/git/torvalds/linux.git/)[refs](/pub/scm/linux/kernel/git/torvalds/linux.git/refs/?id=9eac1904d3364254d622bf2c771c4f85cd435fc2)[log](/pub/scm/linux/kernel/git/torvalds/linux.git/log/)[tree](/pub/scm/linux/kernel/git/torvalds/linux.git/tree/?id=9eac1904d3364254d622bf2c771c4f85cd435fc2)[commit](/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=9eac1904d3364254d622bf2c771c4f85cd435fc2)[diff](/pub/scm/linux/kernel/git/torvalds/linux.git/diff/?id=9eac1904d3364254d622bf2c771c4f85cd435fc2)[stats](/pub/scm/linux/kernel/git/torvalds/linux.git/stats/) | log msg author committer range |
| --- | --- |

**diff options**

|  | |
| --- | --- |
| context: | 12345678910152025303540 |
| space: | includeignore |
| mode: | unifiedssdiffstat only |
|  |  |

| author | Jens Axboe <axboe@kernel.dk> | 2021-01-19 15:50:37 -0700 |
| --- | --- | --- |
| committer | Jens Axboe <axboe@kernel.dk> | 2021-02-01 10:02:43 -0700 |
| commit | [9eac1904d3364254d622bf2c771c4f85cd435fc2](/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=9eac1904d3364254d622bf2c771c4f85cd435fc2) ([patch](/pub/scm/linux/kernel/git/torvalds/linux.git/patch/?id=9eac1904d3364254d622bf2c771c4f85cd435fc2)) | |
| tree | [0430f5c8591c27fa6aae45318975888de70e7543](/pub/scm/linux/kernel/git/torvalds/linux.git/tree/?id=9eac1904d3364254d622bf2c771c4f85cd435fc2) | |
| parent | [53dec2ea74f2ef360e8455439be96a780baa6097](/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=53dec2ea74f2ef360e8455439be96a780baa6097) ([diff](/pub/scm/linux/kernel/git/torvalds/linux.git/diff/?id=9eac1904d3364254d622bf2c771c4f85cd435fc2&id2=53dec2ea74f2ef360e8455439be96a780baa6097)) | |
| download | [linux-9eac1904d3364254d622bf2c771c4f85cd435fc2.tar.gz](/pub/scm/linux/kernel/git/torvalds/linux.git/snapshot/linux-9eac1904d3364254d622bf2c771c4f85cd435fc2.tar.gz) | |

io\_uring: get rid of intermediate IORING\_OP\_CLOSE stageWe currently split the close into two, in case we have a ->flush op
that we can't safely handle from non-blocking context. This requires
us to flag the op as uncancelable if we do need to punt it async, and
that means special handling for just this op type.
Use \_\_close\_fd\_get\_file() and grab the files lock so we can get the file
and check if we need to go async in one atomic operation. That gets rid
of the need for splitting this into two steps, and hence the need for
IO\_WQ\_WORK\_NO\_CANCEL.
Signed-off-by: Jens Axboe <axboe@kernel.dk>
[Diffstat](/pub/scm/linux/kernel/git/torvalds/linux.git/diff/?id=9eac1904d3364254d622bf2c771c4f85cd435fc2)

| -rw-r--r-- | [fs/io\_uring.c](/pub/scm/linux/kernel/git/torvalds/linux.git/diff/fs/io_uring.c?id=9eac1904d3364254d622bf2c771c4f85cd435fc2) | 64 | |  |  |  | | --- | --- | --- | |
| --- | --- | --- | --- | --- | --- | --- |

1 files changed, 35 insertions, 29 deletions

| diff --git a/fs/io\_uring.c b/fs/io\_uring.cindex 3f6d055eb6d44f..4dd18c81789c1a 100644--- a/[fs/io\_uring.c](/pub/scm/linux/kernel/git/torvalds/linux.git/tree/fs/io_uring.c?id=53dec2ea74f2ef360e8455439be96a780baa6097)+++ b/[fs/io\_uring.c](/pub/scm/linux/kernel/git/torvalds/linux.git/tree/fs/io_uring.c?id=9eac1904d3364254d622bf2c771c4f85cd435fc2)@@ -423,7 +423,6 @@ struct io\_poll\_remove {  struct io\_close { struct file \*file;- struct file \*put\_file; int fd; }; @@ -920,8 +919,6 @@ static const struct io\_op\_def io\_op\_defs[] = { IO\_WQ\_WORK\_FS | IO\_WQ\_WORK\_MM, }, [IORING\_OP\_CLOSE] = {- .needs\_file = 1,- .needs\_file\_no\_error = 1, .work\_flags = IO\_WQ\_WORK\_FILES | IO\_WQ\_WORK\_BLKCG, }, [IORING\_OP\_FILES\_UPDATE] = {@@ -4475,13 +4472,6 @@ static int io\_statx(struct io\_kiocb \*req, bool force\_nonblock)  static int io\_close\_prep(struct io\_kiocb \*req, const struct io\_uring\_sqe \*sqe) {- /\*- \* If we queue this for async, it must not be cancellable. That would- \* leave the 'file' in an undeterminate state, and here need to modify- \* io\_wq\_work.flags, so initialize io\_wq\_work firstly.- \*/- io\_req\_init\_async(req);- if (unlikely(req->ctx->flags & IORING\_SETUP\_IOPOLL)) return -EINVAL; if (sqe->ioprio || sqe->off || sqe->addr || sqe->len ||@@ -4491,43 +4481,59 @@ static int io\_close\_prep(struct io\_kiocb \*req, const struct io\_uring\_sqe \*sqe) return -EBADF;  req->close.fd = READ\_ONCE(sqe->fd);- if ((req->file && req->file->f\_op == &io\_uring\_fops))- return -EBADF;-- req->close.put\_file = NULL; return 0; }  static int io\_close(struct io\_kiocb \*req, bool force\_nonblock, struct io\_comp\_state \*cs) {+ struct files\_struct \*files = current->files; struct io\_close \*close = &req->close;+ struct fdtable \*fdt;+ struct file \*file; int ret; - /\* might be already done during nonblock submission \*/- if (!close->put\_file) {- ret = close\_fd\_get\_file(close->fd, &close->put\_file);- if (ret < 0)- return (ret == -ENOENT) ? -EBADF : ret;+ file = NULL;+ ret = -EBADF;+ spin\_lock(&files->file\_lock);+ fdt = files\_fdtable(files);+ if (close->fd >= fdt->max\_fds) {+ spin\_unlock(&files->file\_lock);+ goto err;+ }+ file = fdt->fd[close->fd];+ if (!file) {+ spin\_unlock(&files->file\_lock);+ goto err;+ }++ if (file->f\_op == &io\_uring\_fops) {+ spin\_unlock(&files->file\_lock);+ file = NULL;+ goto err; }  /\* if the file has a flush method, be safe and punt to async \*/- if (close->put\_file->f\_op->flush && force\_nonblock) {- /\* not safe to cancel at this point \*/- req->work.flags |= IO\_WQ\_WORK\_NO\_CANCEL;- /\* was never set, but play safe \*/- req->flags &= ~REQ\_F\_NOWAIT;- /\* avoid grabbing files - we don't need the files \*/- req->flags |= REQ\_F\_NO\_FILE\_TABLE;+ if (file->f\_op->flush && force\_nonblock) {+ spin\_unlock(&files->file\_lock); return -EAGAIN; } + ret = \_\_close\_fd\_get\_file(close->fd, &file);+ spin\_unlock(&files->file\_lock);+ if (ret < 0) {+ if (ret == -ENOENT)+ ret = -EBADF;+ goto err;+ }+ /\* No ->flush() or already async, safely close from here \*/- ret = filp\_close(close->put\_file, req->work.identity->files);+ ret = filp\_close(file, current->files);+err: if (ret < 0) req\_set\_fail\_links(req);- fput(close->put\_file);- close->put\_file = NULL;+ if (file)+ fput(file); \_\_io\_req\_complete(req, ret, 0, cs); return 0; } |
| --- |

generated by [cgit 1.2.3-korg](https://git.zx2c4.com/cgit/about/) ([git 2.43.0](https://git-scm.com/)) at 2025-01-14 20:01:10 +0000



=== Content from git.kernel.org_3edf6e6a_20250114_200232.html ===


| [cgit logo](/) | [index](/) : [kernel/git/stable/linux.git](/pub/scm/linux/kernel/git/stable/linux.git/) | linux-2.6.11.y linux-2.6.12.y linux-2.6.13.y linux-2.6.14.y linux-2.6.15.y linux-2.6.16.y linux-2.6.17.y linux-2.6.18.y linux-2.6.19.y linux-2.6.20.y linux-2.6.21.y linux-2.6.22.y linux-2.6.23.y linux-2.6.24.y linux-2.6.25.y linux-2.6.26.y linux-2.6.27.y linux-2.6.28.y linux-2.6.29.y linux-2.6.30.y linux-2.6.31.y linux-2.6.32.y linux-2.6.33.y linux-2.6.34.y linux-2.6.35.y linux-2.6.36.y linux-2.6.37.y linux-2.6.38.y linux-2.6.39.y linux-3.0.y linux-3.1.y linux-3.10.y linux-3.11.y linux-3.12.y linux-3.13.y linux-3.14.y linux-3.15.y linux-3.16.y linux-3.17.y linux-3.18.y linux-3.19.y linux-3.2.y linux-3.3.y linux-3.4.y linux-3.5.y linux-3.6.y linux-3.7.y linux-3.8.y linux-3.9.y linux-4.0.y linux-4.1.y linux-4.10.y linux-4.11.y linux-4.12.y linux-4.13.y linux-4.14.y linux-4.15.y linux-4.16.y linux-4.17.y linux-4.18.y linux-4.19.y linux-4.2.y linux-4.20.y linux-4.3.y linux-4.4.y linux-4.5.y linux-4.6.y linux-4.7.y linux-4.8.y linux-4.9.y linux-5.0.y linux-5.1.y linux-5.10.y linux-5.11.y linux-5.12.y linux-5.13.y linux-5.14.y linux-5.15.y linux-5.16.y linux-5.17.y linux-5.18.y linux-5.19.y linux-5.2.y linux-5.3.y linux-5.4.y linux-5.5.y linux-5.6.y linux-5.7.y linux-5.8.y linux-5.9.y linux-6.0.y linux-6.1.y linux-6.10.y linux-6.11.y linux-6.12.y linux-6.2.y linux-6.3.y linux-6.4.y linux-6.5.y linux-6.6.y linux-6.7.y linux-6.8.y linux-6.9.y linux-rolling-lts linux-rolling-stable master |
| --- | --- | --- |
| Linux kernel stable tree | Stable Group |

| [about](/pub/scm/linux/kernel/git/stable/linux.git/about/)[summary](/pub/scm/linux/kernel/git/stable/linux.git/)[refs](/pub/scm/linux/kernel/git/stable/linux.git/refs/?id=788d0824269bef539fe31a785b1517882eafed93)[log](/pub/scm/linux/kernel/git/stable/linux.git/log/)[tree](/pub/scm/linux/kernel/git/stable/linux.git/tree/?id=788d0824269bef539fe31a785b1517882eafed93)[commit](/pub/scm/linux/kernel/git/stable/linux.git/commit/?id=788d0824269bef539fe31a785b1517882eafed93)[diff](/pub/scm/linux/kernel/git/stable/linux.git/diff/?id=788d0824269bef539fe31a785b1517882eafed93)[stats](/pub/scm/linux/kernel/git/stable/linux.git/stats/) | log msg author committer range |
| --- | --- |

**diff options**

|  | |
| --- | --- |
| context: | 12345678910152025303540 |
| space: | includeignore |
| mode: | unifiedssdiffstat only |
|  |  |

| author | Jens Axboe <axboe@kernel.dk> | 2022-12-22 14:30:11 -0700 |
| --- | --- | --- |
| committer | Greg Kroah-Hartman <gregkh@linuxfoundation.org> | 2023-01-04 11:39:23 +0100 |
| commit | [788d0824269bef539fe31a785b1517882eafed93](/pub/scm/linux/kernel/git/stable/linux.git/commit/?id=788d0824269bef539fe31a785b1517882eafed93) ([patch](/pub/scm/linux/kernel/git/stable/linux.git/patch/?id=788d0824269bef539fe31a785b1517882eafed93)) | |
| tree | [8adc181aa1785ab1478cfe22ffdc7f0a65b3c6d3](/pub/scm/linux/kernel/git/stable/linux.git/tree/?id=788d0824269bef539fe31a785b1517882eafed93) | |
| parent | [ed3005032993da7a3fe2e6095436e0bc2e83d011](/pub/scm/linux/kernel/git/stable/linux.git/commit/?id=ed3005032993da7a3fe2e6095436e0bc2e83d011) ([diff](/pub/scm/linux/kernel/git/stable/linux.git/diff/?id=788d0824269bef539fe31a785b1517882eafed93&id2=ed3005032993da7a3fe2e6095436e0bc2e83d011)) | |
| download | [linux-788d0824269bef539fe31a785b1517882eafed93.tar.gz](/pub/scm/linux/kernel/git/stable/linux.git/snapshot/linux-788d0824269bef539fe31a785b1517882eafed93.tar.gz) | |

io\_uring: import 5.15-stable io\_uringNo upstream commit exists.
This imports the io\_uring codebase from 5.15.85, wholesale. Changes
from that code base:
- Drop IOCB\_ALLOC\_CACHE, we don't have that in 5.10.
- Drop MKDIRAT/SYMLINKAT/LINKAT. Would require further VFS backports,
and we don't support these in 5.10 to begin with.
- sock\_from\_file() old style calling convention.
- Use compat\_get\_bitmap() only for CONFIG\_COMPAT=y
Signed-off-by: Jens Axboe <axboe@kernel.dk>
Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
[Diffstat](/pub/scm/linux/kernel/git/stable/linux.git/diff/?id=788d0824269bef539fe31a785b1517882eafed93)

| -rw-r--r-- | [Makefile](/pub/scm/linux/kernel/git/stable/linux.git/diff/Makefile?id=788d0824269bef539fe31a785b1517882eafed93) | 2 | |  |  |  | | --- | --- | --- | |
| --- | --- | --- | --- | --- | --- | --- |
| -rw-r--r-- | [fs/Makefile](/pub/scm/linux/kernel/git/stable/linux.git/diff/fs/Makefile?id=788d0824269bef539fe31a785b1517882eafed93) | 2 | |  |  |  | | --- | --- | --- | |
| -rw-r--r-- | [fs/io-wq.c](/pub/scm/linux/kernel/git/stable/linux.git/diff/fs/io-wq.c?id=788d0824269bef539fe31a785b1517882eafed93) | 1242 | |  |  |  | | --- | --- | --- | |
| -rw-r--r-- | [include/linux/io\_uring.h](/pub/scm/linux/kernel/git/stable/linux.git/diff/include/linux/io_uring.h?id=788d0824269bef539fe31a785b1517882eafed93) | 46 | |  |  |  | | --- | --- | --- | |
| -rw-r--r-- | [include/linux/sched.h](/pub/scm/linux/kernel/git/stable/linux.git/diff/include/linux/sched.h?id=788d0824269bef539fe31a785b1517882eafed93) | 3 | |  |  |  | | --- | --- | --- | |
| -rw-r--r-- | [include/linux/syscalls.h](/pub/scm/linux/kernel/git/stable/linux.git/diff/include/linux/syscalls.h?id=788d0824269bef539fe31a785b1517882eafed93) | 2 | |  |  |  | | --- | --- | --- | |
| -rw-r--r-- | [include/trace/events/io\_uring.h](/pub/scm/linux/kernel/git/stable/linux.git/diff/include/trace/events/io_uring.h?id=788d0824269bef539fe31a785b1517882eafed93) | 121 | |  |  |  | | --- | --- | --- | |
| -rw-r--r-- | [include/uapi/linux/io\_uring.h](/pub/scm/linux/kernel/git/stable/linux.git/diff/include/uapi/linux/io_uring.h?id=788d0824269bef539fe31a785b1517882eafed93) | 115 | |  |  |  | | --- | --- | --- | |
| -rw-r--r-- | [io\_uring/Makefile](/pub/scm/linux/kernel/git/stable/linux.git/diff/io_uring/Makefile?id=788d0824269bef539fe31a785b1517882eafed93) | 6 | |  |  |  | | --- | --- | --- | |
| -rw-r--r-- | [io\_uring/io-wq.c](/pub/scm/linux/kernel/git/stable/linux.git/diff/io_uring/io-wq.c?id=788d0824269bef539fe31a785b1517882eafed93) | 1398 | |  |  |  | | --- | --- | --- | |
| -rw-r--r-- | [io\_uring/io-wq.h](/pub/scm/linux/kernel/git/stable/linux.git/diff/io_uring/io-wq.h?id=788d0824269bef539fe31a785b1517882eafed93) (renamed from fs/io-wq.h) | 47 | |  |  |  | | --- | --- | --- | |
| -rw-r--r-- | [io\_uring/io\_uring.c](/pub/scm/linux/kernel/git/stable/linux.git/diff/io_uring/io_uring.c?id=788d0824269bef539fe31a785b1517882eafed93) (renamed from fs/io\_uring.c) | 9112 | |  |  |  | | --- | --- | --- | |
| -rw-r--r-- | [kernel/exit.c](/pub/scm/linux/kernel/git/stable/linux.git/diff/kernel/exit.c?id=788d0824269bef539fe31a785b1517882eafed93) | 2 | |  |  |  | | --- | --- | --- | |
| -rw-r--r-- | [kernel/fork.c](/pub/scm/linux/kernel/git/stable/linux.git/diff/kernel/fork.c?id=788d0824269bef539fe31a785b1517882eafed93) | 1 | |  |  |  | | --- | --- | --- | |
| -rw-r--r-- | [kernel/sched/core.c](/pub/scm/linux/kernel/git/stable/linux.git/diff/kernel/sched/core.c?id=788d0824269bef539fe31a785b1517882eafed93) | 2 | |  |  |  | | --- | --- | --- | |

15 files changed, 6667 insertions, 5434 deletions

| diff --git a/Makefile b/Makefileindex 68f8efa0cc3019..14bb1bb37770cd 100644--- a/[Makefile](/pub/scm/linux/kernel/git/stable/linux.git/tree/Makefile?id=ed3005032993da7a3fe2e6095436e0bc2e83d011)+++ b/[Makefile](/pub/scm/linux/kernel/git/stable/linux.git/tree/Makefile?id=788d0824269bef539fe31a785b1517882eafed93)@@ -1128,7 +1128,7 @@ export MODORDER := $(extmod-prefix)modules.order export MODULES\_NSDEPS := $(extmod-prefix)modules.nsdeps  ifeq ($(KBUILD\_EXTMOD),)-core-y += kernel/ certs/ mm/ fs/ ipc/ security/ crypto/ block/+core-y += kernel/ certs/ mm/ fs/ ipc/ security/ crypto/ block/ io\_uring/  vmlinux-dirs := $(patsubst %/,%,$(filter %/, \ $(core-y) $(core-m) $(drivers-y) $(drivers-m) \diff --git a/fs/Makefile b/fs/Makefileindex 999d1a23f036c9..c660ce28f14980 100644--- a/[fs/Makefile](/pub/scm/linux/kernel/git/stable/linux.git/tree/fs/Makefile?id=ed3005032993da7a3fe2e6095436e0bc2e83d011)+++ b/[fs/Makefile](/pub/scm/linux/kernel/git/stable/linux.git/tree/fs/Makefile?id=788d0824269bef539fe31a785b1517882eafed93)@@ -32,8 +32,6 @@ obj-$(CONFIG\_TIMERFD) += timerfd.o obj-$(CONFIG\_EVENTFD) += eventfd.o obj-$(CONFIG\_USERFAULTFD) += userfaultfd.o obj-$(CONFIG\_AIO) += aio.o-obj-$(CONFIG\_IO\_URING) += io\_uring.o-obj-$(CONFIG\_IO\_WQ) += io-wq.o obj-$(CONFIG\_FS\_DAX) += dax.o obj-$(CONFIG\_FS\_ENCRYPTION) += crypto/ obj-$(CONFIG\_FS\_VERITY) += verity/diff --git a/fs/io-wq.c b/fs/io-wq.cdeleted file mode 100644index 3d5fc76b92d014..00000000000000--- a/[fs/io-wq.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/fs/io-wq.c?id=ed3005032993da7a3fe2e6095436e0bc2e83d011)+++ /dev/null@@ -1,1242 +0,0 @@-// SPDX-License-Identifier: GPL-2.0-/\*- \* Basic worker thread pool for io\_uring- \*- \* Copyright (C) 2019 Jens Axboe- \*- \*/-#include <linux/kernel.h>-#include <linux/init.h>-#include <linux/errno.h>-#include <linux/sched/signal.h>-#include <linux/mm.h>-#include <linux/sched/mm.h>-#include <linux/percpu.h>-#include <linux/slab.h>-#include <linux/kthread.h>-#include <linux/rculist\_nulls.h>-#include <linux/fs\_struct.h>-#include <linux/task\_work.h>-#include <linux/blk-cgroup.h>-#include <linux/audit.h>-#include <linux/cpu.h>--#include "../kernel/sched/sched.h"-#include "io-wq.h"--#define WORKER\_IDLE\_TIMEOUT (5 \* HZ)--enum {- IO\_WORKER\_F\_UP = 1, /\* up and active \*/- IO\_WORKER\_F\_RUNNING = 2, /\* account as running \*/- IO\_WORKER\_F\_FREE = 4, /\* worker on free list \*/- IO\_WORKER\_F\_FIXED = 8, /\* static idle worker \*/- IO\_WORKER\_F\_BOUND = 16, /\* is doing bounded work \*/-};--enum {- IO\_WQ\_BIT\_EXIT = 0, /\* wq exiting \*/- IO\_WQ\_BIT\_CANCEL = 1, /\* cancel work on list \*/- IO\_WQ\_BIT\_ERROR = 2, /\* error on setup \*/-};--enum {- IO\_WQE\_FLAG\_STALLED = 1, /\* stalled on hash \*/-};--/\*- \* One for each thread in a wqe pool- \*/-struct io\_worker {- refcount\_t ref;- unsigned flags;- struct hlist\_nulls\_node nulls\_node;- struct list\_head all\_list;- struct task\_struct \*task;- struct io\_wqe \*wqe;-- struct io\_wq\_work \*cur\_work;- spinlock\_t lock;-- struct rcu\_head rcu;- struct mm\_struct \*mm;-#ifdef CONFIG\_BLK\_CGROUP- struct cgroup\_subsys\_state \*blkcg\_css;-#endif- const struct cred \*cur\_creds;- const struct cred \*saved\_creds;- struct files\_struct \*restore\_files;- struct nsproxy \*restore\_nsproxy;- struct fs\_struct \*restore\_fs;-};--#if BITS\_PER\_LONG == 64-#define IO\_WQ\_HASH\_ORDER 6-#else-#define IO\_WQ\_HASH\_ORDER 5-#endif--#define IO\_WQ\_NR\_HASH\_BUCKETS (1u << IO\_WQ\_HASH\_ORDER)--struct io\_wqe\_acct {- unsigned nr\_workers;- unsigned max\_workers;- atomic\_t nr\_running;-};--enum {- IO\_WQ\_ACCT\_BOUND,- IO\_WQ\_ACCT\_UNBOUND,-};--/\*- \* Per-node worker thread pool- \*/-struct io\_wqe {- struct {- raw\_spinlock\_t lock;- struct io\_wq\_work\_list work\_list;- unsigned long hash\_map;- unsigned flags;- } \_\_\_\_cacheline\_aligned\_in\_smp;-- int node;- struct io\_wqe\_acct acct[2];-- struct hlist\_nulls\_head free\_list;- struct list\_head all\_list;-- struct io\_wq \*wq;- struct io\_wq\_work \*hash\_tail[IO\_WQ\_NR\_HASH\_BUCKETS];-};--/\*- \* Per io\_wq state- \*/-struct io\_wq {- struct io\_wqe \*\*wqes;- unsigned long state;-- free\_work\_fn \*free\_work;- io\_wq\_work\_fn \*do\_work;-- struct task\_struct \*manager;- struct user\_struct \*user;- refcount\_t refs;- struct completion done;-- struct hlist\_node cpuhp\_node;-- refcount\_t use\_refs;-};--static enum cpuhp\_state io\_wq\_online;--static bool io\_worker\_get(struct io\_worker \*worker)-{- return refcount\_inc\_not\_zero(&worker->ref);-}--static void io\_worker\_release(struct io\_worker \*worker)-{- if (refcount\_dec\_and\_test(&worker->ref))- wake\_up\_process(worker->task);-}--/\*- \* Note: drops the wqe->lock if returning true! The caller must re-acquire- \* the lock in that case. Some callers need to restart handling if this- \* happens, so we can't just re-acquire the lock on behalf of the caller.- \*/-static bool \_\_io\_worker\_unuse(struct io\_wqe \*wqe, struct io\_worker \*worker)-{- bool dropped\_lock = false;-- if (worker->saved\_creds) {- revert\_creds(worker->saved\_creds);- worker->cur\_creds = worker->saved\_creds = NULL;- }-- if (current->files != worker->restore\_files) {- \_\_acquire(&wqe->lock);- raw\_spin\_unlock\_irq(&wqe->lock);- dropped\_lock = true;-- task\_lock(current);- current->files = worker->restore\_files;- current->nsproxy = worker->restore\_nsproxy;- task\_unlock(current);- }-- if (current->fs != worker->restore\_fs)- current->fs = worker->restore\_fs;-- /\*- \* If we have an active mm, we need to drop the wq lock before unusing- \* it. If we do, return true and let the caller retry the idle loop.- \*/- if (worker->mm) {- if (!dropped\_lock) {- \_\_acquire(&wqe->lock);- raw\_spin\_unlock\_irq(&wqe->lock);- dropped\_lock = true;- }- \_\_set\_current\_state(TASK\_RUNNING);- kthread\_unuse\_mm(worker->mm);- mmput(worker->mm);- worker->mm = NULL;- }--#ifdef CONFIG\_BLK\_CGROUP- if (worker->blkcg\_css) {- kthread\_associate\_blkcg(NULL);- worker->blkcg\_css = NULL;- }-#endif- if (current->signal->rlim[RLIMIT\_FSIZE].rlim\_cur != RLIM\_INFINITY)- current->signal->rlim[RLIMIT\_FSIZE].rlim\_cur = RLIM\_INFINITY;- return dropped\_lock;-}--static inline struct io\_wqe\_acct \*io\_work\_get\_acct(struct io\_wqe \*wqe,- struct io\_wq\_work \*work)-{- if (work->flags & IO\_WQ\_WORK\_UNBOUND)- return &wqe->acct[IO\_WQ\_ACCT\_UNBOUND];-- return &wqe->acct[IO\_WQ\_ACCT\_BOUND];-}--static inline struct io\_wqe\_acct \*io\_wqe\_get\_acct(struct io\_wqe \*wqe,- struct io\_worker \*worker)-{- if (worker->flags & IO\_WORKER\_F\_BOUND)- return &wqe->acct[IO\_WQ\_ACCT\_BOUND];-- return &wqe->acct[IO\_WQ\_ACCT\_UNBOUND];-}--static void io\_worker\_exit(struct io\_worker \*worker)-{- struct io\_wqe \*wqe = worker->wqe;- struct io\_wqe\_acct \*acct = io\_wqe\_get\_acct(wqe, worker);-- /\*- \* If we're not at zero, someone else is holding a brief reference- \* to the worker. Wait for that to go away.- \*/- set\_current\_state(TASK\_INTERRUPTIBLE);- if (!refcount\_dec\_and\_test(&worker->ref))- schedule();- \_\_set\_current\_state(TASK\_RUNNING);-- preempt\_disable();- current->flags &= ~PF\_IO\_WORKER;- if (worker->flags & IO\_WORKER\_F\_RUNNING)- atomic\_dec(&acct->nr\_running);- if (!(worker->flags & IO\_WORKER\_F\_BOUND))- atomic\_dec(&wqe->wq->user->processes);- worker->flags = 0;- preempt\_enable();-- raw\_spin\_lock\_irq(&wqe->lock);- hlist\_nulls\_del\_rcu(&worker->nulls\_node);- list\_del\_rcu(&worker->all\_list);- if (\_\_io\_worker\_unuse(wqe, worker)) {- \_\_release(&wqe->lock);- raw\_spin\_lock\_irq(&wqe->lock);- }- acct->nr\_workers--;- raw\_spin\_unlock\_irq(&wqe->lock);-- kfree\_rcu(worker, rcu);- if (refcount\_dec\_and\_test(&wqe->wq->refs))- complete(&wqe->wq->done);-}--static inline bool io\_wqe\_run\_queue(struct io\_wqe \*wqe)- \_\_must\_hold(wqe->lock)-{- if (!wq\_list\_empty(&wqe->work\_list) &&- !(wqe->flags & IO\_WQE\_FLAG\_STALLED))- return true;- return false;-}--/\*- \* Check head of free list for an available worker. If one isn't available,- \* caller must wake up the wq manager to create one.- \*/-static bool io\_wqe\_activate\_free\_worker(struct io\_wqe \*wqe)- \_\_must\_hold(RCU)-{- struct hlist\_nulls\_node \*n;- struct io\_worker \*worker;-- n = rcu\_dereference(hlist\_nulls\_first\_rcu(&wqe->free\_list));- if (is\_a\_nulls(n))- return false;-- worker = hlist\_nulls\_entry(n, struct io\_worker, nulls\_node);- if (io\_worker\_get(worker)) {- wake\_up\_process(worker->task);- io\_worker\_release(worker);- return true;- }-- return false;-}--/\*- \* We need a worker. If we find a free one, we're good. If not, and we're- \* below the max number of workers, wake up the manager to create one.- \*/-static void io\_wqe\_wake\_worker(struct io\_wqe \*wqe, struct io\_wqe\_acct \*acct)-{- bool ret;-- /\*- \* Most likely an attempt to queue unbounded work on an io\_wq that- \* wasn't setup with any unbounded workers.- \*/- if (unlikely(!acct->max\_workers))- pr\_warn\_once("io-wq is not configured for unbound workers");-- rcu\_read\_lock();- ret = io\_wqe\_activate\_free\_worker(wqe);- rcu\_read\_unlock();-- if (!ret && acct->nr\_workers < acct->max\_workers)- wake\_up\_process(wqe->wq->manager);-}--static void io\_wqe\_inc\_running(struct io\_wqe \*wqe, struct io\_worker \*worker)-{- struct io\_wqe\_acct \*acct = io\_wqe\_get\_acct(wqe, worker);-- atomic\_inc(&acct->nr\_running);-}--static void io\_wqe\_dec\_running(struct io\_wqe \*wqe, struct io\_worker \*worker)- \_\_must\_hold(wqe->lock)-{- struct io\_wqe\_acct \*acct = io\_wqe\_get\_acct(wqe, worker);-- if (atomic\_dec\_and\_test(&acct->nr\_running) && io\_wqe\_run\_queue(wqe))- io\_wqe\_wake\_worker(wqe, acct);-}--static void io\_worker\_start(struct io\_wqe \*wqe, struct io\_worker \*worker)-{- allow\_kernel\_signal(SIGINT);-- current->flags |= PF\_IO\_WORKER;-- worker->flags |= (IO\_WORKER\_F\_UP | IO\_WORKER\_F\_RUNNING);- worker->restore\_files = current->files;- worker->restore\_nsproxy = current->nsproxy;- worker->restore\_fs = current->fs;- io\_wqe\_inc\_running(wqe, worker);-}--/\*- \* Worker will start processing some work. Move it to the busy list, if- \* it's currently on the freelist- \*/-static void \_\_io\_worker\_busy(struct io\_wqe \*wqe, struct io\_worker \*worker,- struct io\_wq\_work \*work)- \_\_must\_hold(wqe->lock)-{- bool worker\_bound, work\_bound;-- if (worker->flags & IO\_WORKER\_F\_FREE) {- worker->flags &= ~IO\_WORKER\_F\_FREE;- hlist\_nulls\_del\_init\_rcu(&worker->nulls\_node);- }-- /\*- \* If worker is moving from bound to unbound (or vice versa), then- \* ensure we update the running accounting.- \*/- worker\_bound = (worker->flags & IO\_WORKER\_F\_BOUND) != 0;- work\_bound = (work->flags & IO\_WQ\_WORK\_UNBOUND) == 0;- if (worker\_bound != work\_bound) {- io\_wqe\_dec\_running(wqe, worker);- if (work\_bound) {- worker->flags |= IO\_WORKER\_F\_BOUND;- wqe->acct[IO\_WQ\_ACCT\_UNBOUND].nr\_workers--;- wqe->acct[IO\_WQ\_ACCT\_BOUND].nr\_workers++;- atomic\_dec(&wqe->wq->user->processes);- } else {- worker->flags &= ~IO\_WORKER\_F\_BOUND;- wqe->acct[IO\_WQ\_ACCT\_UNBOUND].nr\_workers++;- wqe->acct[IO\_WQ\_ACCT\_BOUND].nr\_workers--;- atomic\_inc(&wqe->wq->user->processes);- }- io\_wqe\_inc\_running(wqe, worker);- }-}--/\*- \* No work, worker going to sleep. Move to freelist, and unuse mm if we- \* have one attached. Dropping the mm may potentially sleep, so we drop- \* the lock in that case and return success. Since the caller has to- \* retry the loop in that case (we changed task state), we don't regrab- \* the lock if we return success.- \*/-static bool \_\_io\_worker\_idle(struct io\_wqe \*wqe, struct io\_worker \*worker)- \_\_must\_hold(wqe->lock)-{- if (!(worker->flags & IO\_WORKER\_F\_FREE)) {- worker->flags |= IO\_WORKER\_F\_FREE;- hlist\_nulls\_add\_head\_rcu(&worker->nulls\_node, &wqe->free\_list);- }-- return \_\_io\_worker\_unuse(wqe, worker);-}--static inline unsigned int io\_get\_work\_hash(struct io\_wq\_work \*work)-{- return work->flags >> IO\_WQ\_HASH\_SHIFT;-}--static struct io\_wq\_work \*io\_get\_next\_work(struct io\_wqe \*wqe)- \_\_must\_hold(wqe->lock)-{- struct io\_wq\_work\_node \*node, \*prev;- struct io\_wq\_work \*work, \*tail;- unsigned int hash;-- wq\_list\_for\_each(node, prev, &wqe->work\_list) {- work = container\_of(node, struct io\_wq\_work, list);-- /\* not hashed, can run anytime \*/- if (!io\_wq\_is\_hashed(work)) {- wq\_list\_del(&wqe->work\_list, node, prev);- return work;- }-- /\* hashed, can run if not already running \*/- hash = io\_get\_work\_hash(work);- if (!(wqe->hash\_map & BIT(hash))) {- wqe->hash\_map |= BIT(hash);- /\* all items with this hash lie in [work, tail] \*/- tail = wqe->hash\_tail[hash];- wqe->hash\_tail[hash] = NULL;- wq\_list\_cut(&wqe->work\_list, &tail->list, prev);- return work;- }- }-- return NULL;-}--static void io\_wq\_switch\_mm(struct io\_worker \*worker, struct io\_wq\_work \*work)-{- if (worker->mm) {- kthread\_unuse\_mm(worker->mm);- mmput(worker->mm);- worker->mm = NULL;- }-- if (mmget\_not\_zero(work->identity->mm)) {- kthread\_use\_mm(work->identity->mm);- worker->mm = work->identity->mm;- return;- }-- /\* failed grabbing mm, ensure work gets cancelled \*/- work->flags |= IO\_WQ\_WORK\_CANCEL;-}--static inline void io\_wq\_switch\_blkcg(struct io\_worker \*worker,- struct io\_wq\_work \*work)-{-#ifdef CONFIG\_BLK\_CGROUP- if (!(work->flags & IO\_WQ\_WORK\_BLKCG))- return;- if (work->identity->blkcg\_css != worker->blkcg\_css) {- kthread\_associate\_blkcg(work->identity->blkcg\_css);- worker->blkcg\_css = work->identity->blkcg\_css;- }-#endif-}--static void io\_wq\_switch\_creds(struct io\_worker \*worker,- struct io\_wq\_work \*work)-{- const struct cred \*old\_creds = override\_creds(work->identity->creds);-- worker->cur\_creds = work->identity->creds;- if (worker->saved\_creds)- put\_cred(old\_creds); /\* creds set by previous switch \*/- else- worker->saved\_creds = old\_creds;-}--static void io\_impersonate\_work(struct io\_worker \*worker,- struct io\_wq\_work \*work)-{- if ((work->flags & IO\_WQ\_WORK\_FILES) &&- current->files != work->identity->files) {- task\_lock(current);- current->files = work->identity->files;- current->nsproxy = work->identity->nsproxy;- task\_unlock(current);- if (!work->identity->files) {- /\* failed grabbing files, ensure work gets cancelled \*/- work->flags |= IO\_WQ\_WORK\_CANCEL;- }- }- if ((work->flags & IO\_WQ\_WORK\_FS) && current->fs != work->identity->fs)- current->fs = work->identity->fs;- if ((work->flags & IO\_WQ\_WORK\_MM) && work->identity->mm != worker->mm)- io\_wq\_switch\_mm(worker, work);- if ((work->flags & IO\_WQ\_WORK\_CREDS) &&- worker->cur\_creds != work->identity->creds)- io\_wq\_switch\_creds(worker, work);- if (work->flags & IO\_WQ\_WORK\_FSIZE)- current->signal->rlim[RLIMIT\_FSIZE].rlim\_cur = work->identity->fsize;- else if (current->signal->rlim[RLIMIT\_FSIZE].rlim\_cur != RLIM\_INFINITY)- current->signal->rlim[RLIMIT\_FSIZE].rlim\_cur = RLIM\_INFINITY;- io\_wq\_switch\_blkcg(worker, work);-#ifdef CONFIG\_AUDIT- current->loginuid = work->identity->loginuid;- current->sessionid = work->identity->sessionid;-#endif-}--static void io\_assign\_current\_work(struct io\_worker \*worker,- struct io\_wq\_work \*work)-{- if (work) {- /\* flush pending signals before assigning new work \*/- if (signal\_pending(current))- flush\_signals(current);- cond\_resched();- }--#ifdef CONFIG\_AUDIT- current->loginuid = KUIDT\_INIT(AUDIT\_UID\_UNSET);- current->sessionid = AUDIT\_SID\_UNSET;-#endif-- spin\_lock\_irq(&worker->lock);- worker->cur\_work = work;- spin\_unlock\_irq(&worker->lock);-}--static void io\_wqe\_enqueue(struct io\_wqe \*wqe, struct io\_wq\_work \*work);--static void io\_worker\_handle\_work(struct io\_worker \*worker)- \_\_releases(wqe->lock)-{- struct io\_wqe \*wqe = worker->wqe;- struct io\_wq \*wq = wqe->wq;-- do {- struct io\_wq\_work \*work;-get\_next:- /\*- \* If we got some work, mark us as busy. If we didn't, but- \* the list isn't empty, it means we stalled on hashed work.- \* Mark us stalled so we don't keep looking for work when we- \* can't make progress, any work completion or insertion will- \* clear the stalled flag.- \*/- work = io\_get\_next\_work(wqe);- if (work)- \_\_io\_worker\_busy(wqe, worker, work);- else if (!wq\_list\_empty(&wqe->work\_list))- wqe->flags |= IO\_WQE\_FLAG\_STALLED;-- raw\_spin\_unlock\_irq(&wqe->lock);- if (!work)- break;- io\_assign\_current\_work(worker, work);-- /\* handle a whole dependent link \*/- do {- struct io\_wq\_work \*old\_work, \*next\_hashed, \*linked;- unsigned int hash = io\_get\_work\_hash(work);-- next\_hashed = wq\_next\_work(work);- io\_impersonate\_work(worker, work);- /\*- \* OK to set IO\_WQ\_WORK\_CANCEL even for uncancellable- \* work, the worker function will do the right thing.- \*/- if (test\_bit(IO\_WQ\_BIT\_CANCEL, &wq->state))- work->flags |= IO\_WQ\_WORK\_CANCEL;-- old\_work = work;- linked = wq->do\_work(work);-- work = next\_hashed;- if (!work && linked && !io\_wq\_is\_hashed(linked)) {- work = linked;- linked = NULL;- }- io\_assign\_current\_work(worker, work);- wq->free\_work(old\_work);-- if (linked)- io\_wqe\_enqueue(wqe, linked);-- if (hash != -1U && !next\_hashed) {- raw\_spin\_lock\_irq(&wqe->lock);- wqe->hash\_map &= ~BIT\_ULL(hash);- wqe->flags &= ~IO\_WQE\_FLAG\_STALLED;- /\* skip unnecessary unlock-lock wqe->lock \*/- if (!work)- goto get\_next;- raw\_spin\_unlock\_irq(&wqe->lock);- }- } while (work);-- raw\_spin\_lock\_irq(&wqe->lock);- } while (1);-}--static int io\_wqe\_worker(void \*data)-{- struct io\_worker \*worker = data;- struct io\_wqe \*wqe = worker->wqe;- struct io\_wq \*wq = wqe->wq;-- io\_worker\_start(wqe, worker);-- while (!test\_bit(IO\_WQ\_BIT\_EXIT, &wq->state)) {- set\_current\_state(TASK\_INTERRUPTIBLE);-loop:- raw\_spin\_lock\_irq(&wqe->lock);- if (io\_wqe\_run\_queue(wqe)) {- \_\_set\_current\_state(TASK\_RUNNING);- io\_worker\_handle\_work(worker);- goto loop;- }- /\* drops the lock on success, retry \*/- if (\_\_io\_worker\_idle(wqe, worker)) {- \_\_release(&wqe->lock);- goto loop;- }- raw\_spin\_unlock\_irq(&wqe->lock);- if (signal\_pending(current))- flush\_signals(current);- if (schedule\_timeout(WORKER\_IDLE\_TIMEOUT))- continue;- /\* timed out, exit unless we're the fixed worker \*/- if (test\_bit(IO\_WQ\_BIT\_EXIT, &wq->state) ||- !(worker->flags & IO\_WORKER\_F\_FIXED))- break;- }-- if (test\_bit(IO\_WQ\_BIT\_EXIT, &wq->state)) {- raw\_spin\_lock\_irq(&wqe->lock);- if (!wq\_list\_empty(&wqe->work\_list))- io\_worker\_handle\_work(worker);- else- raw\_spin\_unlock\_irq(&wqe->lock);- }-- io\_worker\_exit(worker);- return 0;-}--/\*- \* Called when a worker is scheduled in. Mark us as currently running.- \*/-void io\_wq\_worker\_running(struct task\_struct \*tsk)-{- struct io\_worker \*worker = kthread\_data(tsk);- struct io\_wqe \*wqe = worker->wqe;-- if (!(worker->flags & IO\_WORKER\_F\_UP))- return;- if (worker->flags & IO\_WORKER\_F\_RUNNING)- return;- worker->flags |= IO\_WORKER\_F\_RUNNING;- io\_wqe\_inc\_running(wqe, worker);-}--/\*- \* Called when worker is going to sleep. If there are no workers currently- \* running and we have work pending, wake up a free one or have the manager- \* set one up.- \*/-void io\_wq\_worker\_sleeping(struct task\_struct \*tsk)-{- struct io\_worker \*worker = kthread\_data(tsk);- struct io\_wqe \*wqe = worker->wqe;-- if (!(worker->flags & IO\_WORKER\_F\_UP))- return;- if (!(worker->flags & IO\_WORKER\_F\_RUNNING))- return;-- worker->flags &= ~IO\_WORKER\_F\_RUNNING;-- raw\_spin\_lock\_irq(&wqe->lock);- io\_wqe\_dec\_running(wqe, worker);- raw\_spin\_unlock\_irq(&wqe->lock);-}--static bool create\_io\_worker(struct io\_wq \*wq, struct io\_wqe \*wqe, int index)-{- struct io\_wqe\_acct \*acct = &wqe->acct[index];- struct io\_worker \*worker;-- worker = kzalloc\_node(sizeof(\*worker), GFP\_KERNEL, wqe->node);- if (!worker)- return false;-- refcount\_set(&worker->ref, 1);- worker->nulls\_node.pprev = NULL;- worker->wqe = wqe;- spin\_lock\_init(&worker->lock);-- worker->task = kthread\_create\_on\_node(io\_wqe\_worker, worker, wqe->node,- "io\_wqe\_worker-%d/%d", index, wqe->node);- if (IS\_ERR(worker->task)) {- kfree(worker);- return false;- }- kthread\_bind\_mask(worker->task, cpumask\_of\_node(wqe->node));-- raw\_spin\_lock\_irq(&wqe->lock);- hlist\_nulls\_add\_head\_rcu(&worker->nulls\_node, &wqe->free\_list);- list\_add\_tail\_rcu(&worker->all\_list, &wqe->all\_list);- worker->flags |= IO\_WORKER\_F\_FREE;- if (index == IO\_WQ\_ACCT\_BOUND)- worker->flags |= IO\_WORKER\_F\_BOUND;- if (!acct->nr\_workers && (worker->flags & IO\_WORKER\_F\_BOUND))- worker->flags |= IO\_WORKER\_F\_FIXED;- acct->nr\_workers++;- raw\_spin\_unlock\_irq(&wqe->lock);-- if (index == IO\_WQ\_ACCT\_UNBOUND)- atomic\_inc(&wq->user->processes);-- refcount\_inc(&wq->refs);- wake\_up\_process(worker->task);- return true;-}--static inline bool io\_wqe\_need\_worker(struct io\_wqe \*wqe, int index)- \_\_must\_hold(wqe->lock)-{- struct io\_wqe\_acct \*acct = &wqe->acct[index];-- /\* if we have available workers or no work, no need \*/- if (!hlist\_nulls\_empty(&wqe->free\_list) || !io\_wqe\_run\_queue(wqe))- return false;- return acct->nr\_workers < acct->max\_workers;-}--static bool io\_wqe\_worker\_send\_sig(struct io\_worker \*worker, void \*data)-{- send\_sig(SIGINT, worker->task, 1);- return false;-}--/\*- \* Iterate the passed in list and call the specific function for each- \* worker that isn't exiting- \*/-static bool io\_wq\_for\_each\_worker(struct io\_wqe \*wqe,- bool (\*func)(struct io\_worker \*, void \*),- void \*data)-{- struct io\_worker \*worker;- bool ret = false;-- list\_for\_each\_entry\_rcu(worker, &wqe->all\_list, all\_list) {- if (io\_worker\_get(worker)) {- /\* no task if node is/was offline \*/- if (worker->task)- ret = func(worker, data);- io\_worker\_release(worker);- if (ret)- break;- }- }-- return ret;-}--static bool io\_wq\_worker\_wake(struct io\_worker \*worker, void \*data)-{- wake\_up\_process(worker->task);- return false;-}--/\*- \* Manager thread. Tasked with creating new workers, if we need them.- \*/-static int io\_wq\_manager(void \*data)-{- struct io\_wq \*wq = data;- int node;-- /\* create fixed workers \*/- refcount\_set(&wq->refs, 1);- for\_each\_node(node) {- if (!node\_online(node))- continue;- if (create\_io\_worker(wq, wq->wqes[node], IO\_WQ\_ACCT\_BOUND))- continue;- set\_bit(IO\_WQ\_BIT\_ERROR, &wq->state);- set\_bit(IO\_WQ\_BIT\_EXIT, &wq->state);- goto out;- }-- complete(&wq->done);-- while (!kthread\_should\_stop()) {- if (current->task\_works)- task\_work\_run();-- for\_each\_node(node) {- struct io\_wqe \*wqe = wq->wqes[node];- bool fork\_worker[2] = { false, false };-- if (!node\_online(node))- continue;-- raw\_spin\_lock\_irq(&wqe->lock);- if (io\_wqe\_need\_worker(wqe, IO\_WQ\_ACCT\_BOUND))- fork\_worker[IO\_WQ\_ACCT\_BOUND] = true;- if (io\_wqe\_need\_worker(wqe, IO\_WQ\_ACCT\_UNBOUND))- fork\_worker[IO\_WQ\_ACCT\_UNBOUND] = true;- raw\_spin\_unlock\_irq(&wqe->lock);- if (fork\_worker[IO\_WQ\_ACCT\_BOUND])- create\_io\_worker(wq, wqe, IO\_WQ\_ACCT\_BOUND);- if (fork\_worker[IO\_WQ\_ACCT\_UNBOUND])- create\_io\_worker(wq, wqe, IO\_WQ\_ACCT\_UNBOUND);- }- set\_current\_state(TASK\_INTERRUPTIBLE);- schedule\_timeout(HZ);- }-- if (current->task\_works)- task\_work\_run();--out:- if (refcount\_dec\_and\_test(&wq->refs)) {- complete(&wq->done);- return 0;- }- /\* if ERROR is set and we get here, we have workers to wake \*/- if (test\_bit(IO\_WQ\_BIT\_ERROR, &wq->state)) {- rcu\_read\_lock();- for\_each\_node(node)- io\_wq\_for\_each\_worker(wq->wqes[node], io\_wq\_worker\_wake, NULL);- rcu\_read\_unlock();- }- return 0;-}--static bool io\_wq\_can\_queue(struct io\_wqe \*wqe, struct io\_wqe\_acct \*acct,- struct io\_wq\_work \*work)-{- bool free\_worker;-- if (!(work->flags & IO\_WQ\_WORK\_UNBOUND))- return true;- if (atomic\_read(&acct->nr\_running))- return true;-- rcu\_read\_lock();- free\_worker = !hlist\_nulls\_empty(&wqe->free\_list);- rcu\_read\_unlock();- if (free\_worker)- return true;-- if (atomic\_read(&wqe->wq->user->processes) >= acct->max\_workers &&- !(capable(CAP\_SYS\_RESOURCE) || capable(CAP\_SYS\_ADMIN)))- return false;-- return true;-}--static void io\_run\_cancel(struct io\_wq\_work \*work, struct io\_wqe \*wqe)-{- struct io\_wq \*wq = wqe->wq;-- do {- struct io\_wq\_work \*old\_work = work;-- work->flags |= IO\_WQ\_WORK\_CANCEL;- work = wq->do\_work(work);- wq->free\_work(old\_work);- } while (work);-}--static void io\_wqe\_insert\_work(struct io\_wqe \*wqe, struct io\_wq\_work \*work)-{- unsigned int hash;- struct io\_wq\_work \*tail;-- if (!io\_wq\_is\_hashed(work)) {-append:- wq\_list\_add\_tail(&work->list, &wqe->work\_list);- return;- }-- hash = io\_get\_work\_hash(work);- tail = wqe->hash\_tail[hash];- wqe->hash\_tail[hash] = work;- if (!tail)- goto append;-- wq\_list\_add\_after(&work->list, &tail->list, &wqe->work\_list);-}--static void io\_wqe\_enqueue(struct io\_wqe \*wqe, struct io\_wq\_work \*work)-{- struct io\_wqe\_acct \*acct = io\_work\_get\_acct(wqe, work);- bool do\_wake;- unsigned long flags;-- /\*- \* Do early check to see if we need a new unbound worker, and if we do,- \* if we're allowed to do so. This isn't 100% accurate as there's a- \* gap between this check and incrementing the value, but that's OK.- \* It's close enough to not be an issue, fork() has the same delay.- \*/- if (unlikely(!io\_wq\_can\_queue(wqe, acct, work))) {- io\_run\_cancel(work, wqe);- return;- }-- raw\_spin\_lock\_irqsave(&wqe->lock, flags);- io\_wqe\_insert\_work(wqe, work);- wqe->flags &= ~IO\_WQE\_FLAG\_STALLED;- do\_wake = (work->flags & IO\_WQ\_WORK\_CONCURRENT) ||- !atomic\_read(&acct->nr\_running);- raw\_spin\_unlock\_irqrestore(&wqe->lock, flags);-- if (do\_wake)- io\_wqe\_wake\_worker(wqe, acct);-}--void io\_wq\_enqueue(struct io\_wq \*wq, struct io\_wq\_work \*work)-{- struct io\_wqe \*wqe = wq->wqes[numa\_node\_id()];-- io\_wqe\_enqueue(wqe, work);-}--/\*- \* Work items that hash to the same value will not be done in parallel.- \* Used to limit concurrent writes, generally hashed by inode.- \*/-void io\_wq\_hash\_work(struct io\_wq\_work \*work, void \*val)-{- unsigned int bit;-- bit = hash\_ptr(val, IO\_WQ\_HASH\_ORDER);- work->flags |= (IO\_WQ\_WORK\_HASHED | (bit << IO\_WQ\_HASH\_SHIFT));-}--void io\_wq\_cancel\_all(struct io\_wq \*wq)-{- int node;-- set\_bit(IO\_WQ\_BIT\_CANCEL, &wq->state);-- rcu\_read\_lock();- for\_each\_node(node) {- struct io\_wqe \*wqe = wq->wqes[node];-- io\_wq\_for\_each\_worker(wqe, io\_wqe\_worker\_send\_sig, NULL);- }- rcu\_read\_unlock();-}--struct io\_cb\_cancel\_data {- work\_cancel\_fn \*fn;- void \*data;- int nr\_running;- int nr\_pending;- bool cancel\_all;-};--static bool io\_wq\_worker\_cancel(struct io\_worker \*worker, void \*data)-{- struct io\_cb\_cancel\_data \*match = data;- unsigned long flags;-- /\*- \* Hold the lock to avoid ->cur\_work going out of scope, caller- \* may dereference the passed in work.- \*/- spin\_lock\_irqsave(&worker->lock, flags);- if (worker->cur\_work &&- !(worker->cur\_work->flags & IO\_WQ\_WORK\_NO\_CANCEL) &&- match->fn(worker->cur\_work, match->data)) {- send\_sig(SIGINT, worker->task, 1);- match->nr\_running++;- }- spin\_unlock\_irqrestore(&worker->lock, flags);-- return match->nr\_running && !match->cancel\_all;-}--static inline void io\_wqe\_remove\_pending(struct io\_wqe \*wqe,- struct io\_wq\_work \*work,- struct io\_wq\_work\_node \*prev)-{- unsigned int hash = io\_get\_work\_hash(work);- struct io\_wq\_work \*prev\_work = NULL;-- if (io\_wq\_is\_hashed(work) && work == wqe->hash\_tail[hash]) {- if (prev)- prev\_work = container\_of(prev, struct io\_wq\_work, list);- if (prev\_work && io\_get\_work\_hash(prev\_work) == hash)- wqe->hash\_tail[hash] = prev\_work;- else- wqe->hash\_tail[hash] = NULL;- }- wq\_list\_del(&wqe->work\_list, &work->list, prev);-}--static void io\_wqe\_cancel\_pending\_work(struct io\_wqe \*wqe,- struct io\_cb\_cancel\_data \*match)-{- struct io\_wq\_work\_node \*node, \*prev;- struct io\_wq\_work \*work;- unsigned long flags;--retry:- raw\_spin\_lock\_irqsave(&wqe->lock, flags);- wq\_list\_for\_each(node, prev, &wqe->work\_list) {- work = container\_of(node, struct io\_wq\_work, list);- if (!match->fn(work, match->data))- continue;- io\_wqe\_remove\_pending(wqe, work, prev);- raw\_spin\_unlock\_irqrestore(&wqe->lock, flags);- io\_run\_cancel(work, wqe);- match->nr\_pending++;- if (!match->cancel\_all)- return;-- /\* not safe to continue after unlock \*/- goto retry;- }- raw\_spin\_unlock\_irqrestore(&wqe->lock, flags);-}--static void io\_wqe\_cancel\_running\_work(struct io\_wqe \*wqe,- struct io\_cb\_cancel\_data \*match)-{- rcu\_read\_lock();- io\_wq\_for\_each\_worker(wqe, io\_wq\_worker\_cancel, match);- rcu\_read\_unlock();-}--enum io\_wq\_cancel io\_wq\_cancel\_cb(struct io\_wq \*wq, work\_cancel\_fn \*cancel,- void \*data, bool cancel\_all)-{- struct io\_cb\_cancel\_data match = {- .fn = cancel,- .data = data,- .cancel\_all = cancel\_all,- };- int node;-- /\*- \* First check pending list, if we're lucky we can just remove it- \* from there. CANCEL\_OK means that the work is returned as-new,- \* no completion will be posted for it.- \*/- for\_each\_node(node) {- struct io\_wqe \*wqe = wq->wqes[node];-- io\_wqe\_cancel\_pending\_work(wqe, &match);- if (match.nr\_pending && !match.cancel\_all)- return IO\_WQ\_CANCEL\_OK;- }-- /\*- \* Now check if a free (going busy) or busy worker has the work- \* currently running. If we find it there, we'll return CANCEL\_RUNNING- \* as an indication that we attempt to signal cancellation. The- \* completion will run normally in this case.- \*/- for\_each\_node(node) {- struct io\_wqe \*wqe = wq->wqes[node];-- io\_wqe\_cancel\_running\_work(wqe, &match);- if (match.nr\_running && !match.cancel\_all)- return IO\_WQ\_CANCEL\_RUNNING;- }-- if (match.nr\_running)- return IO\_WQ\_CANCEL\_RUNNING;- if (match.nr\_pending)- return IO\_WQ\_CANCEL\_OK;- return IO\_WQ\_CANCEL\_NOTFOUND;-}--struct io\_wq \*io\_wq\_create(unsigned bounded, struct io\_wq\_data \*data)-{- int ret = -ENOMEM, node;- struct io\_wq \*wq;-- if (WARN\_ON\_ONCE(!data->free\_work || !data->do\_work))- return ERR\_PTR(-EINVAL);- if (WARN\_ON\_ONCE(!bounded))- return ERR\_PTR(-EINVAL);-- wq = kzalloc(sizeof(\*wq), GFP\_KERNEL);- if (!wq)- return ERR\_PTR(-ENOMEM);-- wq->wqes = kcalloc(nr\_node\_ids, sizeof(struct io\_wqe \*), GFP\_KERNEL);- if (!wq->wqes)- goto err\_wq;-- ret = cpuhp\_state\_add\_instance\_nocalls(io\_wq\_online, &wq->cpuhp\_node);- if (ret)- goto err\_wqes;-- wq->free\_work = data->free\_work;- wq->do\_work = data->do\_work;-- /\* caller must already hold a reference to this \*/- wq->user = data->user;-- ret = -ENOMEM;- for\_each\_node(node) {- struct io\_wqe \*wqe;- int alloc\_node = node;-- if (!node\_online(alloc\_node))- alloc\_node = NUMA\_NO\_NODE;- wqe = kzalloc\_node(sizeof(struct io\_wqe), GFP\_KERNEL, alloc\_node);- if (!wqe)- goto err;- wq->wqes[node] = wqe;- wqe->node = alloc\_node;- wqe->acct[IO\_WQ\_ACCT\_BOUND].max\_workers = bounded;- atomic\_set(&wqe->acct[IO\_WQ\_ACCT\_BOUND].nr\_running, 0);- if (wq->user) {- wqe->acct[IO\_WQ\_ACCT\_UNBOUND].max\_workers =- task\_rlimit(current, RLIMIT\_NPROC);- }- atomic\_set(&wqe->acct[IO\_WQ\_ACCT\_UNBOUND].nr\_running, 0);- wqe->wq = wq;- raw\_spin\_lock\_init(&wqe->lock);- INIT\_WQ\_LIST(&wqe->work\_list);- INIT\_HLIST\_NULLS\_HEAD(&wqe->free\_list, 0);- INIT\_LIST\_HEAD(&wqe->all\_list);- }-- init\_completion(&wq->done);-- wq->manager = kthread\_create(io\_wq\_manager, wq, "io\_wq\_manager");- if (!IS\_ERR(wq->manager)) {- wake\_up\_process(wq->manager);- wait\_for\_completion(&wq->done);- if (test\_bit(IO\_WQ\_BIT\_ERROR, &wq->state)) {- ret = -ENOMEM;- goto err;- }- refcount\_set(&wq->use\_refs, 1);- reinit\_completion(&wq->done);- return wq;- }-- ret = PTR\_ERR(wq->manager);- complete(&wq->done);-err:- cpuhp\_state\_remove\_instance\_nocalls(io\_wq\_online, &wq->cpuhp\_node);- for\_each\_node(node)- kfree(wq->wqes[node]);-err\_wqes:- kfree(wq->wqes);-err\_wq:- kfree(wq);- return ERR\_PTR(ret);-}--bool io\_wq\_get(struct io\_wq \*wq, struct io\_wq\_data \*data)-{- if (data->free\_work != wq->free\_work || data->do\_work != wq->do\_work)- return false;-- return refcount\_inc\_not\_zero(&wq->use\_refs);-}--static void \_\_io\_wq\_destroy(struct io\_wq \*wq)-{- int node;-- cpuhp\_state\_remove\_instance\_nocalls(io\_wq\_online, &wq->cpuhp\_node);-- set\_bit(IO\_WQ\_BIT\_EXIT, &wq->state);- if (wq->manager)- kthread\_stop(wq->manager);-- rcu\_read\_lock();- for\_each\_node(node)- io\_wq\_for\_each\_worker(wq->wqes[node], io\_wq\_worker\_wake, NULL);- rcu\_read\_unlock();-- wait\_for\_completion(&wq->done);-- for\_each\_node(node)- kfree(wq->wqes[node]);- kfree(wq->wqes);- kfree(wq);-}--void io\_wq\_destroy(struct io\_wq \*wq)-{- if (refcount\_dec\_and\_test(&wq->use\_refs))- \_\_io\_wq\_destroy(wq);-}--struct task\_struct \*io\_wq\_get\_task(struct io\_wq \*wq)-{- return wq->manager;-}--static bool io\_wq\_worker\_affinity(struct io\_worker \*worker, void \*data)-{- struct task\_struct \*task = worker->task;- struct rq\_flags rf;- struct rq \*rq;-- rq = task\_rq\_lock(task, &rf);- do\_set\_cpus\_allowed(task, cpumask\_of\_node(worker->wqe->node));- task->flags |= PF\_NO\_SETAFFINITY;- task\_rq\_unlock(rq, task, &rf);- return false;-}--static int io\_wq\_cpu\_online(unsigned int cpu, struct hlist\_node \*node)-{- struct io\_wq \*wq = hlist\_entry\_safe(node, struct io\_wq, cpuhp\_node);- int i;-- rcu\_read\_lock();- for\_each\_node(i)- io\_wq\_for\_each\_worker(wq->wqes[i], io\_wq\_worker\_affinity, NULL);- rcu\_read\_unlock();- return 0;-}--static \_\_init int io\_wq\_init(void)-{- int ret;-- ret = cpuhp\_setup\_state\_multi(CPUHP\_AP\_ONLINE\_DYN, "io-wq/online",- io\_wq\_cpu\_online, NULL);- if (ret < 0)- return ret;- io\_wq\_online = ret;- return 0;-}-subsys\_initcall(io\_wq\_init);diff --git a/include/linux/io\_uring.h b/include/linux/io\_uring.hindex 35b2d845704d91..649a4d7c241bcc 100644--- a/[include/linux/io\_uring.h](/pub/scm/linux/kernel/git/stable/linux.git/tree/include/linux/io_uring.h?id=ed3005032993da7a3fe2e6095436e0bc2e83d011)+++ b/[include/linux/io\_uring.h](/pub/scm/linux/kernel/git/stable/linux.git/tree/include/linux/io_uring.h?id=788d0824269bef539fe31a785b1517882eafed93)@@ -5,50 +5,20 @@ #include <linux/sched.h> #include <linux/xarray.h> -struct io\_identity {- struct files\_struct \*files;- struct mm\_struct \*mm;-#ifdef CONFIG\_BLK\_CGROUP- struct cgroup\_subsys\_state \*blkcg\_css;-#endif- const struct cred \*creds;- struct nsproxy \*nsproxy;- struct fs\_struct \*fs;- unsigned long fsize;-#ifdef CONFIG\_AUDIT- kuid\_t loginuid;- unsigned int sessionid;-#endif- refcount\_t count;-};--struct io\_uring\_task {- /\* submission side \*/- struct xarray xa;- struct wait\_queue\_head wait;- struct file \*last;- struct percpu\_counter inflight;- struct io\_identity \_\_identity;- struct io\_identity \*identity;- atomic\_t in\_idle;- bool sqpoll;-};- #if defined(CONFIG\_IO\_URING) struct sock \*io\_uring\_get\_socket(struct file \*file);-void \_\_io\_uring\_task\_cancel(void);-void \_\_io\_uring\_files\_cancel(struct files\_struct \*files);+void \_\_io\_uring\_cancel(bool cancel\_all); void \_\_io\_uring\_free(struct task\_struct \*tsk); -static inline void io\_uring\_task\_cancel(void)+static inline void io\_uring\_files\_cancel(void) {- if (current->io\_uring && !xa\_empty(&current->io\_uring->xa))- \_\_io\_uring\_task\_cancel();+ if (current->io\_uring)+ \_\_io\_uring\_cancel(false); }-static inline void io\_uring\_files\_cancel(struct files\_struct \*files)+static inline void io\_uring\_task\_cancel(void) {- if (current->io\_uring && !xa\_empty(&current->io\_uring->xa))- \_\_io\_uring\_files\_cancel(files);+ if (current->io\_uring)+ \_\_io\_uring\_cancel(true); } static inline void io\_uring\_free(struct task\_struct \*tsk) {@@ -63,7 +33,7 @@ static inline struct sock \*io\_uring\_get\_socket(struct file \*file) static inline void io\_uring\_task\_cancel(void) { }-static inline void io\_uring\_files\_cancel(struct files\_struct \*files)+static inline void io\_uring\_files\_cancel(void) { } static inline void io\_uring\_free(struct task\_struct \*tsk)diff --git a/include/linux/sched.h b/include/linux/sched.hindex b055c217eb0be2..5da4b3c89f6368 100644--- a/[include/linux/sched.h](/pub/scm/linux/kernel/git/stable/linux.git/tree/include/linux/sched.h?id=ed3005032993da7a3fe2e6095436e0bc2e83d011)+++ b/[include/linux/sched.h](/pub/scm/linux/kernel/git/stable/linux.git/tree/include/linux/sched.h?id=788d0824269bef539fe31a785b1517882eafed93)@@ -885,6 +885,9 @@ struct task\_struct { /\* CLONE\_CHILD\_CLEARTID: \*/ int \_\_user \*clear\_child\_tid; + /\* PF\_IO\_WORKER \*/+ void \*pf\_io\_worker;+ u64 utime; u64 stime; #ifdef CONFIG\_ARCH\_HAS\_SCALED\_CPUTIMEdiff --git a/include/linux/syscalls.h b/include/linux/syscalls.hindex aea0ce9f3b745a..a058c96cf21386 100644--- a/[include/linux/syscalls.h](/pub/scm/linux/kernel/git/stable/linux.git/tree/include/linux/syscalls.h?id=ed3005032993da7a3fe2e6095436e0bc2e83d011)+++ b/[include/linux/syscalls.h](/pub/scm/linux/kernel/git/stable/linux.git/tree/include/linux/syscalls.h?id=788d0824269bef539fe31a785b1517882eafed93)@@ -341,7 +341,7 @@ asmlinkage long sys\_io\_uring\_setup(u32 entries, struct io\_uring\_params \_\_user \*p); asmlinkage long sys\_io\_uring\_enter(unsigned int fd, u32 to\_submit, u32 min\_complete, u32 flags,- const sigset\_t \_\_user \*sig, size\_t sigsz);+ const void \_\_user \*argp, size\_t argsz); asmlinkage long sys\_io\_uring\_register(unsigned int fd, unsigned int op, void \_\_user \*arg, unsigned int nr\_args); diff --git a/include/trace/events/io\_uring.h b/include/trace/events/io\_uring.hindex 9f0d3b7d56b0f3..0dd30de00e5b44 100644--- a/[include/trace/events/io\_uring.h](/pub/scm/linux/kernel/git/stable/linux.git/tree/include/trace/events/io_uring.h?id=ed3005032993da7a3fe2e6095436e0bc2e83d011)+++ b/[include/trace/events/io\_uring.h](/pub/scm/linux/kernel/git/stable/linux.git/tree/include/trace/events/io_uring.h?id=788d0824269bef539fe31a785b1517882eafed93)@@ -12,11 +12,11 @@ struct io\_wq\_work; /\*\* \* io\_uring\_create - called after a new io\_uring context was prepared \*- \* @fd: corresponding file descriptor- \* @ctx: pointer to a ring context structure+ \* @fd: corresponding file descriptor+ \* @ctx: pointer to a ring context structure \* @sq\_entries: actual SQ size \* @cq\_entries: actual CQ size- \* @flags: SQ ring flags, provided to io\_uring\_setup(2)+ \* @flags: SQ ring flags, provided to io\_uring\_setup(2) \* \* Allows to trace io\_uring creation and provide pointer to a context, that can \* be used later to find correlated events.@@ -49,15 +49,15 @@ TRACE\_EVENT(io\_uring\_create, );  /\*\*- \* io\_uring\_register - called after a buffer/file/eventfd was succesfully+ \* io\_uring\_register - called after a buffer/file/eventfd was successfully \* registered for a ring \*- \* @ctx: pointer to a ring context structure- \* @opcode: describes which operation to perform+ \* @ctx: pointer to a ring context structure+ \* @opcode: describes which operation to perform \* @nr\_user\_files: number of registered files \* @nr\_user\_bufs: number of registered buffers \* @cq\_ev\_fd: whether eventfs registered or not- \* @ret: return code+ \* @ret: return code \* \* Allows to trace fixed files/buffers/eventfds, that could be registered to \* avoid an overhead of getting references to them for every operation. This@@ -142,16 +142,16 @@ TRACE\_EVENT(io\_uring\_queue\_async\_work, TP\_ARGS(ctx, rw, req, work, flags),  TP\_STRUCT\_\_entry (- \_\_field( void \*, ctx )- \_\_field( int, rw )- \_\_field( void \*, req )+ \_\_field( void \*, ctx )+ \_\_field( int, rw )+ \_\_field( void \*, req ) \_\_field( struct io\_wq\_work \*, work ) \_\_field( unsigned int, flags ) ),  TP\_fast\_assign( \_\_entry->ctx = ctx;- \_\_entry->rw = rw;+ \_\_entry->rw = rw; \_\_entry->req = req; \_\_entry->work = work; \_\_entry->flags = flags;@@ -196,10 +196,10 @@ TRACE\_EVENT(io\_uring\_defer,  /\*\* \* io\_uring\_link - called before the io\_uring request added into link\_list of- \* another request+ \* another request \*- \* @ctx: pointer to a ring context structure- \* @req: pointer to a linked request+ \* @ctx: pointer to a ring context structure+ \* @req: pointer to a linked request \* @target\_req: pointer to a previous request, that would contain @req \* \* Allows to track linked requests, to understand dependencies between requests@@ -212,8 +212,8 @@ TRACE\_EVENT(io\_uring\_link, TP\_ARGS(ctx, req, target\_req),  TP\_STRUCT\_\_entry (- \_\_field( void \*, ctx )- \_\_field( void \*, req )+ \_\_field( void \*, ctx )+ \_\_field( void \*, req ) \_\_field( void \*, target\_req ) ), @@ -244,7 +244,7 @@ TRACE\_EVENT(io\_uring\_cqring\_wait, TP\_ARGS(ctx, min\_events),  TP\_STRUCT\_\_entry (- \_\_field( void \*, ctx )+ \_\_field( void \*, ctx ) \_\_field( int, min\_events ) ), @@ -272,7 +272,7 @@ TRACE\_EVENT(io\_uring\_fail\_link, TP\_ARGS(req, link),  TP\_STRUCT\_\_entry (- \_\_field( void \*, req )+ \_\_field( void \*, req ) \_\_field( void \*, link ) ), @@ -290,38 +290,42 @@ TRACE\_EVENT(io\_uring\_fail\_link, \* @ctx: pointer to a ring context structure \* @user\_data: user data associated with the request \* @res: result of the request+ \* @cflags: completion flags \* \*/ TRACE\_EVENT(io\_uring\_complete, - TP\_PROTO(void \*ctx, u64 user\_data, long res),+ TP\_PROTO(void \*ctx, u64 user\_data, int res, unsigned cflags), - TP\_ARGS(ctx, user\_data, res),+ TP\_ARGS(ctx, user\_data, res, cflags),  TP\_STRUCT\_\_entry ( \_\_field( void \*, ctx ) \_\_field( u64, user\_data )- \_\_field( long, res )+ \_\_field( int, res )+ \_\_field( unsigned, cflags ) ),  TP\_fast\_assign( \_\_entry->ctx = ctx; \_\_entry->user\_data = user\_data; \_\_entry->res = res;+ \_\_entry->cflags = cflags; ), - TP\_printk("ring %p, user\_data 0x%llx, result %ld",+ TP\_printk("ring %p, user\_data 0x%llx, result %d, cflags %x", \_\_entry->ctx, (unsigned long long)\_\_entry->user\_data,- \_\_entry->res)+ \_\_entry->res, \_\_entry->cflags) ); - /\*\* \* io\_uring\_submit\_sqe - called before submitting one SQE \* \* @ctx: pointer to a ring context structure+ \* @req: pointer to a submitted request \* @opcode: opcode of request \* @user\_data: user data associated with the request+ \* @flags request flags \* @force\_nonblock: whether a context blocking or not \* @sq\_thread: true if sq\_thread has submitted this SQE \*@@ -330,41 +334,60 @@ TRACE\_EVENT(io\_uring\_complete, \*/ TRACE\_EVENT(io\_uring\_submit\_sqe, - TP\_PROTO(void \*ctx, u8 opcode, u64 user\_data, bool force\_nonblock,- bool sq\_thread),+ TP\_PROTO(void \*ctx, void \*req, u8 opcode, u64 user\_data, u32 flags,+ bool force\_nonblock, bool sq\_thread), - TP\_ARGS(ctx, opcode, user\_data, force\_nonblock, sq\_thread),+ TP\_ARGS(ctx, req, opcode, user\_data, flags, force\_nonblock, sq\_thread),  TP\_STRUCT\_\_entry ( \_\_field( void \*, ctx )+ \_\_field( void \*, req ) \_\_field( u8, opcode ) \_\_field( u64, user\_data )+ \_\_field( u32, flags ) \_\_field( bool, force\_nonblock ) \_\_field( bool, sq\_thread ) ),  TP\_fast\_assign( \_\_entry->ctx = ctx;+ \_\_entry->req = req; \_\_entry->opcode = opcode; \_\_entry->user\_data = user\_data;+ \_\_entry->flags = flags; \_\_entry->force\_nonblock = force\_nonblock; \_\_entry->sq\_thread = sq\_thread; ), - TP\_printk("ring %p, op %d, data 0x%llx, non block %d, sq\_thread %d",- \_\_entry->ctx, \_\_entry->opcode,- (unsigned long long) \_\_entry->user\_data,- \_\_entry->force\_nonblock, \_\_entry->sq\_thread)+ TP\_printk("ring %p, req %p, op %d, data 0x%llx, flags %u, "+ "non block %d, sq\_thread %d", \_\_entry->ctx, \_\_entry->req,+ \_\_entry->opcode, (unsigned long long)\_\_entry->user\_data,+ \_\_entry->flags, \_\_entry->force\_nonblock, \_\_entry->sq\_thread) ); +/\*+ \* io\_uring\_poll\_arm - called after arming a poll wait if successful+ \*+ \* @ctx: pointer to a ring context structure+ \* @req: pointer to the armed request+ \* @opcode: opcode of request+ \* @user\_data: user data associated with the request+ \* @mask: request poll events mask+ \* @events: registered events of interest+ \*+ \* Allows to track which fds are waiting for and what are the events of+ \* interest.+ \*/ TRACE\_EVENT(io\_uring\_poll\_arm, - TP\_PROTO(void \*ctx, u8 opcode, u64 user\_data, int mask, int events),+ TP\_PROTO(void \*ctx, void \*req, u8 opcode, u64 user\_data,+ int mask, int events), - TP\_ARGS(ctx, opcode, user\_data, mask, events),+ TP\_ARGS(ctx, req, opcode, user\_data, mask, events),  TP\_STRUCT\_\_entry ( \_\_field( void \*, ctx )+ \_\_field( void \*, req ) \_\_field( u8, opcode ) \_\_field( u64, user\_data ) \_\_field( int, mask )@@ -373,16 +396,17 @@ TRACE\_EVENT(io\_uring\_poll\_arm,  TP\_fast\_assign( \_\_entry->ctx = ctx;+ \_\_entry->req = req; \_\_entry->opcode = opcode; \_\_entry->user\_data = user\_data; \_\_entry->mask = mask; \_\_entry->events = events; ), - TP\_printk("ring %p, op %d, data 0x%llx, mask 0x%x, events 0x%x",- \_\_entry->ctx, \_\_entry->opcode,- (unsigned long long) \_\_entry->user\_data,- \_\_entry->mask, \_\_entry->events)+ TP\_printk("ring %p, req %p, op %d, data 0x%llx, mask 0x%x, events 0x%x",+ \_\_entry->ctx, \_\_entry->req, \_\_entry->opcode,+ (unsigned long long) \_\_entry->user\_data,+ \_\_entry->mask, \_\_entry->events) );  TRACE\_EVENT(io\_uring\_poll\_wake,@@ -437,27 +461,40 @@ TRACE\_EVENT(io\_uring\_task\_add, \_\_entry->mask) ); +/\*+ \* io\_uring\_task\_run - called when task\_work\_run() executes the poll events+ \* notification callbacks+ \*+ \* @ctx: pointer to a ring context structure+ \* @req: pointer to the armed request+ \* @opcode: opcode of request+ \* @user\_data: user data associated with the request+ \*+ \* Allows to track when notified poll events are processed+ \*/ TRACE\_EVENT(io\_uring\_task\_run, - TP\_PROTO(void \*ctx, u8 opcode, u64 user\_data),+ TP\_PROTO(void \*ctx, void \*req, u8 opcode, u64 user\_data), - TP\_ARGS(ctx, opcode, user\_data),+ TP\_ARGS(ctx, req, opcode, user\_data),  TP\_STRUCT\_\_entry ( \_\_field( void \*, ctx )+ \_\_field( void \*, req ) \_\_field( u8, opcode ) \_\_field( u64, user\_data ) ),  TP\_fast\_assign( \_\_entry->ctx = ctx;+ \_\_entry->req = req; \_\_entry->opcode = opcode; \_\_entry->user\_data = user\_data; ), - TP\_printk("ring %p, op %d, data 0x%llx",- \_\_entry->ctx, \_\_entry->opcode,- (unsigned long long) \_\_entry->user\_data)+ TP\_printk("ring %p, req %p, op %d, data 0x%llx",+ \_\_entry->ctx, \_\_entry->req, \_\_entry->opcode,+ (unsigned long long) \_\_entry->user\_data) );  #endif /\* \_TRACE\_IO\_URING\_H \*/diff --git a/include/uapi/linux/io\_uring.h b/include/uapi/linux/io\_uring.hindex 98d8e06dea220c..6481db93700287 100644--- a/[include/uapi/linux/io\_uring.h](/pub/scm/linux/kernel/git/stable/linux.git/tree/include/uapi/linux/io_uring.h?id=ed3005032993da7a3fe2e6095436e0bc2e83d011)+++ b/[include/uapi/linux/io\_uring.h](/pub/scm/linux/kernel/git/stable/linux.git/tree/include/uapi/linux/io_uring.h?id=788d0824269bef539fe31a785b1517882eafed93)@@ -42,23 +42,25 @@ struct io\_uring\_sqe { \_\_u32 statx\_flags; \_\_u32 fadvise\_advice; \_\_u32 splice\_flags;+ \_\_u32 rename\_flags;+ \_\_u32 unlink\_flags;+ \_\_u32 hardlink\_flags; }; \_\_u64 user\_data; /\* data to be passed back at completion time \*/+ /\* pack this to avoid bogus arm OABI complaints \*/ union {- struct {- /\* pack this to avoid bogus arm OABI complaints \*/- union {- /\* index into fixed buffers, if used \*/- \_\_u16 buf\_index;- /\* for grouped buffer selection \*/- \_\_u16 buf\_group;- } \_\_attribute\_\_((packed));- /\* personality to use, if used \*/- \_\_u16 personality;- \_\_s32 splice\_fd\_in;- };- \_\_u64 \_\_pad2[3];+ /\* index into fixed buffers, if used \*/+ \_\_u16 buf\_index;+ /\* for grouped buffer selection \*/+ \_\_u16 buf\_group;+ } \_\_attribute\_\_((packed));+ /\* personality to use, if used \*/+ \_\_u16 personality;+ union {+ \_\_s32 splice\_fd\_in;+ \_\_u32 file\_index; };+ \_\_u64 \_\_pad2[2]; };  enum {@@ -132,6 +134,9 @@ enum { IORING\_OP\_PROVIDE\_BUFFERS, IORING\_OP\_REMOVE\_BUFFERS, IORING\_OP\_TEE,+ IORING\_OP\_SHUTDOWN,+ IORING\_OP\_RENAMEAT,+ IORING\_OP\_UNLINKAT,  /\* this goes last, obviously \*/ IORING\_OP\_LAST,@@ -145,8 +150,13 @@ enum { /\* \* sqe->timeout\_flags \*/-#define IORING\_TIMEOUT\_ABS (1U << 0)-+#define IORING\_TIMEOUT\_ABS (1U << 0)+#define IORING\_TIMEOUT\_UPDATE (1U << 1)+#define IORING\_TIMEOUT\_BOOTTIME (1U << 2)+#define IORING\_TIMEOUT\_REALTIME (1U << 3)+#define IORING\_LINK\_TIMEOUT\_UPDATE (1U << 4)+#define IORING\_TIMEOUT\_CLOCK\_MASK (IORING\_TIMEOUT\_BOOTTIME | IORING\_TIMEOUT\_REALTIME)+#define IORING\_TIMEOUT\_UPDATE\_MASK (IORING\_TIMEOUT\_UPDATE | IORING\_LINK\_TIMEOUT\_UPDATE) /\* \* sqe->splice\_flags \* extends splice(2) flags@@ -154,6 +164,21 @@ enum { #define SPLICE\_F\_FD\_IN\_FIXED (1U << 31) /\* the last bit of \_\_u32 \*/  /\*+ \* POLL\_ADD flags. Note that since sqe->poll\_events is the flag space, the+ \* command flags for POLL\_ADD are stored in sqe->len.+ \*+ \* IORING\_POLL\_ADD\_MULTI Multishot poll. Sets IORING\_CQE\_F\_MORE if+ \* the poll handler will continue to report+ \* CQEs on behalf of the same SQE.+ \*+ \* IORING\_POLL\_UPDATE Update existing poll request, matching+ \* sqe->addr as the old user\_data field.+ \*/+#define IORING\_POLL\_ADD\_MULTI (1U << 0)+#define IORING\_POLL\_UPDATE\_EVENTS (1U << 1)+#define IORING\_POLL\_UPDATE\_USER\_DATA (1U << 2)++/\* \* IO completion data structure (Completion Queue Entry) \*/ struct io\_uring\_cqe {@@ -166,8 +191,10 @@ struct io\_uring\_cqe { \* cqe->flags \* \* IORING\_CQE\_F\_BUFFER If set, the upper 16 bits are the buffer ID+ \* IORING\_CQE\_F\_MORE If set, parent SQE will generate more CQE entries \*/ #define IORING\_CQE\_F\_BUFFER (1U << 0)+#define IORING\_CQE\_F\_MORE (1U << 1)  enum { IORING\_CQE\_BUFFER\_SHIFT = 16,@@ -226,6 +253,7 @@ struct io\_cqring\_offsets { #define IORING\_ENTER\_GETEVENTS (1U << 0) #define IORING\_ENTER\_SQ\_WAKEUP (1U << 1) #define IORING\_ENTER\_SQ\_WAIT (1U << 2)+#define IORING\_ENTER\_EXT\_ARG (1U << 3)  /\* \* Passed in for io\_uring\_setup(2). Copied back with updated info on success@@ -253,6 +281,10 @@ struct io\_uring\_params { #define IORING\_FEAT\_CUR\_PERSONALITY (1U << 4) #define IORING\_FEAT\_FAST\_POLL (1U << 5) #define IORING\_FEAT\_POLL\_32BITS (1U << 6)+#define IORING\_FEAT\_SQPOLL\_NONFIXED (1U << 7)+#define IORING\_FEAT\_EXT\_ARG (1U << 8)+#define IORING\_FEAT\_NATIVE\_WORKERS (1U << 9)+#define IORING\_FEAT\_RSRC\_TAGS (1U << 10)  /\* \* io\_uring\_register(2) opcodes and arguments@@ -272,16 +304,62 @@ enum { IORING\_REGISTER\_RESTRICTIONS = 11, IORING\_REGISTER\_ENABLE\_RINGS = 12, + /\* extended with tagging \*/+ IORING\_REGISTER\_FILES2 = 13,+ IORING\_REGISTER\_FILES\_UPDATE2 = 14,+ IORING\_REGISTER\_BUFFERS2 = 15,+ IORING\_REGISTER\_BUFFERS\_UPDATE = 16,++ /\* set/clear io-wq thread affinities \*/+ IORING\_REGISTER\_IOWQ\_AFF = 17,+ IORING\_UNREGISTER\_IOWQ\_AFF = 18,++ /\* set/get max number of io-wq workers \*/+ IORING\_REGISTER\_IOWQ\_MAX\_WORKERS = 19,+ /\* this goes last \*/ IORING\_REGISTER\_LAST }; +/\* io-wq worker categories \*/+enum {+ IO\_WQ\_BOUND,+ IO\_WQ\_UNBOUND,+};++/\* deprecated, see struct io\_uring\_rsrc\_update \*/ struct io\_uring\_files\_update { \_\_u32 offset; \_\_u32 resv; \_\_aligned\_u64 /\* \_\_s32 \* \*/ fds; }; +struct io\_uring\_rsrc\_register {+ \_\_u32 nr;+ \_\_u32 resv;+ \_\_u64 resv2;+ \_\_aligned\_u64 data;+ \_\_aligned\_u64 tags;+};++struct io\_uring\_rsrc\_update {+ \_\_u32 offset;+ \_\_u32 resv;+ \_\_aligned\_u64 data;+};++struct io\_uring\_rsrc\_update2 {+ \_\_u32 offset;+ \_\_u32 resv;+ \_\_aligned\_u64 data;+ \_\_aligned\_u64 tags;+ \_\_u32 nr;+ \_\_u32 resv2;+};++/\* Skip updating fd indexes set to this value in the fd table \*/+#define IORING\_REGISTER\_FILES\_SKIP (-2)+ #define IO\_URING\_OP\_SUPPORTED (1U << 0)  struct io\_uring\_probe\_op {@@ -329,4 +407,11 @@ enum { IORING\_RESTRICTION\_LAST }; +struct io\_uring\_getevents\_arg {+ \_\_u64 sigmask;+ \_\_u32 sigmask\_sz;+ \_\_u32 pad;+ \_\_u64 ts;+};+ #endifdiff --git a/io\_uring/Makefile b/io\_uring/Makefilenew file mode 100644index 00000000000000..3680425df9478b--- /dev/null+++ b/[io\_uring/Makefile](/pub/scm/linux/kernel/git/stable/linux.git/tree/io_uring/Makefile?id=788d0824269bef539fe31a785b1517882eafed93)@@ -0,0 +1,6 @@+# SPDX-License-Identifier: GPL-2.0+#+# Makefile for io\_uring++obj-$(CONFIG\_IO\_URING) += io\_uring.o+obj-$(CONFIG\_IO\_WQ) += io-wq.odiff --git a/io\_uring/io-wq.c b/io\_uring/io-wq.cnew file mode 100644index 00000000000000..6031fb319d8780--- /dev/null+++ b/[io\_uring/io-wq.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/io_uring/io-wq.c?id=788d0824269bef539fe31a785b1517882eafed93)@@ -0,0 +1,1398 @@+// SPDX-License-Identifier: GPL-2.0+/\*+ \* Basic worker thread pool for io\_uring+ \*+ \* Copyright (C) 2019 Jens Axboe+ \*+ \*/+#include <linux/kernel.h>+#include <linux/init.h>+#include <linux/errno.h>+#include <linux/sched/signal.h>+#include <linux/percpu.h>+#include <linux/slab.h>+#include <linux/rculist\_nulls.h>+#include <linux/cpu.h>+#include <linux/tracehook.h>+#include <uapi/linux/io\_uring.h>++#include "io-wq.h"++#define WORKER\_IDLE\_TIMEOUT (5 \* HZ)++enum {+ IO\_WORKER\_F\_UP = 1, /\* up and active \*/+ IO\_WORKER\_F\_RUNNING = 2, /\* account as running \*/+ IO\_WORKER\_F\_FREE = 4, /\* worker on free list \*/+ IO\_WORKER\_F\_BOUND = 8, /\* is doing bounded work \*/+};++enum {+ IO\_WQ\_BIT\_EXIT = 0, /\* wq exiting \*/+};++enum {+ IO\_ACCT\_STALLED\_BIT = 0, /\* stalled on hash \*/+};++/\*+ \* One for each thread in a wqe pool+ \*/+struct io\_worker {+ refcount\_t ref;+ unsigned flags;+ struct hlist\_nulls\_node nulls\_node;+ struct list\_head all\_list;+ struct task\_struct \*task;+ struct io\_wqe \*wqe;++ struct io\_wq\_work \*cur\_work;+ spinlock\_t lock;++ struct completion ref\_done;++ unsigned long create\_state;+ struct callback\_head create\_work;+ int create\_index;++ union {+ struct rcu\_head rcu;+ struct work\_struct work;+ };+};++#if BITS\_PER\_LONG == 64+#define IO\_WQ\_HASH\_ORDER 6+#else+#define IO\_WQ\_HASH\_ORDER 5+#endif++#define IO\_WQ\_NR\_HASH\_BUCKETS (1u << IO\_WQ\_HASH\_ORDER)++struct io\_wqe\_acct {+ unsigned nr\_workers;+ unsigned max\_workers;+ int index;+ atomic\_t nr\_running;+ struct io\_wq\_work\_list work\_list;+ unsigned long flags;+};++enum {+ IO\_WQ\_ACCT\_BOUND,+ IO\_WQ\_ACCT\_UNBOUND,+ IO\_WQ\_ACCT\_NR,+};++/\*+ \* Per-node worker thread pool+ \*/+struct io\_wqe {+ raw\_spinlock\_t lock;+ struct io\_wqe\_acct acct[2];++ int node;++ struct hlist\_nulls\_head free\_list;+ struct list\_head all\_list;++ struct wait\_queue\_entry wait;++ struct io\_wq \*wq;+ struct io\_wq\_work \*hash\_tail[IO\_WQ\_NR\_HASH\_BUCKETS];++ cpumask\_var\_t cpu\_mask;+};++/\*+ \* Per io\_wq state+ \*/+struct io\_wq {+ unsigned long state;++ free\_work\_fn \*free\_work;+ io\_wq\_work\_fn \*do\_work;++ struct io\_wq\_hash \*hash;++ atomic\_t worker\_refs;+ struct completion worker\_done;++ struct hlist\_node cpuhp\_node;++ struct task\_struct \*task;++ struct io\_wqe \*wqes[];+};++static enum cpuhp\_state io\_wq\_online;++struct io\_cb\_cancel\_data {+ work\_cancel\_fn \*fn;+ void \*data;+ int nr\_running;+ int nr\_pending;+ bool cancel\_all;+};++static bool create\_io\_worker(struct io\_wq \*wq, struct io\_wqe \*wqe, int index);+static void io\_wqe\_dec\_running(struct io\_worker \*worker);+static bool io\_acct\_cancel\_pending\_work(struct io\_wqe \*wqe,+ struct io\_wqe\_acct \*acct,+ struct io\_cb\_cancel\_data \*match);+static void create\_worker\_cb(struct callback\_head \*cb);+static void io\_wq\_cancel\_tw\_create(struct io\_wq \*wq);++static bool io\_worker\_get(struct io\_worker \*worker)+{+ return refcount\_inc\_not\_zero(&worker->ref);+}++static void io\_worker\_release(struct io\_worker \*worker)+{+ if (refcount\_dec\_and\_test(&worker->ref))+ complete(&worker->ref\_done);+}++static inline struct io\_wqe\_acct \*io\_get\_acct(struct io\_wqe \*wqe, bool bound)+{+ return &wqe->acct[bound ? IO\_WQ\_ACCT\_BOUND : IO\_WQ\_ACCT\_UNBOUND];+}++static inline struct io\_wqe\_acct \*io\_work\_get\_acct(struct io\_wqe \*wqe,+ struct io\_wq\_work \*work)+{+ return io\_get\_acct(wqe, !(work->flags & IO\_WQ\_WORK\_UNBOUND));+}++static inline struct io\_wqe\_acct \*io\_wqe\_get\_acct(struct io\_worker \*worker)+{+ return io\_get\_acct(worker->wqe, worker->flags & IO\_WORKER\_F\_BOUND);+}++static void io\_worker\_ref\_put(struct io\_wq \*wq)+{+ if (atomic\_dec\_and\_test(&wq->worker\_refs))+ complete(&wq->worker\_done);+}++static void io\_worker\_cancel\_cb(struct io\_worker \*worker)+{+ struct io\_wqe\_acct \*acct = io\_wqe\_get\_acct(worker);+ struct io\_wqe \*wqe = worker->wqe;+ struct io\_wq \*wq = wqe->wq;++ atomic\_dec(&acct->nr\_running);+ raw\_spin\_lock(&worker->wqe->lock);+ acct->nr\_workers--;+ raw\_spin\_unlock(&worker->wqe->lock);+ io\_worker\_ref\_put(wq);+ clear\_bit\_unlock(0, &worker->create\_state);+ io\_worker\_release(worker);+}++static bool io\_task\_worker\_match(struct callback\_head \*cb, void \*data)+{+ struct io\_worker \*worker;++ if (cb->func != create\_worker\_cb)+ return false;+ worker = container\_of(cb, struct io\_worker, create\_work);+ return worker == data;+}++static void io\_worker\_exit(struct io\_worker \*worker)+{+ struct io\_wqe \*wqe = worker->wqe;+ struct io\_wq \*wq = wqe->wq;++ while (1) {+ struct callback\_head \*cb = task\_work\_cancel\_match(wq->task,+ io\_task\_worker\_match, worker);++ if (!cb)+ break;+ io\_worker\_cancel\_cb(worker);+ }++ if (refcount\_dec\_and\_test(&worker->ref))+ complete(&worker->ref\_done);+ wait\_for\_completion(&worker->ref\_done);++ raw\_spin\_lock(&wqe->lock);+ if (worker->flags & IO\_WORKER\_F\_FREE)+ hlist\_nulls\_del\_rcu(&worker->nulls\_node);+ list\_del\_rcu(&worker->all\_list);+ preempt\_disable();+ io\_wqe\_dec\_running(worker);+ worker->flags = 0;+ current->flags &= ~PF\_IO\_WORKER;+ preempt\_enable();+ raw\_spin\_unlock(&wqe->lock);++ kfree\_rcu(worker, rcu);+ io\_worker\_ref\_put(wqe->wq);+ do\_exit(0);+}++static inline bool io\_acct\_run\_queue(struct io\_wqe\_acct \*acct)+{+ if (!wq\_list\_empty(&acct->work\_list) &&+ !test\_bit(IO\_ACCT\_STALLED\_BIT, &acct->flags))+ return true;+ return false;+}++/\*+ \* Check head of free list for an available worker. If one isn't available,+ \* caller must create one.+ \*/+static bool io\_wqe\_activate\_free\_worker(struct io\_wqe \*wqe,+ struct io\_wqe\_acct \*acct)+ \_\_must\_hold(RCU)+{+ struct hlist\_nulls\_node \*n;+ struct io\_worker \*worker;++ /\*+ \* Iterate free\_list and see if we can find an idle worker to+ \* activate. If a given worker is on the free\_list but in the process+ \* of exiting, keep trying.+ \*/+ hlist\_nulls\_for\_each\_entry\_rcu(worker, n, &wqe->free\_list, nulls\_node) {+ if (!io\_worker\_get(worker))+ continue;+ if (io\_wqe\_get\_acct(worker) != acct) {+ io\_worker\_release(worker);+ continue;+ }+ if (wake\_up\_process(worker->task)) {+ io\_worker\_release(worker);+ return true;+ }+ io\_worker\_release(worker);+ }++ return false;+}++/\*+ \* We need a worker. If we find a free one, we're good. If not, and we're+ \* below the max number of workers, create one.+ \*/+static bool io\_wqe\_create\_worker(struct io\_wqe \*wqe, struct io\_wqe\_acct \*acct)+{+ /\*+ \* Most likely an attempt to queue unbounded work on an io\_wq that+ \* wasn't setup with any unbounded workers.+ \*/+ if (unlikely(!acct->max\_workers))+ pr\_warn\_once("io-wq is not configured for unbound workers");++ raw\_spin\_lock(&wqe->lock);+ if (acct->nr\_workers >= acct->max\_workers) {+ raw\_spin\_unlock(&wqe->lock);+ return true;+ }+ acct->nr\_workers++;+ raw\_spin\_unlock(&wqe->lock);+ atomic\_inc(&acct->nr\_running);+ atomic\_inc(&wqe->wq->worker\_refs);+ return create\_io\_worker(wqe->wq, wqe, acct->index);+}++static void io\_wqe\_inc\_running(struct io\_worker \*worker)+{+ struct io\_wqe\_acct \*acct = io\_wqe\_get\_acct(worker);++ atomic\_inc(&acct->nr\_running);+}++static void create\_worker\_cb(struct callback\_head \*cb)+{+ struct io\_worker \*worker;+ struct io\_wq \*wq;+ struct io\_wqe \*wqe;+ struct io\_wqe\_acct \*acct;+ bool do\_create = false;++ worker = container\_of(cb, struct io\_worker, create\_work);+ wqe = worker->wqe;+ wq = wqe->wq;+ acct = &wqe->acct[worker->create\_index];+ raw\_spin\_lock(&wqe->lock);+ if (acct->nr\_workers < acct->max\_workers) {+ acct->nr\_workers++;+ do\_create = true;+ }+ raw\_spin\_unlock(&wqe->lock);+ if (do\_create) {+ create\_io\_worker(wq, wqe, worker->create\_index);+ } else {+ atomic\_dec(&acct->nr\_running);+ io\_worker\_ref\_put(wq);+ }+ clear\_bit\_unlock(0, &worker->create\_state);+ io\_worker\_release(worker);+}++static bool io\_queue\_worker\_create(struct io\_worker \*worker,+ struct io\_wqe\_acct \*acct,+ task\_work\_func\_t func)+{+ struct io\_wqe \*wqe = worker->wqe;+ struct io\_wq \*wq = wqe->wq;++ /\* raced with exit, just ignore create call \*/+ if (test\_bit(IO\_WQ\_BIT\_EXIT, &wq->state))+ goto fail;+ if (!io\_worker\_get(worker))+ goto fail;+ /\*+ \* create\_state manages ownership of create\_work/index. We should+ \* only need one entry per worker, as the worker going to sleep+ \* will trigger the condition, and waking will clear it once it+ \* runs the task\_work.+ \*/+ if (test\_bit(0, &worker->create\_state) ||+ test\_and\_set\_bit\_lock(0, &worker->create\_state))+ goto fail\_release;++ atomic\_inc(&wq->worker\_refs);+ init\_task\_work(&worker->create\_work, func);+ worker->create\_index = acct->index;+ if (!task\_work\_add(wq->task, &worker->create\_work, TWA\_SIGNAL)) {+ /\*+ \* EXIT may have been set after checking it above, check after+ \* adding the task\_work and remove any creation item if it is+ \* now set. wq exit does that too, but we can have added this+ \* work item after we canceled in io\_wq\_exit\_workers().+ \*/+ if (test\_bit(IO\_WQ\_BIT\_EXIT, &wq->state))+ io\_wq\_cancel\_tw\_create(wq);+ io\_worker\_ref\_put(wq);+ return true;+ }+ io\_worker\_ref\_put(wq);+ clear\_bit\_unlock(0, &worker->create\_state);+fail\_release:+ io\_worker\_release(worker);+fail:+ atomic\_dec(&acct->nr\_running);+ io\_worker\_ref\_put(wq);+ return false;+}++static void io\_wqe\_dec\_running(struct io\_worker \*worker)+ \_\_must\_hold(wqe->lock)+{+ struct io\_wqe\_acct \*acct = io\_wqe\_get\_acct(worker);+ struct io\_wqe \*wqe = worker->wqe;++ if (!(worker->flags & IO\_WORKER\_F\_UP))+ return;++ if (atomic\_dec\_and\_test(&acct->nr\_running) && io\_acct\_run\_queue(acct)) {+ atomic\_inc(&acct->nr\_running);+ atomic\_inc(&wqe->wq->worker\_refs);+ raw\_spin\_unlock(&wqe->lock);+ io\_queue\_worker\_create(worker, acct, create\_worker\_cb);+ raw\_spin\_lock(&wqe->lock);+ }+}++/\*+ \* Worker will start processing some work. Move it to the busy list, if+ \* it's currently on the freelist+ \*/+static void \_\_io\_worker\_busy(struct io\_wqe \*wqe, struct io\_worker \*worker,+ struct io\_wq\_work \*work)+ \_\_must\_hold(wqe->lock)+{+ if (worker->flags & IO\_WORKER\_F\_FREE) {+ worker->flags &= ~IO\_WORKER\_F\_FREE;+ hlist\_nulls\_del\_init\_rcu(&worker->nulls\_node);+ }+}++/\*+ \* No work, worker going to sleep. Move to freelist, and unuse mm if we+ \* have one attached. Dropping the mm may potentially sleep, so we drop+ \* the lock in that case and return success. Since the caller has to+ \* retry the loop in that case (we changed task state), we don't regrab+ \* the lock if we return success.+ \*/+static void \_\_io\_worker\_idle(struct io\_wqe \*wqe, struct io\_worker \*worker)+ \_\_must\_hold(wqe->lock)+{+ if (!(worker->flags & IO\_WORKER\_F\_FREE)) {+ worker->flags |= IO\_WORKER\_F\_FREE;+ hlist\_nulls\_add\_head\_rcu(&worker->nulls\_node, &wqe->free\_list);+ }+}++static inline unsigned int io\_get\_work\_hash(struct io\_wq\_work \*work)+{+ return work->flags >> IO\_WQ\_HASH\_SHIFT;+}++static bool io\_wait\_on\_hash(struct io\_wqe \*wqe, unsigned int hash)+{+ struct io\_wq \*wq = wqe->wq;+ bool ret = false;++ spin\_lock\_irq(&wq->hash->wait.lock);+ if (list\_empty(&wqe->wait.entry)) {+ \_\_add\_wait\_queue(&wq->hash->wait, &wqe->wait);+ if (!test\_bit(hash, &wq->hash->map)) {+ \_\_set\_current\_state(TASK\_RUNNING);+ list\_del\_init(&wqe->wait.entry);+ ret = true;+ }+ }+ spin\_unlock\_irq(&wq->hash->wait.lock);+ return ret;+}++static struct io\_wq\_work \*io\_get\_next\_work(struct io\_wqe\_acct \*acct,+ struct io\_worker \*worker)+ \_\_must\_hold(wqe->lock)+{+ struct io\_wq\_work\_node \*node, \*prev;+ struct io\_wq\_work \*work, \*tail;+ unsigned int stall\_hash = -1U;+ struct io\_wqe \*wqe = worker->wqe;++ wq\_list\_for\_each(node, prev, &acct->work\_list) {+ unsigned int hash;++ work = container\_of(node, struct io\_wq\_work, list);++ /\* not hashed, can run anytime \*/+ if (!io\_wq\_is\_hashed(work)) {+ wq\_list\_del(&acct->work\_list, node, prev);+ return work;+ }++ hash = io\_get\_work\_hash(work);+ /\* all items with this hash lie in [work, tail] \*/+ tail = wqe->hash\_tail[hash];++ /\* hashed, can run if not already running \*/+ if (!test\_and\_set\_bit(hash, &wqe->wq->hash->map)) {+ wqe->hash\_tail[hash] = NULL;+ wq\_list\_cut(&acct->work\_list, &tail->list, prev);+ return work;+ }+ if (stall\_hash == -1U)+ stall\_hash = hash;+ /\* fast forward to a next hash, for-each will fix up @prev \*/+ node = &tail->list;+ }++ if (stall\_hash != -1U) {+ bool unstalled;++ /\*+ \* Set this before dropping the lock to avoid racing with new+ \* work being added and clearing the stalled bit.+ \*/+ set\_bit(IO\_ACCT\_STALLED\_BIT, &acct->flags);+ raw\_spin\_unlock(&wqe->lock);+ unstalled = io\_wait\_on\_hash(wqe, stall\_hash);+ raw\_spin\_lock(&wqe->lock);+ if (unstalled) {+ clear\_bit(IO\_ACCT\_STALLED\_BIT, &acct->flags);+ if (wq\_has\_sleeper(&wqe->wq->hash->wait))+ wake\_up(&wqe->wq->hash->wait);+ }+ }++ return NULL;+}++static bool io\_flush\_signals(void)+{+ if (unlikely(test\_thread\_flag(TIF\_NOTIFY\_SIGNAL))) {+ \_\_set\_current\_state(TASK\_RUNNING);+ tracehook\_notify\_signal();+ return true;+ }+ return false;+}++static void io\_assign\_current\_work(struct io\_worker \*worker,+ struct io\_wq\_work \*work)+{+ if (work) {+ io\_flush\_signals();+ cond\_resched();+ }++ spin\_lock(&worker->lock);+ worker->cur\_work = work;+ spin\_unlock(&worker->lock);+}++static void io\_wqe\_enqueue(struct io\_wqe \*wqe, struct io\_wq\_work \*work);++static void io\_worker\_handle\_work(struct io\_worker \*worker)+ \_\_releases(wqe->lock)+{+ struct io\_wqe\_acct \*acct = io\_wqe\_get\_acct(worker);+ struct io\_wqe \*wqe = worker->wqe;+ struct io\_wq \*wq = wqe->wq;+ bool do\_kill = test\_bit(IO\_WQ\_BIT\_EXIT, &wq->state);++ do {+ struct io\_wq\_work \*work;+get\_next:+ /\*+ \* If we got some work, mark us as busy. If we didn't, but+ \* the list isn't empty, it means we stalled on hashed work.+ \* Mark us stalled so we don't keep looking for work when we+ \* can't make progress, any work completion or insertion will+ \* clear the stalled flag.+ \*/+ work = io\_get\_next\_work(acct, worker);+ if (work)+ \_\_io\_worker\_busy(wqe, worker, work);++ raw\_spin\_unlock(&wqe->lock);+ if (!work)+ break;+ io\_assign\_current\_work(worker, work);+ \_\_set\_current\_state(TASK\_RUNNING);++ /\* handle a whole dependent link \*/+ do {+ struct io\_wq\_work \*next\_hashed, \*linked;+ unsigned int hash = io\_get\_work\_hash(work);++ next\_hashed = wq\_next\_work(work);++ if (unlikely(do\_kill) && (work->flags & IO\_WQ\_WORK\_UNBOUND))+ work->flags |= IO\_WQ\_WORK\_CANCEL;+ wq->do\_work(work);+ io\_assign\_current\_work(worker, NULL);++ linked = wq->free\_work(work);+ work = next\_hashed;+ if (!work && linked && !io\_wq\_is\_hashed(linked)) {+ work = linked;+ linked = NULL;+ }+ io\_assign\_current\_work(worker, work);+ if (linked)+ io\_wqe\_enqueue(wqe, linked);++ if (hash != -1U && !next\_hashed) {+ /\* serialize hash clear with wake\_up() \*/+ spin\_lock\_irq(&wq->hash->wait.lock);+ clear\_bit(hash, &wq->hash->map);+ clear\_bit(IO\_ACCT\_STALLED\_BIT, &acct->flags);+ spin\_unlock\_irq(&wq->hash->wait.lock);+ if (wq\_has\_sleeper(&wq->hash->wait))+ wake\_up(&wq->hash->wait);+ raw\_spin\_lock(&wqe->lock);+ /\* skip unnecessary unlock-lock wqe->lock \*/+ if (!work)+ goto get\_next;+ raw\_spin\_unlock(&wqe->lock);+ }+ } while (work);++ raw\_spin\_lock(&wqe->lock);+ } while (1);+}++static int io\_wqe\_worker(void \*data)+{+ struct io\_worker \*worker = data;+ struct io\_wqe\_acct \*acct = io\_wqe\_get\_acct(worker);+ struct io\_wqe \*wqe = worker->wqe;+ struct io\_wq \*wq = wqe->wq;+ bool last\_timeout = false;+ char buf[TASK\_COMM\_LEN];++ worker->flags |= (IO\_WORKER\_F\_UP | IO\_WORKER\_F\_RUNNING);++ snprintf(buf, sizeof(buf), "iou-wrk-%d", wq->task->pid);+ set\_task\_comm(current, buf);++ while (!test\_bit(IO\_WQ\_BIT\_EXIT, &wq->state)) {+ long ret;++ set\_current\_state(TASK\_INTERRUPTIBLE);+loop:+ raw\_spin\_lock(&wqe->lock);+ if (io\_acct\_run\_queue(acct)) {+ io\_worker\_handle\_work(worker);+ goto loop;+ }+ /\* timed out, exit unless we're the last worker \*/+ if (last\_timeout && acct->nr\_workers > 1) {+ acct->nr\_workers--;+ raw\_spin\_unlock(&wqe->lock);+ \_\_set\_current\_state(TASK\_RUNNING);+ break;+ }+ last\_timeout = false;+ \_\_io\_worker\_idle(wqe, worker);+ raw\_spin\_unlock(&wqe->lock);+ if (io\_flush\_signals())+ continue;+ ret = schedule\_timeout(WORKER\_IDLE\_TIMEOUT);+ if (signal\_pending(current)) {+ struct ksignal ksig;++ if (!get\_signal(&ksig))+ continue;+ break;+ }+ last\_timeout = !ret;+ }++ if (test\_bit(IO\_WQ\_BIT\_EXIT, &wq->state)) {+ raw\_spin\_lock(&wqe->lock);+ io\_worker\_handle\_work(worker);+ }++ io\_worker\_exit(worker);+ return 0;+}++/\*+ \* Called when a worker is scheduled in. Mark us as currently running.+ \*/+void io\_wq\_worker\_running(struct task\_struct \*tsk)+{+ struct io\_worker \*worker = tsk->pf\_io\_worker;++ if (!worker)+ return;+ if (!(worker->flags & IO\_WORKER\_F\_UP))+ return;+ if (worker->flags & IO\_WORKER\_F\_RUNNING)+ return;+ worker->flags |= IO\_WORKER\_F\_RUNNING;+ io\_wqe\_inc\_running(worker);+}++/\*+ \* Called when worker is going to sleep. If there are no workers currently+ \* running and we have work pending, wake up a free one or create a new one.+ \*/+void io\_wq\_worker\_sleeping(struct task\_struct \*tsk)+{+ struct io\_worker \*worker = tsk->pf\_io\_worker;++ if (!worker)+ return;+ if (!(worker->flags & IO\_WORKER\_F\_UP))+ return;+ if (!(worker->flags & IO\_WORKER\_F\_RUNNING))+ return;++ worker->flags &= ~IO\_WORKER\_F\_RUNNING;++ raw\_spin\_lock(&worker->wqe->lock);+ io\_wqe\_dec\_running(worker);+ raw\_spin\_unlock(&worker->wqe->lock);+}++static void io\_init\_new\_worker(struct io\_wqe \*wqe, struct io\_worker \*worker,+ struct task\_struct \*tsk)+{+ tsk->pf\_io\_worker = worker;+ worker->task = tsk;+ set\_cpus\_allowed\_ptr(tsk, wqe->cpu\_mask);+ tsk->flags |= PF\_NO\_SETAFFINITY;++ raw\_spin\_lock(&wqe->lock);+ hlist\_nulls\_add\_head\_rcu(&worker->nulls\_node, &wqe->free\_list);+ list\_add\_tail\_rcu(&worker->all\_list, &wqe->all\_list);+ worker->flags |= IO\_WORKER\_F\_FREE;+ raw\_spin\_unlock(&wqe->lock);+ wake\_up\_new\_task(tsk);+}++static bool io\_wq\_work\_match\_all(struct io\_wq\_work \*work, void \*data)+{+ return true;+}++static inline bool io\_should\_retry\_thread(long err)+{+ /\*+ \* Prevent perpetual task\_work retry, if the task (or its group) is+ \* exiting.+ \*/+ if (fatal\_signal\_pending(current))+ return false;++ switch (err) {+ case -EAGAIN:+ case -ERESTARTSYS:+ case -ERESTARTNOINTR:+ case -ERESTARTNOHAND:+ return true;+ default:+ return false;+ }+}++static void create\_worker\_cont(struct callback\_head \*cb)+{+ struct io\_worker \*worker;+ struct task\_struct \*tsk;+ struct io\_wqe \*wqe;++ worker = container\_of(cb, struct io\_worker, create\_work);+ clear\_bit\_unlock(0, &worker->create\_state);+ wqe = worker->wqe;+ tsk = create\_io\_thread(io\_wqe\_worker, worker, wqe->node);+ if (!IS\_ERR(tsk)) {+ io\_init\_new\_worker(wqe, worker, tsk);+ io\_worker\_release(worker);+ return;+ } else if (!io\_should\_retry\_thread(PTR\_ERR(tsk))) {+ struct io\_wqe\_acct \*acct = io\_wqe\_get\_acct(worker);++ atomic\_dec(&acct->nr\_running);+ raw\_spin\_lock(&wqe->lock);+ acct->nr\_workers--;+ if (!acct->nr\_workers) {+ struct io\_cb\_cancel\_data match = {+ .fn = io\_wq\_work\_match\_all,+ .cancel\_all = true,+ };++ while (io\_acct\_cancel\_pending\_work(wqe, acct, &match))+ raw\_spin\_lock(&wqe->lock);+ }+ raw\_spin\_unlock(&wqe->lock);+ io\_worker\_ref\_put(wqe->wq);+ kfree(worker);+ return;+ }++ /\* re-create attempts grab a new worker ref, drop the existing one \*/+ io\_worker\_release(worker);+ schedule\_work(&worker->work);+}++static void io\_workqueue\_create(struct work\_struct \*work)+{+ struct io\_worker \*worker = container\_of(work, struct io\_worker, work);+ struct io\_wqe\_acct \*acct = io\_wqe\_get\_acct(worker);++ if (!io\_queue\_worker\_create(worker, acct, create\_worker\_cont))+ kfree(worker);+}++static bool create\_io\_worker(struct io\_wq \*wq, struct io\_wqe \*wqe, int index)+{+ struct io\_wqe\_acct \*acct = &wqe->acct[index];+ struct io\_worker \*worker;+ struct task\_struct \*tsk;++ \_\_set\_current\_state(TASK\_RUNNING);++ worker = kzalloc\_node(sizeof(\*worker), GFP\_KERNEL, wqe->node);+ if (!worker) {+fail:+ atomic\_dec(&acct->nr\_running);+ raw\_spin\_lock(&wqe->lock);+ acct->nr\_workers--;+ raw\_spin\_unlock(&wqe->lock);+ io\_worker\_ref\_put(wq);+ return false;+ }++ refcount\_set(&worker->ref, 1);+ worker->wqe = wqe;+ spin\_lock\_init(&worker->lock);+ init\_completion(&worker->ref\_done);++ if (index == IO\_WQ\_ACCT\_BOUND)+ worker->flags |= IO\_WORKER\_F\_BOUND;++ tsk = create\_io\_thread(io\_wqe\_worker, worker, wqe->node);+ if (!IS\_ERR(tsk)) {+ io\_init\_new\_worker(wqe, worker, tsk);+ } else if (!io\_should\_retry\_thread(PTR\_ERR(tsk))) {+ kfree(worker);+ goto fail;+ } else {+ INIT\_WORK(&worker->work, io\_workqueue\_create);+ schedule\_work(&worker->work);+ }++ return true;+}++/\*+ \* Iterate the passed in list and call the specific function for each+ \* worker that isn't exiting+ \*/+static bool io\_wq\_for\_each\_worker(struct io\_wqe \*wqe,+ bool (\*func)(struct io\_worker \*, void \*),+ void \*data)+{+ struct io\_worker \*worker;+ bool ret = false;++ list\_for\_each\_entry\_rcu(worker, &wqe->all\_list, all\_list) {+ if (io\_worker\_get(worker)) {+ /\* no task if node is/was offline \*/+ if (worker->task)+ ret = func(worker, data);+ io\_worker\_release(worker);+ if (ret)+ break;+ }+ }++ return ret;+}++static bool io\_wq\_worker\_wake(struct io\_worker \*worker, void \*data)+{+ set\_notify\_signal(worker->task);+ wake\_up\_process(worker->task);+ return false;+}++static void io\_run\_cancel(struct io\_wq\_work \*work, struct io\_wqe \*wqe)+{+ struct io\_wq \*wq = wqe->wq;++ do {+ work->flags |= IO\_WQ\_WORK\_CANCEL;+ wq->do\_work(work);+ work = wq->free\_work(work);+ } while (work);+}++static void io\_wqe\_insert\_work(struct io\_wqe \*wqe, struct io\_wq\_work \*work)+{+ struct io\_wqe\_acct \*acct = io\_work\_get\_acct(wqe, work);+ unsigned int hash;+ struct io\_wq\_work \*tail;++ if (!io\_wq\_is\_hashed(work)) {+append:+ wq\_list\_add\_tail(&work->list, &acct->work\_list);+ return;+ }++ hash = io\_get\_work\_hash(work);+ tail = wqe->hash\_tail[hash];+ wqe->hash\_tail[hash] = work;+ if (!tail)+ goto append;++ wq\_list\_add\_after(&work->list, &tail->list, &acct->work\_list);+}++static bool io\_wq\_work\_match\_item(struct io\_wq\_work \*work, void \*data)+{+ return work == data;+}++static void io\_wqe\_enqueue(struct io\_wqe \*wqe, struct io\_wq\_work \*work)+{+ struct io\_wqe\_acct \*acct = io\_work\_get\_acct(wqe, work);+ unsigned work\_flags = work->flags;+ bool do\_create;++ /\*+ \* If io-wq is exiting for this task, or if the request has explicitly+ \* been marked as one that should not get executed, cancel it here.+ \*/+ if (test\_bit(IO\_WQ\_BIT\_EXIT, &wqe->wq->state) ||+ (work->flags & IO\_WQ\_WORK\_CANCEL)) {+ io\_run\_cancel(work, wqe);+ return;+ }++ raw\_spin\_lock(&wqe->lock);+ io\_wqe\_insert\_work(wqe, work);+ clear\_bit(IO\_ACCT\_STALLED\_BIT, &acct->flags);++ rcu\_read\_lock();+ do\_create = !io\_wqe\_activate\_free\_worker(wqe, acct);+ rcu\_read\_unlock();++ raw\_spin\_unlock(&wqe->lock);++ if (do\_create && ((work\_flags & IO\_WQ\_WORK\_CONCURRENT) ||+ !atomic\_read(&acct->nr\_running))) {+ bool did\_create;++ did\_create = io\_wqe\_create\_worker(wqe, acct);+ if (likely(did\_create))+ return;++ raw\_spin\_lock(&wqe->lock);+ /\* fatal condition, failed to create the first worker \*/+ if (!acct->nr\_workers) {+ struct io\_cb\_cancel\_data match = {+ .fn = io\_wq\_work\_match\_item,+ .data = work,+ .cancel\_all = false,+ };++ if (io\_acct\_cancel\_pending\_work(wqe, acct, &match))+ raw\_spin\_lock(&wqe->lock);+ }+ raw\_spin\_unlock(&wqe->lock);+ }+}++void io\_wq\_enqueue(struct io\_wq \*wq, struct io\_wq\_work \*work)+{+ struct io\_wqe \*wqe = wq->wqes[numa\_node\_id()];++ io\_wqe\_enqueue(wqe, work);+}++/\*+ \* Work items that hash to the same value will not be done in parallel.+ \* Used to limit concurrent writes, generally hashed by inode.+ \*/+void io\_wq\_hash\_work(struct io\_wq\_work \*work, void \*val)+{+ unsigned int bit;++ bit = hash\_ptr(val, IO\_WQ\_HASH\_ORDER);+ work->flags |= (IO\_WQ\_WORK\_HASHED | (bit << IO\_WQ\_HASH\_SHIFT));+}++static bool io\_wq\_worker\_cancel(struct io\_worker \*worker, void \*data)+{+ struct io\_cb\_cancel\_data \*match = data;++ /\*+ \* Hold the lock to avoid ->cur\_work going out of scope, caller+ \* may dereference the passed in work.+ \*/+ spin\_lock(&worker->lock);+ if (worker->cur\_work &&+ match->fn(worker->cur\_work, match->data)) {+ set\_notify\_signal(worker->task);+ match->nr\_running++;+ }+ spin\_unlock(&worker->lock);++ return match->nr\_running && !match->cancel\_all;+}++static inline void io\_wqe\_remove\_pending(struct io\_wqe \*wqe,+ struct io\_wq\_work \*work,+ struct io\_wq\_work\_node \*prev)+{+ struct io\_wqe\_acct \*acct = io\_work\_get\_acct(wqe, work);+ unsigned int hash = io\_get\_work\_hash(work);+ struct io\_wq\_work \*prev\_work = NULL;++ if (io\_wq\_is\_hashed(work) && work == wqe->hash\_tail[hash]) {+ if (prev)+ prev\_work = container\_of(prev, struct io\_wq\_work, list);+ if (prev\_work && io\_get\_work\_hash(prev\_work) == hash)+ wqe->hash\_tail[hash] = prev\_work;+ else+ wqe->hash\_tail[hash] = NULL;+ }+ wq\_list\_del(&acct->work\_list, &work->list, prev);+}++static bool io\_acct\_cancel\_pending\_work(struct io\_wqe \*wqe,+ struct io\_wqe\_acct \*acct,+ struct io\_cb\_cancel\_data \*match)+ \_\_releases(wqe->lock)+{+ struct io\_wq\_work\_node \*node, \*prev;+ struct io\_wq\_work \*work;++ wq\_list\_for\_each(node, prev, &acct->work\_list) {+ work = container\_of(node, struct io\_wq\_work, list);+ if (!match->fn(work, match->data))+ continue;+ io\_wqe\_remove\_pending(wqe, work, prev);+ raw\_spin\_unlock(&wqe->lock);+ io\_run\_cancel(work, wqe);+ match->nr\_pending++;+ /\* not safe to continue after unlock \*/+ return true;+ }++ return false;+}++static void io\_wqe\_cancel\_pending\_work(struct io\_wqe \*wqe,+ struct io\_cb\_cancel\_data \*match)+{+ int i;+retry:+ raw\_spin\_lock(&wqe->lock);+ for (i = 0; i < IO\_WQ\_ACCT\_NR; i++) {+ struct io\_wqe\_acct \*acct = io\_get\_acct(wqe, i == 0);++ if (io\_acct\_cancel\_pending\_work(wqe, acct, match)) {+ if (match->cancel\_all)+ goto retry;+ return;+ }+ }+ raw\_spin\_unlock(&wqe->lock);+}++static void io\_wqe\_cancel\_running\_work(struct io\_wqe \*wqe,+ struct io\_cb\_cancel\_data \*match)+{+ rcu\_read\_lock();+ io\_wq\_for\_each\_worker(wqe, io\_wq\_worker\_cancel, match);+ rcu\_read\_unlock();+}++enum io\_wq\_cancel io\_wq\_cancel\_cb(struct io\_wq \*wq, work\_cancel\_fn \*cancel,+ void \*data, bool cancel\_all)+{+ struct io\_cb\_cancel\_data match = {+ .fn = cancel,+ .data = data,+ .cancel\_all = cancel\_all,+ };+ int node;++ /\*+ \* First check pending list, if we're lucky we can just remove it+ \* from there. CANCEL\_OK means that the work is returned as-new,+ \* no completion will be posted for it.+ \*/+ for\_each\_node(node) {+ struct io\_wqe \*wqe = wq->wqes[node];++ io\_wqe\_cancel\_pending\_work(wqe, &match);+ if (match.nr\_pending && !match.cancel\_all)+ return IO\_WQ\_CANCEL\_OK;+ }++ /\*+ \* Now check if a free (going busy) or busy worker has the work+ \* currently running. If we find it there, we'll return CANCEL\_RUNNING+ \* as an indication that we attempt to signal cancellation. The+ \* completion will run normally in this case.+ \*/+ for\_each\_node(node) {+ struct io\_wqe \*wqe = wq->wqes[node];++ io\_wqe\_cancel\_running\_work(wqe, &match);+ if (match.nr\_running && !match.cancel\_all)+ return IO\_WQ\_CANCEL\_RUNNING;+ }++ if (match.nr\_running)+ return IO\_WQ\_CANCEL\_RUNNING;+ if (match.nr\_pending)+ return IO\_WQ\_CANCEL\_OK;+ return IO\_WQ\_CANCEL\_NOTFOUND;+}++static int io\_wqe\_hash\_wake(struct wait\_queue\_entry \*wait, unsigned mode,+ int sync, void \*key)+{+ struct io\_wqe \*wqe = container\_of(wait, struct io\_wqe, wait);+ int i;++ list\_del\_init(&wait->entry);++ rcu\_read\_lock();+ for (i = 0; i < IO\_WQ\_ACCT\_NR; i++) {+ struct io\_wqe\_acct \*acct = &wqe->acct[i];++ if (test\_and\_clear\_bit(IO\_ACCT\_STALLED\_BIT, &acct->flags))+ io\_wqe\_activate\_free\_worker(wqe, acct);+ }+ rcu\_read\_unlock();+ return 1;+}++struct io\_wq \*io\_wq\_create(unsigned bounded, struct io\_wq\_data \*data)+{+ int ret, node, i;+ struct io\_wq \*wq;++ if (WARN\_ON\_ONCE(!data->free\_work || !data->do\_work))+ return ERR\_PTR(-EINVAL);+ if (WARN\_ON\_ONCE(!bounded))+ return ERR\_PTR(-EINVAL);++ wq = kzalloc(struct\_size(wq, wqes, nr\_node\_ids), GFP\_KERNEL);+ if (!wq)+ return ERR\_PTR(-ENOMEM);+ ret = cpuhp\_state\_add\_instance\_nocalls(io\_wq\_online, &wq->cpuhp\_node);+ if (ret)+ goto err\_wq;++ refcount\_inc(&data->hash->refs);+ wq->hash = data->hash;+ wq->free\_work = data->free\_work;+ wq->do\_work = data->do\_work;++ ret = -ENOMEM;+ for\_each\_node(node) {+ struct io\_wqe \*wqe;+ int alloc\_node = node;++ if (!node\_online(alloc\_node))+ alloc\_node = NUMA\_NO\_NODE;+ wqe = kzalloc\_node(sizeof(struct io\_wqe), GFP\_KERNEL, alloc\_node);+ if (!wqe)+ goto err;+ wq->wqes[node] = wqe;+ if (!alloc\_cpumask\_var(&wqe->cpu\_mask, GFP\_KERNEL))+ goto err;+ cpumask\_copy(wqe->cpu\_mask, cpumask\_of\_node(node));+ wqe->node = alloc\_node;+ wqe->acct[IO\_WQ\_ACCT\_BOUND].max\_workers = bounded;+ wqe->acct[IO\_WQ\_ACCT\_UNBOUND].max\_workers =+ task\_rlimit(current, RLIMIT\_NPROC);+ INIT\_LIST\_HEAD(&wqe->wait.entry);+ wqe->wait.func = io\_wqe\_hash\_wake;+ for (i = 0; i < IO\_WQ\_ACCT\_NR; i++) {+ struct io\_wqe\_acct \*acct = &wqe->acct[i];++ acct->index = i;+ atomic\_set(&acct->nr\_running, 0);+ INIT\_WQ\_LIST(&acct->work\_list);+ }+ wqe->wq = wq;+ raw\_spin\_lock\_init(&wqe->lock);+ INIT\_HLIST\_NULLS\_HEAD(&wqe->free\_list, 0);+ INIT\_LIST\_HEAD(&wqe->all\_list);+ }++ wq->task = get\_task\_struct(data->task);+ atomic\_set(&wq->worker\_refs, 1);+ init\_completion(&wq->worker\_done);+ return wq;+err:+ io\_wq\_put\_hash(data->hash);+ cpuhp\_state\_remove\_instance\_nocalls(io\_wq\_online, &wq->cpuhp\_node);+ for\_each\_node(node) {+ if (!wq->wqes[node])+ continue;+ free\_cpumask\_var(wq->wqes[node]->cpu\_mask);+ kfree(wq->wqes[node]);+ }+err\_wq:+ kfree(wq);+ return ERR\_PTR(ret);+}++static bool io\_task\_work\_match(struct callback\_head \*cb, void \*data)+{+ struct io\_worker \*worker;++ if (cb->func != create\_worker\_cb && cb->func != create\_worker\_cont)+ return false;+ worker = container\_of(cb, struct io\_worker, create\_work);+ return worker->wqe->wq == data;+}++void io\_wq\_exit\_start(struct io\_wq \*wq)+{+ set\_bit(IO\_WQ\_BIT\_EXIT, &wq->state);+}++static void io\_wq\_cancel\_tw\_create(struct io\_wq \*wq)+{+ struct callback\_head \*cb;++ while ((cb = task\_work\_cancel\_match(wq->task, io\_task\_work\_match, wq)) != NULL) {+ struct io\_worker \*worker;++ worker = container\_of(cb, struct io\_worker, create\_work);+ io\_worker\_cancel\_cb(worker);+ }+}++static void io\_wq\_exit\_workers(struct io\_wq \*wq)+{+ int node;++ if (!wq->task)+ return;++ io\_wq\_cancel\_tw\_create(wq);++ rcu\_read\_lock();+ for\_each\_node(node) {+ struct io\_wqe \*wqe = wq->wqes[node];++ io\_wq\_for\_each\_worker(wqe, io\_wq\_worker\_wake, NULL);+ }+ rcu\_read\_unlock();+ io\_worker\_ref\_put(wq);+ wait\_for\_completion(&wq->worker\_done);++ for\_each\_node(node) {+ spin\_lock\_irq(&wq->hash->wait.lock);+ list\_del\_init(&wq->wqes[node]->wait.entry);+ spin\_unlock\_irq(&wq->hash->wait.lock);+ }+ put\_task\_struct(wq->task);+ wq->task = NULL;+}++static void io\_wq\_destroy(struct io\_wq \*wq)+{+ int node;++ cpuhp\_state\_remove\_instance\_nocalls(io\_wq\_online, &wq->cpuhp\_node);++ for\_each\_node(node) {+ struct io\_wqe \*wqe = wq->wqes[node];+ struct io\_cb\_cancel\_data match = {+ .fn = io\_wq\_work\_match\_all,+ .cancel\_all = true,+ };+ io\_wqe\_cancel\_pending\_work(wqe, &match);+ free\_cpumask\_var(wqe->cpu\_mask);+ kfree(wqe);+ }+ io\_wq\_put\_hash(wq->hash);+ kfree(wq);+}++void io\_wq\_put\_and\_exit(struct io\_wq \*wq)+{+ WARN\_ON\_ONCE(!test\_bit(IO\_WQ\_BIT\_EXIT, &wq->state));++ io\_wq\_exit\_workers(wq);+ io\_wq\_destroy(wq);+}++struct online\_data {+ unsigned int cpu;+ bool online;+};++static bool io\_wq\_worker\_affinity(struct io\_worker \*worker, void \*data)+{+ struct online\_data \*od = data;++ if (od->online)+ cpumask\_set\_cpu(od->cpu, worker->wqe->cpu\_mask);+ else+ cpumask\_clear\_cpu(od->cpu, worker->wqe->cpu\_mask);+ return false;+}++static int \_\_io\_wq\_cpu\_online(struct io\_wq \*wq, unsigned int cpu, bool online)+{+ struct online\_data od = {+ .cpu = cpu,+ .online = online+ };+ int i;++ rcu\_read\_lock();+ for\_each\_node(i)+ io\_wq\_for\_each\_worker(wq->wqes[i], io\_wq\_worker\_affinity, &od);+ rcu\_read\_unlock();+ return 0;+}++static int io\_wq\_cpu\_online(unsigned int cpu, struct hlist\_node \*node)+{+ struct io\_wq \*wq = hlist\_entry\_safe(node, struct io\_wq, cpuhp\_node);++ return \_\_io\_wq\_cpu\_online(wq, cpu, true);+}++static int io\_wq\_cpu\_offline(unsigned int cpu, struct hlist\_node \*node)+{+ struct io\_wq \*wq = hlist\_entry\_safe(node, struct io\_wq, cpuhp\_node);++ return \_\_io\_wq\_cpu\_online(wq, cpu, false);+}++int io\_wq\_cpu\_affinity(struct io\_wq \*wq, cpumask\_var\_t mask)+{+ int i;++ rcu\_read\_lock();+ for\_each\_node(i) {+ struct io\_wqe \*wqe = wq->wqes[i];++ if (mask)+ cpumask\_copy(wqe->cpu\_mask, mask);+ else+ cpumask\_copy(wqe->cpu\_mask, cpumask\_of\_node(i));+ }+ rcu\_read\_unlock();+ return 0;+}++/\*+ \* Set max number of unbounded workers, returns old value. If new\_count is 0,+ \* then just return the old value.+ \*/+int io\_wq\_max\_workers(struct io\_wq \*wq, int \*new\_count)+{+ int prev[IO\_WQ\_ACCT\_NR];+ bool first\_node = true;+ int i, node;++ BUILD\_BUG\_ON((int) IO\_WQ\_ACCT\_BOUND != (int) IO\_WQ\_BOUND);+ BUILD\_BUG\_ON((int) IO\_WQ\_ACCT\_UNBOUND != (int) IO\_WQ\_UNBOUND);+ BUILD\_BUG\_ON((int) IO\_WQ\_ACCT\_NR != 2);++ for (i = 0; i < 2; i++) {+ if (new\_count[i] > task\_rlimit(current, RLIMIT\_NPROC))+ new\_count[i] = task\_rlimit(current, RLIMIT\_NPROC);+ }++ for (i = 0; i < IO\_WQ\_ACCT\_NR; i++)+ prev[i] = 0;++ rcu\_read\_lock();+ for\_each\_node(node) {+ struct io\_wqe \*wqe = wq->wqes[node];+ struct io\_wqe\_acct \*acct;++ raw\_spin\_lock(&wqe->lock);+ for (i = 0; i < IO\_WQ\_ACCT\_NR; i++) {+ acct = &wqe->acct[i];+ if (first\_node)+ prev[i] = max\_t(int, acct->max\_workers, prev[i]);+ if (new\_count[i])+ acct->max\_workers = new\_count[i];+ }+ raw\_spin\_unlock(&wqe->lock);+ first\_node = false;+ }+ rcu\_read\_unlock();++ for (i = 0; i < IO\_WQ\_ACCT\_NR; i++)+ new\_count[i] = prev[i];++ return 0;+}++static \_\_init int io\_wq\_init(void)+{+ int ret;++ ret = cpuhp\_setup\_state\_multi(CPUHP\_AP\_ONLINE\_DYN, "io-wq/online",+ io\_wq\_cpu\_online, io\_wq\_cpu\_offline);+ if (ret < 0)+ return ret;+ io\_wq\_online = ret;+ return 0;+}+subsys\_initcall(io\_wq\_init);diff --git a/fs/io-wq.h b/io\_uring/io-wq.hindex 75113bcd5889f4..bf5c4c53376057 100644--- a/[fs/io-wq.h](/pub/scm/linux/kernel/git/stable/linux.git/tree/fs/io-wq.h?id=ed3005032993da7a3fe2e6095436e0bc2e83d011)+++ b/[io\_uring/io-wq.h](/pub/scm/linux/kernel/git/stable/linux.git/tree/io_uring/io-wq.h?id=788d0824269bef539fe31a785b1517882eafed93)@@ -1,7 +1,7 @@ #ifndef INTERNAL\_IO\_WQ\_H #define INTERNAL\_IO\_WQ\_H -#include <linux/io\_uring.h>+#include <linux/refcount.h>  struct io\_wq; @@ -9,16 +9,8 @@ enum { IO\_WQ\_WORK\_CANCEL = 1, IO\_WQ\_WORK\_HASHED = 2, IO\_WQ\_WORK\_UNBOUND = 4,- IO\_WQ\_WORK\_NO\_CANCEL = 8, IO\_WQ\_WORK\_CONCURRENT = 16, - IO\_WQ\_WORK\_FILES = 32,- IO\_WQ\_WORK\_FS = 64,- IO\_WQ\_WORK\_MM = 128,- IO\_WQ\_WORK\_CREDS = 256,- IO\_WQ\_WORK\_BLKCG = 512,- IO\_WQ\_WORK\_FSIZE = 1024,- IO\_WQ\_HASH\_SHIFT = 24, /\* upper 8 bits are used for hash key \*/ }; @@ -52,6 +44,7 @@ static inline void wq\_list\_add\_after(struct io\_wq\_work\_node \*node, static inline void wq\_list\_add\_tail(struct io\_wq\_work\_node \*node, struct io\_wq\_work\_list \*list) {+ node->next = NULL; if (!list->first) { list->last = node; WRITE\_ONCE(list->first, node);@@ -59,7 +52,6 @@ static inline void wq\_list\_add\_tail(struct io\_wq\_work\_node \*node, list->last->next = node; list->last = node; }- node->next = NULL; }  static inline void wq\_list\_cut(struct io\_wq\_work\_list \*list,@@ -95,7 +87,6 @@ static inline void wq\_list\_del(struct io\_wq\_work\_list \*list,  struct io\_wq\_work { struct io\_wq\_work\_node list;- struct io\_identity \*identity; unsigned flags; }; @@ -107,37 +98,48 @@ static inline struct io\_wq\_work \*wq\_next\_work(struct io\_wq\_work \*work) return container\_of(work->list.next, struct io\_wq\_work, list); } -typedef void (free\_work\_fn)(struct io\_wq\_work \*);-typedef struct io\_wq\_work \*(io\_wq\_work\_fn)(struct io\_wq\_work \*);+typedef struct io\_wq\_work \*(free\_work\_fn)(struct io\_wq\_work \*);+typedef void (io\_wq\_work\_fn)(struct io\_wq\_work \*); -struct io\_wq\_data {- struct user\_struct \*user;+struct io\_wq\_hash {+ refcount\_t refs;+ unsigned long map;+ struct wait\_queue\_head wait;+};++static inline void io\_wq\_put\_hash(struct io\_wq\_hash \*hash)+{+ if (refcount\_dec\_and\_test(&hash->refs))+ kfree(hash);+} +struct io\_wq\_data {+ struct io\_wq\_hash \*hash;+ struct task\_struct \*task; io\_wq\_work\_fn \*do\_work; free\_work\_fn \*free\_work; };  struct io\_wq \*io\_wq\_create(unsigned bounded, struct io\_wq\_data \*data);-bool io\_wq\_get(struct io\_wq \*wq, struct io\_wq\_data \*data);-void io\_wq\_destroy(struct io\_wq \*wq);+void io\_wq\_exit\_start(struct io\_wq \*wq);+void io\_wq\_put\_and\_exit(struct io\_wq \*wq);  void io\_wq\_enqueue(struct io\_wq \*wq, struct io\_wq\_work \*work); void io\_wq\_hash\_work(struct io\_wq\_work \*work, void \*val); +int io\_wq\_cpu\_affinity(struct io\_wq \*wq, cpumask\_var\_t mask);+int io\_wq\_max\_workers(struct io\_wq \*wq, int \*new\_count);+ static inline bool io\_wq\_is\_hashed(struct io\_wq\_work \*work) { return work->flags & IO\_WQ\_WORK\_HASHED; } -void io\_wq\_cancel\_all(struct io\_wq \*wq);- typedef bool (work\_cancel\_fn)(struct io\_wq\_work \*, void \*);  enum io\_wq\_cancel io\_wq\_cancel\_cb(struct io\_wq \*wq, work\_cancel\_fn \*cancel, void \*data, bool cancel\_all); -struct task\_struct \*io\_wq\_get\_task(struct io\_wq \*wq);- #if defined(CONFIG\_IO\_WQ) extern void io\_wq\_worker\_sleeping(struct task\_struct \*); extern void io\_wq\_worker\_running(struct task\_struct \*);@@ -152,6 +154,7 @@ static inline void io\_wq\_worker\_running(struct task\_struct \*tsk)  static inline bool io\_wq\_current\_is\_worker(void) {- return in\_task() && (current->flags & PF\_IO\_WORKER);+ return in\_task() && (current->flags & PF\_IO\_WORKER) &&+ current->pf\_io\_worker; } #endifdiff --git a/fs/io\_uring.c b/io\_uring/io\_uring.cindex 1d817374131097..473dbd1830a3b6 100644--- a/[fs/io\_uring.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/fs/io_uring.c?id=ed3005032993da7a3fe2e6095436e0bc2e83d011)+++ b/[io\_uring/io\_uring.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/io_uring/io_uring.c?id=788d0824269bef539fe31a785b1517882eafed93)@@ -11,7 +11,7 @@ \* before writing the tail (using smp\_load\_acquire to read the tail will \* do). It also needs a smp\_mb() before updating CQ head (ordering the \* entry load(s) with the head store), pairing with an implicit barrier- \* through a control-dependency in io\_get\_cqring (smp\_store\_release to+ \* through a control-dependency in io\_get\_cqe (smp\_store\_release to \* store head will do). Failure to do so could lead to reading invalid \* CQ entries. \*@@ -57,7 +57,6 @@ #include <linux/mman.h> #include <linux/percpu.h> #include <linux/slab.h>-#include <linux/kthread.h> #include <linux/blkdev.h> #include <linux/bvec.h> #include <linux/net.h>@@ -75,35 +74,43 @@ #include <linux/fsnotify.h> #include <linux/fadvise.h> #include <linux/eventpoll.h>-#include <linux/fs\_struct.h> #include <linux/splice.h> #include <linux/task\_work.h> #include <linux/pagemap.h> #include <linux/io\_uring.h>-#include <linux/blk-cgroup.h>-#include <linux/audit.h>+#include <linux/tracehook.h>  #define CREATE\_TRACE\_POINTS #include <trace/events/io\_uring.h>  #include <uapi/linux/io\_uring.h> -#include "internal.h"+#include "../fs/internal.h" #include "io-wq.h"  #define IORING\_MAX\_ENTRIES 32768 #define IORING\_MAX\_CQ\_ENTRIES (2 \* IORING\_MAX\_ENTRIES)+#define IORING\_SQPOLL\_CAP\_ENTRIES\_VALUE 8 -/\*- \* Shift of 9 is 512 entries, or exactly one page on 64-bit archs- \*/-#define IORING\_FILE\_TABLE\_SHIFT 9-#define IORING\_MAX\_FILES\_TABLE (1U << IORING\_FILE\_TABLE\_SHIFT)-#define IORING\_FILE\_TABLE\_MASK (IORING\_MAX\_FILES\_TABLE - 1)-#define IORING\_MAX\_FIXED\_FILES (64 \* IORING\_MAX\_FILES\_TABLE)+/\* only define max \*/+#define IORING\_MAX\_FIXED\_FILES (1U << 15) #define IORING\_MAX\_RESTRICTIONS (IORING\_RESTRICTION\_LAST + \ IORING\_REGISTER\_LAST + IORING\_OP\_LAST) +#define IO\_RSRC\_TAG\_TABLE\_SHIFT (PAGE\_SHIFT - 3)+#define IO\_RSRC\_TAG\_TABLE\_MAX (1U << IO\_RSRC\_TAG\_TABLE\_SHIFT)+#define IO\_RSRC\_TAG\_TABLE\_MASK (IO\_RSRC\_TAG\_TABLE\_MAX - 1)++#define IORING\_MAX\_REG\_BUFFERS (1U << 14)++#define SQE\_VALID\_FLAGS (IOSQE\_FIXED\_FILE|IOSQE\_IO\_DRAIN|IOSQE\_IO\_LINK| \+ IOSQE\_IO\_HARDLINK | IOSQE\_ASYNC | \+ IOSQE\_BUFFER\_SELECT)+#define IO\_REQ\_CLEAN\_FLAGS (REQ\_F\_BUFFER\_SELECTED | REQ\_F\_NEED\_CLEANUP | \+ REQ\_F\_POLLED | REQ\_F\_INFLIGHT | REQ\_F\_CREDS)++#define IO\_TCTX\_REFS\_CACHE\_NR (1U << 10)+ struct io\_uring { u32 head \_\_\_\_cacheline\_aligned\_in\_smp; u32 tail \_\_\_\_cacheline\_aligned\_in\_smp;@@ -162,7 +169,7 @@ struct io\_rings { \* Written by the application, shouldn't be modified by the \* kernel. \*/- u32 cq\_flags;+ u32 cq\_flags; /\* \* Number of completion events lost because the queue was full; \* this should be avoided by the application by making sure@@ -187,36 +194,64 @@ struct io\_rings { struct io\_uring\_cqe cqes[] \_\_\_\_cacheline\_aligned\_in\_smp; }; +enum io\_uring\_cmd\_flags {+ IO\_URING\_F\_NONBLOCK = 1,+ IO\_URING\_F\_COMPLETE\_DEFER = 2,+};+ struct io\_mapped\_ubuf { u64 ubuf;- size\_t len;- struct bio\_vec \*bvec;+ u64 ubuf\_end; unsigned int nr\_bvecs; unsigned long acct\_pages;+ struct bio\_vec bvec[]; }; -struct fixed\_file\_table {- struct file \*\*files;+struct io\_ring\_ctx;++struct io\_overflow\_cqe {+ struct io\_uring\_cqe cqe;+ struct list\_head list; }; -struct fixed\_file\_ref\_node {+struct io\_fixed\_file {+ /\* file \* with additional FFS\_\* flags \*/+ unsigned long file\_ptr;+};++struct io\_rsrc\_put {+ struct list\_head list;+ u64 tag;+ union {+ void \*rsrc;+ struct file \*file;+ struct io\_mapped\_ubuf \*buf;+ };+};++struct io\_file\_table {+ struct io\_fixed\_file \*files;+};++struct io\_rsrc\_node { struct percpu\_ref refs; struct list\_head node;- struct list\_head file\_list;- struct fixed\_file\_data \*file\_data;+ struct list\_head rsrc\_list;+ struct io\_rsrc\_data \*rsrc\_data; struct llist\_node llist; bool done; }; -struct fixed\_file\_data {- struct fixed\_file\_table \*table;+typedef void (rsrc\_put\_fn)(struct io\_ring\_ctx \*ctx, struct io\_rsrc\_put \*prsrc);++struct io\_rsrc\_data { struct io\_ring\_ctx \*ctx; - struct fixed\_file\_ref\_node \*node;- struct percpu\_ref refs;+ u64 \*\*tags;+ unsigned int nr;+ rsrc\_put\_fn \*do\_put;+ atomic\_t refs; struct completion done;- struct list\_head ref\_list;- spinlock\_t lock; bool quiesce; }; @@ -235,33 +270,81 @@ struct io\_restriction { bool registered; }; +enum {+ IO\_SQ\_THREAD\_SHOULD\_STOP = 0,+ IO\_SQ\_THREAD\_SHOULD\_PARK,+};+ struct io\_sq\_data { refcount\_t refs;+ atomic\_t park\_pending; struct mutex lock;  /\* ctx's that are using this sqd \*/ struct list\_head ctx\_list;- struct list\_head ctx\_new\_list;- struct mutex ctx\_lock;  struct task\_struct \*thread; struct wait\_queue\_head wait;++ unsigned sq\_thread\_idle;+ int sq\_cpu;+ pid\_t task\_pid;+ pid\_t task\_tgid;++ unsigned long state;+ struct completion exited;+};++#define IO\_COMPL\_BATCH 32+#define IO\_REQ\_CACHE\_SIZE 32+#define IO\_REQ\_ALLOC\_BATCH 8++struct io\_submit\_link {+ struct io\_kiocb \*head;+ struct io\_kiocb \*last;+};++struct io\_submit\_state {+ struct blk\_plug plug;+ struct io\_submit\_link link;++ /\*+ \* io\_kiocb alloc cache+ \*/+ void \*reqs[IO\_REQ\_CACHE\_SIZE];+ unsigned int free\_reqs;++ bool plug\_started;++ /\*+ \* Batch completion logic+ \*/+ struct io\_kiocb \*compl\_reqs[IO\_COMPL\_BATCH];+ unsigned int compl\_nr;+ /\* inline/task\_work completion list, under ->uring\_lock \*/+ struct list\_head free\_list;++ unsigned int ios\_left; };  struct io\_ring\_ctx {+ /\* const or read-mostly hot data \*/ struct { struct percpu\_ref refs;- } \_\_\_\_cacheline\_aligned\_in\_smp; - struct {+ struct io\_rings \*rings; unsigned int flags; unsigned int compat: 1;- unsigned int limit\_mem: 1;- unsigned int cq\_overflow\_flushed: 1; unsigned int drain\_next: 1; unsigned int eventfd\_async: 1; unsigned int restricted: 1;- unsigned int sqo\_dead: 1;+ unsigned int off\_timeout\_used: 1;+ unsigned int drain\_active: 1;+ } \_\_\_\_cacheline\_aligned\_in\_smp;++ /\* submission data \*/+ struct {+ struct mutex uring\_lock;  /\* \* Ring buffer of indices into array of io\_uring\_sqe, which is@@ -275,101 +358,59 @@ struct io\_ring\_ctx { \* array. \*/ u32 \*sq\_array;+ struct io\_uring\_sqe \*sq\_sqes; unsigned cached\_sq\_head; unsigned sq\_entries;- unsigned sq\_mask;- unsigned sq\_thread\_idle;- unsigned cached\_sq\_dropped;- unsigned cached\_cq\_overflow;- unsigned long sq\_check\_overflow;- struct list\_head defer\_list;++ /\*+ \* Fixed resources fast path, should be accessed only under+ \* uring\_lock, and updated through io\_uring\_register(2)+ \*/+ struct io\_rsrc\_node \*rsrc\_node;+ struct io\_file\_table file\_table;+ unsigned nr\_user\_files;+ unsigned nr\_user\_bufs;+ struct io\_mapped\_ubuf \*\*user\_bufs;++ struct io\_submit\_state submit\_state; struct list\_head timeout\_list;+ struct list\_head ltimeout\_list; struct list\_head cq\_overflow\_list;-- struct io\_uring\_sqe \*sq\_sqes;+ struct xarray io\_buffers;+ struct xarray personalities;+ u32 pers\_next;+ unsigned sq\_thread\_idle; } \_\_\_\_cacheline\_aligned\_in\_smp; - struct io\_rings \*rings;-- /\* IO offload \*/- struct io\_wq \*io\_wq;-- /\*- \* For SQPOLL usage - we hold a reference to the parent task, so we- \* have access to the ->files- \*/- struct task\_struct \*sqo\_task;-- /\* Only used for accounting purposes \*/- struct mm\_struct \*mm\_account;--#ifdef CONFIG\_BLK\_CGROUP- struct cgroup\_subsys\_state \*sqo\_blkcg\_css;-#endif+ /\* IRQ completion list, under ->completion\_lock \*/+ struct list\_head locked\_free\_list;+ unsigned int locked\_free\_nr; + const struct cred \*sq\_creds; /\* cred used for \_\_io\_sq\_thread() \*/ struct io\_sq\_data \*sq\_data; /\* if using sq thread polling \*/  struct wait\_queue\_head sqo\_sq\_wait;- struct wait\_queue\_entry sqo\_wait\_entry; struct list\_head sqd\_list; - /\*- \* If used, fixed file set. Writers must ensure that ->refs is dead,- \* readers must ensure that ->refs is alive as long as the file\* is- \* used. Only updated through io\_uring\_register(2).- \*/- struct fixed\_file\_data \*file\_data;- unsigned nr\_user\_files;-- /\* if used, fixed mapped user buffers \*/- unsigned nr\_user\_bufs;- struct io\_mapped\_ubuf \*user\_bufs;-- struct user\_struct \*user;-- const struct cred \*creds;--#ifdef CONFIG\_AUDIT- kuid\_t loginuid;- unsigned int sessionid;-#endif-- struct completion ref\_comp;- struct completion sq\_thread\_comp;-- /\* if all else fails... \*/- struct io\_kiocb \*fallback\_req;--#if defined(CONFIG\_UNIX)- struct socket \*ring\_sock;-#endif-- struct xarray io\_buffers;-- struct xarray personalities;- u32 pers\_next;+ unsigned long check\_cq\_overflow;  struct { unsigned cached\_cq\_tail; unsigned cq\_entries;- unsigned cq\_mask;+ struct eventfd\_ctx \*cq\_ev\_fd;+ struct wait\_queue\_head poll\_wait;+ struct wait\_queue\_head cq\_wait;+ unsigned cq\_extra; atomic\_t cq\_timeouts; unsigned cq\_last\_tm\_flush;- unsigned long cq\_check\_overflow;- struct wait\_queue\_head cq\_wait;- struct fasync\_struct \*cq\_fasync;- struct eventfd\_ctx \*cq\_ev\_fd;- } \_\_\_\_cacheline\_aligned\_in\_smp;-- struct {- struct mutex uring\_lock;- wait\_queue\_head\_t wait; } \_\_\_\_cacheline\_aligned\_in\_smp;  struct { spinlock\_t completion\_lock; + spinlock\_t timeout\_lock;+ /\* \* ->iopoll\_list is protected by the ctx->uring\_lock for \* io\_uring instances that don't use IORING\_SETUP\_SQPOLL.@@ -379,17 +420,62 @@ struct io\_ring\_ctx { struct list\_head iopoll\_list; struct hlist\_head \*cancel\_hash; unsigned cancel\_hash\_bits;- bool poll\_multi\_file;-- spinlock\_t inflight\_lock;- struct list\_head inflight\_list;+ bool poll\_multi\_queue; } \_\_\_\_cacheline\_aligned\_in\_smp; - struct delayed\_work file\_put\_work;- struct llist\_head file\_put\_llist;-- struct work\_struct exit\_work; struct io\_restriction restrictions;++ /\* slow path rsrc auxilary data, used by update/register \*/+ struct {+ struct io\_rsrc\_node \*rsrc\_backup\_node;+ struct io\_mapped\_ubuf \*dummy\_ubuf;+ struct io\_rsrc\_data \*file\_data;+ struct io\_rsrc\_data \*buf\_data;++ struct delayed\_work rsrc\_put\_work;+ struct llist\_head rsrc\_put\_llist;+ struct list\_head rsrc\_ref\_list;+ spinlock\_t rsrc\_ref\_lock;+ };++ /\* Keep this last, we don't need it for the fast path \*/+ struct {+ #if defined(CONFIG\_UNIX)+ struct socket \*ring\_sock;+ #endif+ /\* hashed buffered write serialization \*/+ struct io\_wq\_hash \*hash\_map;++ /\* Only used for accounting purposes \*/+ struct user\_struct \*user;+ struct mm\_struct \*mm\_account;++ /\* ctx exit and cancelation \*/+ struct llist\_head fallback\_llist;+ struct delayed\_work fallback\_work;+ struct work\_struct exit\_work;+ struct list\_head tctx\_list;+ struct completion ref\_comp;+ u32 iowq\_limits[2];+ bool iowq\_limits\_set;+ };+};++struct io\_uring\_task {+ /\* submission side \*/+ int cached\_refs;+ struct xarray xa;+ struct wait\_queue\_head wait;+ const struct io\_ring\_ctx \*last;+ struct io\_wq \*io\_wq;+ struct percpu\_counter inflight;+ atomic\_t inflight\_tracked;+ atomic\_t in\_idle;++ spinlock\_t task\_lock;+ struct io\_wq\_work\_list task\_list;+ struct callback\_head task\_work;+ bool task\_running; };  /\*@@ -398,20 +484,24 @@ struct io\_ring\_ctx { \*/ struct io\_poll\_iocb { struct file \*file;- union {- struct wait\_queue\_head \*head;- u64 addr;- };+ struct wait\_queue\_head \*head; \_\_poll\_t events;- bool done;- bool canceled; struct wait\_queue\_entry wait; }; +struct io\_poll\_update {+ struct file \*file;+ u64 old\_user\_data;+ u64 new\_user\_data;+ \_\_poll\_t events;+ bool update\_events;+ bool update\_user\_data;+};+ struct io\_close { struct file \*file;- struct file \*put\_file; int fd;+ u32 file\_slot; };  struct io\_timeout\_data {@@ -419,6 +509,7 @@ struct io\_timeout\_data { struct hrtimer timer; struct timespec64 ts; enum hrtimer\_mode mode;+ u32 flags; };  struct io\_accept {@@ -426,6 +517,7 @@ struct io\_accept { struct sockaddr \_\_user \*addr; int \_\_user \*addr\_len; int flags;+ u32 file\_slot; unsigned long nofile; }; @@ -447,11 +539,20 @@ struct io\_timeout { u32 off; u32 target\_seq; struct list\_head list;+ /\* head of the link, used by linked timeouts only \*/+ struct io\_kiocb \*head;+ /\* for linked completions \*/+ struct io\_kiocb \*prev; };  struct io\_timeout\_rem { struct file \*file; u64 addr;++ /\* timeout update \*/+ struct timespec64 ts;+ u32 flags;+ bool ltimeout; };  struct io\_rw {@@ -470,8 +571,9 @@ struct io\_connect { struct io\_sr\_msg { struct file \*file; union {- struct user\_msghdr \_\_user \*umsg;- void \_\_user \*buf;+ struct compat\_msghdr \_\_user \*umsg\_compat;+ struct user\_msghdr \_\_user \*umsg;+ void \_\_user \*buf; }; int msg\_flags; int bgid;@@ -482,13 +584,13 @@ struct io\_sr\_msg { struct io\_open { struct file \*file; int dfd;- bool ignore\_nonblock;+ u32 file\_slot; struct filename \*filename; struct open\_how how; unsigned long nofile; }; -struct io\_files\_update {+struct io\_rsrc\_update { struct file \*file; u64 arg; u32 nr\_args;@@ -519,10 +621,10 @@ struct io\_epoll {  struct io\_splice { struct file \*file\_out;- struct file \*file\_in; loff\_t off\_out; loff\_t off\_in; u64 len;+ int splice\_fd\_in; unsigned int flags; }; @@ -544,9 +646,52 @@ struct io\_statx { struct statx \_\_user \*buffer; }; +struct io\_shutdown {+ struct file \*file;+ int how;+};++struct io\_rename {+ struct file \*file;+ int old\_dfd;+ int new\_dfd;+ struct filename \*oldpath;+ struct filename \*newpath;+ int flags;+};++struct io\_unlink {+ struct file \*file;+ int dfd;+ int flags;+ struct filename \*filename;+};++struct io\_mkdir {+ struct file \*file;+ int dfd;+ umode\_t mode;+ struct filename \*filename;+};++struct io\_symlink {+ struct file \*file;+ int new\_dfd;+ struct filename \*oldpath;+ struct filename \*newpath;+};++struct io\_hardlink {+ struct file \*file;+ int old\_dfd;+ int new\_dfd;+ struct filename \*oldpath;+ struct filename \*newpath;+ int flags;+};+ struct io\_completion { struct file \*file;- struct list\_head list; u32 cflags; }; @@ -556,7 +701,8 @@ struct io\_async\_connect {  struct io\_async\_msghdr { struct iovec fast\_iov[UIO\_FASTIOV];- struct iovec \*iov;+ /\* points to an allocated iov, if NULL we use fast\_iov instead \*/+ struct iovec \*free\_iov; struct sockaddr \_\_user \*uaddr; struct msghdr msg; struct sockaddr\_storage addr;@@ -566,6 +712,7 @@ struct io\_async\_rw { struct iovec fast\_iov[UIO\_FASTIOV]; const struct iovec \*free\_iovec; struct iov\_iter iter;+ struct iov\_iter\_state iter\_state; size\_t bytes\_done; struct wait\_page\_queue wpq; };@@ -578,19 +725,24 @@ enum { REQ\_F\_FORCE\_ASYNC\_BIT = IOSQE\_ASYNC\_BIT, REQ\_F\_BUFFER\_SELECT\_BIT = IOSQE\_BUFFER\_SELECT\_BIT, - REQ\_F\_LINK\_HEAD\_BIT,- REQ\_F\_FAIL\_LINK\_BIT,+ /\* first byte is taken by user flags, shift it to not overlap \*/+ REQ\_F\_FAIL\_BIT = 8, REQ\_F\_INFLIGHT\_BIT, REQ\_F\_CUR\_POS\_BIT, REQ\_F\_NOWAIT\_BIT, REQ\_F\_LINK\_TIMEOUT\_BIT,- REQ\_F\_ISREG\_BIT, REQ\_F\_NEED\_CLEANUP\_BIT, REQ\_F\_POLLED\_BIT, REQ\_F\_BUFFER\_SELECTED\_BIT,- REQ\_F\_NO\_FILE\_TABLE\_BIT,- REQ\_F\_WORK\_INITIALIZED\_BIT,- REQ\_F\_LTIMEOUT\_ACTIVE\_BIT,+ REQ\_F\_COMPLETE\_INLINE\_BIT,+ REQ\_F\_REISSUE\_BIT,+ REQ\_F\_CREDS\_BIT,+ REQ\_F\_REFCOUNT\_BIT,+ REQ\_F\_ARM\_LTIMEOUT\_BIT,+ /\* keep async read/write and isreg together and in order \*/+ REQ\_F\_NOWAIT\_READ\_BIT,+ REQ\_F\_NOWAIT\_WRITE\_BIT,+ REQ\_F\_ISREG\_BIT,  /\* not a real bit, just to check we're not overflowing the space \*/ \_\_REQ\_F\_LAST\_BIT,@@ -610,11 +762,9 @@ enum { /\* IOSQE\_BUFFER\_SELECT \*/ REQ\_F\_BUFFER\_SELECT = BIT(REQ\_F\_BUFFER\_SELECT\_BIT), - /\* head of a link \*/- REQ\_F\_LINK\_HEAD = BIT(REQ\_F\_LINK\_HEAD\_BIT), /\* fail rest of links \*/- REQ\_F\_FAIL\_LINK = BIT(REQ\_F\_FAIL\_LINK\_BIT),- /\* on inflight list \*/+ REQ\_F\_FAIL = BIT(REQ\_F\_FAIL\_BIT),+ /\* on inflight list, should be cancelled and waited on exit reliably \*/ REQ\_F\_INFLIGHT = BIT(REQ\_F\_INFLIGHT\_BIT), /\* read/write uses file position \*/ REQ\_F\_CUR\_POS = BIT(REQ\_F\_CUR\_POS\_BIT),@@ -622,20 +772,28 @@ enum { REQ\_F\_NOWAIT = BIT(REQ\_F\_NOWAIT\_BIT), /\* has or had linked timeout \*/ REQ\_F\_LINK\_TIMEOUT = BIT(REQ\_F\_LINK\_TIMEOUT\_BIT),- /\* regular file \*/- REQ\_F\_ISREG = BIT(REQ\_F\_ISREG\_BIT), /\* needs cleanup \*/ REQ\_F\_NEED\_CLEANUP = BIT(REQ\_F\_NEED\_CLEANUP\_BIT), /\* already went through poll handler \*/ REQ\_F\_POLLED = BIT(REQ\_F\_POLLED\_BIT), /\* buffer already selected \*/ REQ\_F\_BUFFER\_SELECTED = BIT(REQ\_F\_BUFFER\_SELECTED\_BIT),- /\* doesn't need file table for this request \*/- REQ\_F\_NO\_FILE\_TABLE = BIT(REQ\_F\_NO\_FILE\_TABLE\_BIT),- /\* io\_wq\_work is initialized \*/- REQ\_F\_WORK\_INITIALIZED = BIT(REQ\_F\_WORK\_INITIALIZED\_BIT),- /\* linked timeout is active, i.e. prepared by link's head \*/- REQ\_F\_LTIMEOUT\_ACTIVE = BIT(REQ\_F\_LTIMEOUT\_ACTIVE\_BIT),+ /\* completion is deferred through io\_comp\_state \*/+ REQ\_F\_COMPLETE\_INLINE = BIT(REQ\_F\_COMPLETE\_INLINE\_BIT),+ /\* caller should reissue async \*/+ REQ\_F\_REISSUE = BIT(REQ\_F\_REISSUE\_BIT),+ /\* supports async reads \*/+ REQ\_F\_NOWAIT\_READ = BIT(REQ\_F\_NOWAIT\_READ\_BIT),+ /\* supports async writes \*/+ REQ\_F\_NOWAIT\_WRITE = BIT(REQ\_F\_NOWAIT\_WRITE\_BIT),+ /\* regular file \*/+ REQ\_F\_ISREG = BIT(REQ\_F\_ISREG\_BIT),+ /\* has creds assigned \*/+ REQ\_F\_CREDS = BIT(REQ\_F\_CREDS\_BIT),+ /\* skip refcounting if not set \*/+ REQ\_F\_REFCOUNT = BIT(REQ\_F\_REFCOUNT\_BIT),+ /\* there is a linked timeout that has to be armed \*/+ REQ\_F\_ARM\_LTIMEOUT = BIT(REQ\_F\_ARM\_LTIMEOUT\_BIT), };  struct async\_poll {@@ -643,6 +801,21 @@ struct async\_poll { struct io\_poll\_iocb \*double\_poll; }; +typedef void (\*io\_req\_tw\_func\_t)(struct io\_kiocb \*req, bool \*locked);++struct io\_task\_work {+ union {+ struct io\_wq\_work\_node node;+ struct llist\_node fallback\_node;+ };+ io\_req\_tw\_func\_t func;+};++enum {+ IORING\_RSRC\_FILE = 0,+ IORING\_RSRC\_BUFFER = 1,+};+ /\* \* NOTE! Each of the iocb union members has the file pointer \* as the first entry in their struct definition. So you can@@ -654,6 +827,7 @@ struct io\_kiocb { struct file \*file; struct io\_rw rw; struct io\_poll\_iocb poll;+ struct io\_poll\_update poll\_update; struct io\_accept accept; struct io\_sync sync; struct io\_cancel cancel;@@ -663,13 +837,19 @@ struct io\_kiocb { struct io\_sr\_msg sr\_msg; struct io\_open open; struct io\_close close;- struct io\_files\_update files\_update;+ struct io\_rsrc\_update rsrc\_update; struct io\_fadvise fadvise; struct io\_madvise madvise; struct io\_epoll epoll; struct io\_splice splice; struct io\_provide\_buf pbuf; struct io\_statx statx;+ struct io\_shutdown shutdown;+ struct io\_rename rename;+ struct io\_unlink unlink;+ struct io\_mkdir mkdir;+ struct io\_symlink symlink;+ struct io\_hardlink hardlink; /\* use only after cleaning per-op data, see io\_clean\_op() \*/ struct io\_completion compl; };@@ -685,70 +865,44 @@ struct io\_kiocb {  struct io\_ring\_ctx \*ctx; unsigned int flags;- refcount\_t refs;+ atomic\_t refs; struct task\_struct \*task; u64 user\_data; - struct list\_head link\_list;+ struct io\_kiocb \*link;+ struct percpu\_ref \*fixed\_rsrc\_refs; - /\*- \* 1. used with ctx->iopoll\_list with reads/writes- \* 2. to track reqs with ->files (see io\_op\_def::file\_table)- \*/+ /\* used with ctx->iopoll\_list with reads/writes \*/ struct list\_head inflight\_entry;-- struct list\_head iopoll\_entry;-- struct percpu\_ref \*fixed\_file\_refs;- struct callback\_head task\_work;+ struct io\_task\_work io\_task\_work; /\* for polled requests, i.e. IORING\_OP\_POLL\_ADD and async armed poll \*/ struct hlist\_node hash\_node; struct async\_poll \*apoll; struct io\_wq\_work work;-};+ const struct cred \*creds; -struct io\_defer\_entry {- struct list\_head list;- struct io\_kiocb \*req;- u32 seq;+ /\* store used ubuf, so we can prevent reloading \*/+ struct io\_mapped\_ubuf \*imu;+ /\* stores selected buf, valid IFF REQ\_F\_BUFFER\_SELECTED is set \*/+ struct io\_buffer \*kbuf;+ atomic\_t poll\_refs; }; -#define IO\_IOPOLL\_BATCH 8--struct io\_comp\_state {- unsigned int nr;- struct list\_head list;+struct io\_tctx\_node {+ struct list\_head ctx\_node;+ struct task\_struct \*task; struct io\_ring\_ctx \*ctx; }; -struct io\_submit\_state {- struct blk\_plug plug;-- /\*- \* io\_kiocb alloc cache- \*/- void \*reqs[IO\_IOPOLL\_BATCH];- unsigned int free\_reqs;-- /\*- \* Batch completion logic- \*/- struct io\_comp\_state comp;-- /\*- \* File reference cache- \*/- struct file \*file;- unsigned int fd;- unsigned int has\_refs;- unsigned int ios\_left;+struct io\_defer\_entry {+ struct list\_head list;+ struct io\_kiocb \*req;+ u32 seq; };  struct io\_op\_def { /\* needs req->file assigned \*/ unsigned needs\_file : 1;- /\* don't fail if file grab fails \*/- unsigned needs\_file\_no\_error : 1; /\* hash wq insertion if file is a regular file \*/ unsigned hash\_reg\_file : 1; /\* unbound wq insertion if file is a non-regular file \*/@@ -760,11 +914,12 @@ struct io\_op\_def { unsigned pollout : 1; /\* op supports buffer selection \*/ unsigned buffer\_select : 1;- /\* must always have async data allocated \*/- unsigned needs\_async\_data : 1;+ /\* do prep async if is going to be punted \*/+ unsigned needs\_async\_setup : 1;+ /\* should block plug \*/+ unsigned plug : 1; /\* size of async data needed, if any \*/ unsigned short async\_size;- unsigned work\_flags; };  static const struct io\_op\_def io\_op\_defs[] = {@@ -774,41 +929,36 @@ static const struct io\_op\_def io\_op\_defs[] = { .unbound\_nonreg\_file = 1, .pollin = 1, .buffer\_select = 1,- .needs\_async\_data = 1,+ .needs\_async\_setup = 1,+ .plug = 1, .async\_size = sizeof(struct io\_async\_rw),- .work\_flags = IO\_WQ\_WORK\_MM | IO\_WQ\_WORK\_BLKCG |- IO\_WQ\_WORK\_FILES, }, [IORING\_OP\_WRITEV] = { .needs\_file = 1, .hash\_reg\_file = 1, .unbound\_nonreg\_file = 1, .pollout = 1,- .needs\_async\_data = 1,+ .needs\_async\_setup = 1,+ .plug = 1, .async\_size = sizeof(struct io\_async\_rw),- .work\_flags = IO\_WQ\_WORK\_MM | IO\_WQ\_WORK\_BLKCG |- IO\_WQ\_WORK\_FSIZE | IO\_WQ\_WORK\_FILES, }, [IORING\_OP\_FSYNC] = { .needs\_file = 1,- .work\_flags = IO\_WQ\_WORK\_BLKCG, }, [IORING\_OP\_READ\_FIXED] = { .needs\_file = 1, .unbound\_nonreg\_file = 1, .pollin = 1,+ .plug = 1, .async\_size = sizeof(struct io\_async\_rw),- .work\_flags = IO\_WQ\_WORK\_BLKCG | IO\_WQ\_WORK\_MM |- IO\_WQ\_WORK\_FILES, }, [IORING\_OP\_WRITE\_FIXED] = { .needs\_file = 1, .hash\_reg\_file = 1, .unbound\_nonreg\_file = 1, .pollout = 1,+ .plug = 1, .async\_size = sizeof(struct io\_async\_rw),- .work\_flags = IO\_WQ\_WORK\_BLKCG | IO\_WQ\_WORK\_FSIZE |- IO\_WQ\_WORK\_MM | IO\_WQ\_WORK\_FILES, }, [IORING\_OP\_POLL\_ADD] = { .needs\_file = 1,@@ -817,126 +967,91 @@ static const struct io\_op\_def io\_op\_defs[] = { [IORING\_OP\_POLL\_REMOVE] = {}, [IORING\_OP\_SYNC\_FILE\_RANGE] = { .needs\_file = 1,- .work\_flags = IO\_WQ\_WORK\_BLKCG, }, [IORING\_OP\_SENDMSG] = { .needs\_file = 1, .unbound\_nonreg\_file = 1, .pollout = 1,- .needs\_async\_data = 1,+ .needs\_async\_setup = 1, .async\_size = sizeof(struct io\_async\_msghdr),- .work\_flags = IO\_WQ\_WORK\_MM | IO\_WQ\_WORK\_BLKCG |- IO\_WQ\_WORK\_FS, }, [IORING\_OP\_RECVMSG] = { .needs\_file = 1, .unbound\_nonreg\_file = 1, .pollin = 1, .buffer\_select = 1,- .needs\_async\_data = 1,+ .needs\_async\_setup = 1, .async\_size = sizeof(struct io\_async\_msghdr),- .work\_flags = IO\_WQ\_WORK\_MM | IO\_WQ\_WORK\_BLKCG |- IO\_WQ\_WORK\_FS, }, [IORING\_OP\_TIMEOUT] = {- .needs\_async\_data = 1, .async\_size = sizeof(struct io\_timeout\_data),- .work\_flags = IO\_WQ\_WORK\_MM, },- [IORING\_OP\_TIMEOUT\_REMOVE] = {},+ [IORING\_OP\_TIMEOUT\_REMOVE] = {+ /\* used by timeout updates' prep() \*/+ }, [IORING\_OP\_ACCEPT] = { .needs\_file = 1, .unbound\_nonreg\_file = 1, .pollin = 1,- .work\_flags = IO\_WQ\_WORK\_MM | IO\_WQ\_WORK\_FILES, }, [IORING\_OP\_ASYNC\_CANCEL] = {}, [IORING\_OP\_LINK\_TIMEOUT] = {- .needs\_async\_data = 1, .async\_size = sizeof(struct io\_timeout\_data),- .work\_flags = IO\_WQ\_WORK\_MM, }, [IORING\_OP\_CONNECT] = { .needs\_file = 1, .unbound\_nonreg\_file = 1, .pollout = 1,- .needs\_async\_data = 1,+ .needs\_async\_setup = 1, .async\_size = sizeof(struct io\_async\_connect),- .work\_flags = IO\_WQ\_WORK\_MM | IO\_WQ\_WORK\_FS, }, [IORING\_OP\_FALLOCATE] = { .needs\_file = 1,- .work\_flags = IO\_WQ\_WORK\_BLKCG | IO\_WQ\_WORK\_FSIZE,- },- [IORING\_OP\_OPENAT] = {- .work\_flags = IO\_WQ\_WORK\_FILES | IO\_WQ\_WORK\_BLKCG |- IO\_WQ\_WORK\_FS,- },- [IORING\_OP\_CLOSE] = {- .needs\_file = 1,- .needs\_file\_no\_error = 1,- .work\_flags = IO\_WQ\_WORK\_FILES | IO\_WQ\_WORK\_BLKCG,- },- [IORING\_OP\_FILES\_UPDATE] = {- .work\_flags = IO\_WQ\_WORK\_FILES | IO\_WQ\_WORK\_MM,- },- [IORING\_OP\_STATX] = {- .work\_flags = IO\_WQ\_WORK\_FILES | IO\_WQ\_WORK\_MM |- IO\_WQ\_WORK\_FS | IO\_WQ\_WORK\_BLKCG, },+ [IORING\_OP\_OPENAT] = {},+ [IORING\_OP\_CLOSE] = {},+ [IORING\_OP\_FILES\_UPDATE] = {},+ [IORING\_OP\_STATX] = {}, [IORING\_OP\_READ] = { .needs\_file = 1, .unbound\_nonreg\_file = 1, .pollin = 1, .buffer\_select = 1,+ .plug = 1, .async\_size = sizeof(struct io\_async\_rw),- .work\_flags = IO\_WQ\_WORK\_MM | IO\_WQ\_WORK\_BLKCG |- IO\_WQ\_WORK\_FILES, }, [IORING\_OP\_WRITE] = { .needs\_file = 1, .hash\_reg\_file = 1, .unbound\_nonreg\_file = 1, .pollout = 1,+ .plug = 1, .async\_size = sizeof(struct io\_async\_rw),- .work\_flags = IO\_WQ\_WORK\_MM | IO\_WQ\_WORK\_BLKCG |- IO\_WQ\_WORK\_FSIZE | IO\_WQ\_WORK\_FILES, }, [IORING\_OP\_FADVISE] = { .needs\_file = 1,- .work\_flags = IO\_WQ\_WORK\_BLKCG,- },- [IORING\_OP\_MADVISE] = {- .work\_flags = IO\_WQ\_WORK\_MM | IO\_WQ\_WORK\_BLKCG, },+ [IORING\_OP\_MADVISE] = {}, [IORING\_OP\_SEND] = { .needs\_file = 1, .unbound\_nonreg\_file = 1, .pollout = 1,- .work\_flags = IO\_WQ\_WORK\_MM | IO\_WQ\_WORK\_BLKCG |- IO\_WQ\_WORK\_FS, }, [IORING\_OP\_RECV] = { .needs\_file = 1, .unbound\_nonreg\_file = 1, .pollin = 1, .buffer\_select = 1,- .work\_flags = IO\_WQ\_WORK\_MM | IO\_WQ\_WORK\_BLKCG |- IO\_WQ\_WORK\_FS, }, [IORING\_OP\_OPENAT2] = {- .work\_flags = IO\_WQ\_WORK\_FILES | IO\_WQ\_WORK\_FS |- IO\_WQ\_WORK\_BLKCG, }, [IORING\_OP\_EPOLL\_CTL] = { .unbound\_nonreg\_file = 1,- .work\_flags = IO\_WQ\_WORK\_FILES, }, [IORING\_OP\_SPLICE] = { .needs\_file = 1, .hash\_reg\_file = 1, .unbound\_nonreg\_file = 1,- .work\_flags = IO\_WQ\_WORK\_BLKCG | IO\_WQ\_WORK\_FILES, }, [IORING\_OP\_PROVIDE\_BUFFERS] = {}, [IORING\_OP\_REMOVE\_BUFFERS] = {},@@ -945,43 +1060,47 @@ static const struct io\_op\_def io\_op\_defs[] = { .hash\_reg\_file = 1, .unbound\_nonreg\_file = 1, },+ [IORING\_OP\_SHUTDOWN] = {+ .needs\_file = 1,+ },+ [IORING\_OP\_RENAMEAT] = {},+ [IORING\_OP\_UNLINKAT] = {}, }; -enum io\_mem\_account {- ACCT\_LOCKED,- ACCT\_PINNED,-};+/\* requests with any of those set should undergo io\_disarm\_next() \*/+#define IO\_DISARM\_MASK (REQ\_F\_ARM\_LTIMEOUT | REQ\_F\_LINK\_TIMEOUT | REQ\_F\_FAIL) -static void destroy\_fixed\_file\_ref\_node(struct fixed\_file\_ref\_node \*ref\_node);-static struct fixed\_file\_ref\_node \*alloc\_fixed\_file\_ref\_node(- struct io\_ring\_ctx \*ctx);+static bool io\_disarm\_next(struct io\_kiocb \*req);+static void io\_uring\_del\_tctx\_node(unsigned long index);+static void io\_uring\_try\_cancel\_requests(struct io\_ring\_ctx \*ctx,+ struct task\_struct \*task,+ bool cancel\_all);+static void io\_uring\_cancel\_generic(bool cancel\_all, struct io\_sq\_data \*sqd);++static void io\_fill\_cqe\_req(struct io\_kiocb \*req, s32 res, u32 cflags); -static void \_\_io\_complete\_rw(struct io\_kiocb \*req, long res, long res2,- struct io\_comp\_state \*cs);-static void io\_cqring\_fill\_event(struct io\_kiocb \*req, long res); static void io\_put\_req(struct io\_kiocb \*req);-static void io\_put\_req\_deferred(struct io\_kiocb \*req, int nr);-static void io\_double\_put\_req(struct io\_kiocb \*req);-static struct io\_kiocb \*io\_prep\_linked\_timeout(struct io\_kiocb \*req);-static void \_\_io\_queue\_linked\_timeout(struct io\_kiocb \*req);+static void io\_put\_req\_deferred(struct io\_kiocb \*req);+static void io\_dismantle\_req(struct io\_kiocb \*req); static void io\_queue\_linked\_timeout(struct io\_kiocb \*req);-static int \_\_io\_sqe\_files\_update(struct io\_ring\_ctx \*ctx,- struct io\_uring\_files\_update \*ip,- unsigned nr\_args);-static void \_\_io\_clean\_op(struct io\_kiocb \*req);-static struct file \*io\_file\_get(struct io\_submit\_state \*state,+static int \_\_io\_register\_rsrc\_update(struct io\_ring\_ctx \*ctx, unsigned type,+ struct io\_uring\_rsrc\_update2 \*up,+ unsigned nr\_args);+static void io\_clean\_op(struct io\_kiocb \*req);+static struct file \*io\_file\_get(struct io\_ring\_ctx \*ctx, struct io\_kiocb \*req, int fd, bool fixed);-static void \_\_io\_queue\_sqe(struct io\_kiocb \*req, struct io\_comp\_state \*cs);-static void io\_file\_put\_work(struct work\_struct \*work);+static void \_\_io\_queue\_sqe(struct io\_kiocb \*req);+static void io\_rsrc\_put\_work(struct work\_struct \*work); -static ssize\_t io\_import\_iovec(int rw, struct io\_kiocb \*req,- struct iovec \*\*iovec, struct iov\_iter \*iter,- bool needs\_lock);-static int io\_setup\_async\_rw(struct io\_kiocb \*req, const struct iovec \*iovec,- const struct iovec \*fast\_iov,- struct iov\_iter \*iter, bool force);-static void io\_req\_drop\_files(struct io\_kiocb \*req); static void io\_req\_task\_queue(struct io\_kiocb \*req);+static void io\_submit\_flush\_completions(struct io\_ring\_ctx \*ctx);+static int io\_req\_prep\_async(struct io\_kiocb \*req);++static int io\_install\_fixed\_file(struct io\_kiocb \*req, struct file \*file,+ unsigned int issue\_flags, u32 slot\_index);+static int io\_close\_fixed(struct io\_kiocb \*req, unsigned int issue\_flags);++static enum hrtimer\_restart io\_link\_timeout\_fn(struct hrtimer \*timer);  static struct kmem\_cache \*req\_cachep; @@ -1000,186 +1119,146 @@ struct sock \*io\_uring\_get\_socket(struct file \*file) } EXPORT\_SYMBOL(io\_uring\_get\_socket); -static inline void io\_clean\_op(struct io\_kiocb \*req)+static inline void io\_tw\_lock(struct io\_ring\_ctx \*ctx, bool \*locked) {- if (req->flags & (REQ\_F\_NEED\_CLEANUP | REQ\_F\_BUFFER\_SELECTED))- \_\_io\_clean\_op(req);+ if (!\*locked) {+ mutex\_lock(&ctx->uring\_lock);+ \*locked = true;+ } } -static inline bool \_\_io\_match\_files(struct io\_kiocb \*req,- struct files\_struct \*files)-{- if (req->file && req->file->f\_op == &io\_uring\_fops)- return true;+#define io\_for\_each\_link(pos, head) \+ for (pos = (head); pos; pos = pos->link) - return ((req->flags & REQ\_F\_WORK\_INITIALIZED) &&- (req->work.flags & IO\_WQ\_WORK\_FILES)) &&- req->work.identity->files == files;-}+/\*+ \* Shamelessly stolen from the mm implementation of page reference checking,+ \* see commit f958d7b528b1 for details.+ \*/+#define req\_ref\_zero\_or\_close\_to\_overflow(req) \+ ((unsigned int) atomic\_read(&(req->refs)) + 127u <= 127u) -static void io\_refs\_resurrect(struct percpu\_ref \*ref, struct completion \*compl)+static inline bool req\_ref\_inc\_not\_zero(struct io\_kiocb \*req) {- bool got = percpu\_ref\_tryget(ref);-- /\* already at zero, wait for ->release() \*/- if (!got)- wait\_for\_completion(compl);- percpu\_ref\_resurrect(ref);- if (got)- percpu\_ref\_put(ref);+ WARN\_ON\_ONCE(!(req->flags & REQ\_F\_REFCOUNT));+ return atomic\_inc\_not\_zero(&req->refs); } -static bool io\_match\_task(struct io\_kiocb \*head,- struct task\_struct \*task,- struct files\_struct \*files)+static inline bool req\_ref\_put\_and\_test(struct io\_kiocb \*req) {- struct io\_kiocb \*link;-- if (task && head->task != task) {- /\* in terms of cancelation, always match if req task is dead \*/- if (head->task->flags & PF\_EXITING)- return true;- return false;- }- if (!files)+ if (likely(!(req->flags & REQ\_F\_REFCOUNT))) return true;- if (\_\_io\_match\_files(head, files))- return true;- if (head->flags & REQ\_F\_LINK\_HEAD) {- list\_for\_each\_entry(link, &head->link\_list, link\_list) {- if (\_\_io\_match\_files(link, files))- return true;- }- }- return false;-} + WARN\_ON\_ONCE(req\_ref\_zero\_or\_close\_to\_overflow(req));+ return atomic\_dec\_and\_test(&req->refs);+} -static void io\_sq\_thread\_drop\_mm(void)+static inline void req\_ref\_get(struct io\_kiocb \*req) {- struct mm\_struct \*mm = current->mm;-- if (mm) {- kthread\_unuse\_mm(mm);- mmput(mm);- current->mm = NULL;- }+ WARN\_ON\_ONCE(!(req->flags & REQ\_F\_REFCOUNT));+ WARN\_ON\_ONCE(req\_ref\_zero\_or\_close\_to\_overflow(req));+ atomic\_inc(&req->refs); } -static int \_\_io\_sq\_thread\_acquire\_mm(struct io\_ring\_ctx \*ctx)+static inline void \_\_io\_req\_set\_refcount(struct io\_kiocb \*req, int nr) {- struct mm\_struct \*mm;-- if (current->flags & PF\_EXITING)- return -EFAULT;- if (current->mm)- return 0;-- /\* Should never happen \*/- if (unlikely(!(ctx->flags & IORING\_SETUP\_SQPOLL)))- return -EFAULT;-- task\_lock(ctx->sqo\_task);- mm = ctx->sqo\_task->mm;- if (unlikely(!mm || !mmget\_not\_zero(mm)))- mm = NULL;- task\_unlock(ctx->sqo\_task);-- if (mm) {- kthread\_use\_mm(mm);- return 0;+ if (!(req->flags & REQ\_F\_REFCOUNT)) {+ req->flags |= REQ\_F\_REFCOUNT;+ atomic\_set(&req->refs, nr); }-- return -EFAULT; } -static int io\_sq\_thread\_acquire\_mm(struct io\_ring\_ctx \*ctx,- struct io\_kiocb \*req)+static inline void io\_req\_set\_refcount(struct io\_kiocb \*req) {- if (!(io\_op\_defs[req->opcode].work\_flags & IO\_WQ\_WORK\_MM))- return 0;- return \_\_io\_sq\_thread\_acquire\_mm(ctx);+ \_\_io\_req\_set\_refcount(req, 1); } -static void io\_sq\_thread\_associate\_blkcg(struct io\_ring\_ctx \*ctx,- struct cgroup\_subsys\_state \*\*cur\_css)-+static inline void io\_req\_set\_rsrc\_node(struct io\_kiocb \*req) {-#ifdef CONFIG\_BLK\_CGROUP- /\* puts the old one when swapping \*/- if (\*cur\_css != ctx->sqo\_blkcg\_css) {- kthread\_associate\_blkcg(ctx->sqo\_blkcg\_css);- \*cur\_css = ctx->sqo\_blkcg\_css;+ struct io\_ring\_ctx \*ctx = req->ctx;++ if (!req->fixed\_rsrc\_refs) {+ req->fixed\_rsrc\_refs = &ctx->rsrc\_node->refs;+ percpu\_ref\_get(req->fixed\_rsrc\_refs); }-#endif } -static void io\_sq\_thread\_unassociate\_blkcg(void)+static void io\_refs\_resurrect(struct percpu\_ref \*ref, struct completion \*compl) {-#ifdef CONFIG\_BLK\_CGROUP- kthread\_associate\_blkcg(NULL);-#endif-}+ bool got = percpu\_ref\_tryget(ref); -static inline void req\_set\_fail\_links(struct io\_kiocb \*req)-{- if ((req->flags & (REQ\_F\_LINK | REQ\_F\_HARDLINK)) == REQ\_F\_LINK)- req->flags |= REQ\_F\_FAIL\_LINK;+ /\* already at zero, wait for ->release() \*/+ if (!got)+ wait\_for\_completion(compl);+ percpu\_ref\_resurrect(ref);+ if (got)+ percpu\_ref\_put(ref); } -/\*- \* None of these are dereferenced, they are simply used to check if any of- \* them have changed. If we're under current and check they are still the- \* same, we're fine to grab references to them for actual out-of-line use.- \*/-static void io\_init\_identity(struct io\_identity \*id)+static bool io\_match\_task(struct io\_kiocb \*head, struct task\_struct \*task,+ bool cancel\_all)+ \_\_must\_hold(&req->ctx->timeout\_lock) {- id->files = current->files;- id->mm = current->mm;-#ifdef CONFIG\_BLK\_CGROUP- rcu\_read\_lock();- id->blkcg\_css = blkcg\_css();- rcu\_read\_unlock();-#endif- id->creds = current\_cred();- id->nsproxy = current->nsproxy;- id->fs = current->fs;- id->fsize = rlimit(RLIMIT\_FSIZE);-#ifdef CONFIG\_AUDIT- id->loginuid = current->loginuid;- id->sessionid = current->sessionid;-#endif- refcount\_set(&id->count, 1);+ struct io\_kiocb \*req;++ if (task && head->task != task)+ return false;+ if (cancel\_all)+ return true;++ io\_for\_each\_link(req, head) {+ if (req->flags & REQ\_F\_INFLIGHT)+ return true;+ }+ return false; } -static inline void \_\_io\_req\_init\_async(struct io\_kiocb \*req)+static bool io\_match\_linked(struct io\_kiocb \*head) {- memset(&req->work, 0, sizeof(req->work));- req->flags |= REQ\_F\_WORK\_INITIALIZED;+ struct io\_kiocb \*req;++ io\_for\_each\_link(req, head) {+ if (req->flags & REQ\_F\_INFLIGHT)+ return true;+ }+ return false; }  /\*- \* Note: must call io\_req\_init\_async() for the first time you- \* touch any members of io\_wq\_work.+ \* As io\_match\_task() but protected against racing with linked timeouts.+ \* User must not hold timeout\_lock. \*/-static inline void io\_req\_init\_async(struct io\_kiocb \*req)+static bool io\_match\_task\_safe(struct io\_kiocb \*head, struct task\_struct \*task,+ bool cancel\_all) {- struct io\_uring\_task \*tctx = req->task->io\_uring;+ bool matched; - if (req->flags & REQ\_F\_WORK\_INITIALIZED)- return;+ if (task && head->task != task)+ return false;+ if (cancel\_all)+ return true;++ if (head->flags & REQ\_F\_LINK\_TIMEOUT) {+ struct io\_ring\_ctx \*ctx = head->ctx; - \_\_io\_req\_init\_async(req);+ /\* protect against races with linked timeouts \*/+ spin\_lock\_irq(&ctx->timeout\_lock);+ matched = io\_match\_linked(head);+ spin\_unlock\_irq(&ctx->timeout\_lock);+ } else {+ matched = io\_match\_linked(head);+ }+ return matched;+} - /\* Grab a ref if this isn't our static identity \*/- req->work.identity = tctx->identity;- if (tctx->identity != &tctx->\_\_identity)- refcount\_inc(&req->work.identity->count);+static inline void req\_set\_fail(struct io\_kiocb \*req)+{+ req->flags |= REQ\_F\_FAIL; } -static inline bool io\_async\_submit(struct io\_ring\_ctx \*ctx)+static inline void req\_fail\_link\_node(struct io\_kiocb \*req, int res) {- return ctx->flags & IORING\_SETUP\_SQPOLL;+ req\_set\_fail(req);+ req->result = res; }  static void io\_ring\_ctx\_ref\_free(struct percpu\_ref \*ref)@@ -1194,6 +1273,27 @@ static inline bool io\_is\_timeout\_noseq(struct io\_kiocb \*req) return !req->timeout.off; } +static void io\_fallback\_req\_func(struct work\_struct \*work)+{+ struct io\_ring\_ctx \*ctx = container\_of(work, struct io\_ring\_ctx,+ fallback\_work.work);+ struct llist\_node \*node = llist\_del\_all(&ctx->fallback\_llist);+ struct io\_kiocb \*req, \*tmp;+ bool locked = false;++ percpu\_ref\_get(&ctx->refs);+ llist\_for\_each\_entry\_safe(req, tmp, node, io\_task\_work.fallback\_node)+ req->io\_task\_work.func(req, &locked);++ if (locked) {+ if (ctx->submit\_state.compl\_nr)+ io\_submit\_flush\_completions(ctx);+ mutex\_unlock(&ctx->uring\_lock);+ }+ percpu\_ref\_put(&ctx->refs);++}+ static struct io\_ring\_ctx \*io\_ring\_ctx\_alloc(struct io\_uring\_params \*p) { struct io\_ring\_ctx \*ctx;@@ -1203,10 +1303,6 @@ static struct io\_ring\_ctx \*io\_ring\_ctx\_alloc(struct io\_uring\_params \*p) if (!ctx) return NULL; - ctx->fallback\_req = kmem\_cache\_alloc(req\_cachep, GFP\_KERNEL);- if (!ctx->fallback\_req)- goto err;- /\* \* Use 5 bits less than the max cq entries, that should give us around \* 32 entries per hash list if totally full and uniformly spread.@@ -1222,6 +1318,12 @@ static struct io\_ring\_ctx \*io\_ring\_ctx\_alloc(struct io\_uring\_params \*p) goto err; \_\_hash\_init(ctx->cancel\_hash, 1U << hash\_bits); + ctx->dummy\_ubuf = kzalloc(sizeof(\*ctx->dummy\_ubuf), GFP\_KERNEL);+ if (!ctx->dummy\_ubuf)+ goto err;+ /\* set invalid range, so io\_import\_fixed() fails meeting it \*/+ ctx->dummy\_ubuf->ubuf = -1UL;+ if (percpu\_ref\_init(&ctx->refs, io\_ring\_ctx\_ref\_free, PERCPU\_REF\_ALLOW\_REINIT, GFP\_KERNEL)) goto err;@@ -1229,232 +1331,109 @@ static struct io\_ring\_ctx \*io\_ring\_ctx\_alloc(struct io\_uring\_params \*p) ctx->flags = p->flags; init\_waitqueue\_head(&ctx->sqo\_sq\_wait); INIT\_LIST\_HEAD(&ctx->sqd\_list);- init\_waitqueue\_head(&ctx->cq\_wait);+ init\_waitqueue\_head(&ctx->poll\_wait); INIT\_LIST\_HEAD(&ctx->cq\_overflow\_list); init\_completion(&ctx->ref\_comp);- init\_completion(&ctx->sq\_thread\_comp); xa\_init\_flags(&ctx->io\_buffers, XA\_FLAGS\_ALLOC1); xa\_init\_flags(&ctx->personalities, XA\_FLAGS\_ALLOC1); mutex\_init(&ctx->uring\_lock);- init\_waitqueue\_head(&ctx->wait);+ init\_waitqueue\_head(&ctx->cq\_wait); spin\_lock\_init(&ctx->completion\_lock);+ spin\_lock\_init(&ctx->timeout\_lock); INIT\_LIST\_HEAD(&ctx->iopoll\_list); INIT\_LIST\_HEAD(&ctx->defer\_list); INIT\_LIST\_HEAD(&ctx->timeout\_list);- spin\_lock\_init(&ctx->inflight\_lock);- INIT\_LIST\_HEAD(&ctx->inflight\_list);- INIT\_DELAYED\_WORK(&ctx->file\_put\_work, io\_file\_put\_work);- init\_llist\_head(&ctx->file\_put\_llist);+ INIT\_LIST\_HEAD(&ctx->ltimeout\_list);+ spin\_lock\_init(&ctx->rsrc\_ref\_lock);+ INIT\_LIST\_HEAD(&ctx->rsrc\_ref\_list);+ INIT\_DELAYED\_WORK(&ctx->rsrc\_put\_work, io\_rsrc\_put\_work);+ init\_llist\_head(&ctx->rsrc\_put\_llist);+ INIT\_LIST\_HEAD(&ctx->tctx\_list);+ INIT\_LIST\_HEAD(&ctx->submit\_state.free\_list);+ INIT\_LIST\_HEAD(&ctx->locked\_free\_list);+ INIT\_DELAYED\_WORK(&ctx->fallback\_work, io\_fallback\_req\_func); return ctx; err:- if (ctx->fallback\_req)- kmem\_cache\_free(req\_cachep, ctx->fallback\_req);+ kfree(ctx->dummy\_ubuf); kfree(ctx->cancel\_hash); kfree(ctx); return NULL; } +static void io\_account\_cq\_overflow(struct io\_ring\_ctx \*ctx)+{+ struct io\_rings \*r = ctx->rings;++ WRITE\_ONCE(r->cq\_overflow, READ\_ONCE(r->cq\_overflow) + 1);+ ctx->cq\_extra--;+}+ static bool req\_need\_defer(struct io\_kiocb \*req, u32 seq) { if (unlikely(req->flags & REQ\_F\_IO\_DRAIN)) { struct io\_ring\_ctx \*ctx = req->ctx; - return seq != ctx->cached\_cq\_tail- + READ\_ONCE(ctx->cached\_cq\_overflow);+ return seq + READ\_ONCE(ctx->cq\_extra) != ctx->cached\_cq\_tail; }  return false; } -static void \_\_io\_commit\_cqring(struct io\_ring\_ctx \*ctx)-{- struct io\_rings \*rings = ctx->rings;-- /\* order cqe stores with ring update \*/- smp\_store\_release(&rings->cq.tail, ctx->cached\_cq\_tail);-}+#define FFS\_ASYNC\_READ 0x1UL+#define FFS\_ASYNC\_WRITE 0x2UL+#ifdef CONFIG\_64BIT+#define FFS\_ISREG 0x4UL+#else+#define FFS\_ISREG 0x0UL+#endif+#define FFS\_MASK ~(FFS\_ASYNC\_READ|FFS\_ASYNC\_WRITE|FFS\_ISREG) -static void io\_put\_identity(struct io\_uring\_task \*tctx, struct io\_kiocb \*req)+static inline bool io\_req\_ffs\_set(struct io\_kiocb \*req) {- if (req->work.identity == &tctx->\_\_identity)- return;- if (refcount\_dec\_and\_test(&req->work.identity->count))- kfree(req->work.identity);+ return IS\_ENABLED(CONFIG\_64BIT) && (req->flags & REQ\_F\_FIXED\_FILE); } -static void io\_req\_clean\_work(struct io\_kiocb \*req)+static void io\_req\_track\_inflight(struct io\_kiocb \*req) {- if (!(req->flags & REQ\_F\_WORK\_INITIALIZED))- return;-- req->flags &= ~REQ\_F\_WORK\_INITIALIZED;-- if (req->work.flags & IO\_WQ\_WORK\_MM) {- mmdrop(req->work.identity->mm);- req->work.flags &= ~IO\_WQ\_WORK\_MM;- }-#ifdef CONFIG\_BLK\_CGROUP- if (req->work.flags & IO\_WQ\_WORK\_BLKCG) {- css\_put(req->work.identity->blkcg\_css);- req->work.flags &= ~IO\_WQ\_WORK\_BLKCG;- }-#endif- if (req->work.flags & IO\_WQ\_WORK\_CREDS) {- put\_cred(req->work.identity->creds);- req->work.flags &= ~IO\_WQ\_WORK\_CREDS;- }- if (req->work.flags & IO\_WQ\_WORK\_FS) {- struct fs\_struct \*fs = req->work.identity->fs;-- spin\_lock(&req->work.identity->fs->lock);- if (--fs->users)- fs = NULL;- spin\_unlock(&req->work.identity->fs->lock);- if (fs)- free\_fs\_struct(fs);- req->work.flags &= ~IO\_WQ\_WORK\_FS;+ if (!(req->flags & REQ\_F\_INFLIGHT)) {+ req->flags |= REQ\_F\_INFLIGHT;+ atomic\_inc(&req->task->io\_uring->inflight\_tracked); }- if (req->flags & REQ\_F\_INFLIGHT)- io\_req\_drop\_files(req);-- io\_put\_identity(req->task->io\_uring, req); } -/\*- \* Create a private copy of io\_identity, since some fields don't match- \* the current context.- \*/-static bool io\_identity\_cow(struct io\_kiocb \*req)+static struct io\_kiocb \*\_\_io\_prep\_linked\_timeout(struct io\_kiocb \*req) {- struct io\_uring\_task \*tctx = req->task->io\_uring;- const struct cred \*creds = NULL;- struct io\_identity \*id;-- if (req->work.flags & IO\_WQ\_WORK\_CREDS)- creds = req->work.identity->creds;-- id = kmemdup(req->work.identity, sizeof(\*id), GFP\_KERNEL);- if (unlikely(!id)) {- req->work.flags |= IO\_WQ\_WORK\_CANCEL;- return false;- }-- /\*- \* We can safely just re-init the creds we copied Either the field- \* matches the current one, or we haven't grabbed it yet. The only- \* exception is ->creds, through registered personalities, so handle- \* that one separately.- \*/- io\_init\_identity(id);- if (creds)- id->creds = creds;-- /\* add one for this request \*/- refcount\_inc(&id->count);+ if (WARN\_ON\_ONCE(!req->link))+ return NULL; - /\* drop tctx and req identity references, if needed \*/- if (tctx->identity != &tctx->\_\_identity &&- refcount\_dec\_and\_test(&tctx->identity->count))- kfree(tctx->identity);- if (req->work.identity != &tctx->\_\_identity &&- refcount\_dec\_and\_test(&req->work.identity->count))- kfree(req->work.identity);+ req->flags &= ~REQ\_F\_ARM\_LTIMEOUT;+ req->flags |= REQ\_F\_LINK\_TIMEOUT; - req->work.identity = id;- tctx->identity = id;- return true;+ /\* linked timeouts should have two refs once prep'ed \*/+ io\_req\_set\_refcount(req);+ \_\_io\_req\_set\_refcount(req->link, 2);+ return req->link; } -static bool io\_grab\_identity(struct io\_kiocb \*req)+static inline struct io\_kiocb \*io\_prep\_linked\_timeout(struct io\_kiocb \*req) {- const struct io\_op\_def \*def = &io\_op\_defs[req->opcode];- struct io\_identity \*id = req->work.identity;- struct io\_ring\_ctx \*ctx = req->ctx;-- if (def->work\_flags & IO\_WQ\_WORK\_FSIZE) {- if (id->fsize != rlimit(RLIMIT\_FSIZE))- return false;- req->work.flags |= IO\_WQ\_WORK\_FSIZE;- }-#ifdef CONFIG\_BLK\_CGROUP- if (!(req->work.flags & IO\_WQ\_WORK\_BLKCG) &&- (def->work\_flags & IO\_WQ\_WORK\_BLKCG)) {- rcu\_read\_lock();- if (id->blkcg\_css != blkcg\_css()) {- rcu\_read\_unlock();- return false;- }- /\*- \* This should be rare, either the cgroup is dying or the task- \* is moving cgroups. Just punt to root for the handful of ios.- \*/- if (css\_tryget\_online(id->blkcg\_css))- req->work.flags |= IO\_WQ\_WORK\_BLKCG;- rcu\_read\_unlock();- }-#endif- if (!(req->work.flags & IO\_WQ\_WORK\_CREDS)) {- if (id->creds != current\_cred())- return false;- get\_cred(id->creds);- req->work.flags |= IO\_WQ\_WORK\_CREDS;- }-#ifdef CONFIG\_AUDIT- if (!uid\_eq(current->loginuid, id->loginuid) ||- current->sessionid != id->sessionid)- return false;-#endif- if (!(req->work.flags & IO\_WQ\_WORK\_FS) &&- (def->work\_flags & IO\_WQ\_WORK\_FS)) {- if (current->fs != id->fs)- return false;- spin\_lock(&id->fs->lock);- if (!id->fs->in\_exec) {- id->fs->users++;- req->work.flags |= IO\_WQ\_WORK\_FS;- } else {- req->work.flags |= IO\_WQ\_WORK\_CANCEL;- }- spin\_unlock(&current->fs->lock);- }- if (!(req->work.flags & IO\_WQ\_WORK\_FILES) &&- (def->work\_flags & IO\_WQ\_WORK\_FILES) &&- !(req->flags & REQ\_F\_NO\_FILE\_TABLE)) {- if (id->files != current->files ||- id->nsproxy != current->nsproxy)- return false;- atomic\_inc(&id->files->count);- get\_nsproxy(id->nsproxy);-- if (!(req->flags & REQ\_F\_INFLIGHT)) {- req->flags |= REQ\_F\_INFLIGHT;-- spin\_lock\_irq(&ctx->inflight\_lock);- list\_add(&req->inflight\_entry, &ctx->inflight\_list);- spin\_unlock\_irq(&ctx->inflight\_lock);- }- req->work.flags |= IO\_WQ\_WORK\_FILES;- }- if (!(req->work.flags & IO\_WQ\_WORK\_MM) &&- (def->work\_flags & IO\_WQ\_WORK\_MM)) {- if (id->mm != current->mm)- return false;- mmgrab(id->mm);- req->work.flags |= IO\_WQ\_WORK\_MM;- }-- return true;+ if (likely(!(req->flags & REQ\_F\_ARM\_LTIMEOUT)))+ return NULL;+ return \_\_io\_prep\_linked\_timeout(req); }  static void io\_prep\_async\_work(struct io\_kiocb \*req) { const struct io\_op\_def \*def = &io\_op\_defs[req->opcode]; struct io\_ring\_ctx \*ctx = req->ctx;- struct io\_identity \*id; - io\_req\_init\_async(req);- id = req->work.identity;+ if (!(req->flags & REQ\_F\_CREDS)) {+ req->flags |= REQ\_F\_CREDS;+ req->creds = get\_current\_cred();+ } + req->work.list.next = NULL;+ req->work.flags = 0; if (req->flags & REQ\_F\_FORCE\_ASYNC) req->work.flags |= IO\_WQ\_WORK\_CONCURRENT; @@ -1465,92 +1444,77 @@ static void io\_prep\_async\_work(struct io\_kiocb \*req) if (def->unbound\_nonreg\_file) req->work.flags |= IO\_WQ\_WORK\_UNBOUND; }-- /\* if we fail grabbing identity, we must COW, regrab, and retry \*/- if (io\_grab\_identity(req))- return;-- if (!io\_identity\_cow(req))- return;-- /\* can't fail at this point \*/- if (!io\_grab\_identity(req))- WARN\_ON(1); }  static void io\_prep\_async\_link(struct io\_kiocb \*req) { struct io\_kiocb \*cur; - io\_prep\_async\_work(req);- if (req->flags & REQ\_F\_LINK\_HEAD)- list\_for\_each\_entry(cur, &req->link\_list, link\_list)+ if (req->flags & REQ\_F\_LINK\_TIMEOUT) {+ struct io\_ring\_ctx \*ctx = req->ctx;++ spin\_lock\_irq(&ctx->timeout\_lock);+ io\_for\_each\_link(cur, req)+ io\_prep\_async\_work(cur);+ spin\_unlock\_irq(&ctx->timeout\_lock);+ } else {+ io\_for\_each\_link(cur, req) io\_prep\_async\_work(cur);+ } } -static struct io\_kiocb \*\_\_io\_queue\_async\_work(struct io\_kiocb \*req)+static void io\_queue\_async\_work(struct io\_kiocb \*req, bool \*locked) { struct io\_ring\_ctx \*ctx = req->ctx; struct io\_kiocb \*link = io\_prep\_linked\_timeout(req);+ struct io\_uring\_task \*tctx = req->task->io\_uring; - trace\_io\_uring\_queue\_async\_work(ctx, io\_wq\_is\_hashed(&req->work), req,- &req->work, req->flags);- io\_wq\_enqueue(ctx->io\_wq, &req->work);- return link;-}+ /\* must not take the lock, NULL it as a precaution \*/+ locked = NULL; -static void io\_queue\_async\_work(struct io\_kiocb \*req)-{- struct io\_kiocb \*link;+ BUG\_ON(!tctx);+ BUG\_ON(!tctx->io\_wq);  /\* init ->work of the whole link before punting \*/ io\_prep\_async\_link(req);- link = \_\_io\_queue\_async\_work(req); + /\*+ \* Not expected to happen, but if we do have a bug where this \_can\_+ \* happen, catch it here and ensure the request is marked as+ \* canceled. That will make io-wq go through the usual work cancel+ \* procedure rather than attempt to run this request (or create a new+ \* worker for it).+ \*/+ if (WARN\_ON\_ONCE(!same\_thread\_group(req->task, current)))+ req->work.flags |= IO\_WQ\_WORK\_CANCEL;++ trace\_io\_uring\_queue\_async\_work(ctx, io\_wq\_is\_hashed(&req->work), req,+ &req->work, req->flags);+ io\_wq\_enqueue(tctx->io\_wq, &req->work); if (link) io\_queue\_linked\_timeout(link); }  static void io\_kill\_timeout(struct io\_kiocb \*req, int status)+ \_\_must\_hold(&req->ctx->completion\_lock)+ \_\_must\_hold(&req->ctx->timeout\_lock) { struct io\_timeout\_data \*io = req->async\_data;- int ret; - ret = hrtimer\_try\_to\_cancel(&io->timer);- if (ret != -1) {+ if (hrtimer\_try\_to\_cancel(&io->timer) != -1) { if (status)- req\_set\_fail\_links(req);+ req\_set\_fail(req); atomic\_set(&req->ctx->cq\_timeouts, atomic\_read(&req->ctx->cq\_timeouts) + 1); list\_del\_init(&req->timeout.list);- io\_cqring\_fill\_event(req, status);- io\_put\_req\_deferred(req, 1);- }-}--/\*- \* Returns true if we found and killed one or more timeouts- \*/-static bool io\_kill\_timeouts(struct io\_ring\_ctx \*ctx, struct task\_struct \*tsk,- struct files\_struct \*files)-{- struct io\_kiocb \*req, \*tmp;- int canceled = 0;-- spin\_lock\_irq(&ctx->completion\_lock);- list\_for\_each\_entry\_safe(req, tmp, &ctx->timeout\_list, timeout.list) {- if (io\_match\_task(req, tsk, files)) {- io\_kill\_timeout(req, -ECANCELED);- canceled++;- }+ io\_fill\_cqe\_req(req, status, 0);+ io\_put\_req\_deferred(req); }- spin\_unlock\_irq(&ctx->completion\_lock);- return canceled != 0; } -static void \_\_io\_queue\_deferred(struct io\_ring\_ctx \*ctx)+static void io\_queue\_deferred(struct io\_ring\_ctx \*ctx) {- do {+ while (!list\_empty(&ctx->defer\_list)) { struct io\_defer\_entry \*de = list\_first\_entry(&ctx->defer\_list, struct io\_defer\_entry, list); @@ -1559,19 +1523,16 @@ static void \_\_io\_queue\_deferred(struct io\_ring\_ctx \*ctx) list\_del\_init(&de->list); io\_req\_task\_queue(de->req); kfree(de);- } while (!list\_empty(&ctx->defer\_list));+ } }  static void io\_flush\_timeouts(struct io\_ring\_ctx \*ctx)+ \_\_must\_hold(&ctx->completion\_lock) {+ u32 seq = ctx->cached\_cq\_tail - atomic\_read(&ctx->cq\_timeouts); struct io\_kiocb \*req, \*tmp;- u32 seq;-- if (list\_empty(&ctx->timeout\_list))- return;-- seq = ctx->cached\_cq\_tail - atomic\_read(&ctx->cq\_timeouts); + spin\_lock\_irq(&ctx->timeout\_lock); list\_for\_each\_entry\_safe(req, tmp, &ctx->timeout\_list, timeout.list) { u32 events\_needed, events\_got; @@ -1592,441 +1553,564 @@ static void io\_flush\_timeouts(struct io\_ring\_ctx \*ctx)  io\_kill\_timeout(req, 0); }- ctx->cq\_last\_tm\_flush = seq;+ spin\_unlock\_irq(&ctx->timeout\_lock); } -static void io\_commit\_cqring(struct io\_ring\_ctx \*ctx)+static void \_\_io\_commit\_cqring\_flush(struct io\_ring\_ctx \*ctx) {- io\_flush\_timeouts(ctx);- \_\_io\_commit\_cqring(ctx);+ if (ctx->off\_timeout\_used)+ io\_flush\_timeouts(ctx);+ if (ctx->drain\_active)+ io\_queue\_deferred(ctx);+} - if (unlikely(!list\_empty(&ctx->defer\_list)))- \_\_io\_queue\_deferred(ctx);+static inline void io\_commit\_cqring(struct io\_ring\_ctx \*ctx)+{+ if (unlikely(ctx->off\_timeout\_used || ctx->drain\_active))+ \_\_io\_commit\_cqring\_flush(ctx);+ /\* order cqe stores with ring update \*/+ smp\_store\_release(&ctx->rings->cq.tail, ctx->cached\_cq\_tail); }  static inline bool io\_sqring\_full(struct io\_ring\_ctx \*ctx) { struct io\_rings \*r = ctx->rings; - return READ\_ONCE(r->sq.tail) - ctx->cached\_sq\_head == r->sq\_ring\_entries;+ return READ\_ONCE(r->sq.tail) - ctx->cached\_sq\_head == ctx->sq\_entries; } -static struct io\_uring\_cqe \*io\_get\_cqring(struct io\_ring\_ctx \*ctx)+static inline unsigned int \_\_io\_cqring\_events(struct io\_ring\_ctx \*ctx)+{+ return ctx->cached\_cq\_tail - READ\_ONCE(ctx->rings->cq.head);+}++static inline struct io\_uring\_cqe \*io\_get\_cqe(struct io\_ring\_ctx \*ctx) { struct io\_rings \*rings = ctx->rings;- unsigned tail;+ unsigned tail, mask = ctx->cq\_entries - 1; - tail = ctx->cached\_cq\_tail; /\* \* writes to the cq entry need to come after reading head; the \* control dependency is enough as we're using WRITE\_ONCE to \* fill the cq entry \*/- if (tail - READ\_ONCE(rings->cq.head) == rings->cq\_ring\_entries)+ if (\_\_io\_cqring\_events(ctx) == ctx->cq\_entries) return NULL; - ctx->cached\_cq\_tail++;- return &rings->cqes[tail & ctx->cq\_mask];+ tail = ctx->cached\_cq\_tail++;+ return &rings->cqes[tail & mask]; }  static inline bool io\_should\_trigger\_evfd(struct io\_ring\_ctx \*ctx) {- if (!ctx->cq\_ev\_fd)+ if (likely(!ctx->cq\_ev\_fd)) return false; if (READ\_ONCE(ctx->rings->cq\_flags) & IORING\_CQ\_EVENTFD\_DISABLED) return false;- if (!ctx->eventfd\_async)- return true;- return io\_wq\_current\_is\_worker();+ return !ctx->eventfd\_async || io\_wq\_current\_is\_worker(); } +/\*+ \* This should only get called when at least one event has been posted.+ \* Some applications rely on the eventfd notification count only changing+ \* IFF a new CQE has been added to the CQ ring. There's no depedency on+ \* 1:1 relationship between how many times this function is called (and+ \* hence the eventfd count) and number of CQEs posted to the CQ ring.+ \*/ static void io\_cqring\_ev\_posted(struct io\_ring\_ctx \*ctx) {- if (wq\_has\_sleeper(&ctx->cq\_wait)) {- wake\_up\_interruptible(&ctx->cq\_wait);- kill\_fasync(&ctx->cq\_fasync, SIGIO, POLL\_IN);- }- if (waitqueue\_active(&ctx->wait))- wake\_up(&ctx->wait);+ /\*+ \* wake\_up\_all() may seem excessive, but io\_wake\_function() and+ \* io\_should\_wake() handle the termination of the loop and only+ \* wake as many waiters as we need to.+ \*/+ if (wq\_has\_sleeper(&ctx->cq\_wait))+ wake\_up\_all(&ctx->cq\_wait); if (ctx->sq\_data && waitqueue\_active(&ctx->sq\_data->wait)) wake\_up(&ctx->sq\_data->wait); if (io\_should\_trigger\_evfd(ctx)) eventfd\_signal(ctx->cq\_ev\_fd, 1);+ if (waitqueue\_active(&ctx->poll\_wait))+ wake\_up\_interruptible(&ctx->poll\_wait); } -static void io\_cqring\_mark\_overflow(struct io\_ring\_ctx \*ctx)+static void io\_cqring\_ev\_posted\_iopoll(struct io\_ring\_ctx \*ctx) {- if (list\_empty(&ctx->cq\_overflow\_list)) {- clear\_bit(0, &ctx->sq\_check\_overflow);- clear\_bit(0, &ctx->cq\_check\_overflow);- ctx->rings->sq\_flags &= ~IORING\_SQ\_CQ\_OVERFLOW;+ /\* see waitqueue\_active() comment \*/+ smp\_mb();++ if (ctx->flags & IORING\_SETUP\_SQPOLL) {+ if (waitqueue\_active(&ctx->cq\_wait))+ wake\_up\_all(&ctx->cq\_wait); }+ if (io\_should\_trigger\_evfd(ctx))+ eventfd\_signal(ctx->cq\_ev\_fd, 1);+ if (waitqueue\_active(&ctx->poll\_wait))+ wake\_up\_interruptible(&ctx->poll\_wait); }  /\* Returns true if there are no backlogged entries after the flush \*/-static bool \_\_io\_cqring\_overflow\_flush(struct io\_ring\_ctx \*ctx, bool force,- struct task\_struct \*tsk,- struct files\_struct \*files)+static bool \_\_io\_cqring\_overflow\_flush(struct io\_ring\_ctx \*ctx, bool force) {- struct io\_rings \*rings = ctx->rings;- struct io\_kiocb \*req, \*tmp;- struct io\_uring\_cqe \*cqe;- unsigned long flags;- LIST\_HEAD(list);-- if (!force) {- if ((ctx->cached\_cq\_tail - READ\_ONCE(rings->cq.head) ==- rings->cq\_ring\_entries))- return false;- }+ bool all\_flushed, posted; - spin\_lock\_irqsave(&ctx->completion\_lock, flags);+ if (!force && \_\_io\_cqring\_events(ctx) == ctx->cq\_entries)+ return false; - cqe = NULL;- list\_for\_each\_entry\_safe(req, tmp, &ctx->cq\_overflow\_list, compl.list) {- if (!io\_match\_task(req, tsk, files))- continue;+ posted = false;+ spin\_lock(&ctx->completion\_lock);+ while (!list\_empty(&ctx->cq\_overflow\_list)) {+ struct io\_uring\_cqe \*cqe = io\_get\_cqe(ctx);+ struct io\_overflow\_cqe \*ocqe; - cqe = io\_get\_cqring(ctx); if (!cqe && !force) break;+ ocqe = list\_first\_entry(&ctx->cq\_overflow\_list,+ struct io\_overflow\_cqe, list);+ if (cqe)+ memcpy(cqe, &ocqe->cqe, sizeof(\*cqe));+ else+ io\_account\_cq\_overflow(ctx); - list\_move(&req->compl.list, &list);- if (cqe) {- WRITE\_ONCE(cqe->user\_data, req->user\_data);- WRITE\_ONCE(cqe->res, req->result);- WRITE\_ONCE(cqe->flags, req->compl.cflags);- } else {- ctx->cached\_cq\_overflow++;- WRITE\_ONCE(ctx->rings->cq\_overflow,- ctx->cached\_cq\_overflow);- }+ posted = true;+ list\_del(&ocqe->list);+ kfree(ocqe); } - io\_commit\_cqring(ctx);- io\_cqring\_mark\_overflow(ctx);-- spin\_unlock\_irqrestore(&ctx->completion\_lock, flags);- io\_cqring\_ev\_posted(ctx);-- while (!list\_empty(&list)) {- req = list\_first\_entry(&list, struct io\_kiocb, compl.list);- list\_del(&req->compl.list);- io\_put\_req(req);+ all\_flushed = list\_empty(&ctx->cq\_overflow\_list);+ if (all\_flushed) {+ clear\_bit(0, &ctx->check\_cq\_overflow);+ WRITE\_ONCE(ctx->rings->sq\_flags,+ ctx->rings->sq\_flags & ~IORING\_SQ\_CQ\_OVERFLOW); } - return cqe != NULL;+ if (posted)+ io\_commit\_cqring(ctx);+ spin\_unlock(&ctx->completion\_lock);+ if (posted)+ io\_cqring\_ev\_posted(ctx);+ return all\_flushed; } -static void io\_cqring\_overflow\_flush(struct io\_ring\_ctx \*ctx, bool force,- struct task\_struct \*tsk,- struct files\_struct \*files)+static bool io\_cqring\_overflow\_flush(struct io\_ring\_ctx \*ctx) {- if (test\_bit(0, &ctx->cq\_check\_overflow)) {+ bool ret = true;++ if (test\_bit(0, &ctx->check\_cq\_overflow)) { /\* iopoll syncs against uring\_lock, not completion\_lock \*/ if (ctx->flags & IORING\_SETUP\_IOPOLL) mutex\_lock(&ctx->uring\_lock);- \_\_io\_cqring\_overflow\_flush(ctx, force, tsk, files);+ ret = \_\_io\_cqring\_overflow\_flush(ctx, false); if (ctx->flags & IORING\_SETUP\_IOPOLL) mutex\_unlock(&ctx->uring\_lock); }++ return ret; } -static void \_\_io\_cqring\_fill\_event(struct io\_kiocb \*req, long res,- unsigned int cflags)+/\* must to be called somewhat shortly after putting a request \*/+static inline void io\_put\_task(struct task\_struct \*task, int nr)+{+ struct io\_uring\_task \*tctx = task->io\_uring;++ if (likely(task == current)) {+ tctx->cached\_refs += nr;+ } else {+ percpu\_counter\_sub(&tctx->inflight, nr);+ if (unlikely(atomic\_read(&tctx->in\_idle)))+ wake\_up(&tctx->wait);+ put\_task\_struct\_many(task, nr);+ }+}++static void io\_task\_refs\_refill(struct io\_uring\_task \*tctx)+{+ unsigned int refill = -tctx->cached\_refs + IO\_TCTX\_REFS\_CACHE\_NR;++ percpu\_counter\_add(&tctx->inflight, refill);+ refcount\_add(refill, &current->usage);+ tctx->cached\_refs += refill;+}++static inline void io\_get\_task\_refs(int nr)+{+ struct io\_uring\_task \*tctx = current->io\_uring;++ tctx->cached\_refs -= nr;+ if (unlikely(tctx->cached\_refs < 0))+ io\_task\_refs\_refill(tctx);+}++static \_\_cold void io\_uring\_drop\_tctx\_refs(struct task\_struct \*task)+{+ struct io\_uring\_task \*tctx = task->io\_uring;+ unsigned int refs = tctx->cached\_refs;++ if (refs) {+ tctx->cached\_refs = 0;+ percpu\_counter\_sub(&tctx->inflight, refs);+ put\_task\_struct\_many(task, refs);+ }+}++static bool io\_cqring\_event\_overflow(struct io\_ring\_ctx \*ctx, u64 user\_data,+ s32 res, u32 cflags)+{+ struct io\_overflow\_cqe \*ocqe;++ ocqe = kmalloc(sizeof(\*ocqe), GFP\_ATOMIC | \_\_GFP\_ACCOUNT);+ if (!ocqe) {+ /\*+ \* If we're in ring overflow flush mode, or in task cancel mode,+ \* or cannot allocate an overflow entry, then we need to drop it+ \* on the floor.+ \*/+ io\_account\_cq\_overflow(ctx);+ return false;+ }+ if (list\_empty(&ctx->cq\_overflow\_list)) {+ set\_bit(0, &ctx->check\_cq\_overflow);+ WRITE\_ONCE(ctx->rings->sq\_flags,+ ctx->rings->sq\_flags | IORING\_SQ\_CQ\_OVERFLOW);++ }+ ocqe->cqe.user\_data = user\_data;+ ocqe->cqe.res = res;+ ocqe->cqe.flags = cflags;+ list\_add\_tail(&ocqe->list, &ctx->cq\_overflow\_list);+ return true;+}++static inline bool \_\_io\_fill\_cqe(struct io\_ring\_ctx \*ctx, u64 user\_data,+ s32 res, u32 cflags) {- struct io\_ring\_ctx \*ctx = req->ctx; struct io\_uring\_cqe \*cqe; - trace\_io\_uring\_complete(ctx, req->user\_data, res);+ trace\_io\_uring\_complete(ctx, user\_data, res, cflags);  /\* \* If we can't get a cq entry, userspace overflowed the \* submission (by quite a lot). Increment the overflow count in \* the ring. \*/- cqe = io\_get\_cqring(ctx);+ cqe = io\_get\_cqe(ctx); if (likely(cqe)) {- WRITE\_ONCE(cqe->user\_data, req->user\_data);+ WRITE\_ONCE(cqe->user\_data, user\_data); WRITE\_ONCE(cqe->res, res); WRITE\_ONCE(cqe->flags, cflags);- } else if (ctx->cq\_overflow\_flushed ||- atomic\_read(&req->task->io\_uring->in\_idle)) {- /\*- \* If we're in ring overflow flush mode, or in task cancel mode,- \* then we cannot store the request for later flushing, we need- \* to drop it on the floor.- \*/- ctx->cached\_cq\_overflow++;- WRITE\_ONCE(ctx->rings->cq\_overflow, ctx->cached\_cq\_overflow);- } else {- if (list\_empty(&ctx->cq\_overflow\_list)) {- set\_bit(0, &ctx->sq\_check\_overflow);- set\_bit(0, &ctx->cq\_check\_overflow);- ctx->rings->sq\_flags |= IORING\_SQ\_CQ\_OVERFLOW;- }- io\_clean\_op(req);- req->result = res;- req->compl.cflags = cflags;- refcount\_inc(&req->refs);- list\_add\_tail(&req->compl.list, &ctx->cq\_overflow\_list);+ return true; }+ return io\_cqring\_event\_overflow(ctx, user\_data, res, cflags); } -static void io\_cqring\_fill\_event(struct io\_kiocb \*req, long res)+static noinline void io\_fill\_cqe\_req(struct io\_kiocb \*req, s32 res, u32 cflags) {- \_\_io\_cqring\_fill\_event(req, res, 0);+ \_\_io\_fill\_cqe(req->ctx, req->user\_data, res, cflags); } -static void io\_cqring\_add\_event(struct io\_kiocb \*req, long res, long cflags)+static noinline bool io\_fill\_cqe\_aux(struct io\_ring\_ctx \*ctx, u64 user\_data,+ s32 res, u32 cflags)+{+ ctx->cq\_extra++;+ return \_\_io\_fill\_cqe(ctx, user\_data, res, cflags);+}++static void io\_req\_complete\_post(struct io\_kiocb \*req, s32 res,+ u32 cflags) { struct io\_ring\_ctx \*ctx = req->ctx;- unsigned long flags; - spin\_lock\_irqsave(&ctx->completion\_lock, flags);- \_\_io\_cqring\_fill\_event(req, res, cflags);+ spin\_lock(&ctx->completion\_lock);+ \_\_io\_fill\_cqe(ctx, req->user\_data, res, cflags);+ /\*+ \* If we're the last reference to this request, add to our locked+ \* free\_list cache.+ \*/+ if (req\_ref\_put\_and\_test(req)) {+ if (req->flags & (REQ\_F\_LINK | REQ\_F\_HARDLINK)) {+ if (req->flags & IO\_DISARM\_MASK)+ io\_disarm\_next(req);+ if (req->link) {+ io\_req\_task\_queue(req->link);+ req->link = NULL;+ }+ }+ io\_dismantle\_req(req);+ io\_put\_task(req->task, 1);+ list\_add(&req->inflight\_entry, &ctx->locked\_free\_list);+ ctx->locked\_free\_nr++;+ } else {+ if (!percpu\_ref\_tryget(&ctx->refs))+ req = NULL;+ } io\_commit\_cqring(ctx);- spin\_unlock\_irqrestore(&ctx->completion\_lock, flags);+ spin\_unlock(&ctx->completion\_lock); - io\_cqring\_ev\_posted(ctx);+ if (req) {+ io\_cqring\_ev\_posted(ctx);+ percpu\_ref\_put(&ctx->refs);+ } } -static void io\_submit\_flush\_completions(struct io\_comp\_state \*cs)+static inline bool io\_req\_needs\_clean(struct io\_kiocb \*req) {- struct io\_ring\_ctx \*ctx = cs->ctx;+ return req->flags & IO\_REQ\_CLEAN\_FLAGS;+} - spin\_lock\_irq(&ctx->completion\_lock);- while (!list\_empty(&cs->list)) {- struct io\_kiocb \*req;+static inline void io\_req\_complete\_state(struct io\_kiocb \*req, s32 res,+ u32 cflags)+{+ if (io\_req\_needs\_clean(req))+ io\_clean\_op(req);+ req->result = res;+ req->compl.cflags = cflags;+ req->flags |= REQ\_F\_COMPLETE\_INLINE;+} - req = list\_first\_entry(&cs->list, struct io\_kiocb, compl.list);- list\_del(&req->compl.list);- \_\_io\_cqring\_fill\_event(req, req->result, req->compl.cflags);+static inline void \_\_io\_req\_complete(struct io\_kiocb \*req, unsigned issue\_flags,+ s32 res, u32 cflags)+{+ if (issue\_flags & IO\_URING\_F\_COMPLETE\_DEFER)+ io\_req\_complete\_state(req, res, cflags);+ else+ io\_req\_complete\_post(req, res, cflags);+} - /\*- \* io\_free\_req() doesn't care about completion\_lock unless one- \* of these flags is set. REQ\_F\_WORK\_INITIALIZED is in the list- \* because of a potential deadlock with req->work.fs->lock- \*/- if (req->flags & (REQ\_F\_FAIL\_LINK|REQ\_F\_LINK\_TIMEOUT- |REQ\_F\_WORK\_INITIALIZED)) {- spin\_unlock\_irq(&ctx->completion\_lock);- io\_put\_req(req);- spin\_lock\_irq(&ctx->completion\_lock);- } else {- io\_put\_req(req);- }- }- io\_commit\_cqring(ctx);- spin\_unlock\_irq(&ctx->completion\_lock);+static inline void io\_req\_complete(struct io\_kiocb \*req, s32 res)+{+ \_\_io\_req\_complete(req, 0, res, 0);+} - io\_cqring\_ev\_posted(ctx);- cs->nr = 0;+static void io\_req\_complete\_failed(struct io\_kiocb \*req, s32 res)+{+ req\_set\_fail(req);+ io\_req\_complete\_post(req, res, 0); } -static void \_\_io\_req\_complete(struct io\_kiocb \*req, long res, unsigned cflags,- struct io\_comp\_state \*cs)+static void io\_req\_complete\_fail\_submit(struct io\_kiocb \*req) {- if (!cs) {- io\_cqring\_add\_event(req, res, cflags);- io\_put\_req(req);- } else {- io\_clean\_op(req);- req->result = res;- req->compl.cflags = cflags;- list\_add\_tail(&req->compl.list, &cs->list);- if (++cs->nr >= 32)- io\_submit\_flush\_completions(cs);- }+ /\*+ \* We don't submit, fail them all, for that replace hardlinks with+ \* normal links. Extra REQ\_F\_LINK is tolerated.+ \*/+ req->flags &= ~REQ\_F\_HARDLINK;+ req->flags |= REQ\_F\_LINK;+ io\_req\_complete\_failed(req, req->result); } -static void io\_req\_complete(struct io\_kiocb \*req, long res)+/\*+ \* Don't initialise the fields below on every allocation, but do that in+ \* advance and keep them valid across allocations.+ \*/+static void io\_preinit\_req(struct io\_kiocb \*req, struct io\_ring\_ctx \*ctx) {- \_\_io\_req\_complete(req, res, 0, NULL);+ req->ctx = ctx;+ req->link = NULL;+ req->async\_data = NULL;+ /\* not necessary, but safer to zero \*/+ req->result = 0; } -static inline bool io\_is\_fallback\_req(struct io\_kiocb \*req)+static void io\_flush\_cached\_locked\_reqs(struct io\_ring\_ctx \*ctx,+ struct io\_submit\_state \*state) {- return req == (struct io\_kiocb \*)- ((unsigned long) req->ctx->fallback\_req & ~1UL);+ spin\_lock(&ctx->completion\_lock);+ list\_splice\_init(&ctx->locked\_free\_list, &state->free\_list);+ ctx->locked\_free\_nr = 0;+ spin\_unlock(&ctx->completion\_lock); } -static struct io\_kiocb \*io\_get\_fallback\_req(struct io\_ring\_ctx \*ctx)+/\* Returns true IFF there are requests in the cache \*/+static bool io\_flush\_cached\_reqs(struct io\_ring\_ctx \*ctx) {- struct io\_kiocb \*req;+ struct io\_submit\_state \*state = &ctx->submit\_state;+ int nr; - req = ctx->fallback\_req;- if (!test\_and\_set\_bit\_lock(0, (unsigned long \*) &ctx->fallback\_req))- return req;+ /\*+ \* If we have more than a batch's worth of requests in our IRQ side+ \* locked cache, grab the lock and move them over to our submission+ \* side cache.+ \*/+ if (READ\_ONCE(ctx->locked\_free\_nr) > IO\_COMPL\_BATCH)+ io\_flush\_cached\_locked\_reqs(ctx, state); - return NULL;+ nr = state->free\_reqs;+ while (!list\_empty(&state->free\_list)) {+ struct io\_kiocb \*req = list\_first\_entry(&state->free\_list,+ struct io\_kiocb, inflight\_entry);++ list\_del(&req->inflight\_entry);+ state->reqs[nr++] = req;+ if (nr == ARRAY\_SIZE(state->reqs))+ break;+ }++ state->free\_reqs = nr;+ return nr != 0; } -static struct io\_kiocb \*io\_alloc\_req(struct io\_ring\_ctx \*ctx,- struct io\_submit\_state \*state)+/\*+ \* A request might get retired back into the request caches even before opcode+ \* handlers and io\_issue\_sqe() are done with it, e.g. inline completion path.+ \* Because of that, io\_alloc\_req() should be called only under ->uring\_lock+ \* and with extra caution to not get a request that is still worked on.+ \*/+static struct io\_kiocb \*io\_alloc\_req(struct io\_ring\_ctx \*ctx)+ \_\_must\_hold(&ctx->uring\_lock) {- if (!state->free\_reqs) {- gfp\_t gfp = GFP\_KERNEL | \_\_GFP\_NOWARN;- size\_t sz;- int ret;+ struct io\_submit\_state \*state = &ctx->submit\_state;+ gfp\_t gfp = GFP\_KERNEL | \_\_GFP\_NOWARN;+ int ret, i; - sz = min\_t(size\_t, state->ios\_left, ARRAY\_SIZE(state->reqs));- ret = kmem\_cache\_alloc\_bulk(req\_cachep, gfp, sz, state->reqs);+ BUILD\_BUG\_ON(ARRAY\_SIZE(state->reqs) < IO\_REQ\_ALLOC\_BATCH); - /\*- \* Bulk alloc is all-or-nothing. If we fail to get a batch,- \* retry single alloc to be on the safe side.- \*/- if (unlikely(ret <= 0)) {- state->reqs[0] = kmem\_cache\_alloc(req\_cachep, gfp);- if (!state->reqs[0])- goto fallback;- ret = 1;- }- state->free\_reqs = ret;+ if (likely(state->free\_reqs || io\_flush\_cached\_reqs(ctx)))+ goto got\_req;++ ret = kmem\_cache\_alloc\_bulk(req\_cachep, gfp, IO\_REQ\_ALLOC\_BATCH,+ state->reqs);++ /\*+ \* Bulk alloc is all-or-nothing. If we fail to get a batch,+ \* retry single alloc to be on the safe side.+ \*/+ if (unlikely(ret <= 0)) {+ state->reqs[0] = kmem\_cache\_alloc(req\_cachep, gfp);+ if (!state->reqs[0])+ return NULL;+ ret = 1; } + for (i = 0; i < ret; i++)+ io\_preinit\_req(state->reqs[i], ctx);+ state->free\_reqs = ret;+got\_req: state->free\_reqs--; return state->reqs[state->free\_reqs];-fallback:- return io\_get\_fallback\_req(ctx); } -static inline void io\_put\_file(struct io\_kiocb \*req, struct file \*file,- bool fixed)+static inline void io\_put\_file(struct file \*file) {- if (fixed)- percpu\_ref\_put(req->fixed\_file\_refs);- else+ if (file) fput(file); }  static void io\_dismantle\_req(struct io\_kiocb \*req) {- io\_clean\_op(req);+ unsigned int flags = req->flags; - if (req->async\_data)+ if (io\_req\_needs\_clean(req))+ io\_clean\_op(req);+ if (!(flags & REQ\_F\_FIXED\_FILE))+ io\_put\_file(req->file);+ if (req->fixed\_rsrc\_refs)+ percpu\_ref\_put(req->fixed\_rsrc\_refs);+ if (req->async\_data) { kfree(req->async\_data);- if (req->file)- io\_put\_file(req, req->file, (req->flags & REQ\_F\_FIXED\_FILE));-- io\_req\_clean\_work(req);+ req->async\_data = NULL;+ } }  static void \_\_io\_free\_req(struct io\_kiocb \*req) {- struct io\_uring\_task \*tctx = req->task->io\_uring; struct io\_ring\_ctx \*ctx = req->ctx;  io\_dismantle\_req(req);+ io\_put\_task(req->task, 1); - percpu\_counter\_dec(&tctx->inflight);- if (atomic\_read(&tctx->in\_idle))- wake\_up(&tctx->wait);- put\_task\_struct(req->task);+ spin\_lock(&ctx->completion\_lock);+ list\_add(&req->inflight\_entry, &ctx->locked\_free\_list);+ ctx->locked\_free\_nr++;+ spin\_unlock(&ctx->completion\_lock); - if (likely(!io\_is\_fallback\_req(req)))- kmem\_cache\_free(req\_cachep, req);- else- clear\_bit\_unlock(0, (unsigned long \*) &ctx->fallback\_req); percpu\_ref\_put(&ctx->refs); } -static void io\_kill\_linked\_timeout(struct io\_kiocb \*req)+static inline void io\_remove\_next\_linked(struct io\_kiocb \*req) {- struct io\_ring\_ctx \*ctx = req->ctx;- struct io\_kiocb \*link;- bool cancelled = false;- unsigned long flags;-- spin\_lock\_irqsave(&ctx->completion\_lock, flags);- link = list\_first\_entry\_or\_null(&req->link\_list, struct io\_kiocb,- link\_list);- /\*- \* Can happen if a linked timeout fired and link had been like- \* req -> link t-out -> link t-out [-> ...]- \*/- if (link && (link->flags & REQ\_F\_LTIMEOUT\_ACTIVE)) {- struct io\_timeout\_data \*io = link->async\_data;- int ret;-- list\_del\_init(&link->link\_list);- ret = hrtimer\_try\_to\_cancel(&io->timer);- if (ret != -1) {- io\_cqring\_fill\_event(link, -ECANCELED);- io\_commit\_cqring(ctx);- cancelled = true;- }- }- req->flags &= ~REQ\_F\_LINK\_TIMEOUT;- spin\_unlock\_irqrestore(&ctx->completion\_lock, flags);+ struct io\_kiocb \*nxt = req->link; - if (cancelled) {- io\_cqring\_ev\_posted(ctx);- io\_put\_req(link);- }+ req->link = nxt->link;+ nxt->link = NULL; } -static struct io\_kiocb \*io\_req\_link\_next(struct io\_kiocb \*req)+static bool io\_kill\_linked\_timeout(struct io\_kiocb \*req)+ \_\_must\_hold(&req->ctx->completion\_lock)+ \_\_must\_hold(&req->ctx->timeout\_lock) {- struct io\_kiocb \*nxt;+ struct io\_kiocb \*link = req->link; - /\*- \* The list should never be empty when we are called here. But could- \* potentially happen if the chain is messed up, check to be on the- \* safe side.- \*/- if (unlikely(list\_empty(&req->link\_list)))- return NULL;+ if (link && link->opcode == IORING\_OP\_LINK\_TIMEOUT) {+ struct io\_timeout\_data \*io = link->async\_data; - nxt = list\_first\_entry(&req->link\_list, struct io\_kiocb, link\_list);- list\_del\_init(&req->link\_list);- if (!list\_empty(&nxt->link\_list))- nxt->flags |= REQ\_F\_LINK\_HEAD;- return nxt;+ io\_remove\_next\_linked(req);+ link->timeout.head = NULL;+ if (hrtimer\_try\_to\_cancel(&io->timer) != -1) {+ list\_del(&link->timeout.list);+ io\_fill\_cqe\_req(link, -ECANCELED, 0);+ io\_put\_req\_deferred(link);+ return true;+ }+ }+ return false; } -/\*- \* Called if REQ\_F\_LINK\_HEAD is set, and we fail the head request- \*/ static void io\_fail\_links(struct io\_kiocb \*req)+ \_\_must\_hold(&req->ctx->completion\_lock) {- struct io\_ring\_ctx \*ctx = req->ctx;- unsigned long flags;+ struct io\_kiocb \*nxt, \*link = req->link; - spin\_lock\_irqsave(&ctx->completion\_lock, flags);- while (!list\_empty(&req->link\_list)) {- struct io\_kiocb \*link = list\_first\_entry(&req->link\_list,- struct io\_kiocb, link\_list);+ req->link = NULL;+ while (link) {+ long res = -ECANCELED; - list\_del\_init(&link->link\_list);- trace\_io\_uring\_fail\_link(req, link);+ if (link->flags & REQ\_F\_FAIL)+ res = link->result; - io\_cqring\_fill\_event(link, -ECANCELED);+ nxt = link->link;+ link->link = NULL; - /\*- \* It's ok to free under spinlock as they're not linked anymore,- \* but avoid REQ\_F\_WORK\_INITIALIZED because it may deadlock on- \* work.fs->lock.- \*/- if (link->flags & REQ\_F\_WORK\_INITIALIZED)- io\_put\_req\_deferred(link, 2);- else- io\_double\_put\_req(link);+ trace\_io\_uring\_fail\_link(req, link);+ io\_fill\_cqe\_req(link, res, 0);+ io\_put\_req\_deferred(link);+ link = nxt; }+} - io\_commit\_cqring(ctx);- spin\_unlock\_irqrestore(&ctx->completion\_lock, flags);+static bool io\_disarm\_next(struct io\_kiocb \*req)+ \_\_must\_hold(&req->ctx->completion\_lock)+{+ bool posted = false; - io\_cqring\_ev\_posted(ctx);+ if (req->flags & REQ\_F\_ARM\_LTIMEOUT) {+ struct io\_kiocb \*link = req->link;++ req->flags &= ~REQ\_F\_ARM\_LTIMEOUT;+ if (link && link->opcode == IORING\_OP\_LINK\_TIMEOUT) {+ io\_remove\_next\_linked(req);+ io\_fill\_cqe\_req(link, -ECANCELED, 0);+ io\_put\_req\_deferred(link);+ posted = true;+ }+ } else if (req->flags & REQ\_F\_LINK\_TIMEOUT) {+ struct io\_ring\_ctx \*ctx = req->ctx;++ spin\_lock\_irq(&ctx->timeout\_lock);+ posted = io\_kill\_linked\_timeout(req);+ spin\_unlock\_irq(&ctx->timeout\_lock);+ }+ if (unlikely((req->flags & REQ\_F\_FAIL) &&+ !(req->flags & REQ\_F\_HARDLINK))) {+ posted |= (req->link != NULL);+ io\_fail\_links(req);+ }+ return posted; }  static struct io\_kiocb \*\_\_io\_req\_find\_next(struct io\_kiocb \*req) {- req->flags &= ~REQ\_F\_LINK\_HEAD;- if (req->flags & REQ\_F\_LINK\_TIMEOUT)- io\_kill\_linked\_timeout(req);+ struct io\_kiocb \*nxt;  /\* \* If LINK is set, we have dependent requests in this chain. If we@@ -2034,28 +2118,112 @@ static struct io\_kiocb \*\_\_io\_req\_find\_next(struct io\_kiocb \*req) \* dependencies to the next request. In case of failure, fail the rest \* of the chain. \*/- if (likely(!(req->flags & REQ\_F\_FAIL\_LINK)))- return io\_req\_link\_next(req);- io\_fail\_links(req);- return NULL;+ if (req->flags & IO\_DISARM\_MASK) {+ struct io\_ring\_ctx \*ctx = req->ctx;+ bool posted;++ spin\_lock(&ctx->completion\_lock);+ posted = io\_disarm\_next(req);+ if (posted)+ io\_commit\_cqring(req->ctx);+ spin\_unlock(&ctx->completion\_lock);+ if (posted)+ io\_cqring\_ev\_posted(ctx);+ }+ nxt = req->link;+ req->link = NULL;+ return nxt; } -static struct io\_kiocb \*io\_req\_find\_next(struct io\_kiocb \*req)+static inline struct io\_kiocb \*io\_req\_find\_next(struct io\_kiocb \*req) {- if (likely(!(req->flags & REQ\_F\_LINK\_HEAD)))+ if (likely(!(req->flags & (REQ\_F\_LINK|REQ\_F\_HARDLINK)))) return NULL; return \_\_io\_req\_find\_next(req); } -static int io\_req\_task\_work\_add(struct io\_kiocb \*req, bool twa\_signal\_ok)+static void ctx\_flush\_and\_put(struct io\_ring\_ctx \*ctx, bool \*locked)+{+ if (!ctx)+ return;+ if (\*locked) {+ if (ctx->submit\_state.compl\_nr)+ io\_submit\_flush\_completions(ctx);+ mutex\_unlock(&ctx->uring\_lock);+ \*locked = false;+ }+ percpu\_ref\_put(&ctx->refs);+}++static void tctx\_task\_work(struct callback\_head \*cb)+{+ bool locked = false;+ struct io\_ring\_ctx \*ctx = NULL;+ struct io\_uring\_task \*tctx = container\_of(cb, struct io\_uring\_task,+ task\_work);++ while (1) {+ struct io\_wq\_work\_node \*node;++ if (!tctx->task\_list.first && locked && ctx->submit\_state.compl\_nr)+ io\_submit\_flush\_completions(ctx);++ spin\_lock\_irq(&tctx->task\_lock);+ node = tctx->task\_list.first;+ INIT\_WQ\_LIST(&tctx->task\_list);+ if (!node)+ tctx->task\_running = false;+ spin\_unlock\_irq(&tctx->task\_lock);+ if (!node)+ break;++ do {+ struct io\_wq\_work\_node \*next = node->next;+ struct io\_kiocb \*req = container\_of(node, struct io\_kiocb,+ io\_task\_work.node);++ if (req->ctx != ctx) {+ ctx\_flush\_and\_put(ctx, &locked);+ ctx = req->ctx;+ /\* if not contended, grab and improve batching \*/+ locked = mutex\_trylock(&ctx->uring\_lock);+ percpu\_ref\_get(&ctx->refs);+ }+ req->io\_task\_work.func(req, &locked);+ node = next;+ } while (node);++ cond\_resched();+ }++ ctx\_flush\_and\_put(ctx, &locked);++ /\* relaxed read is enough as only the task itself sets ->in\_idle \*/+ if (unlikely(atomic\_read(&tctx->in\_idle)))+ io\_uring\_drop\_tctx\_refs(current);+}++static void io\_req\_task\_work\_add(struct io\_kiocb \*req) { struct task\_struct \*tsk = req->task;- struct io\_ring\_ctx \*ctx = req->ctx;+ struct io\_uring\_task \*tctx = tsk->io\_uring; enum task\_work\_notify\_mode notify;- int ret;+ struct io\_wq\_work\_node \*node;+ unsigned long flags;+ bool running; - if (tsk->flags & PF\_EXITING)- return -ESRCH;+ WARN\_ON\_ONCE(!tctx);++ spin\_lock\_irqsave(&tctx->task\_lock, flags);+ wq\_list\_add\_tail(&req->io\_task\_work.node, &tctx->task\_list);+ running = tctx->task\_running;+ if (!running)+ tctx->task\_running = true;+ spin\_unlock\_irqrestore(&tctx->task\_lock, flags);++ /\* task\_work already pending, we're done \*/+ if (running)+ return;  /\* \* SQPOLL kernel thread doesn't need notification, just a wakeup. For@@ -2063,85 +2231,68 @@ static int io\_req\_task\_work\_add(struct io\_kiocb \*req, bool twa\_signal\_ok) \* processing task\_work. There's no reliable way to tell if TWA\_RESUME \* will do the job. \*/- notify = TWA\_NONE;- if (!(ctx->flags & IORING\_SETUP\_SQPOLL) && twa\_signal\_ok)- notify = TWA\_SIGNAL;-- ret = task\_work\_add(tsk, &req->task\_work, notify);- if (!ret)+ notify = (req->ctx->flags & IORING\_SETUP\_SQPOLL) ? TWA\_NONE : TWA\_SIGNAL;+ if (!task\_work\_add(tsk, &tctx->task\_work, notify)) { wake\_up\_process(tsk);+ return;+ } - return ret;-}--static void \_\_io\_req\_task\_cancel(struct io\_kiocb \*req, int error)-{- struct io\_ring\_ctx \*ctx = req->ctx;-- spin\_lock\_irq(&ctx->completion\_lock);- io\_cqring\_fill\_event(req, error);- io\_commit\_cqring(ctx);- spin\_unlock\_irq(&ctx->completion\_lock);+ spin\_lock\_irqsave(&tctx->task\_lock, flags);+ tctx->task\_running = false;+ node = tctx->task\_list.first;+ INIT\_WQ\_LIST(&tctx->task\_list);+ spin\_unlock\_irqrestore(&tctx->task\_lock, flags); - io\_cqring\_ev\_posted(ctx);- req\_set\_fail\_links(req);- io\_double\_put\_req(req);+ while (node) {+ req = container\_of(node, struct io\_kiocb, io\_task\_work.node);+ node = node->next;+ if (llist\_add(&req->io\_task\_work.fallback\_node,+ &req->ctx->fallback\_llist))+ schedule\_delayed\_work(&req->ctx->fallback\_work, 1);+ } } -static void io\_req\_task\_cancel(struct callback\_head \*cb)+static void io\_req\_task\_cancel(struct io\_kiocb \*req, bool \*locked) {- struct io\_kiocb \*req = container\_of(cb, struct io\_kiocb, task\_work); struct io\_ring\_ctx \*ctx = req->ctx; - mutex\_lock(&ctx->uring\_lock);- \_\_io\_req\_task\_cancel(req, -ECANCELED);- mutex\_unlock(&ctx->uring\_lock);- percpu\_ref\_put(&ctx->refs);+ /\* not needed for normal modes, but SQPOLL depends on it \*/+ io\_tw\_lock(ctx, locked);+ io\_req\_complete\_failed(req, req->result); } -static void \_\_io\_req\_task\_submit(struct io\_kiocb \*req)+static void io\_req\_task\_submit(struct io\_kiocb \*req, bool \*locked) { struct io\_ring\_ctx \*ctx = req->ctx; - mutex\_lock(&ctx->uring\_lock);- if (!ctx->sqo\_dead && !\_\_io\_sq\_thread\_acquire\_mm(ctx))- \_\_io\_queue\_sqe(req, NULL);+ io\_tw\_lock(ctx, locked);+ /\* req->task == current here, checking PF\_EXITING is safe \*/+ if (likely(!(req->task->flags & PF\_EXITING)))+ \_\_io\_queue\_sqe(req); else- \_\_io\_req\_task\_cancel(req, -EFAULT);- mutex\_unlock(&ctx->uring\_lock);-- if (ctx->flags & IORING\_SETUP\_SQPOLL)- io\_sq\_thread\_drop\_mm();+ io\_req\_complete\_failed(req, -EFAULT); } -static void io\_req\_task\_submit(struct callback\_head \*cb)+static void io\_req\_task\_queue\_fail(struct io\_kiocb \*req, int ret) {- struct io\_kiocb \*req = container\_of(cb, struct io\_kiocb, task\_work);- struct io\_ring\_ctx \*ctx = req->ctx;-- \_\_io\_req\_task\_submit(req);- percpu\_ref\_put(&ctx->refs);+ req->result = ret;+ req->io\_task\_work.func = io\_req\_task\_cancel;+ io\_req\_task\_work\_add(req); }  static void io\_req\_task\_queue(struct io\_kiocb \*req) {- int ret;-- init\_task\_work(&req->task\_work, io\_req\_task\_submit);- percpu\_ref\_get(&req->ctx->refs);-- ret = io\_req\_task\_work\_add(req, true);- if (unlikely(ret)) {- struct task\_struct \*tsk;+ req->io\_task\_work.func = io\_req\_task\_submit;+ io\_req\_task\_work\_add(req);+} - init\_task\_work(&req->task\_work, io\_req\_task\_cancel);- tsk = io\_wq\_get\_task(req->ctx->io\_wq);- task\_work\_add(tsk, &req->task\_work, TWA\_NONE);- wake\_up\_process(tsk);- }+static void io\_req\_task\_queue\_reissue(struct io\_kiocb \*req)+{+ req->io\_task\_work.func = io\_queue\_async\_work;+ io\_req\_task\_work\_add(req); } -static void io\_queue\_next(struct io\_kiocb \*req)+static inline void io\_queue\_next(struct io\_kiocb \*req) { struct io\_kiocb \*nxt = io\_req\_find\_next(req); @@ -2155,153 +2306,118 @@ static void io\_free\_req(struct io\_kiocb \*req) \_\_io\_free\_req(req); } -struct req\_batch {- void \*reqs[IO\_IOPOLL\_BATCH];- int to\_free;+static void io\_free\_req\_work(struct io\_kiocb \*req, bool \*locked)+{+ io\_free\_req(req);+} +struct req\_batch { struct task\_struct \*task; int task\_refs;+ int ctx\_refs; };  static inline void io\_init\_req\_batch(struct req\_batch \*rb) {- rb->to\_free = 0; rb->task\_refs = 0;+ rb->ctx\_refs = 0; rb->task = NULL; } -static void \_\_io\_req\_free\_batch\_flush(struct io\_ring\_ctx \*ctx,- struct req\_batch \*rb)+static void io\_req\_free\_batch\_finish(struct io\_ring\_ctx \*ctx,+ struct req\_batch \*rb) {- kmem\_cache\_free\_bulk(req\_cachep, rb->to\_free, rb->reqs);- percpu\_ref\_put\_many(&ctx->refs, rb->to\_free);- rb->to\_free = 0;+ if (rb->ctx\_refs)+ percpu\_ref\_put\_many(&ctx->refs, rb->ctx\_refs);+ if (rb->task)+ io\_put\_task(rb->task, rb->task\_refs); } -static void io\_req\_free\_batch\_finish(struct io\_ring\_ctx \*ctx,- struct req\_batch \*rb)+static void io\_req\_free\_batch(struct req\_batch \*rb, struct io\_kiocb \*req,+ struct io\_submit\_state \*state) {- if (rb->to\_free)- \_\_io\_req\_free\_batch\_flush(ctx, rb);- if (rb->task) {- struct io\_uring\_task \*tctx = rb->task->io\_uring;+ io\_queue\_next(req);+ io\_dismantle\_req(req); - percpu\_counter\_sub(&tctx->inflight, rb->task\_refs);- if (atomic\_read(&tctx->in\_idle))- wake\_up(&tctx->wait);- put\_task\_struct\_many(rb->task, rb->task\_refs);- rb->task = NULL;+ if (req->task != rb->task) {+ if (rb->task)+ io\_put\_task(rb->task, rb->task\_refs);+ rb->task = req->task;+ rb->task\_refs = 0; }+ rb->task\_refs++;+ rb->ctx\_refs++;++ if (state->free\_reqs != ARRAY\_SIZE(state->reqs))+ state->reqs[state->free\_reqs++] = req;+ else+ list\_add(&req->inflight\_entry, &state->free\_list); } -static void io\_req\_free\_batch(struct req\_batch \*rb, struct io\_kiocb \*req)+static void io\_submit\_flush\_completions(struct io\_ring\_ctx \*ctx)+ \_\_must\_hold(&ctx->uring\_lock) {- if (unlikely(io\_is\_fallback\_req(req))) {- io\_free\_req(req);- return;+ struct io\_submit\_state \*state = &ctx->submit\_state;+ int i, nr = state->compl\_nr;+ struct req\_batch rb;++ spin\_lock(&ctx->completion\_lock);+ for (i = 0; i < nr; i++) {+ struct io\_kiocb \*req = state->compl\_reqs[i];++ \_\_io\_fill\_cqe(ctx, req->user\_data, req->result,+ req->compl.cflags); }- if (req->flags & REQ\_F\_LINK\_HEAD)- io\_queue\_next(req);+ io\_commit\_cqring(ctx);+ spin\_unlock(&ctx->completion\_lock);+ io\_cqring\_ev\_posted(ctx); - if (req->task != rb->task) {- if (rb->task) {- struct io\_uring\_task \*tctx = rb->task->io\_uring;+ io\_init\_req\_batch(&rb);+ for (i = 0; i < nr; i++) {+ struct io\_kiocb \*req = state->compl\_reqs[i]; - percpu\_counter\_sub(&tctx->inflight, rb->task\_refs);- if (atomic\_read(&tctx->in\_idle))- wake\_up(&tctx->wait);- put\_task\_struct\_many(rb->task, rb->task\_refs);- }- rb->task = req->task;- rb->task\_refs = 0;+ if (req\_ref\_put\_and\_test(req))+ io\_req\_free\_batch(&rb, req, &ctx->submit\_state); }- rb->task\_refs++; - io\_dismantle\_req(req);- rb->reqs[rb->to\_free++] = req;- if (unlikely(rb->to\_free == ARRAY\_SIZE(rb->reqs)))- \_\_io\_req\_free\_batch\_flush(req->ctx, rb);+ io\_req\_free\_batch\_finish(ctx, &rb);+ state->compl\_nr = 0; }  /\* \* Drop reference to request, return next in chain (if there is one) if this \* was the last reference to this request. \*/-static struct io\_kiocb \*io\_put\_req\_find\_next(struct io\_kiocb \*req)+static inline struct io\_kiocb \*io\_put\_req\_find\_next(struct io\_kiocb \*req) { struct io\_kiocb \*nxt = NULL; - if (refcount\_dec\_and\_test(&req->refs)) {+ if (req\_ref\_put\_and\_test(req)) { nxt = io\_req\_find\_next(req); \_\_io\_free\_req(req); } return nxt; } -static void io\_put\_req(struct io\_kiocb \*req)+static inline void io\_put\_req(struct io\_kiocb \*req) {- if (refcount\_dec\_and\_test(&req->refs))+ if (req\_ref\_put\_and\_test(req)) io\_free\_req(req); } -static void io\_put\_req\_deferred\_cb(struct callback\_head \*cb)+static inline void io\_put\_req\_deferred(struct io\_kiocb \*req) {- struct io\_kiocb \*req = container\_of(cb, struct io\_kiocb, task\_work);-- io\_free\_req(req);-}--static void io\_free\_req\_deferred(struct io\_kiocb \*req)-{- int ret;-- init\_task\_work(&req->task\_work, io\_put\_req\_deferred\_cb);- ret = io\_req\_task\_work\_add(req, true);- if (unlikely(ret)) {- struct task\_struct \*tsk;-- tsk = io\_wq\_get\_task(req->ctx->io\_wq);- task\_work\_add(tsk, &req->task\_work, TWA\_NONE);- wake\_up\_process(tsk);+ if (req\_ref\_put\_and\_test(req)) {+ req->io\_task\_work.func = io\_free\_req\_work;+ io\_req\_task\_work\_add(req); } } -static inline void io\_put\_req\_deferred(struct io\_kiocb \*req, int refs)-{- if (refcount\_sub\_and\_test(refs, &req->refs))- io\_free\_req\_deferred(req);-}--static struct io\_wq\_work \*io\_steal\_work(struct io\_kiocb \*req)-{- struct io\_kiocb \*nxt;-- /\*- \* A ref is owned by io-wq in which context we're. So, if that's the- \* last one, it's safe to steal next work. False negatives are Ok,- \* it just will be re-punted async in io\_put\_work()- \*/- if (refcount\_read(&req->refs) != 1)- return NULL;-- nxt = io\_req\_find\_next(req);- return nxt ? &nxt->work : NULL;-}--static void io\_double\_put\_req(struct io\_kiocb \*req)-{- /\* drop both submit and complete references \*/- if (refcount\_sub\_and\_test(2, &req->refs))- io\_free\_req(req);-}- static unsigned io\_cqring\_events(struct io\_ring\_ctx \*ctx) {- struct io\_rings \*rings = ctx->rings;- /\* See comment at the top of this file \*/ smp\_rmb();- return ctx->cached\_cq\_tail - READ\_ONCE(rings->cq.head);+ return \_\_io\_cqring\_events(ctx); }  static inline unsigned int io\_sqring\_entries(struct io\_ring\_ctx \*ctx)@@ -2327,38 +2443,23 @@ static inline unsigned int io\_put\_rw\_kbuf(struct io\_kiocb \*req) { struct io\_buffer \*kbuf; + if (likely(!(req->flags & REQ\_F\_BUFFER\_SELECTED)))+ return 0; kbuf = (struct io\_buffer \*) (unsigned long) req->rw.addr; return io\_put\_kbuf(req, kbuf); }  static inline bool io\_run\_task\_work(void) {- /\*- \* Not safe to run on exiting task, and the task\_work handling will- \* not add work to such a task.- \*/- if (unlikely(current->flags & PF\_EXITING))- return false;- if (current->task\_works) {+ if (test\_thread\_flag(TIF\_NOTIFY\_SIGNAL) || current->task\_works) { \_\_set\_current\_state(TASK\_RUNNING);- task\_work\_run();+ tracehook\_notify\_signal(); return true; }  return false; } -static void io\_iopoll\_queue(struct list\_head \*again)-{- struct io\_kiocb \*req;-- do {- req = list\_first\_entry(again, struct io\_kiocb, iopoll\_entry);- list\_del(&req->iopoll\_entry);- \_\_io\_complete\_rw(req, -EAGAIN, 0, NULL);- } while (!list\_empty(again));-}- /\* \* Find and free completed poll iocbs \*/@@ -2367,41 +2468,25 @@ static void io\_iopoll\_complete(struct io\_ring\_ctx \*ctx, unsigned int \*nr\_events, { struct req\_batch rb; struct io\_kiocb \*req;- LIST\_HEAD(again);  /\* order with ->result store in io\_complete\_rw\_iopoll() \*/ smp\_rmb();  io\_init\_req\_batch(&rb); while (!list\_empty(done)) {- int cflags = 0;+ req = list\_first\_entry(done, struct io\_kiocb, inflight\_entry);+ list\_del(&req->inflight\_entry); - req = list\_first\_entry(done, struct io\_kiocb, iopoll\_entry);- if (READ\_ONCE(req->result) == -EAGAIN) {- req->result = 0;- req->iopoll\_completed = 0;- list\_move\_tail(&req->iopoll\_entry, &again);- continue;- }- list\_del(&req->iopoll\_entry);-- if (req->flags & REQ\_F\_BUFFER\_SELECTED)- cflags = io\_put\_rw\_kbuf(req);-- \_\_io\_cqring\_fill\_event(req, req->result, cflags);+ io\_fill\_cqe\_req(req, req->result, io\_put\_rw\_kbuf(req)); (\*nr\_events)++; - if (refcount\_dec\_and\_test(&req->refs))- io\_req\_free\_batch(&rb, req);+ if (req\_ref\_put\_and\_test(req))+ io\_req\_free\_batch(&rb, req, &ctx->submit\_state); }  io\_commit\_cqring(ctx);- if (ctx->flags & IORING\_SETUP\_SQPOLL)- io\_cqring\_ev\_posted(ctx);+ io\_cqring\_ev\_posted\_iopoll(ctx); io\_req\_free\_batch\_finish(ctx, &rb);-- if (!list\_empty(&again))- io\_iopoll\_queue(&again); }  static int io\_do\_iopoll(struct io\_ring\_ctx \*ctx, unsigned int \*nr\_events,@@ -2410,17 +2495,16 @@ static int io\_do\_iopoll(struct io\_ring\_ctx \*ctx, unsigned int \*nr\_events, struct io\_kiocb \*req, \*tmp; LIST\_HEAD(done); bool spin;- int ret;  /\* \* Only spin for completions if we don't have multiple devices hanging \* off our complete list, and we're under the requested amount. \*/- spin = !ctx->poll\_multi\_file && \*nr\_events < min;+ spin = !ctx->poll\_multi\_queue && \*nr\_events < min; - ret = 0;- list\_for\_each\_entry\_safe(req, tmp, &ctx->iopoll\_list, iopoll\_entry) {+ list\_for\_each\_entry\_safe(req, tmp, &ctx->iopoll\_list, inflight\_entry) { struct kiocb \*kiocb = &req->rw.kiocb;+ int ret;  /\* \* Move completed and retryable entries to our local lists.@@ -2428,50 +2512,27 @@ static int io\_do\_iopoll(struct io\_ring\_ctx \*ctx, unsigned int \*nr\_events, \* and complete those lists first, if we have entries there. \*/ if (READ\_ONCE(req->iopoll\_completed)) {- list\_move\_tail(&req->iopoll\_entry, &done);+ list\_move\_tail(&req->inflight\_entry, &done); continue; } if (!list\_empty(&done)) break;  ret = kiocb->ki\_filp->f\_op->iopoll(kiocb, spin);- if (ret < 0)- break;+ if (unlikely(ret < 0))+ return ret;+ else if (ret)+ spin = false;  /\* iopoll may have completed current req \*/ if (READ\_ONCE(req->iopoll\_completed))- list\_move\_tail(&req->iopoll\_entry, &done);-- if (ret && spin)- spin = false;- ret = 0;+ list\_move\_tail(&req->inflight\_entry, &done); }  if (!list\_empty(&done)) io\_iopoll\_complete(ctx, nr\_events, &done); - return ret;-}--/\*- \* Poll for a minimum of 'min' events. Note that if min == 0 we consider that a- \* non-spinning poll check - we'll still enter the driver poll loop, but only- \* as a non-spinning completion check.- \*/-static int io\_iopoll\_getevents(struct io\_ring\_ctx \*ctx, unsigned int \*nr\_events,- long min)-{- while (!list\_empty(&ctx->iopoll\_list) && !need\_resched()) {- int ret;-- ret = io\_do\_iopoll(ctx, nr\_events, min);- if (ret < 0)- return ret;- if (\*nr\_events >= min)- return 0;- }-- return 1;+ return 0; }  /\*@@ -2509,7 +2570,7 @@ static void io\_iopoll\_try\_reap\_events(struct io\_ring\_ctx \*ctx) static int io\_iopoll\_check(struct io\_ring\_ctx \*ctx, long min) { unsigned int nr\_events = 0;- int iters = 0, ret = 0;+ int ret = 0;  /\* \* We disallow the app entering submit/complete with polling, but we@@ -2517,18 +2578,17 @@ static int io\_iopoll\_check(struct io\_ring\_ctx \*ctx, long min) \* that got punted to a workqueue. \*/ mutex\_lock(&ctx->uring\_lock);+ /\*+ \* Don't enter poll loop if we already have events pending.+ \* If we do, we can potentially be spinning for commands that+ \* already triggered a CQE (eg in error).+ \*/+ if (test\_bit(0, &ctx->check\_cq\_overflow))+ \_\_io\_cqring\_overflow\_flush(ctx, false);+ if (io\_cqring\_events(ctx))+ goto out; do { /\*- \* Don't enter poll loop if we already have events pending.- \* If we do, we can potentially be spinning for commands that- \* already triggered a CQE (eg in error).- \*/- if (test\_bit(0, &ctx->cq\_check\_overflow))- \_\_io\_cqring\_overflow\_flush(ctx, false, NULL, NULL);- if (io\_cqring\_events(ctx))- break;-- /\* \* If a submit got punted to a workqueue, we can have the \* application entering polling for a command before it gets \* issued. That app will hold the uring\_lock for the duration@@ -2538,18 +2598,21 @@ static int io\_iopoll\_check(struct io\_ring\_ctx \*ctx, long min) \* forever, while the workqueue is stuck trying to acquire the \* very same mutex. \*/- if (!(++iters & 7)) {+ if (list\_empty(&ctx->iopoll\_list)) {+ u32 tail = ctx->cached\_cq\_tail;+ mutex\_unlock(&ctx->uring\_lock); io\_run\_task\_work(); mutex\_lock(&ctx->uring\_lock);- }-- ret = io\_iopoll\_getevents(ctx, &nr\_events, min);- if (ret <= 0)- break;- ret = 0;- } while (min && !nr\_events && !need\_resched()); + /\* some requests don't go through iopoll\_list \*/+ if (tail != ctx->cached\_cq\_tail ||+ list\_empty(&ctx->iopoll\_list))+ break;+ }+ ret = io\_do\_iopoll(ctx, &nr\_events, min);+ } while (!ret && nr\_events < min && !need\_resched());+out: mutex\_unlock(&ctx->uring\_lock); return ret; }@@ -2561,79 +2624,129 @@ static void kiocb\_end\_write(struct io\_kiocb \*req) \* thread. \*/ if (req->flags & REQ\_F\_ISREG) {- struct inode \*inode = file\_inode(req->file);+ struct super\_block \*sb = file\_inode(req->file)->i\_sb; - \_\_sb\_writers\_acquired(inode->i\_sb, SB\_FREEZE\_WRITE);+ \_\_sb\_writers\_acquired(sb, SB\_FREEZE\_WRITE);+ sb\_end\_write(sb); }- file\_end\_write(req->file);-}--static void io\_complete\_rw\_common(struct kiocb \*kiocb, long res,- struct io\_comp\_state \*cs)-{- struct io\_kiocb \*req = container\_of(kiocb, struct io\_kiocb, rw.kiocb);- int cflags = 0;-- if (kiocb->ki\_flags & IOCB\_WRITE)- kiocb\_end\_write(req);-- if (res != req->result)- req\_set\_fail\_links(req);- if (req->flags & REQ\_F\_BUFFER\_SELECTED)- cflags = io\_put\_rw\_kbuf(req);- \_\_io\_req\_complete(req, res, cflags, cs); }  #ifdef CONFIG\_BLOCK-static bool io\_resubmit\_prep(struct io\_kiocb \*req, int error)+static bool io\_resubmit\_prep(struct io\_kiocb \*req) {- req\_set\_fail\_links(req);- return false;+ struct io\_async\_rw \*rw = req->async\_data;++ if (!rw)+ return !io\_req\_prep\_async(req);+ iov\_iter\_restore(&rw->iter, &rw->iter\_state);+ return true; }-#endif -static bool io\_rw\_reissue(struct io\_kiocb \*req, long res)+static bool io\_rw\_should\_reissue(struct io\_kiocb \*req) {-#ifdef CONFIG\_BLOCK umode\_t mode = file\_inode(req->file)->i\_mode;- int ret;+ struct io\_ring\_ctx \*ctx = req->ctx;  if (!S\_ISBLK(mode) && !S\_ISREG(mode)) return false;- if ((res != -EAGAIN && res != -EOPNOTSUPP) || io\_wq\_current\_is\_worker())+ if ((req->flags & REQ\_F\_NOWAIT) || (io\_wq\_current\_is\_worker() &&+ !(ctx->flags & IORING\_SETUP\_IOPOLL))) return false; /\* \* If ref is dying, we might be running poll reap from the exit work. \* Don't attempt to reissue from that path, just let it fail with \* -EAGAIN. \*/- if (percpu\_ref\_is\_dying(&req->ctx->refs))+ if (percpu\_ref\_is\_dying(&ctx->refs))+ return false;+ /\*+ \* Play it safe and assume not safe to re-import and reissue if we're+ \* not in the original thread group (or in task context).+ \*/+ if (!same\_thread\_group(req->task, current) || !in\_task()) return false;+ return true;+}+#else+static bool io\_resubmit\_prep(struct io\_kiocb \*req)+{+ return false;+}+static bool io\_rw\_should\_reissue(struct io\_kiocb \*req)+{+ return false;+}+#endif - ret = io\_sq\_thread\_acquire\_mm(req->ctx, req);+static bool \_\_io\_complete\_rw\_common(struct io\_kiocb \*req, long res)+{+ if (req->rw.kiocb.ki\_flags & IOCB\_WRITE) {+ kiocb\_end\_write(req);+ fsnotify\_modify(req->file);+ } else {+ fsnotify\_access(req->file);+ }+ if (res != req->result) {+ if ((res == -EAGAIN || res == -EOPNOTSUPP) &&+ io\_rw\_should\_reissue(req)) {+ req->flags |= REQ\_F\_REISSUE;+ return true;+ }+ req\_set\_fail(req);+ req->result = res;+ }+ return false;+} - if (io\_resubmit\_prep(req, ret)) {- refcount\_inc(&req->refs);- io\_queue\_async\_work(req);- return true;+static inline int io\_fixup\_rw\_res(struct io\_kiocb \*req, unsigned res)+{+ struct io\_async\_rw \*io = req->async\_data;++ /\* add previously done IO, if any \*/+ if (io && io->bytes\_done > 0) {+ if (res < 0)+ res = io->bytes\_done;+ else+ res += io->bytes\_done; }+ return res;+} -#endif- return false;+static void io\_req\_task\_complete(struct io\_kiocb \*req, bool \*locked)+{+ unsigned int cflags = io\_put\_rw\_kbuf(req);+ int res = req->result;++ if (\*locked) {+ struct io\_ring\_ctx \*ctx = req->ctx;+ struct io\_submit\_state \*state = &ctx->submit\_state;++ io\_req\_complete\_state(req, res, cflags);+ state->compl\_reqs[state->compl\_nr++] = req;+ if (state->compl\_nr == ARRAY\_SIZE(state->compl\_reqs))+ io\_submit\_flush\_completions(ctx);+ } else {+ io\_req\_complete\_post(req, res, cflags);+ } }  static void \_\_io\_complete\_rw(struct io\_kiocb \*req, long res, long res2,- struct io\_comp\_state \*cs)+ unsigned int issue\_flags) {- if (!io\_rw\_reissue(req, res))- io\_complete\_rw\_common(&req->rw.kiocb, res, cs);+ if (\_\_io\_complete\_rw\_common(req, res))+ return;+ \_\_io\_req\_complete(req, issue\_flags, io\_fixup\_rw\_res(req, res), io\_put\_rw\_kbuf(req)); }  static void io\_complete\_rw(struct kiocb \*kiocb, long res, long res2) { struct io\_kiocb \*req = container\_of(kiocb, struct io\_kiocb, rw.kiocb); - \_\_io\_complete\_rw(req, res, res2, NULL);+ if (\_\_io\_complete\_rw\_common(req, res))+ return;+ req->result = io\_fixup\_rw\_res(req, res);+ req->io\_task\_work.func = io\_req\_task\_complete;+ io\_req\_task\_work\_add(req); }  static void io\_complete\_rw\_iopoll(struct kiocb \*kiocb, long res, long res2)@@ -2642,12 +2755,15 @@ static void io\_complete\_rw\_iopoll(struct kiocb \*kiocb, long res, long res2)  if (kiocb->ki\_flags & IOCB\_WRITE) kiocb\_end\_write(req);-- if (res != -EAGAIN && res != req->result)- req\_set\_fail\_links(req);+ if (unlikely(res != req->result)) {+ if (res == -EAGAIN && io\_rw\_should\_reissue(req)) {+ req->flags |= REQ\_F\_REISSUE;+ return;+ }+ }  WRITE\_ONCE(req->result, res);- /\* order with io\_poll\_complete() checking ->result \*/+ /\* order with io\_iopoll\_complete() checking ->result \*/ smp\_wmb(); WRITE\_ONCE(req->iopoll\_completed, 1); }@@ -2655,12 +2771,17 @@ static void io\_complete\_rw\_iopoll(struct kiocb \*kiocb, long res, long res2) /\* \* After the iocb has been issued, it's safe to be found on the poll list. \* Adding the kiocb to the list AFTER submission ensures that we don't- \* find it from a io\_iopoll\_getevents() thread before the issuer is done+ \* find it from a io\_do\_iopoll() thread before the issuer is done \* accessing the kiocb cookie. \*/ static void io\_iopoll\_req\_issued(struct io\_kiocb \*req) { struct io\_ring\_ctx \*ctx = req->ctx;+ const bool in\_async = io\_wq\_current\_is\_worker();++ /\* workqueue context doesn't hold uring\_lock, grab it now \*/+ if (unlikely(in\_async))+ mutex\_lock(&ctx->uring\_lock);  /\* \* Track whether we have multiple files in our lists. This will impact@@ -2668,14 +2789,22 @@ static void io\_iopoll\_req\_issued(struct io\_kiocb \*req) \* different devices. \*/ if (list\_empty(&ctx->iopoll\_list)) {- ctx->poll\_multi\_file = false;- } else if (!ctx->poll\_multi\_file) {+ ctx->poll\_multi\_queue = false;+ } else if (!ctx->poll\_multi\_queue) { struct io\_kiocb \*list\_req;+ unsigned int queue\_num0, queue\_num1;  list\_req = list\_first\_entry(&ctx->iopoll\_list, struct io\_kiocb,- iopoll\_entry);- if (list\_req->file != req->file)- ctx->poll\_multi\_file = true;+ inflight\_entry);++ if (list\_req->file != req->file) {+ ctx->poll\_multi\_queue = true;+ } else {+ queue\_num0 = blk\_qc\_t\_to\_queue\_num(list\_req->rw.kiocb.ki\_cookie);+ queue\_num1 = blk\_qc\_t\_to\_queue\_num(req->rw.kiocb.ki\_cookie);+ if (queue\_num0 != queue\_num1)+ ctx->poll\_multi\_queue = true;+ } }  /\*@@ -2683,61 +2812,28 @@ static void io\_iopoll\_req\_issued(struct io\_kiocb \*req) \* it to the front so we find it first. \*/ if (READ\_ONCE(req->iopoll\_completed))- list\_add(&req->iopoll\_entry, &ctx->iopoll\_list);+ list\_add(&req->inflight\_entry, &ctx->iopoll\_list); else- list\_add\_tail(&req->iopoll\_entry, &ctx->iopoll\_list);-- if ((ctx->flags & IORING\_SETUP\_SQPOLL) &&- wq\_has\_sleeper(&ctx->sq\_data->wait))- wake\_up(&ctx->sq\_data->wait);-}+ list\_add\_tail(&req->inflight\_entry, &ctx->iopoll\_list); -static void \_\_io\_state\_file\_put(struct io\_submit\_state \*state)-{- if (state->has\_refs)- fput\_many(state->file, state->has\_refs);- state->file = NULL;-}--static inline void io\_state\_file\_put(struct io\_submit\_state \*state)-{- if (state->file)- \_\_io\_state\_file\_put(state);-}--/\*- \* Get as many references to a file as we have IOs left in this submission,- \* assuming most submissions are for one file, or at least that each file- \* has more than one submission.- \*/-static struct file \*\_\_io\_file\_get(struct io\_submit\_state \*state, int fd)-{- if (!state)- return fget(fd);+ if (unlikely(in\_async)) {+ /\*+ \* If IORING\_SETUP\_SQPOLL is enabled, sqes are either handle+ \* in sq thread task context or in io worker task context. If+ \* current task context is sq thread, we don't need to check+ \* whether should wake up sq thread.+ \*/+ if ((ctx->flags & IORING\_SETUP\_SQPOLL) &&+ wq\_has\_sleeper(&ctx->sq\_data->wait))+ wake\_up(&ctx->sq\_data->wait); - if (state->file) {- if (state->fd == fd) {- state->has\_refs--;- return state->file;- }- \_\_io\_state\_file\_put(state);+ mutex\_unlock(&ctx->uring\_lock); }- state->file = fget\_many(fd, state->ios\_left);- if (!state->file)- return NULL;-- state->fd = fd;- state->has\_refs = state->ios\_left - 1;- return state->file; }  static bool io\_bdev\_nowait(struct block\_device \*bdev) {-#ifdef CONFIG\_BLOCK return !bdev || blk\_queue\_nowait(bdev\_get\_queue(bdev));-#else- return true;-#endif }  /\*@@ -2745,19 +2841,21 @@ static bool io\_bdev\_nowait(struct block\_device \*bdev) \* any file. For now, just ensure that anything potentially problematic is done \* inline. \*/-static bool io\_file\_supports\_async(struct file \*file, int rw)+static bool \_\_io\_file\_supports\_nowait(struct file \*file, int rw) { umode\_t mode = file\_inode(file)->i\_mode;  if (S\_ISBLK(mode)) {- if (io\_bdev\_nowait(file->f\_inode->i\_bdev))+ if (IS\_ENABLED(CONFIG\_BLOCK) &&+ io\_bdev\_nowait(I\_BDEV(file->f\_mapping->host))) return true; return false; } if (S\_ISSOCK(mode)) return true; if (S\_ISREG(mode)) {- if (io\_bdev\_nowait(file->f\_inode->i\_sb->s\_bdev) &&+ if (IS\_ENABLED(CONFIG\_BLOCK) &&+ io\_bdev\_nowait(file->f\_inode->i\_sb->s\_bdev) && file->f\_op != &io\_uring\_fops) return true; return false;@@ -2776,20 +2874,36 @@ static bool io\_file\_supports\_async(struct file \*file, int rw) return file->f\_op->write\_iter != NULL; } -static int io\_prep\_rw(struct io\_kiocb \*req, const struct io\_uring\_sqe \*sqe)+static bool io\_file\_supports\_nowait(struct io\_kiocb \*req, int rw)+{+ if (rw == READ && (req->flags & REQ\_F\_NOWAIT\_READ))+ return true;+ else if (rw == WRITE && (req->flags & REQ\_F\_NOWAIT\_WRITE))+ return true;++ return \_\_io\_file\_supports\_nowait(req->file, rw);+}++static int io\_prep\_rw(struct io\_kiocb \*req, const struct io\_uring\_sqe \*sqe,+ int rw) { struct io\_ring\_ctx \*ctx = req->ctx; struct kiocb \*kiocb = &req->rw.kiocb;+ struct file \*file = req->file; unsigned ioprio; int ret; - if (S\_ISREG(file\_inode(req->file)->i\_mode))+ if (!io\_req\_ffs\_set(req) && S\_ISREG(file\_inode(file)->i\_mode)) req->flags |= REQ\_F\_ISREG;  kiocb->ki\_pos = READ\_ONCE(sqe->off);- if (kiocb->ki\_pos == -1 && !(req->file->f\_mode & FMODE\_STREAM)) {- req->flags |= REQ\_F\_CUR\_POS;- kiocb->ki\_pos = req->file->f\_pos;+ if (kiocb->ki\_pos == -1) {+ if (!(file->f\_mode & FMODE\_STREAM)) {+ req->flags |= REQ\_F\_CUR\_POS;+ kiocb->ki\_pos = file->f\_pos;+ } else {+ kiocb->ki\_pos = 0;+ } } kiocb->ki\_hint = ki\_hint\_validate(file\_write\_hint(kiocb->ki\_filp)); kiocb->ki\_flags = iocb\_flags(kiocb->ki\_filp);@@ -2797,6 +2911,15 @@ static int io\_prep\_rw(struct io\_kiocb \*req, const struct io\_uring\_sqe \*sqe) if (unlikely(ret)) return ret; + /\*+ \* If the file is marked O\_NONBLOCK, still allow retry for it if it+ \* supports async. Otherwise it's impossible to use O\_NONBLOCK files+ \* reliably. If not, or it IOCB\_NOWAIT is set, don't retry.+ \*/+ if ((kiocb->ki\_flags & IOCB\_NOWAIT) ||+ ((file->f\_flags & O\_NONBLOCK) && !io\_file\_supports\_nowait(req, rw)))+ req->flags |= REQ\_F\_NOWAIT;+ ioprio = READ\_ONCE(sqe->ioprio); if (ioprio) { ret = ioprio\_check\_cap(ioprio);@@ -2807,10 +2930,6 @@ static int io\_prep\_rw(struct io\_kiocb \*req, const struct io\_uring\_sqe \*sqe) } else kiocb->ki\_ioprio = get\_current\_ioprio(); - /\* don't allow async punt if RWF\_NOWAIT was requested \*/- if (kiocb->ki\_flags & IOCB\_NOWAIT)- req->flags |= REQ\_F\_NOWAIT;- if (ctx->flags & IORING\_SETUP\_IOPOLL) { if (!(kiocb->ki\_flags & IOCB\_DIRECT) || !kiocb->ki\_filp->f\_op->iopoll)@@ -2825,9 +2944,24 @@ static int io\_prep\_rw(struct io\_kiocb \*req, const struct io\_uring\_sqe \*sqe) kiocb->ki\_complete = io\_complete\_rw; } + /\* used for fixed read/write too - just read unconditionally \*/+ req->buf\_index = READ\_ONCE(sqe->buf\_index);+ req->imu = NULL;++ if (req->opcode == IORING\_OP\_READ\_FIXED ||+ req->opcode == IORING\_OP\_WRITE\_FIXED) {+ struct io\_ring\_ctx \*ctx = req->ctx;+ u16 index;++ if (unlikely(req->buf\_index >= ctx->nr\_user\_bufs))+ return -EFAULT;+ index = array\_index\_nospec(req->buf\_index, ctx->nr\_user\_bufs);+ req->imu = ctx->user\_bufs[index];+ io\_req\_set\_rsrc\_node(req);+ }+ req->rw.addr = READ\_ONCE(sqe->addr); req->rw.len = READ\_ONCE(sqe->len);- req->buf\_index = READ\_ONCE(sqe->buf\_index); return 0; } @@ -2853,48 +2987,49 @@ static inline void io\_rw\_done(struct kiocb \*kiocb, ssize\_t ret) }  static void kiocb\_done(struct kiocb \*kiocb, ssize\_t ret,- struct io\_comp\_state \*cs)+ unsigned int issue\_flags) { struct io\_kiocb \*req = container\_of(kiocb, struct io\_kiocb, rw.kiocb);- struct io\_async\_rw \*io = req->async\_data;-- /\* add previously done IO, if any \*/- if (io && io->bytes\_done > 0) {- if (ret < 0)- ret = io->bytes\_done;- else- ret += io->bytes\_done;- }  if (req->flags & REQ\_F\_CUR\_POS) req->file->f\_pos = kiocb->ki\_pos;- if (ret >= 0 && kiocb->ki\_complete == io\_complete\_rw)- \_\_io\_complete\_rw(req, ret, 0, cs);+ if (ret >= 0 && (kiocb->ki\_complete == io\_complete\_rw))+ \_\_io\_complete\_rw(req, ret, 0, issue\_flags); else io\_rw\_done(kiocb, ret);++ if (req->flags & REQ\_F\_REISSUE) {+ req->flags &= ~REQ\_F\_REISSUE;+ if (io\_resubmit\_prep(req)) {+ io\_req\_task\_queue\_reissue(req);+ } else {+ unsigned int cflags = io\_put\_rw\_kbuf(req);+ struct io\_ring\_ctx \*ctx = req->ctx;++ ret = io\_fixup\_rw\_res(req, ret);+ req\_set\_fail(req);+ if (!(issue\_flags & IO\_URING\_F\_NONBLOCK)) {+ mutex\_lock(&ctx->uring\_lock);+ \_\_io\_req\_complete(req, issue\_flags, ret, cflags);+ mutex\_unlock(&ctx->uring\_lock);+ } else {+ \_\_io\_req\_complete(req, issue\_flags, ret, cflags);+ }+ }+ } } -static ssize\_t io\_import\_fixed(struct io\_kiocb \*req, int rw,- struct iov\_iter \*iter)+static int \_\_io\_import\_fixed(struct io\_kiocb \*req, int rw, struct iov\_iter \*iter,+ struct io\_mapped\_ubuf \*imu) {- struct io\_ring\_ctx \*ctx = req->ctx; size\_t len = req->rw.len;- struct io\_mapped\_ubuf \*imu;- u16 index, buf\_index = req->buf\_index;+ u64 buf\_end, buf\_addr = req->rw.addr; size\_t offset;- u64 buf\_addr;-- if (unlikely(buf\_index >= ctx->nr\_user\_bufs))- return -EFAULT;- index = array\_index\_nospec(buf\_index, ctx->nr\_user\_bufs);- imu = &ctx->user\_bufs[index];- buf\_addr = req->rw.addr; - /\* overflow \*/- if (buf\_addr + len < buf\_addr)+ if (unlikely(check\_add\_overflow(buf\_addr, (u64)len, &buf\_end))) return -EFAULT; /\* not inside the mapped region \*/- if (buf\_addr < imu->ubuf || buf\_addr + len > imu->ubuf + imu->len)+ if (unlikely(buf\_addr < imu->ubuf || buf\_end > imu->ubuf\_end)) return -EFAULT;  /\*@@ -2939,7 +3074,14 @@ static ssize\_t io\_import\_fixed(struct io\_kiocb \*req, int rw, } } - return len;+ return 0;+}++static int io\_import\_fixed(struct io\_kiocb \*req, int rw, struct iov\_iter \*iter)+{+ if (WARN\_ON\_ONCE(!req->imu))+ return -EFAULT;+ return \_\_io\_import\_fixed(req, rw, iter, req->imu); }  static void io\_ring\_submit\_unlock(struct io\_ring\_ctx \*ctx, bool needs\_lock)@@ -3080,16 +3222,14 @@ static ssize\_t io\_iov\_buffer\_select(struct io\_kiocb \*req, struct iovec \*iov, return \_\_io\_iov\_buffer\_select(req, iov, needs\_lock); } -static ssize\_t \_\_io\_import\_iovec(int rw, struct io\_kiocb \*req,- struct iovec \*\*iovec, struct iov\_iter \*iter,- bool needs\_lock)+static int io\_import\_iovec(int rw, struct io\_kiocb \*req, struct iovec \*\*iovec,+ struct iov\_iter \*iter, bool needs\_lock) { void \_\_user \*buf = u64\_to\_user\_ptr(req->rw.addr); size\_t sqe\_len = req->rw.len;+ u8 opcode = req->opcode; ssize\_t ret;- u8 opcode; - opcode = req->opcode; if (opcode == IORING\_OP\_READ\_FIXED || opcode == IORING\_OP\_WRITE\_FIXED) { \*iovec = NULL; return io\_import\_fixed(req, rw, iter);@@ -3114,10 +3254,8 @@ static ssize\_t \_\_io\_import\_iovec(int rw, struct io\_kiocb \*req,  if (req->flags & REQ\_F\_BUFFER\_SELECT) { ret = io\_iov\_buffer\_select(req, \*iovec, needs\_lock);- if (!ret) {- ret = (\*iovec)->iov\_len;- iov\_iter\_init(iter, rw, \*iovec, 1, ret);- }+ if (!ret)+ iov\_iter\_init(iter, rw, \*iovec, 1, (\*iovec)->iov\_len); \*iovec = NULL; return ret; }@@ -3126,18 +3264,6 @@ static ssize\_t \_\_io\_import\_iovec(int rw, struct io\_kiocb \*req, req->ctx->compat); } -static ssize\_t io\_import\_iovec(int rw, struct io\_kiocb \*req,- struct iovec \*\*iovec, struct iov\_iter \*iter,- bool needs\_lock)-{- struct io\_async\_rw \*iorw = req->async\_data;-- if (!iorw)- return \_\_io\_import\_iovec(rw, req, iovec, iter, needs\_lock);- \*iovec = NULL;- return 0;-}- static inline loff\_t \*io\_kiocb\_ppos(struct kiocb \*kiocb) { return (kiocb->ki\_filp->f\_mode & FMODE\_STREAM) ? NULL : &kiocb->ki\_pos;@@ -3230,32 +3356,31 @@ static void io\_req\_map\_rw(struct io\_kiocb \*req, const struct iovec \*iovec, } } -static inline int \_\_io\_alloc\_async\_data(struct io\_kiocb \*req)+static inline int io\_alloc\_async\_data(struct io\_kiocb \*req) { WARN\_ON\_ONCE(!io\_op\_defs[req->opcode].async\_size); req->async\_data = kmalloc(io\_op\_defs[req->opcode].async\_size, GFP\_KERNEL); return req->async\_data == NULL; } -static int io\_alloc\_async\_data(struct io\_kiocb \*req)-{- if (!io\_op\_defs[req->opcode].needs\_async\_data)- return 0;-- return \_\_io\_alloc\_async\_data(req);-}- static int io\_setup\_async\_rw(struct io\_kiocb \*req, const struct iovec \*iovec, const struct iovec \*fast\_iov, struct iov\_iter \*iter, bool force) {- if (!force && !io\_op\_defs[req->opcode].needs\_async\_data)+ if (!force && !io\_op\_defs[req->opcode].needs\_async\_setup) return 0; if (!req->async\_data) {- if (\_\_io\_alloc\_async\_data(req))+ struct io\_async\_rw \*iorw;++ if (io\_alloc\_async\_data(req)) {+ kfree(iovec); return -ENOMEM;+ }  io\_req\_map\_rw(req, iovec, fast\_iov, iter);+ iorw = req->async\_data;+ /\* we've copied and mapped the iter, ensure state is saved \*/+ iov\_iter\_save\_state(&iorw->iter, &iorw->iter\_state); } return 0; }@@ -3264,9 +3389,9 @@ static inline int io\_rw\_prep\_async(struct io\_kiocb \*req, int rw) { struct io\_async\_rw \*iorw = req->async\_data; struct iovec \*iov = iorw->fast\_iov;- ssize\_t ret;+ int ret; - ret = \_\_io\_import\_iovec(rw, req, &iov, &iorw->iter, false);+ ret = io\_import\_iovec(rw, req, &iov, &iorw->iter, false); if (unlikely(ret < 0)) return ret; @@ -3274,24 +3399,15 @@ static inline int io\_rw\_prep\_async(struct io\_kiocb \*req, int rw) iorw->free\_iovec = iov; if (iov) req->flags |= REQ\_F\_NEED\_CLEANUP;+ iov\_iter\_save\_state(&iorw->iter, &iorw->iter\_state); return 0; }  static int io\_read\_prep(struct io\_kiocb \*req, const struct io\_uring\_sqe \*sqe) {- ssize\_t ret;-- ret = io\_prep\_rw(req, sqe);- if (ret)- return ret;- if (unlikely(!(req->file->f\_mode & FMODE\_READ))) return -EBADF;-- /\* either don't need iovec imported or already have it \*/- if (!req->async\_data)- return 0;- return io\_rw\_prep\_async(req, READ);+ return io\_prep\_rw(req, sqe, READ); }  /\*@@ -3310,7 +3426,6 @@ static int io\_async\_buf\_func(struct wait\_queue\_entry \*wait, unsigned mode, struct wait\_page\_queue \*wpq; struct io\_kiocb \*req = wait->private; struct wait\_page\_key \*key = arg;- int ret;  wpq = container\_of(wait, struct wait\_page\_queue, wait); @@ -3319,22 +3434,7 @@ static int io\_async\_buf\_func(struct wait\_queue\_entry \*wait, unsigned mode,  req->rw.kiocb.ki\_flags &= ~IOCB\_WAITQ; list\_del\_init(&wait->entry);-- init\_task\_work(&req->task\_work, io\_req\_task\_submit);- percpu\_ref\_get(&req->ctx->refs);-- /\* submit ref gets dropped, acquire a new one \*/- refcount\_inc(&req->refs);- ret = io\_req\_task\_work\_add(req, true);- if (unlikely(ret)) {- struct task\_struct \*tsk;-- /\* queue just for cancelation \*/- init\_task\_work(&req->task\_work, io\_req\_task\_cancel);- tsk = io\_wq\_get\_task(req->ctx->io\_wq);- task\_work\_add(tsk, &req->task\_work, TWA\_NONE);- wake\_up\_process(tsk);- }+ io\_req\_task\_queue(req); return 1; } @@ -3381,7 +3481,7 @@ static bool io\_rw\_should\_retry(struct io\_kiocb \*req) return true; } -static int io\_iter\_do\_read(struct io\_kiocb \*req, struct iov\_iter \*iter)+static inline int io\_iter\_do\_read(struct io\_kiocb \*req, struct iov\_iter \*iter) { if (req->file->f\_op->read\_iter) return call\_read\_iter(req->file, &req->rw.kiocb, iter);@@ -3391,27 +3491,40 @@ static int io\_iter\_do\_read(struct io\_kiocb \*req, struct iov\_iter \*iter) return -EINVAL; } -static int io\_read(struct io\_kiocb \*req, bool force\_nonblock,- struct io\_comp\_state \*cs)+static bool need\_read\_all(struct io\_kiocb \*req)+{+ return req->flags & REQ\_F\_ISREG ||+ S\_ISBLK(file\_inode(req->file)->i\_mode);+}++static int io\_read(struct io\_kiocb \*req, unsigned int issue\_flags) { struct iovec inline\_vecs[UIO\_FASTIOV], \*iovec = inline\_vecs; struct kiocb \*kiocb = &req->rw.kiocb; struct iov\_iter \_\_iter, \*iter = &\_\_iter;- struct iov\_iter iter\_cp; struct io\_async\_rw \*rw = req->async\_data;- ssize\_t io\_size, ret, ret2;- bool no\_async;+ bool force\_nonblock = issue\_flags & IO\_URING\_F\_NONBLOCK;+ struct iov\_iter\_state \_\_state, \*state;+ ssize\_t ret, ret2; - if (rw)+ if (rw) { iter = &rw->iter;-- ret = io\_import\_iovec(READ, req, &iovec, iter, !force\_nonblock);- if (ret < 0)- return ret;- iter\_cp = \*iter;- io\_size = iov\_iter\_count(iter);- req->result = io\_size;- ret = 0;+ state = &rw->iter\_state;+ /\*+ \* We come here from an earlier attempt, restore our state to+ \* match in case it doesn't. It's cheap enough that we don't+ \* need to make this conditional.+ \*/+ iov\_iter\_restore(iter, state);+ iovec = NULL;+ } else {+ ret = io\_import\_iovec(READ, req, &iovec, iter, !force\_nonblock);+ if (ret < 0)+ return ret;+ state = &\_\_state;+ iov\_iter\_save\_state(iter, state);+ }+ req->result = iov\_iter\_count(iter);  /\* Ensure we clear previously set non-block flag \*/ if (!force\_nonblock)@@ -3419,127 +3532,130 @@ static int io\_read(struct io\_kiocb \*req, bool force\_nonblock, else kiocb->ki\_flags |= IOCB\_NOWAIT; - /\* If the file doesn't support async, just async punt \*/- no\_async = force\_nonblock && !io\_file\_supports\_async(req->file, READ);- if (no\_async)- goto copy\_iov;+ if (force\_nonblock && !io\_file\_supports\_nowait(req, READ)) {+ ret = io\_setup\_async\_rw(req, iovec, inline\_vecs, iter, true);+ return ret ?: -EAGAIN;+ } - ret = rw\_verify\_area(READ, req->file, io\_kiocb\_ppos(kiocb), io\_size);- if (unlikely(ret))- goto out\_free;+ ret = rw\_verify\_area(READ, req->file, io\_kiocb\_ppos(kiocb), req->result);+ if (unlikely(ret)) {+ kfree(iovec);+ return ret;+ }  ret = io\_iter\_do\_read(req, iter); - if (!ret) {- goto done;- } else if (ret == -EIOCBQUEUED) {- ret = 0;- goto out\_free;- } else if (ret == -EAGAIN) {+ if (ret == -EAGAIN || (req->flags & REQ\_F\_REISSUE)) {+ req->flags &= ~REQ\_F\_REISSUE; /\* IOPOLL retry should happen for io-wq threads \*/ if (!force\_nonblock && !(req->ctx->flags & IORING\_SETUP\_IOPOLL)) goto done;- /\* no retry on NONBLOCK marked file \*/- if (req->file->f\_flags & O\_NONBLOCK)+ /\* no retry on NONBLOCK nor RWF\_NOWAIT \*/+ if (req->flags & REQ\_F\_NOWAIT) goto done;- /\* some cases will consume bytes even on error returns \*/- \*iter = iter\_cp; ret = 0;- goto copy\_iov;- } else if (ret < 0) {- /\* make sure -ERESTARTSYS -> -EINTR is done \*/+ } else if (ret == -EIOCBQUEUED) {+ goto out\_free;+ } else if (ret <= 0 || ret == req->result || !force\_nonblock ||+ (req->flags & REQ\_F\_NOWAIT) || !need\_read\_all(req)) {+ /\* read all, failed, already did sync or don't want to retry \*/ goto done; } - /\* read it all, or we did blocking attempt. no retry. \*/- if (!iov\_iter\_count(iter) || !force\_nonblock ||- (req->file->f\_flags & O\_NONBLOCK) || !(req->flags & REQ\_F\_ISREG))- goto done;+ /\*+ \* Don't depend on the iter state matching what was consumed, or being+ \* untouched in case of error. Restore it and we'll advance it+ \* manually if we need to.+ \*/+ iov\_iter\_restore(iter, state); - io\_size -= ret;-copy\_iov: ret2 = io\_setup\_async\_rw(req, iovec, inline\_vecs, iter, true);- if (ret2) {- ret = ret2;- goto out\_free;- }- if (no\_async)- return -EAGAIN;- rw = req->async\_data;- /\* it's copied and will be cleaned with ->io \*/- iovec = NULL;- /\* now use our persistent iterator, if we aren't already \*/- iter = &rw->iter;-retry:- rw->bytes\_done += ret;- /\* if we can retry, do so with the callbacks armed \*/- if (!io\_rw\_should\_retry(req)) {- kiocb->ki\_flags &= ~IOCB\_WAITQ;- return -EAGAIN;- }+ if (ret2)+ return ret2; + iovec = NULL;+ rw = req->async\_data; /\*- \* Now retry read with the IOCB\_WAITQ parts set in the iocb. If we- \* get -EIOCBQUEUED, then we'll get a notification when the desired- \* page gets unlocked. We can also get a partial read here, and if we- \* do, then just retry at the new offset.+ \* Now use our persistent iterator and state, if we aren't already.+ \* We've restored and mapped the iter to match. \*/- ret = io\_iter\_do\_read(req, iter);- if (ret == -EIOCBQUEUED) {- ret = 0;- goto out\_free;- } else if (ret > 0 && ret < io\_size) {+ if (iter != &rw->iter) {+ iter = &rw->iter;+ state = &rw->iter\_state;+ }++ do {+ /\*+ \* We end up here because of a partial read, either from+ \* above or inside this loop. Advance the iter by the bytes+ \* that were consumed.+ \*/+ iov\_iter\_advance(iter, ret);+ if (!iov\_iter\_count(iter))+ break;+ rw->bytes\_done += ret;+ iov\_iter\_save\_state(iter, state);++ /\* if we can retry, do so with the callbacks armed \*/+ if (!io\_rw\_should\_retry(req)) {+ kiocb->ki\_flags &= ~IOCB\_WAITQ;+ return -EAGAIN;+ }++ req->result = iov\_iter\_count(iter);+ /\*+ \* Now retry read with the IOCB\_WAITQ parts set in the iocb. If+ \* we get -EIOCBQUEUED, then we'll get a notification when the+ \* desired page gets unlocked. We can also get a partial read+ \* here, and if we do, then just retry at the new offset.+ \*/+ ret = io\_iter\_do\_read(req, iter);+ if (ret == -EIOCBQUEUED)+ return 0; /\* we got some bytes, but not all. retry. \*/ kiocb->ki\_flags &= ~IOCB\_WAITQ;- goto retry;- }+ iov\_iter\_restore(iter, state);+ } while (ret > 0); done:- kiocb\_done(kiocb, ret, cs);- ret = 0;+ kiocb\_done(kiocb, ret, issue\_flags); out\_free:- /\* it's reportedly faster than delegating the null check to kfree() \*/+ /\* it's faster to check here then delegate to kfree \*/ if (iovec) kfree(iovec);- return ret;+ return 0; }  static int io\_write\_prep(struct io\_kiocb \*req, const struct io\_uring\_sqe \*sqe) {- ssize\_t ret;-- ret = io\_prep\_rw(req, sqe);- if (ret)- return ret;- if (unlikely(!(req->file->f\_mode & FMODE\_WRITE))) return -EBADF;-- /\* either don't need iovec imported or already have it \*/- if (!req->async\_data)- return 0;- return io\_rw\_prep\_async(req, WRITE);+ return io\_prep\_rw(req, sqe, WRITE); } -static int io\_write(struct io\_kiocb \*req, bool force\_nonblock,- struct io\_comp\_state \*cs)+static int io\_write(struct io\_kiocb \*req, unsigned int issue\_flags) { struct iovec inline\_vecs[UIO\_FASTIOV], \*iovec = inline\_vecs; struct kiocb \*kiocb = &req->rw.kiocb; struct iov\_iter \_\_iter, \*iter = &\_\_iter;- struct iov\_iter iter\_cp; struct io\_async\_rw \*rw = req->async\_data;- ssize\_t ret, ret2, io\_size;+ bool force\_nonblock = issue\_flags & IO\_URING\_F\_NONBLOCK;+ struct iov\_iter\_state \_\_state, \*state;+ ssize\_t ret, ret2; - if (rw)+ if (rw) { iter = &rw->iter;-- ret = io\_import\_iovec(WRITE, req, &iovec, iter, !force\_nonblock);- if (ret < 0)- return ret;- iter\_cp = \*iter;- io\_size = iov\_iter\_count(iter);- req->result = io\_size;+ state = &rw->iter\_state;+ iov\_iter\_restore(iter, state);+ iovec = NULL;+ } else {+ ret = io\_import\_iovec(WRITE, req, &iovec, iter, !force\_nonblock);+ if (ret < 0)+ return ret;+ state = &\_\_state;+ iov\_iter\_save\_state(iter, state);+ }+ req->result = iov\_iter\_count(iter);  /\* Ensure we clear previously set non-block flag \*/ if (!force\_nonblock)@@ -3548,7 +3664,7 @@ static int io\_write(struct io\_kiocb \*req, bool force\_nonblock, kiocb->ki\_flags |= IOCB\_NOWAIT;  /\* If the file doesn't support async, just async punt \*/- if (force\_nonblock && !io\_file\_supports\_async(req->file, WRITE))+ if (force\_nonblock && !io\_file\_supports\_nowait(req, WRITE)) goto copy\_iov;  /\* file path doesn't support NOWAIT for non-direct\_IO \*/@@ -3556,7 +3672,7 @@ static int io\_write(struct io\_kiocb \*req, bool force\_nonblock, (req->flags & REQ\_F\_ISREG)) goto copy\_iov; - ret = rw\_verify\_area(WRITE, req->file, io\_kiocb\_ppos(kiocb), io\_size);+ ret = rw\_verify\_area(WRITE, req->file, io\_kiocb\_ppos(kiocb), req->result); if (unlikely(ret)) goto out\_free; @@ -3581,28 +3697,36 @@ static int io\_write(struct io\_kiocb \*req, bool force\_nonblock, else ret2 = -EINVAL; + if (req->flags & REQ\_F\_REISSUE) {+ req->flags &= ~REQ\_F\_REISSUE;+ ret2 = -EAGAIN;+ }+ /\* \* Raw bdev writes will return -EOPNOTSUPP for IOCB\_NOWAIT. Just \* retry them without IOCB\_NOWAIT. \*/ if (ret2 == -EOPNOTSUPP && (kiocb->ki\_flags & IOCB\_NOWAIT)) ret2 = -EAGAIN;- /\* no retry on NONBLOCK marked file \*/- if (ret2 == -EAGAIN && (req->file->f\_flags & O\_NONBLOCK))+ /\* no retry on NONBLOCK nor RWF\_NOWAIT \*/+ if (ret2 == -EAGAIN && (req->flags & REQ\_F\_NOWAIT)) goto done; if (!force\_nonblock || ret2 != -EAGAIN) { /\* IOPOLL retry should happen for io-wq threads \*/ if ((req->ctx->flags & IORING\_SETUP\_IOPOLL) && ret2 == -EAGAIN) goto copy\_iov; done:- kiocb\_done(kiocb, ret2, cs);+ kiocb\_done(kiocb, ret2, issue\_flags); } else { copy\_iov:- /\* some cases will consume bytes even on error returns \*/- \*iter = iter\_cp;+ iov\_iter\_restore(iter, state); ret = io\_setup\_async\_rw(req, iovec, inline\_vecs, iter, false);- if (!ret)+ if (!ret) {+ if (kiocb->ki\_flags & IOCB\_WRITE)+ kiocb\_end\_write(req); return -EAGAIN;+ }+ return ret; } out\_free: /\* it's reportedly faster than delegating the null check to kfree() \*/@@ -3611,37 +3735,160 @@ out\_free: return ret; } -static int \_\_io\_splice\_prep(struct io\_kiocb \*req,+static int io\_renameat\_prep(struct io\_kiocb \*req, const struct io\_uring\_sqe \*sqe) {- struct io\_splice\* sp = &req->splice;- unsigned int valid\_flags = SPLICE\_F\_FD\_IN\_FIXED | SPLICE\_F\_ALL;+ struct io\_rename \*ren = &req->rename;+ const char \_\_user \*oldf, \*newf;  if (unlikely(req->ctx->flags & IORING\_SETUP\_IOPOLL)) return -EINVAL;+ if (sqe->ioprio || sqe->buf\_index || sqe->splice\_fd\_in)+ return -EINVAL;+ if (unlikely(req->flags & REQ\_F\_FIXED\_FILE))+ return -EBADF; - sp->file\_in = NULL;- sp->len = READ\_ONCE(sqe->len);- sp->flags = READ\_ONCE(sqe->splice\_flags);+ ren->old\_dfd = READ\_ONCE(sqe->fd);+ oldf = u64\_to\_user\_ptr(READ\_ONCE(sqe->addr));+ newf = u64\_to\_user\_ptr(READ\_ONCE(sqe->addr2));+ ren->new\_dfd = READ\_ONCE(sqe->len);+ ren->flags = READ\_ONCE(sqe->rename\_flags); - if (unlikely(sp->flags & ~valid\_flags))- return -EINVAL;+ ren->oldpath = getname(oldf);+ if (IS\_ERR(ren->oldpath))+ return PTR\_ERR(ren->oldpath); - sp->file\_in = io\_file\_get(NULL, req, READ\_ONCE(sqe->splice\_fd\_in),- (sp->flags & SPLICE\_F\_FD\_IN\_FIXED));- if (!sp->file\_in)+ ren->newpath = getname(newf);+ if (IS\_ERR(ren->newpath)) {+ putname(ren->oldpath);+ return PTR\_ERR(ren->newpath);+ }++ req->flags |= REQ\_F\_NEED\_CLEANUP;+ return 0;+}++static int io\_renameat(struct io\_kiocb \*req, unsigned int issue\_flags)+{+ struct io\_rename \*ren = &req->rename;+ int ret;++ if (issue\_flags & IO\_URING\_F\_NONBLOCK)+ return -EAGAIN;++ ret = do\_renameat2(ren->old\_dfd, ren->oldpath, ren->new\_dfd,+ ren->newpath, ren->flags);++ req->flags &= ~REQ\_F\_NEED\_CLEANUP;+ if (ret < 0)+ req\_set\_fail(req);+ io\_req\_complete(req, ret);+ return 0;+}++static int io\_unlinkat\_prep(struct io\_kiocb \*req,+ const struct io\_uring\_sqe \*sqe)+{+ struct io\_unlink \*un = &req->unlink;+ const char \_\_user \*fname;++ if (unlikely(req->ctx->flags & IORING\_SETUP\_IOPOLL))+ return -EINVAL;+ if (sqe->ioprio || sqe->off || sqe->len || sqe->buf\_index ||+ sqe->splice\_fd\_in)+ return -EINVAL;+ if (unlikely(req->flags & REQ\_F\_FIXED\_FILE)) return -EBADF;++ un->dfd = READ\_ONCE(sqe->fd);++ un->flags = READ\_ONCE(sqe->unlink\_flags);+ if (un->flags & ~AT\_REMOVEDIR)+ return -EINVAL;++ fname = u64\_to\_user\_ptr(READ\_ONCE(sqe->addr));+ un->filename = getname(fname);+ if (IS\_ERR(un->filename))+ return PTR\_ERR(un->filename);+ req->flags |= REQ\_F\_NEED\_CLEANUP;+ return 0;+} - if (!S\_ISREG(file\_inode(sp->file\_in)->i\_mode)) {- /\*- \* Splice operation will be punted aync, and here need to- \* modify io\_wq\_work.flags, so initialize io\_wq\_work firstly.- \*/- io\_req\_init\_async(req);- req->work.flags |= IO\_WQ\_WORK\_UNBOUND;- }+static int io\_unlinkat(struct io\_kiocb \*req, unsigned int issue\_flags)+{+ struct io\_unlink \*un = &req->unlink;+ int ret;++ if (issue\_flags & IO\_URING\_F\_NONBLOCK)+ return -EAGAIN;++ if (un->flags & AT\_REMOVEDIR)+ ret = do\_rmdir(un->dfd, un->filename);+ else+ ret = do\_unlinkat(un->dfd, un->filename);++ req->flags &= ~REQ\_F\_NEED\_CLEANUP;+ if (ret < 0)+ req\_set\_fail(req);+ io\_req\_complete(req, ret);+ return 0;+}++static int io\_shutdown\_prep(struct io\_kiocb \*req,+ const struct io\_uring\_sqe \*sqe)+{+#if defined(CONFIG\_NET)+ if (unlikely(req->ctx->flags & IORING\_SETUP\_IOPOLL))+ return -EINVAL;+ if (unlikely(sqe->ioprio || sqe->off || sqe->addr || sqe->rw\_flags ||+ sqe->buf\_index || sqe->splice\_fd\_in))+ return -EINVAL;++ req->shutdown.how = READ\_ONCE(sqe->len);+ return 0;+#else+ return -EOPNOTSUPP;+#endif+}++static int io\_shutdown(struct io\_kiocb \*req, unsigned int issue\_flags)+{+#if defined(CONFIG\_NET)+ struct socket \*sock;+ int ret;++ if (issue\_flags & IO\_URING\_F\_NONBLOCK)+ return -EAGAIN;++ sock = sock\_from\_file(req->file, &ret);+ if (unlikely(!sock))+ return ret;++ ret = \_\_sys\_shutdown\_sock(sock, req->shutdown.how);+ if (ret < 0)+ req\_set\_fail(req);+ io\_req\_complete(req, ret);+ return 0;+#else+ return -EOPNOTSUPP;+#endif+}++static int \_\_io\_splice\_prep(struct io\_kiocb \*req,+ const struct io\_uring\_sqe \*sqe)+{+ struct io\_splice \*sp = &req->splice;+ unsigned int valid\_flags = SPLICE\_F\_FD\_IN\_FIXED | SPLICE\_F\_ALL;++ if (unlikely(req->ctx->flags & IORING\_SETUP\_IOPOLL))+ return -EINVAL; + sp->len = READ\_ONCE(sqe->len);+ sp->flags = READ\_ONCE(sqe->splice\_flags);+ if (unlikely(sp->flags & ~valid\_flags))+ return -EINVAL;+ sp->splice\_fd\_in = READ\_ONCE(sqe->splice\_fd\_in); return 0; } @@ -3653,60 +3900,75 @@ static int io\_tee\_prep(struct io\_kiocb \*req, return \_\_io\_splice\_prep(req, sqe); } -static int io\_tee(struct io\_kiocb \*req, bool force\_nonblock)+static int io\_tee(struct io\_kiocb \*req, unsigned int issue\_flags) { struct io\_splice \*sp = &req->splice;- struct file \*in = sp->file\_in; struct file \*out = sp->file\_out; unsigned int flags = sp->flags & ~SPLICE\_F\_FD\_IN\_FIXED;+ struct file \*in; long ret = 0; - if (force\_nonblock)+ if (issue\_flags & IO\_URING\_F\_NONBLOCK) return -EAGAIN;++ in = io\_file\_get(req->ctx, req, sp->splice\_fd\_in,+ (sp->flags & SPLICE\_F\_FD\_IN\_FIXED));+ if (!in) {+ ret = -EBADF;+ goto done;+ }+ if (sp->len) ret = do\_tee(in, out, sp->len, flags); - io\_put\_file(req, in, (sp->flags & SPLICE\_F\_FD\_IN\_FIXED));- req->flags &= ~REQ\_F\_NEED\_CLEANUP;-+ if (!(sp->flags & SPLICE\_F\_FD\_IN\_FIXED))+ io\_put\_file(in);+done: if (ret != sp->len)- req\_set\_fail\_links(req);+ req\_set\_fail(req); io\_req\_complete(req, ret); return 0; }  static int io\_splice\_prep(struct io\_kiocb \*req, const struct io\_uring\_sqe \*sqe) {- struct io\_splice\* sp = &req->splice;+ struct io\_splice \*sp = &req->splice;  sp->off\_in = READ\_ONCE(sqe->splice\_off\_in); sp->off\_out = READ\_ONCE(sqe->off); return \_\_io\_splice\_prep(req, sqe); } -static int io\_splice(struct io\_kiocb \*req, bool force\_nonblock)+static int io\_splice(struct io\_kiocb \*req, unsigned int issue\_flags) { struct io\_splice \*sp = &req->splice;- struct file \*in = sp->file\_in; struct file \*out = sp->file\_out; unsigned int flags = sp->flags & ~SPLICE\_F\_FD\_IN\_FIXED; loff\_t \*poff\_in, \*poff\_out;+ struct file \*in; long ret = 0; - if (force\_nonblock)+ if (issue\_flags & IO\_URING\_F\_NONBLOCK) return -EAGAIN; + in = io\_file\_get(req->ctx, req, sp->splice\_fd\_in,+ (sp->flags & SPLICE\_F\_FD\_IN\_FIXED));+ if (!in) {+ ret = -EBADF;+ goto done;+ }+ poff\_in = (sp->off\_in == -1) ? NULL : &sp->off\_in; poff\_out = (sp->off\_out == -1) ? NULL : &sp->off\_out;  if (sp->len) ret = do\_splice(in, poff\_in, out, poff\_out, sp->len, flags); - io\_put\_file(req, in, (sp->flags & SPLICE\_F\_FD\_IN\_FIXED));- req->flags &= ~REQ\_F\_NEED\_CLEANUP;-+ if (!(sp->flags & SPLICE\_F\_FD\_IN\_FIXED))+ io\_put\_file(in);+done: if (ret != sp->len)- req\_set\_fail\_links(req);+ req\_set\_fail(req); io\_req\_complete(req, ret); return 0; }@@ -3714,24 +3976,21 @@ static int io\_splice(struct io\_kiocb \*req, bool force\_nonblock) /\* \* IORING\_OP\_NOP just posts a completion event, nothing else. \*/-static int io\_nop(struct io\_kiocb \*req, struct io\_comp\_state \*cs)+static int io\_nop(struct io\_kiocb \*req, unsigned int issue\_flags) { struct io\_ring\_ctx \*ctx = req->ctx;  if (unlikely(ctx->flags & IORING\_SETUP\_IOPOLL)) return -EINVAL; - \_\_io\_req\_complete(req, 0, 0, cs);+ \_\_io\_req\_complete(req, issue\_flags, 0, 0); return 0; } -static int io\_prep\_fsync(struct io\_kiocb \*req, const struct io\_uring\_sqe \*sqe)+static int io\_fsync\_prep(struct io\_kiocb \*req, const struct io\_uring\_sqe \*sqe) { struct io\_ring\_ctx \*ctx = req->ctx; - if (!req->file)- return -EBADF;- if (unlikely(ctx->flags & IORING\_SETUP\_IOPOLL)) return -EINVAL; if (unlikely(sqe->addr || sqe->ioprio || sqe->buf\_index ||@@ -3747,20 +4006,20 @@ static int io\_prep\_fsync(struct io\_kiocb \*req, const struct io\_uring\_sqe \*sqe) return 0; } -static int io\_fsync(struct io\_kiocb \*req, bool force\_nonblock)+static int io\_fsync(struct io\_kiocb \*req, unsigned int issue\_flags) { loff\_t end = req->sync.off + req->sync.len; int ret;  /\* fsync always requires a blocking context \*/- if (force\_nonblock)+ if (issue\_flags & IO\_URING\_F\_NONBLOCK) return -EAGAIN;  ret = vfs\_fsync\_range(req->file, req->sync.off, end > 0 ? end : LLONG\_MAX, req->sync.flags & IORING\_FSYNC\_DATASYNC); if (ret < 0)- req\_set\_fail\_links(req);+ req\_set\_fail(req); io\_req\_complete(req, ret); return 0; }@@ -3780,17 +4039,19 @@ static int io\_fallocate\_prep(struct io\_kiocb \*req, return 0; } -static int io\_fallocate(struct io\_kiocb \*req, bool force\_nonblock)+static int io\_fallocate(struct io\_kiocb \*req, unsigned int issue\_flags) { int ret;  /\* fallocate always requiring blocking context \*/- if (force\_nonblock)+ if (issue\_flags & IO\_URING\_F\_NONBLOCK) return -EAGAIN; ret = vfs\_fallocate(req->file, req->sync.mode, req->sync.off, req->sync.len); if (ret < 0)- req\_set\_fail\_links(req);+ req\_set\_fail(req);+ else+ fsnotify\_modify(req->file); io\_req\_complete(req, ret); return 0; }@@ -3800,7 +4061,9 @@ static int \_\_io\_openat\_prep(struct io\_kiocb \*req, const struct io\_uring\_sqe \*sqe const char \_\_user \*fname; int ret; - if (unlikely(sqe->ioprio || sqe->buf\_index || sqe->splice\_fd\_in))+ if (unlikely(req->ctx->flags & IORING\_SETUP\_IOPOLL))+ return -EINVAL;+ if (unlikely(sqe->ioprio || sqe->buf\_index)) return -EINVAL; if (unlikely(req->flags & REQ\_F\_FIXED\_FILE)) return -EBADF;@@ -3817,20 +4080,21 @@ static int \_\_io\_openat\_prep(struct io\_kiocb \*req, const struct io\_uring\_sqe \*sqe req->open.filename = NULL; return ret; }++ req->open.file\_slot = READ\_ONCE(sqe->file\_index);+ if (req->open.file\_slot && (req->open.how.flags & O\_CLOEXEC))+ return -EINVAL;+ req->open.nofile = rlimit(RLIMIT\_NOFILE);- req->open.ignore\_nonblock = false; req->flags |= REQ\_F\_NEED\_CLEANUP; return 0; }  static int io\_openat\_prep(struct io\_kiocb \*req, const struct io\_uring\_sqe \*sqe) {- u64 flags, mode;+ u64 mode = READ\_ONCE(sqe->len);+ u64 flags = READ\_ONCE(sqe->open\_flags); - if (unlikely(req->ctx->flags & (IORING\_SETUP\_IOPOLL|IORING\_SETUP\_SQPOLL)))- return -EINVAL;- mode = READ\_ONCE(sqe->len);- flags = READ\_ONCE(sqe->open\_flags); req->open.how = build\_open\_how(flags, mode); return \_\_io\_openat\_prep(req, sqe); }@@ -3841,8 +4105,6 @@ static int io\_openat2\_prep(struct io\_kiocb \*req, const struct io\_uring\_sqe \*sqe) size\_t len; int ret; - if (unlikely(req->ctx->flags & (IORING\_SETUP\_IOPOLL|IORING\_SETUP\_SQPOLL)))- return -EINVAL; how = u64\_to\_user\_ptr(READ\_ONCE(sqe->addr2)); len = READ\_ONCE(sqe->len); if (len < OPEN\_HOW\_SIZE\_VER0)@@ -3856,58 +4118,75 @@ static int io\_openat2\_prep(struct io\_kiocb \*req, const struct io\_uring\_sqe \*sqe) return \_\_io\_openat\_prep(req, sqe); } -static int io\_openat2(struct io\_kiocb \*req, bool force\_nonblock)+static int io\_openat2(struct io\_kiocb \*req, unsigned int issue\_flags) { struct open\_flags op; struct file \*file;+ bool resolve\_nonblock, nonblock\_set;+ bool fixed = !!req->open.file\_slot; int ret; - if (force\_nonblock && !req->open.ignore\_nonblock)- return -EAGAIN;- ret = build\_open\_flags(&req->open.how, &op); if (ret) goto err;+ nonblock\_set = op.open\_flag & O\_NONBLOCK;+ resolve\_nonblock = req->open.how.resolve & RESOLVE\_CACHED;+ if (issue\_flags & IO\_URING\_F\_NONBLOCK) {+ /\*+ \* Don't bother trying for O\_TRUNC, O\_CREAT, or O\_TMPFILE open,+ \* it'll always -EAGAIN+ \*/+ if (req->open.how.flags & (O\_TRUNC | O\_CREAT | O\_TMPFILE))+ return -EAGAIN;+ op.lookup\_flags |= LOOKUP\_CACHED;+ op.open\_flag |= O\_NONBLOCK;+ } - ret = \_\_get\_unused\_fd\_flags(req->open.how.flags, req->open.nofile);- if (ret < 0)- goto err;+ if (!fixed) {+ ret = \_\_get\_unused\_fd\_flags(req->open.how.flags, req->open.nofile);+ if (ret < 0)+ goto err;+ }  file = do\_filp\_open(req->open.dfd, req->open.filename, &op); if (IS\_ERR(file)) {- put\_unused\_fd(ret);- ret = PTR\_ERR(file); /\*- \* A work-around to ensure that /proc/self works that way- \* that it should - if we get -EOPNOTSUPP back, then assume- \* that proc\_self\_get\_link() failed us because we're in async- \* context. We should be safe to retry this from the task- \* itself with force\_nonblock == false set, as it should not- \* block on lookup. Would be nice to know this upfront and- \* avoid the async dance, but doesn't seem feasible.+ \* We could hang on to this 'fd' on retrying, but seems like+ \* marginal gain for something that is now known to be a slower+ \* path. So just put it, and we'll get a new one when we retry. \*/- if (ret == -EOPNOTSUPP && io\_wq\_current\_is\_worker()) {- req->open.ignore\_nonblock = true;- refcount\_inc(&req->refs);- io\_req\_task\_queue(req);- return 0;- }- } else {- fsnotify\_open(file);- fd\_install(ret, file);+ if (!fixed)+ put\_unused\_fd(ret);++ ret = PTR\_ERR(file);+ /\* only retry if RESOLVE\_CACHED wasn't already set by application \*/+ if (ret == -EAGAIN &&+ (!resolve\_nonblock && (issue\_flags & IO\_URING\_F\_NONBLOCK)))+ return -EAGAIN;+ goto err; }++ if ((issue\_flags & IO\_URING\_F\_NONBLOCK) && !nonblock\_set)+ file->f\_flags &= ~O\_NONBLOCK;+ fsnotify\_open(file);++ if (!fixed)+ fd\_install(ret, file);+ else+ ret = io\_install\_fixed\_file(req, file, issue\_flags,+ req->open.file\_slot - 1); err: putname(req->open.filename); req->flags &= ~REQ\_F\_NEED\_CLEANUP; if (ret < 0)- req\_set\_fail\_links(req);- io\_req\_complete(req, ret);+ req\_set\_fail(req);+ \_\_io\_req\_complete(req, issue\_flags, ret, 0); return 0; } -static int io\_openat(struct io\_kiocb \*req, bool force\_nonblock)+static int io\_openat(struct io\_kiocb \*req, unsigned int issue\_flags) {- return io\_openat2(req, force\_nonblock);+ return io\_openat2(req, issue\_flags); }  static int io\_remove\_buffers\_prep(struct io\_kiocb \*req,@@ -3948,6 +4227,7 @@ static int \_\_io\_remove\_buffers(struct io\_ring\_ctx \*ctx, struct io\_buffer \*buf, kfree(nxt); if (++i == nbufs) return i;+ cond\_resched(); } i++; kfree(buf);@@ -3956,13 +4236,13 @@ static int \_\_io\_remove\_buffers(struct io\_ring\_ctx \*ctx, struct io\_buffer \*buf, return i; } -static int io\_remove\_buffers(struct io\_kiocb \*req, bool force\_nonblock,- struct io\_comp\_state \*cs)+static int io\_remove\_buffers(struct io\_kiocb \*req, unsigned int issue\_flags) { struct io\_provide\_buf \*p = &req->pbuf; struct io\_ring\_ctx \*ctx = req->ctx; struct io\_buffer \*head; int ret = 0;+ bool force\_nonblock = issue\_flags & IO\_URING\_F\_NONBLOCK;  io\_ring\_submit\_lock(ctx, !force\_nonblock); @@ -3973,16 +4253,11 @@ static int io\_remove\_buffers(struct io\_kiocb \*req, bool force\_nonblock, if (head) ret = \_\_io\_remove\_buffers(ctx, head, p->bgid, p->nbufs); if (ret < 0)- req\_set\_fail\_links(req);+ req\_set\_fail(req); - /\* need to hold the lock to complete IOPOLL requests \*/- if (ctx->flags & IORING\_SETUP\_IOPOLL) {- \_\_io\_req\_complete(req, ret, 0, cs);- io\_ring\_submit\_unlock(ctx, !force\_nonblock);- } else {- io\_ring\_submit\_unlock(ctx, !force\_nonblock);- \_\_io\_req\_complete(req, ret, 0, cs);- }+ /\* complete before unlock, IOPOLL may need the lock \*/+ \_\_io\_req\_complete(req, issue\_flags, ret, 0);+ io\_ring\_submit\_unlock(ctx, !force\_nonblock); return 0; } @@ -4049,13 +4324,13 @@ static int io\_add\_buffers(struct io\_provide\_buf \*pbuf, struct io\_buffer \*\*head) return i ? i : -ENOMEM; } -static int io\_provide\_buffers(struct io\_kiocb \*req, bool force\_nonblock,- struct io\_comp\_state \*cs)+static int io\_provide\_buffers(struct io\_kiocb \*req, unsigned int issue\_flags) { struct io\_provide\_buf \*p = &req->pbuf; struct io\_ring\_ctx \*ctx = req->ctx; struct io\_buffer \*head, \*list; int ret = 0;+ bool force\_nonblock = issue\_flags & IO\_URING\_F\_NONBLOCK;  io\_ring\_submit\_lock(ctx, !force\_nonblock); @@ -4065,21 +4340,16 @@ static int io\_provide\_buffers(struct io\_kiocb \*req, bool force\_nonblock,  ret = io\_add\_buffers(p, &head); if (ret >= 0 && !list) {- ret = xa\_insert(&ctx->io\_buffers, p->bgid, head, GFP\_KERNEL);+ ret = xa\_insert(&ctx->io\_buffers, p->bgid, head,+ GFP\_KERNEL\_ACCOUNT); if (ret < 0) \_\_io\_remove\_buffers(ctx, head, p->bgid, -1U); } if (ret < 0)- req\_set\_fail\_links(req);-- /\* need to hold the lock to complete IOPOLL requests \*/- if (ctx->flags & IORING\_SETUP\_IOPOLL) {- \_\_io\_req\_complete(req, ret, 0, cs);- io\_ring\_submit\_unlock(ctx, !force\_nonblock);- } else {- io\_ring\_submit\_unlock(ctx, !force\_nonblock);- \_\_io\_req\_complete(req, ret, 0, cs);- }+ req\_set\_fail(req);+ /\* complete before unlock, IOPOLL may need the lock \*/+ \_\_io\_req\_complete(req, issue\_flags, ret, 0);+ io\_ring\_submit\_unlock(ctx, !force\_nonblock); return 0; } @@ -4089,7 +4359,7 @@ static int io\_epoll\_ctl\_prep(struct io\_kiocb \*req, #if defined(CONFIG\_EPOLL) if (sqe->ioprio || sqe->buf\_index || sqe->splice\_fd\_in) return -EINVAL;- if (unlikely(req->ctx->flags & (IORING\_SETUP\_IOPOLL | IORING\_SETUP\_SQPOLL)))+ if (unlikely(req->ctx->flags & IORING\_SETUP\_IOPOLL)) return -EINVAL;  req->epoll.epfd = READ\_ONCE(sqe->fd);@@ -4110,20 +4380,20 @@ static int io\_epoll\_ctl\_prep(struct io\_kiocb \*req, #endif } -static int io\_epoll\_ctl(struct io\_kiocb \*req, bool force\_nonblock,- struct io\_comp\_state \*cs)+static int io\_epoll\_ctl(struct io\_kiocb \*req, unsigned int issue\_flags) { #if defined(CONFIG\_EPOLL) struct io\_epoll \*ie = &req->epoll; int ret;+ bool force\_nonblock = issue\_flags & IO\_URING\_F\_NONBLOCK;  ret = do\_epoll\_ctl(ie->epfd, ie->op, ie->fd, &ie->event, force\_nonblock); if (force\_nonblock && ret == -EAGAIN) return -EAGAIN;  if (ret < 0)- req\_set\_fail\_links(req);- \_\_io\_req\_complete(req, ret, 0, cs);+ req\_set\_fail(req);+ \_\_io\_req\_complete(req, issue\_flags, ret, 0); return 0; #else return -EOPNOTSUPP;@@ -4147,18 +4417,18 @@ static int io\_madvise\_prep(struct io\_kiocb \*req, const struct io\_uring\_sqe \*sqe) #endif } -static int io\_madvise(struct io\_kiocb \*req, bool force\_nonblock)+static int io\_madvise(struct io\_kiocb \*req, unsigned int issue\_flags) { #if defined(CONFIG\_ADVISE\_SYSCALLS) && defined(CONFIG\_MMU) struct io\_madvise \*ma = &req->madvise; int ret; - if (force\_nonblock)+ if (issue\_flags & IO\_URING\_F\_NONBLOCK) return -EAGAIN;  ret = do\_madvise(current->mm, ma->addr, ma->len, ma->advice); if (ret < 0)- req\_set\_fail\_links(req);+ req\_set\_fail(req); io\_req\_complete(req, ret); return 0; #else@@ -4179,12 +4449,12 @@ static int io\_fadvise\_prep(struct io\_kiocb \*req, const struct io\_uring\_sqe \*sqe) return 0; } -static int io\_fadvise(struct io\_kiocb \*req, bool force\_nonblock)+static int io\_fadvise(struct io\_kiocb \*req, unsigned int issue\_flags) { struct io\_fadvise \*fa = &req->fadvise; int ret; - if (force\_nonblock) {+ if (issue\_flags & IO\_URING\_F\_NONBLOCK) { switch (fa->advice) { case POSIX\_FADV\_NORMAL: case POSIX\_FADV\_RANDOM:@@ -4197,14 +4467,14 @@ static int io\_fadvise(struct io\_kiocb \*req, bool force\_nonblock)  ret = vfs\_fadvise(req->file, fa->offset, fa->len, fa->advice); if (ret < 0)- req\_set\_fail\_links(req);- io\_req\_complete(req, ret);+ req\_set\_fail(req);+ \_\_io\_req\_complete(req, issue\_flags, ret, 0); return 0; }  static int io\_statx\_prep(struct io\_kiocb \*req, const struct io\_uring\_sqe \*sqe) {- if (unlikely(req->ctx->flags & (IORING\_SETUP\_IOPOLL | IORING\_SETUP\_SQPOLL)))+ if (unlikely(req->ctx->flags & IORING\_SETUP\_IOPOLL)) return -EINVAL; if (sqe->ioprio || sqe->buf\_index || sqe->splice\_fd\_in) return -EINVAL;@@ -4220,89 +4490,96 @@ static int io\_statx\_prep(struct io\_kiocb \*req, const struct io\_uring\_sqe \*sqe) return 0; } -static int io\_statx(struct io\_kiocb \*req, bool force\_nonblock)+static int io\_statx(struct io\_kiocb \*req, unsigned int issue\_flags) { struct io\_statx \*ctx = &req->statx; int ret; - if (force\_nonblock)+ if (issue\_flags & IO\_URING\_F\_NONBLOCK) return -EAGAIN;  ret = do\_statx(ctx->dfd, ctx->filename, ctx->flags, ctx->mask, ctx->buffer);  if (ret < 0)- req\_set\_fail\_links(req);+ req\_set\_fail(req); io\_req\_complete(req, ret); return 0; }  static int io\_close\_prep(struct io\_kiocb \*req, const struct io\_uring\_sqe \*sqe) {- /\*- \* If we queue this for async, it must not be cancellable. That would- \* leave the 'file' in an undeterminate state, and here need to modify- \* io\_wq\_work.flags, so initialize io\_wq\_work firstly.- \*/- io\_req\_init\_async(req);-- if (unlikely(req->ctx->flags & (IORING\_SETUP\_IOPOLL|IORING\_SETUP\_SQPOLL)))+ if (unlikely(req->ctx->flags & IORING\_SETUP\_IOPOLL)) return -EINVAL; if (sqe->ioprio || sqe->off || sqe->addr || sqe->len ||- sqe->rw\_flags || sqe->buf\_index || sqe->splice\_fd\_in)+ sqe->rw\_flags || sqe->buf\_index) return -EINVAL; if (req->flags & REQ\_F\_FIXED\_FILE) return -EBADF;  req->close.fd = READ\_ONCE(sqe->fd);- if ((req->file && req->file->f\_op == &io\_uring\_fops))- return -EBADF;+ req->close.file\_slot = READ\_ONCE(sqe->file\_index);+ if (req->close.file\_slot && req->close.fd)+ return -EINVAL; - req->close.put\_file = NULL; return 0; } -static int io\_close(struct io\_kiocb \*req, bool force\_nonblock,- struct io\_comp\_state \*cs)+static int io\_close(struct io\_kiocb \*req, unsigned int issue\_flags) {+ struct files\_struct \*files = current->files; struct io\_close \*close = &req->close;- int ret;+ struct fdtable \*fdt;+ struct file \*file = NULL;+ int ret = -EBADF; - /\* might be already done during nonblock submission \*/- if (!close->put\_file) {- ret = close\_fd\_get\_file(close->fd, &close->put\_file);- if (ret < 0)- return (ret == -ENOENT) ? -EBADF : ret;+ if (req->close.file\_slot) {+ ret = io\_close\_fixed(req, issue\_flags);+ goto err;+ }++ spin\_lock(&files->file\_lock);+ fdt = files\_fdtable(files);+ if (close->fd >= fdt->max\_fds) {+ spin\_unlock(&files->file\_lock);+ goto err;+ }+ file = fdt->fd[close->fd];+ if (!file || file->f\_op == &io\_uring\_fops) {+ spin\_unlock(&files->file\_lock);+ file = NULL;+ goto err; }  /\* if the file has a flush method, be safe and punt to async \*/- if (close->put\_file->f\_op->flush && force\_nonblock) {- /\* not safe to cancel at this point \*/- req->work.flags |= IO\_WQ\_WORK\_NO\_CANCEL;- /\* was never set, but play safe \*/- req->flags &= ~REQ\_F\_NOWAIT;- /\* avoid grabbing files - we don't need the files \*/- req->flags |= REQ\_F\_NO\_FILE\_TABLE;+ if (file->f\_op->flush && (issue\_flags & IO\_URING\_F\_NONBLOCK)) {+ spin\_unlock(&files->file\_lock); return -EAGAIN; } + ret = \_\_close\_fd\_get\_file(close->fd, &file);+ spin\_unlock(&files->file\_lock);+ if (ret < 0) {+ if (ret == -ENOENT)+ ret = -EBADF;+ goto err;+ }+ /\* No ->flush() or already async, safely close from here \*/- ret = filp\_close(close->put\_file, req->work.identity->files);+ ret = filp\_close(file, current->files);+err: if (ret < 0)- req\_set\_fail\_links(req);- fput(close->put\_file);- close->put\_file = NULL;- \_\_io\_req\_complete(req, ret, 0, cs);+ req\_set\_fail(req);+ if (file)+ fput(file);+ \_\_io\_req\_complete(req, issue\_flags, ret, 0); return 0; } -static int io\_prep\_sfr(struct io\_kiocb \*req, const struct io\_uring\_sqe \*sqe)+static int io\_sfr\_prep(struct io\_kiocb \*req, const struct io\_uring\_sqe \*sqe) { struct io\_ring\_ctx \*ctx = req->ctx; - if (!req->file)- return -EBADF;- if (unlikely(ctx->flags & IORING\_SETUP\_IOPOLL)) return -EINVAL; if (unlikely(sqe->addr || sqe->ioprio || sqe->buf\_index ||@@ -4315,18 +4592,18 @@ static int io\_prep\_sfr(struct io\_kiocb \*req, const struct io\_uring\_sqe \*sqe) return 0; } -static int io\_sync\_file\_range(struct io\_kiocb \*req, bool force\_nonblock)+static int io\_sync\_file\_range(struct io\_kiocb \*req, unsigned int issue\_flags) { int ret;  /\* sync\_file\_range always requires a blocking context \*/- if (force\_nonblock)+ if (issue\_flags & IO\_URING\_F\_NONBLOCK) return -EAGAIN;  ret = sync\_file\_range(req->file, req->sync.off, req->sync.len, req->sync.flags); if (ret < 0)- req\_set\_fail\_links(req);+ req\_set\_fail(req); io\_req\_complete(req, ret); return 0; }@@ -4340,55 +4617,65 @@ static int io\_setup\_async\_msg(struct io\_kiocb \*req, if (async\_msg) return -EAGAIN; if (io\_alloc\_async\_data(req)) {- if (kmsg->iov != kmsg->fast\_iov)- kfree(kmsg->iov);+ kfree(kmsg->free\_iov); return -ENOMEM; } async\_msg = req->async\_data; req->flags |= REQ\_F\_NEED\_CLEANUP; memcpy(async\_msg, kmsg, sizeof(\*kmsg));+ if (async\_msg->msg.msg\_name)+ async\_msg->msg.msg\_name = &async\_msg->addr;+ /\* if were using fast\_iov, set it to the new one \*/+ if (!async\_msg->free\_iov)+ async\_msg->msg.msg\_iter.iov = async\_msg->fast\_iov;+ return -EAGAIN; }  static int io\_sendmsg\_copy\_hdr(struct io\_kiocb \*req, struct io\_async\_msghdr \*iomsg) {- iomsg->iov = iomsg->fast\_iov; iomsg->msg.msg\_name = &iomsg->addr;+ iomsg->free\_iov = iomsg->fast\_iov; return sendmsg\_copy\_msghdr(&iomsg->msg, req->sr\_msg.umsg,- req->sr\_msg.msg\_flags, &iomsg->iov);+ req->sr\_msg.msg\_flags, &iomsg->free\_iov);+}++static int io\_sendmsg\_prep\_async(struct io\_kiocb \*req)+{+ int ret;++ ret = io\_sendmsg\_copy\_hdr(req, req->async\_data);+ if (!ret)+ req->flags |= REQ\_F\_NEED\_CLEANUP;+ return ret; }  static int io\_sendmsg\_prep(struct io\_kiocb \*req, const struct io\_uring\_sqe \*sqe) {- struct io\_async\_msghdr \*async\_msg = req->async\_data; struct io\_sr\_msg \*sr = &req->sr\_msg;- int ret;  if (unlikely(req->ctx->flags & IORING\_SETUP\_IOPOLL)) return -EINVAL;- if (unlikely(sqe->addr2 || sqe->splice\_fd\_in || sqe->ioprio))+ if (unlikely(sqe->addr2 || sqe->file\_index))+ return -EINVAL;+ if (unlikely(sqe->addr2 || sqe->file\_index || sqe->ioprio)) return -EINVAL; - sr->msg\_flags = READ\_ONCE(sqe->msg\_flags); sr->umsg = u64\_to\_user\_ptr(READ\_ONCE(sqe->addr)); sr->len = READ\_ONCE(sqe->len);+ sr->msg\_flags = READ\_ONCE(sqe->msg\_flags) | MSG\_NOSIGNAL;+ if (sr->msg\_flags & MSG\_DONTWAIT)+ req->flags |= REQ\_F\_NOWAIT;  #ifdef CONFIG\_COMPAT if (req->ctx->compat) sr->msg\_flags |= MSG\_CMSG\_COMPAT; #endif-- if (!async\_msg || !io\_op\_defs[req->opcode].needs\_async\_data)- return 0;- ret = io\_sendmsg\_copy\_hdr(req, async\_msg);- if (!ret)- req->flags |= REQ\_F\_NEED\_CLEANUP;- return ret;+ return 0; } -static int io\_sendmsg(struct io\_kiocb \*req, bool force\_nonblock,- struct io\_comp\_state \*cs)+static int io\_sendmsg(struct io\_kiocb \*req, unsigned int issue\_flags) { struct io\_async\_msghdr iomsg, \*kmsg; struct socket \*sock;@@ -4400,46 +4687,37 @@ static int io\_sendmsg(struct io\_kiocb \*req, bool force\_nonblock, if (unlikely(!sock)) return ret; - if (req->async\_data) {- kmsg = req->async\_data;- kmsg->msg.msg\_name = &kmsg->addr;- /\* if iov is set, it's allocated already \*/- if (!kmsg->iov)- kmsg->iov = kmsg->fast\_iov;- kmsg->msg.msg\_iter.iov = kmsg->iov;- } else {+ kmsg = req->async\_data;+ if (!kmsg) { ret = io\_sendmsg\_copy\_hdr(req, &iomsg); if (ret) return ret; kmsg = &iomsg; } - flags = req->sr\_msg.msg\_flags | MSG\_NOSIGNAL;- if (flags & MSG\_DONTWAIT)- req->flags |= REQ\_F\_NOWAIT;- else if (force\_nonblock)+ flags = req->sr\_msg.msg\_flags;+ if (issue\_flags & IO\_URING\_F\_NONBLOCK) flags |= MSG\_DONTWAIT;- if (flags & MSG\_WAITALL) min\_ret = iov\_iter\_count(&kmsg->msg.msg\_iter);  ret = \_\_sys\_sendmsg\_sock(sock, &kmsg->msg, flags);- if (force\_nonblock && ret == -EAGAIN)+ if ((issue\_flags & IO\_URING\_F\_NONBLOCK) && ret == -EAGAIN) return io\_setup\_async\_msg(req, kmsg); if (ret == -ERESTARTSYS) ret = -EINTR; - if (kmsg->iov != kmsg->fast\_iov)- kfree(kmsg->iov);+ /\* fast path, check for non-NULL to avoid function call \*/+ if (kmsg->free\_iov)+ kfree(kmsg->free\_iov); req->flags &= ~REQ\_F\_NEED\_CLEANUP; if (ret < min\_ret)- req\_set\_fail\_links(req);- \_\_io\_req\_complete(req, ret, 0, cs);+ req\_set\_fail(req);+ \_\_io\_req\_complete(req, issue\_flags, ret, 0); return 0; } -static int io\_send(struct io\_kiocb \*req, bool force\_nonblock,- struct io\_comp\_state \*cs)+static int io\_send(struct io\_kiocb \*req, unsigned int issue\_flags) { struct io\_sr\_msg \*sr = &req->sr\_msg; struct msghdr msg;@@ -4462,25 +4740,22 @@ static int io\_send(struct io\_kiocb \*req, bool force\_nonblock, msg.msg\_controllen = 0; msg.msg\_namelen = 0; - flags = req->sr\_msg.msg\_flags | MSG\_NOSIGNAL;- if (flags & MSG\_DONTWAIT)- req->flags |= REQ\_F\_NOWAIT;- else if (force\_nonblock)+ flags = req->sr\_msg.msg\_flags;+ if (issue\_flags & IO\_URING\_F\_NONBLOCK) flags |= MSG\_DONTWAIT;- if (flags & MSG\_WAITALL) min\_ret = iov\_iter\_count(&msg.msg\_iter);  msg.msg\_flags = flags; ret = sock\_sendmsg(sock, &msg);- if (force\_nonblock && ret == -EAGAIN)+ if ((issue\_flags & IO\_URING\_F\_NONBLOCK) && ret == -EAGAIN) return -EAGAIN; if (ret == -ERESTARTSYS) ret = -EINTR;  if (ret < min\_ret)- req\_set\_fail\_links(req);- \_\_io\_req\_complete(req, ret, 0, cs);+ req\_set\_fail(req);+ \_\_io\_req\_complete(req, issue\_flags, ret, 0); return 0; } @@ -4500,15 +4775,14 @@ static int \_\_io\_recvmsg\_copy\_hdr(struct io\_kiocb \*req, if (req->flags & REQ\_F\_BUFFER\_SELECT) { if (iov\_len > 1) return -EINVAL;- if (copy\_from\_user(iomsg->iov, uiov, sizeof(\*uiov)))+ if (copy\_from\_user(iomsg->fast\_iov, uiov, sizeof(\*uiov))) return -EFAULT;- sr->len = iomsg->iov[0].iov\_len;- iov\_iter\_init(&iomsg->msg.msg\_iter, READ, iomsg->iov, 1,- sr->len);- iomsg->iov = NULL;+ sr->len = iomsg->fast\_iov[0].iov\_len;+ iomsg->free\_iov = NULL; } else {+ iomsg->free\_iov = iomsg->fast\_iov; ret = \_\_import\_iovec(READ, uiov, iov\_len, UIO\_FASTIOV,- &iomsg->iov, &iomsg->msg.msg\_iter,+ &iomsg->free\_iov, &iomsg->msg.msg\_iter, false); if (ret > 0) ret = 0;@@ -4521,16 +4795,14 @@ static int \_\_io\_recvmsg\_copy\_hdr(struct io\_kiocb \*req, static int \_\_io\_compat\_recvmsg\_copy\_hdr(struct io\_kiocb \*req, struct io\_async\_msghdr \*iomsg) {- struct compat\_msghdr \_\_user \*msg\_compat; struct io\_sr\_msg \*sr = &req->sr\_msg; struct compat\_iovec \_\_user \*uiov; compat\_uptr\_t ptr; compat\_size\_t len; int ret; - msg\_compat = (struct compat\_msghdr \_\_user \*) sr->umsg;- ret = \_\_get\_compat\_msghdr(&iomsg->msg, msg\_compat, &iomsg->uaddr,- &ptr, &len);+ ret = \_\_get\_compat\_msghdr(&iomsg->msg, sr->umsg\_compat, &iomsg->uaddr,+ &ptr, &len); if (ret) return ret; @@ -4547,11 +4819,11 @@ static int \_\_io\_compat\_recvmsg\_copy\_hdr(struct io\_kiocb \*req, if (clen < 0) return -EINVAL; sr->len = clen;- iomsg->iov[0].iov\_len = clen;- iomsg->iov = NULL;+ iomsg->free\_iov = NULL; } else {+ iomsg->free\_iov = iomsg->fast\_iov; ret = \_\_import\_iovec(READ, (struct iovec \_\_user \*)uiov, len,- UIO\_FASTIOV, &iomsg->iov,+ UIO\_FASTIOV, &iomsg->free\_iov, &iomsg->msg.msg\_iter, true); if (ret < 0) return ret;@@ -4565,7 +4837,6 @@ static int io\_recvmsg\_copy\_hdr(struct io\_kiocb \*req, struct io\_async\_msghdr \*iomsg) { iomsg->msg.msg\_name = &iomsg->addr;- iomsg->iov = iomsg->fast\_iov;  #ifdef CONFIG\_COMPAT if (req->ctx->compat)@@ -4595,38 +4866,42 @@ static inline unsigned int io\_put\_recv\_kbuf(struct io\_kiocb \*req) return io\_put\_kbuf(req, req->sr\_msg.kbuf); } -static int io\_recvmsg\_prep(struct io\_kiocb \*req,- const struct io\_uring\_sqe \*sqe)+static int io\_recvmsg\_prep\_async(struct io\_kiocb \*req) {- struct io\_async\_msghdr \*async\_msg = req->async\_data;- struct io\_sr\_msg \*sr = &req->sr\_msg; int ret; + ret = io\_recvmsg\_copy\_hdr(req, req->async\_data);+ if (!ret)+ req->flags |= REQ\_F\_NEED\_CLEANUP;+ return ret;+}++static int io\_recvmsg\_prep(struct io\_kiocb \*req, const struct io\_uring\_sqe \*sqe)+{+ struct io\_sr\_msg \*sr = &req->sr\_msg;+ if (unlikely(req->ctx->flags & IORING\_SETUP\_IOPOLL)) return -EINVAL;- if (unlikely(sqe->addr2 || sqe->splice\_fd\_in || sqe->ioprio))+ if (unlikely(sqe->addr2 || sqe->file\_index))+ return -EINVAL;+ if (unlikely(sqe->addr2 || sqe->file\_index || sqe->ioprio)) return -EINVAL; - sr->msg\_flags = READ\_ONCE(sqe->msg\_flags); sr->umsg = u64\_to\_user\_ptr(READ\_ONCE(sqe->addr)); sr->len = READ\_ONCE(sqe->len); sr->bgid = READ\_ONCE(sqe->buf\_group);+ sr->msg\_flags = READ\_ONCE(sqe->msg\_flags) | MSG\_NOSIGNAL;+ if (sr->msg\_flags & MSG\_DONTWAIT)+ req->flags |= REQ\_F\_NOWAIT;  #ifdef CONFIG\_COMPAT if (req->ctx->compat) sr->msg\_flags |= MSG\_CMSG\_COMPAT; #endif-- if (!async\_msg || !io\_op\_defs[req->opcode].needs\_async\_data)- return 0;- ret = io\_recvmsg\_copy\_hdr(req, async\_msg);- if (!ret)- req->flags |= REQ\_F\_NEED\_CLEANUP;- return ret;+ return 0; } -static int io\_recvmsg(struct io\_kiocb \*req, bool force\_nonblock,- struct io\_comp\_state \*cs)+static int io\_recvmsg(struct io\_kiocb \*req, unsigned int issue\_flags) { struct io\_async\_msghdr iomsg, \*kmsg; struct socket \*sock;@@ -4634,19 +4909,14 @@ static int io\_recvmsg(struct io\_kiocb \*req, bool force\_nonblock, unsigned flags; int min\_ret = 0; int ret, cflags = 0;+ bool force\_nonblock = issue\_flags & IO\_URING\_F\_NONBLOCK;  sock = sock\_from\_file(req->file, &ret); if (unlikely(!sock)) return ret; - if (req->async\_data) {- kmsg = req->async\_data;- kmsg->msg.msg\_name = &kmsg->addr;- /\* if iov is set, it's allocated already \*/- if (!kmsg->iov)- kmsg->iov = kmsg->fast\_iov;- kmsg->msg.msg\_iter.iov = kmsg->iov;- } else {+ kmsg = req->async\_data;+ if (!kmsg) { ret = io\_recvmsg\_copy\_hdr(req, &iomsg); if (ret) return ret;@@ -4658,16 +4928,14 @@ static int io\_recvmsg(struct io\_kiocb \*req, bool force\_nonblock, if (IS\_ERR(kbuf)) return PTR\_ERR(kbuf); kmsg->fast\_iov[0].iov\_base = u64\_to\_user\_ptr(kbuf->addr);- iov\_iter\_init(&kmsg->msg.msg\_iter, READ, kmsg->iov,+ kmsg->fast\_iov[0].iov\_len = req->sr\_msg.len;+ iov\_iter\_init(&kmsg->msg.msg\_iter, READ, kmsg->fast\_iov, 1, req->sr\_msg.len); } - flags = req->sr\_msg.msg\_flags | MSG\_NOSIGNAL;- if (flags & MSG\_DONTWAIT)- req->flags |= REQ\_F\_NOWAIT;- else if (force\_nonblock)+ flags = req->sr\_msg.msg\_flags;+ if (force\_nonblock) flags |= MSG\_DONTWAIT;- if (flags & MSG\_WAITALL) min\_ret = iov\_iter\_count(&kmsg->msg.msg\_iter); @@ -4680,17 +4948,17 @@ static int io\_recvmsg(struct io\_kiocb \*req, bool force\_nonblock,  if (req->flags & REQ\_F\_BUFFER\_SELECTED) cflags = io\_put\_recv\_kbuf(req);- if (kmsg->iov != kmsg->fast\_iov)- kfree(kmsg->iov);+ /\* fast path, check for non-NULL to avoid function call \*/+ if (kmsg->free\_iov)+ kfree(kmsg->free\_iov); req->flags &= ~REQ\_F\_NEED\_CLEANUP; if (ret < min\_ret || ((flags & MSG\_WAITALL) && (kmsg->msg.msg\_flags & (MSG\_TRUNC | MSG\_CTRUNC))))- req\_set\_fail\_links(req);- \_\_io\_req\_complete(req, ret, cflags, cs);+ req\_set\_fail(req);+ \_\_io\_req\_complete(req, issue\_flags, ret, cflags); return 0; } -static int io\_recv(struct io\_kiocb \*req, bool force\_nonblock,- struct io\_comp\_state \*cs)+static int io\_recv(struct io\_kiocb \*req, unsigned int issue\_flags) { struct io\_buffer \*kbuf; struct io\_sr\_msg \*sr = &req->sr\_msg;@@ -4701,6 +4969,7 @@ static int io\_recv(struct io\_kiocb \*req, bool force\_nonblock, unsigned flags; int min\_ret = 0; int ret, cflags = 0;+ bool force\_nonblock = issue\_flags & IO\_URING\_F\_NONBLOCK;  sock = sock\_from\_file(req->file, &ret); if (unlikely(!sock))@@ -4724,12 +4993,9 @@ static int io\_recv(struct io\_kiocb \*req, bool force\_nonblock, msg.msg\_iocb = NULL; msg.msg\_flags = 0; - flags = req->sr\_msg.msg\_flags | MSG\_NOSIGNAL;- if (flags & MSG\_DONTWAIT)- req->flags |= REQ\_F\_NOWAIT;- else if (force\_nonblock)+ flags = req->sr\_msg.msg\_flags;+ if (force\_nonblock) flags |= MSG\_DONTWAIT;- if (flags & MSG\_WAITALL) min\_ret = iov\_iter\_count(&msg.msg\_iter); @@ -4742,8 +5008,8 @@ out\_free: if (req->flags & REQ\_F\_BUFFER\_SELECTED) cflags = io\_put\_recv\_kbuf(req); if (ret < min\_ret || ((flags & MSG\_WAITALL) && (msg.msg\_flags & (MSG\_TRUNC | MSG\_CTRUNC))))- req\_set\_fail\_links(req);- \_\_io\_req\_complete(req, ret, cflags, cs);+ req\_set\_fail(req);+ \_\_io\_req\_complete(req, issue\_flags, ret, cflags); return 0; } @@ -4751,48 +5017,79 @@ static int io\_accept\_prep(struct io\_kiocb \*req, const struct io\_uring\_sqe \*sqe) { struct io\_accept \*accept = &req->accept; - if (unlikely(req->ctx->flags & (IORING\_SETUP\_IOPOLL|IORING\_SETUP\_SQPOLL)))+ if (unlikely(req->ctx->flags & IORING\_SETUP\_IOPOLL)) return -EINVAL;- if (sqe->ioprio || sqe->len || sqe->buf\_index || sqe->splice\_fd\_in)+ if (sqe->ioprio || sqe->len || sqe->buf\_index) return -EINVAL;  accept->addr = u64\_to\_user\_ptr(READ\_ONCE(sqe->addr)); accept->addr\_len = u64\_to\_user\_ptr(READ\_ONCE(sqe->addr2)); accept->flags = READ\_ONCE(sqe->accept\_flags); accept->nofile = rlimit(RLIMIT\_NOFILE);++ accept->file\_slot = READ\_ONCE(sqe->file\_index);+ if (accept->file\_slot && (accept->flags & SOCK\_CLOEXEC))+ return -EINVAL;+ if (accept->flags & ~(SOCK\_CLOEXEC | SOCK\_NONBLOCK))+ return -EINVAL;+ if (SOCK\_NONBLOCK != O\_NONBLOCK && (accept->flags & SOCK\_NONBLOCK))+ accept->flags = (accept->flags & ~SOCK\_NONBLOCK) | O\_NONBLOCK; return 0; } -static int io\_accept(struct io\_kiocb \*req, bool force\_nonblock,- struct io\_comp\_state \*cs)+static int io\_accept(struct io\_kiocb \*req, unsigned int issue\_flags) { struct io\_accept \*accept = &req->accept;+ bool force\_nonblock = issue\_flags & IO\_URING\_F\_NONBLOCK; unsigned int file\_flags = force\_nonblock ? O\_NONBLOCK : 0;- int ret;+ bool fixed = !!accept->file\_slot;+ struct file \*file;+ int ret, fd;  if (req->file->f\_flags & O\_NONBLOCK) req->flags |= REQ\_F\_NOWAIT; - ret = \_\_sys\_accept4\_file(req->file, file\_flags, accept->addr,- accept->addr\_len, accept->flags,- accept->nofile);- if (ret == -EAGAIN && force\_nonblock)- return -EAGAIN;- if (ret < 0) {+ if (!fixed) {+ fd = \_\_get\_unused\_fd\_flags(accept->flags, accept->nofile);+ if (unlikely(fd < 0))+ return fd;+ }+ file = do\_accept(req->file, file\_flags, accept->addr, accept->addr\_len,+ accept->flags);++ if (IS\_ERR(file)) {+ if (!fixed)+ put\_unused\_fd(fd);+ ret = PTR\_ERR(file);+ if (ret == -EAGAIN && force\_nonblock)+ return -EAGAIN; if (ret == -ERESTARTSYS) ret = -EINTR;- req\_set\_fail\_links(req);+ req\_set\_fail(req);+ } else if (!fixed) {+ fd\_install(fd, file);+ ret = fd;+ } else {+ ret = io\_install\_fixed\_file(req, file, issue\_flags,+ accept->file\_slot - 1); }- \_\_io\_req\_complete(req, ret, 0, cs);+ \_\_io\_req\_complete(req, issue\_flags, ret, 0); return 0; } +static int io\_connect\_prep\_async(struct io\_kiocb \*req)+{+ struct io\_async\_connect \*io = req->async\_data;+ struct io\_connect \*conn = &req->connect;++ return move\_addr\_to\_kernel(conn->addr, conn->addr\_len, &io->address);+}+ static int io\_connect\_prep(struct io\_kiocb \*req, const struct io\_uring\_sqe \*sqe) { struct io\_connect \*conn = &req->connect;- struct io\_async\_connect \*io = req->async\_data; - if (unlikely(req->ctx->flags & (IORING\_SETUP\_IOPOLL|IORING\_SETUP\_SQPOLL)))+ if (unlikely(req->ctx->flags & IORING\_SETUP\_IOPOLL)) return -EINVAL; if (sqe->ioprio || sqe->len || sqe->buf\_index || sqe->rw\_flags || sqe->splice\_fd\_in)@@ -4800,20 +5097,15 @@ static int io\_connect\_prep(struct io\_kiocb \*req, const struct io\_uring\_sqe \*sqe)  conn->addr = u64\_to\_user\_ptr(READ\_ONCE(sqe->addr)); conn->addr\_len = READ\_ONCE(sqe->addr2);-- if (!io)- return 0;-- return move\_addr\_to\_kernel(conn->addr, conn->addr\_len,- &io->address);+ return 0; } -static int io\_connect(struct io\_kiocb \*req, bool force\_nonblock,- struct io\_comp\_state \*cs)+static int io\_connect(struct io\_kiocb \*req, unsigned int issue\_flags) { struct io\_async\_connect \_\_io, \*io; unsigned file\_flags; int ret;+ bool force\_nonblock = issue\_flags & IO\_URING\_F\_NONBLOCK;  if (req->async\_data) { io = req->async\_data;@@ -4837,7 +5129,6 @@ static int io\_connect(struct io\_kiocb \*req, bool force\_nonblock, ret = -ENOMEM; goto out; }- io = req->async\_data; memcpy(req->async\_data, &\_\_io, sizeof(\_\_io)); return -EAGAIN; }@@ -4845,246 +5136,350 @@ static int io\_connect(struct io\_kiocb \*req, bool force\_nonblock, ret = -EINTR; out: if (ret < 0)- req\_set\_fail\_links(req);- \_\_io\_req\_complete(req, ret, 0, cs);+ req\_set\_fail(req);+ \_\_io\_req\_complete(req, issue\_flags, ret, 0); return 0; } #else /\* !CONFIG\_NET \*/-static int io\_sendmsg\_prep(struct io\_kiocb \*req, const struct io\_uring\_sqe \*sqe)+#define IO\_NETOP\_FN(op) \+static int io\_##op(struct io\_kiocb \*req, unsigned int issue\_flags) \+{ \+ return -EOPNOTSUPP; \+}++#define IO\_NETOP\_PREP(op) \+IO\_NETOP\_FN(op) \+static int io\_##op##\_prep(struct io\_kiocb \*req, const struct io\_uring\_sqe \*sqe) \+{ \+ return -EOPNOTSUPP; \+} \++#define IO\_NETOP\_PREP\_ASYNC(op) \+IO\_NETOP\_PREP(op) \+static int io\_##op##\_prep\_async(struct io\_kiocb \*req) \+{ \+ return -EOPNOTSUPP; \+}++IO\_NETOP\_PREP\_ASYNC(sendmsg);+IO\_NETOP\_PREP\_ASYNC(recvmsg);+IO\_NETOP\_PREP\_ASYNC(connect);+IO\_NETOP\_PREP(accept);+IO\_NETOP\_FN(send);+IO\_NETOP\_FN(recv);+#endif /\* CONFIG\_NET \*/++struct io\_poll\_table {+ struct poll\_table\_struct pt;+ struct io\_kiocb \*req;+ int nr\_entries;+ int error;+};++#define IO\_POLL\_CANCEL\_FLAG BIT(31)+#define IO\_POLL\_RETRY\_FLAG BIT(30)+#define IO\_POLL\_REF\_MASK GENMASK(29, 0)++/\*+ \* We usually have 1-2 refs taken, 128 is more than enough and we want to+ \* maximise the margin between this amount and the moment when it overflows.+ \*/+#define IO\_POLL\_REF\_BIAS 128++static bool io\_poll\_get\_ownership\_slowpath(struct io\_kiocb \*req) {- return -EOPNOTSUPP;+ int v;++ /\*+ \* poll\_refs are already elevated and we don't have much hope for+ \* grabbing the ownership. Instead of incrementing set a retry flag+ \* to notify the loop that there might have been some change.+ \*/+ v = atomic\_fetch\_or(IO\_POLL\_RETRY\_FLAG, &req->poll\_refs);+ if (v & IO\_POLL\_REF\_MASK)+ return false;+ return !(atomic\_fetch\_inc(&req->poll\_refs) & IO\_POLL\_REF\_MASK); } -static int io\_sendmsg(struct io\_kiocb \*req, bool force\_nonblock,- struct io\_comp\_state \*cs)+/\*+ \* If refs part of ->poll\_refs (see IO\_POLL\_REF\_MASK) is 0, it's free. We can+ \* bump it and acquire ownership. It's disallowed to modify requests while not+ \* owning it, that prevents from races for enqueueing task\_work's and b/w+ \* arming poll and wakeups.+ \*/+static inline bool io\_poll\_get\_ownership(struct io\_kiocb \*req) {- return -EOPNOTSUPP;+ if (unlikely(atomic\_read(&req->poll\_refs) >= IO\_POLL\_REF\_BIAS))+ return io\_poll\_get\_ownership\_slowpath(req);+ return !(atomic\_fetch\_inc(&req->poll\_refs) & IO\_POLL\_REF\_MASK); } -static int io\_send(struct io\_kiocb \*req, bool force\_nonblock,- struct io\_comp\_state \*cs)+static void io\_poll\_mark\_cancelled(struct io\_kiocb \*req) {- return -EOPNOTSUPP;+ atomic\_or(IO\_POLL\_CANCEL\_FLAG, &req->poll\_refs); } -static int io\_recvmsg\_prep(struct io\_kiocb \*req,- const struct io\_uring\_sqe \*sqe)+static struct io\_poll\_iocb \*io\_poll\_get\_double(struct io\_kiocb \*req) {- return -EOPNOTSUPP;+ /\* pure poll stashes this in ->async\_data, poll driven retry elsewhere \*/+ if (req->opcode == IORING\_OP\_POLL\_ADD)+ return req->async\_data;+ return req->apoll->double\_poll; } -static int io\_recvmsg(struct io\_kiocb \*req, bool force\_nonblock,- struct io\_comp\_state \*cs)+static struct io\_poll\_iocb \*io\_poll\_get\_single(struct io\_kiocb \*req) {- return -EOPNOTSUPP;+ if (req->opcode == IORING\_OP\_POLL\_ADD)+ return &req->poll;+ return &req->apoll->poll; } -static int io\_recv(struct io\_kiocb \*req, bool force\_nonblock,- struct io\_comp\_state \*cs)+static void io\_poll\_req\_insert(struct io\_kiocb \*req) {- return -EOPNOTSUPP;+ struct io\_ring\_ctx \*ctx = req->ctx;+ struct hlist\_head \*list;++ list = &ctx->cancel\_hash[hash\_long(req->user\_data, ctx->cancel\_hash\_bits)];+ hlist\_add\_head(&req->hash\_node, list); } -static int io\_accept\_prep(struct io\_kiocb \*req, const struct io\_uring\_sqe \*sqe)+static void io\_init\_poll\_iocb(struct io\_poll\_iocb \*poll, \_\_poll\_t events,+ wait\_queue\_func\_t wake\_func) {- return -EOPNOTSUPP;+ poll->head = NULL;+#define IO\_POLL\_UNMASK (EPOLLERR|EPOLLHUP|EPOLLNVAL|EPOLLRDHUP)+ /\* mask in events that we always want/need \*/+ poll->events = events | IO\_POLL\_UNMASK;+ INIT\_LIST\_HEAD(&poll->wait.entry);+ init\_waitqueue\_func\_entry(&poll->wait, wake\_func); } -static int io\_accept(struct io\_kiocb \*req, bool force\_nonblock,- struct io\_comp\_state \*cs)+static inline void io\_poll\_remove\_entry(struct io\_poll\_iocb \*poll) {- return -EOPNOTSUPP;+ struct wait\_queue\_head \*head = smp\_load\_acquire(&poll->head);++ if (head) {+ spin\_lock\_irq(&head->lock);+ list\_del\_init(&poll->wait.entry);+ poll->head = NULL;+ spin\_unlock\_irq(&head->lock);+ } } -static int io\_connect\_prep(struct io\_kiocb \*req, const struct io\_uring\_sqe \*sqe)+static void io\_poll\_remove\_entries(struct io\_kiocb \*req) {- return -EOPNOTSUPP;+ struct io\_poll\_iocb \*poll = io\_poll\_get\_single(req);+ struct io\_poll\_iocb \*poll\_double = io\_poll\_get\_double(req);++ /\*+ \* While we hold the waitqueue lock and the waitqueue is nonempty,+ \* wake\_up\_pollfree() will wait for us. However, taking the waitqueue+ \* lock in the first place can race with the waitqueue being freed.+ \*+ \* We solve this as eventpoll does: by taking advantage of the fact that+ \* all users of wake\_up\_pollfree() will RCU-delay the actual free. If+ \* we enter rcu\_read\_lock() and see that the pointer to the queue is+ \* non-NULL, we can then lock it without the memory being freed out from+ \* under us.+ \*+ \* Keep holding rcu\_read\_lock() as long as we hold the queue lock, in+ \* case the caller deletes the entry from the queue, leaving it empty.+ \* In that case, only RCU prevents the queue memory from being freed.+ \*/+ rcu\_read\_lock();+ io\_poll\_remove\_entry(poll);+ if (poll\_double)+ io\_poll\_remove\_entry(poll\_double);+ rcu\_read\_unlock(); } -static int io\_connect(struct io\_kiocb \*req, bool force\_nonblock,- struct io\_comp\_state \*cs)+/\*+ \* All poll tw should go through this. Checks for poll events, manages+ \* references, does rewait, etc.+ \*+ \* Returns a negative error on failure. >0 when no action require, which is+ \* either spurious wakeup or multishot CQE is served. 0 when it's done with+ \* the request, then the mask is stored in req->result.+ \*/+static int io\_poll\_check\_events(struct io\_kiocb \*req) {- return -EOPNOTSUPP;-}-#endif /\* CONFIG\_NET \*/+ struct io\_ring\_ctx \*ctx = req->ctx;+ struct io\_poll\_iocb \*poll = io\_poll\_get\_single(req);+ int v; -struct io\_poll\_table {- struct poll\_table\_struct pt;- struct io\_kiocb \*req;- int nr\_entries;- int error;-};+ /\* req->task == current here, checking PF\_EXITING is safe \*/+ if (unlikely(req->task->flags & PF\_EXITING))+ io\_poll\_mark\_cancelled(req); -static int \_\_io\_async\_wake(struct io\_kiocb \*req, struct io\_poll\_iocb \*poll,- \_\_poll\_t mask, task\_work\_func\_t func)-{- bool twa\_signal\_ok;- int ret;+ do {+ v = atomic\_read(&req->poll\_refs); - /\* for instances that support it check for an event match first: \*/- if (mask && !(mask & poll->events))- return 0;+ /\* tw handler should be the owner, and so have some references \*/+ if (WARN\_ON\_ONCE(!(v & IO\_POLL\_REF\_MASK)))+ return 0;+ if (v & IO\_POLL\_CANCEL\_FLAG)+ return -ECANCELED;+ /\*+ \* cqe.res contains only events of the first wake up+ \* and all others are be lost. Redo vfs\_poll() to get+ \* up to date state.+ \*/+ if ((v & IO\_POLL\_REF\_MASK) != 1)+ req->result = 0;+ if (v & IO\_POLL\_RETRY\_FLAG) {+ req->result = 0;+ /\*+ \* We won't find new events that came in between+ \* vfs\_poll and the ref put unless we clear the+ \* flag in advance.+ \*/+ atomic\_andnot(IO\_POLL\_RETRY\_FLAG, &req->poll\_refs);+ v &= ~IO\_POLL\_RETRY\_FLAG;+ } - trace\_io\_uring\_task\_add(req->ctx, req->opcode, req->user\_data, mask);+ if (!req->result) {+ struct poll\_table\_struct pt = { .\_key = poll->events }; - list\_del\_init(&poll->wait.entry);+ req->result = vfs\_poll(req->file, &pt) & poll->events;+ } - req->result = mask;- init\_task\_work(&req->task\_work, func);- percpu\_ref\_get(&req->ctx->refs);+ /\* multishot, just fill an CQE and proceed \*/+ if (req->result && !(poll->events & EPOLLONESHOT)) {+ \_\_poll\_t mask = mangle\_poll(req->result & poll->events);+ bool filled; - /\*- \* If we using the signalfd wait\_queue\_head for this wakeup, then- \* it's not safe to use TWA\_SIGNAL as we could be recursing on the- \* tsk->sighand->siglock on doing the wakeup. Should not be needed- \* either, as the normal wakeup will suffice.- \*/- twa\_signal\_ok = (poll->head != &req->task->sighand->signalfd\_wqh);+ spin\_lock(&ctx->completion\_lock);+ filled = io\_fill\_cqe\_aux(ctx, req->user\_data, mask,+ IORING\_CQE\_F\_MORE);+ io\_commit\_cqring(ctx);+ spin\_unlock(&ctx->completion\_lock);+ if (unlikely(!filled))+ return -ECANCELED;+ io\_cqring\_ev\_posted(ctx);+ } else if (req->result) {+ return 0;+ } - /\*- \* If this fails, then the task is exiting. When a task exits, the- \* work gets canceled, so just cancel this request as well instead- \* of executing it. We can't safely execute it anyway, as we may not- \* have the needed state needed for it anyway.- \*/- ret = io\_req\_task\_work\_add(req, twa\_signal\_ok);- if (unlikely(ret)) {- struct task\_struct \*tsk;+ /\* force the next iteration to vfs\_poll() \*/+ req->result = 0;++ /\*+ \* Release all references, retry if someone tried to restart+ \* task\_work while we were executing it.+ \*/+ } while (atomic\_sub\_return(v & IO\_POLL\_REF\_MASK, &req->poll\_refs) &+ IO\_POLL\_REF\_MASK); - WRITE\_ONCE(poll->canceled, true);- tsk = io\_wq\_get\_task(req->ctx->io\_wq);- task\_work\_add(tsk, &req->task\_work, TWA\_NONE);- wake\_up\_process(tsk);- } return 1; } -static bool io\_poll\_rewait(struct io\_kiocb \*req, struct io\_poll\_iocb \*poll)- \_\_acquires(&req->ctx->completion\_lock)+static void io\_poll\_task\_func(struct io\_kiocb \*req, bool \*locked) { struct io\_ring\_ctx \*ctx = req->ctx;+ int ret; - if (!req->result && !READ\_ONCE(poll->canceled)) {- struct poll\_table\_struct pt = { .\_key = poll->events };-- req->result = vfs\_poll(req->file, &pt) & poll->events;- }+ ret = io\_poll\_check\_events(req);+ if (ret > 0)+ return; - spin\_lock\_irq(&ctx->completion\_lock);- if (!req->result && !READ\_ONCE(poll->canceled)) {- add\_wait\_queue(poll->head, &poll->wait);- return true;+ if (!ret) {+ req->result = mangle\_poll(req->result & req->poll.events);+ } else {+ req->result = ret;+ req\_set\_fail(req); } - return false;-}--static struct io\_poll\_iocb \*io\_poll\_get\_double(struct io\_kiocb \*req)-{- /\* pure poll stashes this in ->async\_data, poll driven retry elsewhere \*/- if (req->opcode == IORING\_OP\_POLL\_ADD)- return req->async\_data;- return req->apoll->double\_poll;-}--static struct io\_poll\_iocb \*io\_poll\_get\_single(struct io\_kiocb \*req)-{- if (req->opcode == IORING\_OP\_POLL\_ADD)- return &req->poll;- return &req->apoll->poll;+ io\_poll\_remove\_entries(req);+ spin\_lock(&ctx->completion\_lock);+ hash\_del(&req->hash\_node);+ spin\_unlock(&ctx->completion\_lock);+ io\_req\_complete\_post(req, req->result, 0); } -static void io\_poll\_remove\_double(struct io\_kiocb \*req)+static void io\_apoll\_task\_func(struct io\_kiocb \*req, bool \*locked) {- struct io\_poll\_iocb \*poll = io\_poll\_get\_double(req);+ struct io\_ring\_ctx \*ctx = req->ctx;+ int ret; - lockdep\_assert\_held(&req->ctx->completion\_lock);+ ret = io\_poll\_check\_events(req);+ if (ret > 0)+ return; - if (poll && poll->head) {- struct wait\_queue\_head \*head = poll->head;+ io\_poll\_remove\_entries(req);+ spin\_lock(&ctx->completion\_lock);+ hash\_del(&req->hash\_node);+ spin\_unlock(&ctx->completion\_lock); - spin\_lock(&head->lock);- list\_del\_init(&poll->wait.entry);- if (poll->wait.private)- refcount\_dec(&req->refs);- poll->head = NULL;- spin\_unlock(&head->lock);- }+ if (!ret)+ io\_req\_task\_submit(req, locked);+ else+ io\_req\_complete\_failed(req, ret); } -static void io\_poll\_complete(struct io\_kiocb \*req, \_\_poll\_t mask, int error)+static void \_\_io\_poll\_execute(struct io\_kiocb \*req, int mask) {- struct io\_ring\_ctx \*ctx = req->ctx;+ req->result = mask;+ if (req->opcode == IORING\_OP\_POLL\_ADD)+ req->io\_task\_work.func = io\_poll\_task\_func;+ else+ req->io\_task\_work.func = io\_apoll\_task\_func; - io\_poll\_remove\_double(req);- req->poll.done = true;- io\_cqring\_fill\_event(req, error ? error : mangle\_poll(mask));- io\_commit\_cqring(ctx);+ trace\_io\_uring\_task\_add(req->ctx, req->opcode, req->user\_data, mask);+ io\_req\_task\_work\_add(req); } -static void io\_poll\_task\_func(struct callback\_head \*cb)+static inline void io\_poll\_execute(struct io\_kiocb \*req, int res) {- struct io\_kiocb \*req = container\_of(cb, struct io\_kiocb, task\_work);- struct io\_ring\_ctx \*ctx = req->ctx;- struct io\_kiocb \*nxt;-- if (io\_poll\_rewait(req, &req->poll)) {- spin\_unlock\_irq(&ctx->completion\_lock);- } else {- hash\_del(&req->hash\_node);- io\_poll\_complete(req, req->result, 0);- spin\_unlock\_irq(&ctx->completion\_lock);-- nxt = io\_put\_req\_find\_next(req);- io\_cqring\_ev\_posted(ctx);- if (nxt)- \_\_io\_req\_task\_submit(nxt);- }+ if (io\_poll\_get\_ownership(req))+ \_\_io\_poll\_execute(req, res);+} - percpu\_ref\_put(&ctx->refs);+static void io\_poll\_cancel\_req(struct io\_kiocb \*req)+{+ io\_poll\_mark\_cancelled(req);+ /\* kick tw, which should complete the request \*/+ io\_poll\_execute(req, 0); } -static int io\_poll\_double\_wake(struct wait\_queue\_entry \*wait, unsigned mode,- int sync, void \*key)+static int io\_poll\_wake(struct wait\_queue\_entry \*wait, unsigned mode, int sync,+ void \*key) { struct io\_kiocb \*req = wait->private;- struct io\_poll\_iocb \*poll = io\_poll\_get\_single(req);+ struct io\_poll\_iocb \*poll = container\_of(wait, struct io\_poll\_iocb,+ wait); \_\_poll\_t mask = key\_to\_poll(key); - /\* for instances that support it check for an event match first: \*/- if (mask && !(mask & poll->events))- return 0;+ if (unlikely(mask & POLLFREE)) {+ io\_poll\_mark\_cancelled(req);+ /\* we have to kick tw in case it's not already \*/+ io\_poll\_execute(req, 0); - list\_del\_init(&wait->entry);+ /\*+ \* If the waitqueue is being freed early but someone is already+ \* holds ownership over it, we have to tear down the request as+ \* best we can. That means immediately removing the request from+ \* its waitqueue and preventing all further accesses to the+ \* waitqueue via the request.+ \*/+ list\_del\_init(&poll->wait.entry); - if (poll && poll->head) {- bool done;-- spin\_lock(&poll->head->lock);- done = list\_empty(&poll->wait.entry);- if (!done)- list\_del\_init(&poll->wait.entry);- /\* make sure double remove sees this as being gone \*/- wait->private = NULL;- spin\_unlock(&poll->head->lock);- if (!done) {- /\* use wait func handler, so it matches the rq type \*/- poll->wait.func(&poll->wait, mode, sync, key);- }+ /\*+ \* Careful: this \*must\* be the last step, since as soon+ \* as req->head is NULL'ed out, the request can be+ \* completed and freed, since aio\_poll\_complete\_work()+ \* will no longer need to take the waitqueue lock.+ \*/+ smp\_store\_release(&poll->head, NULL);+ return 1; }- refcount\_dec(&req->refs);- return 1;-} -static void io\_init\_poll\_iocb(struct io\_poll\_iocb \*poll, \_\_poll\_t events,- wait\_queue\_func\_t wake\_func)-{- poll->head = NULL;- poll->done = false;- poll->canceled = false;- poll->events = events;- INIT\_LIST\_HEAD(&poll->wait.entry);- init\_waitqueue\_func\_entry(&poll->wait, wake\_func);+ /\* for instances that support it check for an event match first \*/+ if (mask && !(mask & poll->events))+ return 0;++ if (io\_poll\_get\_ownership(req))+ \_\_io\_poll\_execute(req, mask);+ return 1; }  static void \_\_io\_queue\_proc(struct io\_poll\_iocb \*poll, struct io\_poll\_table \*pt,@@ -5099,29 +5494,31 @@ static void \_\_io\_queue\_proc(struct io\_poll\_iocb \*poll, struct io\_poll\_table \*pt, \* if this happens. \*/ if (unlikely(pt->nr\_entries)) {- struct io\_poll\_iocb \*poll\_one = poll;+ struct io\_poll\_iocb \*first = poll; + /\* double add on the same waitqueue head, ignore \*/+ if (first->head == head)+ return; /\* already have a 2nd entry, fail a third attempt \*/ if (\*poll\_ptr) {+ if ((\*poll\_ptr)->head == head)+ return; pt->error = -EINVAL; return; }- /\* double add on the same waitqueue head, ignore \*/- if (poll->head == head)- return;+ poll = kmalloc(sizeof(\*poll), GFP\_ATOMIC); if (!poll) { pt->error = -ENOMEM; return; }- io\_init\_poll\_iocb(poll, poll\_one->events, io\_poll\_double\_wake);- refcount\_inc(&req->refs);- poll->wait.private = req;+ io\_init\_poll\_iocb(poll, first->events, first->wait.func); \*poll\_ptr = poll; }  pt->nr\_entries++; poll->head = head;+ poll->wait.private = req;  if (poll->events & EPOLLEXCLUSIVE) add\_wait\_queue\_exclusive(head, &poll->wait);@@ -5129,83 +5526,23 @@ static void \_\_io\_queue\_proc(struct io\_poll\_iocb \*poll, struct io\_poll\_table \*pt, add\_wait\_queue(head, &poll->wait); } -static void io\_async\_queue\_proc(struct file \*file, struct wait\_queue\_head \*head,+static void io\_poll\_queue\_proc(struct file \*file, struct wait\_queue\_head \*head, struct poll\_table\_struct \*p) { struct io\_poll\_table \*pt = container\_of(p, struct io\_poll\_table, pt);- struct async\_poll \*apoll = pt->req->apoll;-- \_\_io\_queue\_proc(&apoll->poll, pt, head, &apoll->double\_poll);-}--static void io\_async\_task\_func(struct callback\_head \*cb)-{- struct io\_kiocb \*req = container\_of(cb, struct io\_kiocb, task\_work);- struct async\_poll \*apoll = req->apoll;- struct io\_ring\_ctx \*ctx = req->ctx;-- trace\_io\_uring\_task\_run(req->ctx, req->opcode, req->user\_data);-- if (io\_poll\_rewait(req, &apoll->poll)) {- spin\_unlock\_irq(&ctx->completion\_lock);- percpu\_ref\_put(&ctx->refs);- return;- }-- /\* If req is still hashed, it cannot have been canceled. Don't check. \*/- if (hash\_hashed(&req->hash\_node))- hash\_del(&req->hash\_node);-- io\_poll\_remove\_double(req);- spin\_unlock\_irq(&ctx->completion\_lock);-- if (!READ\_ONCE(apoll->poll.canceled))- \_\_io\_req\_task\_submit(req);- else- \_\_io\_req\_task\_cancel(req, -ECANCELED);-- percpu\_ref\_put(&ctx->refs);- kfree(apoll->double\_poll);- kfree(apoll);-}--static int io\_async\_wake(struct wait\_queue\_entry \*wait, unsigned mode, int sync,- void \*key)-{- struct io\_kiocb \*req = wait->private;- struct io\_poll\_iocb \*poll = &req->apoll->poll; - trace\_io\_uring\_poll\_wake(req->ctx, req->opcode, req->user\_data,- key\_to\_poll(key));-- return \_\_io\_async\_wake(req, poll, key\_to\_poll(key), io\_async\_task\_func);+ \_\_io\_queue\_proc(&pt->req->poll, pt, head,+ (struct io\_poll\_iocb \*\*) &pt->req->async\_data); } -static void io\_poll\_req\_insert(struct io\_kiocb \*req)+static int \_\_io\_arm\_poll\_handler(struct io\_kiocb \*req,+ struct io\_poll\_iocb \*poll,+ struct io\_poll\_table \*ipt, \_\_poll\_t mask) { struct io\_ring\_ctx \*ctx = req->ctx;- struct hlist\_head \*list;-- list = &ctx->cancel\_hash[hash\_long(req->user\_data, ctx->cancel\_hash\_bits)];- hlist\_add\_head(&req->hash\_node, list);-}--static \_\_poll\_t \_\_io\_arm\_poll\_handler(struct io\_kiocb \*req,- struct io\_poll\_iocb \*poll,- struct io\_poll\_table \*ipt, \_\_poll\_t mask,- wait\_queue\_func\_t wake\_func)- \_\_acquires(&ctx->completion\_lock)-{- struct io\_ring\_ctx \*ctx = req->ctx;- bool cancel = false;-- if (req->file->f\_op->may\_pollfree) {- spin\_lock\_irq(&ctx->completion\_lock);- return -EOPNOTSUPP;- }  INIT\_HLIST\_NODE(&req->hash\_node);- io\_init\_poll\_iocb(poll, mask, wake\_func);+ io\_init\_poll\_iocb(poll, mask, io\_poll\_wake); poll->file = req->file; poll->wait.private = req; @@ -5214,169 +5551,138 @@ static \_\_poll\_t \_\_io\_arm\_poll\_handler(struct io\_kiocb \*req, ipt->error = 0; ipt->nr\_entries = 0; + /\*+ \* Take the ownership to delay any tw execution up until we're done+ \* with poll arming. see io\_poll\_get\_ownership().+ \*/+ atomic\_set(&req->poll\_refs, 1); mask = vfs\_poll(req->file, &ipt->pt) & poll->events;- if (unlikely(!ipt->nr\_entries) && !ipt->error)- ipt->error = -EINVAL;-- spin\_lock\_irq(&ctx->completion\_lock);- if (ipt->error)- io\_poll\_remove\_double(req);- if (likely(poll->head)) {- spin\_lock(&poll->head->lock);- if (unlikely(list\_empty(&poll->wait.entry))) {- if (ipt->error)- cancel = true;++ if (mask && (poll->events & EPOLLONESHOT)) {+ io\_poll\_remove\_entries(req);+ /\* no one else has access to the req, forget about the ref \*/+ return mask;+ }+ if (!mask && unlikely(ipt->error || !ipt->nr\_entries)) {+ io\_poll\_remove\_entries(req);+ if (!ipt->error)+ ipt->error = -EINVAL;+ return 0;+ }++ spin\_lock(&ctx->completion\_lock);+ io\_poll\_req\_insert(req);+ spin\_unlock(&ctx->completion\_lock);++ if (mask) {+ /\* can't multishot if failed, just queue the event we've got \*/+ if (unlikely(ipt->error || !ipt->nr\_entries)) {+ poll->events |= EPOLLONESHOT; ipt->error = 0;- mask = 0; }- if (mask || ipt->error)- list\_del\_init(&poll->wait.entry);- else if (cancel)- WRITE\_ONCE(poll->canceled, true);- else if (!poll->done) /\* actually waiting for an event \*/- io\_poll\_req\_insert(req);- spin\_unlock(&poll->head->lock);+ \_\_io\_poll\_execute(req, mask);+ return 0; } - return mask;+ /\*+ \* Try to release ownership. If we see a change of state, e.g.+ \* poll was waken up, queue up a tw, it'll deal with it.+ \*/+ if (atomic\_cmpxchg(&req->poll\_refs, 1, 0) != 1)+ \_\_io\_poll\_execute(req, 0);+ return 0;+}++static void io\_async\_queue\_proc(struct file \*file, struct wait\_queue\_head \*head,+ struct poll\_table\_struct \*p)+{+ struct io\_poll\_table \*pt = container\_of(p, struct io\_poll\_table, pt);+ struct async\_poll \*apoll = pt->req->apoll;++ \_\_io\_queue\_proc(&apoll->poll, pt, head, &apoll->double\_poll); } -static bool io\_arm\_poll\_handler(struct io\_kiocb \*req)+enum {+ IO\_APOLL\_OK,+ IO\_APOLL\_ABORTED,+ IO\_APOLL\_READY+};++static int io\_arm\_poll\_handler(struct io\_kiocb \*req) { const struct io\_op\_def \*def = &io\_op\_defs[req->opcode]; struct io\_ring\_ctx \*ctx = req->ctx; struct async\_poll \*apoll; struct io\_poll\_table ipt;- \_\_poll\_t mask, ret;- int rw;+ \_\_poll\_t mask = EPOLLONESHOT | POLLERR | POLLPRI;+ int ret;  if (!req->file || !file\_can\_poll(req->file))- return false;+ return IO\_APOLL\_ABORTED; if (req->flags & REQ\_F\_POLLED)- return false;- if (def->pollin)- rw = READ;- else if (def->pollout)- rw = WRITE;- else- return false;- /\* if we can't nonblock try, then no point in arming a poll handler \*/- if (!io\_file\_supports\_async(req->file, rw))- return false;+ return IO\_APOLL\_ABORTED;+ if (!def->pollin && !def->pollout)+ return IO\_APOLL\_ABORTED;++ if (def->pollin) {+ mask |= POLLIN | POLLRDNORM;++ /\* If reading from MSG\_ERRQUEUE using recvmsg, ignore POLLIN \*/+ if ((req->opcode == IORING\_OP\_RECVMSG) &&+ (req->sr\_msg.msg\_flags & MSG\_ERRQUEUE))+ mask &= ~POLLIN;+ } else {+ mask |= POLLOUT | POLLWRNORM;+ }  apoll = kmalloc(sizeof(\*apoll), GFP\_ATOMIC); if (unlikely(!apoll))- return false;+ return IO\_APOLL\_ABORTED; apoll->double\_poll = NULL;-- req->flags |= REQ\_F\_POLLED; req->apoll = apoll;-- mask = 0;- if (def->pollin)- mask |= POLLIN | POLLRDNORM;- if (def->pollout)- mask |= POLLOUT | POLLWRNORM;-- /\* If reading from MSG\_ERRQUEUE using recvmsg, ignore POLLIN \*/- if ((req->opcode == IORING\_OP\_RECVMSG) &&- (req->sr\_msg.msg\_flags & MSG\_ERRQUEUE))- mask &= ~POLLIN;-- mask |= POLLERR | POLLPRI;-+ req->flags |= REQ\_F\_POLLED; ipt.pt.\_qproc = io\_async\_queue\_proc; - ret = \_\_io\_arm\_poll\_handler(req, &apoll->poll, &ipt, mask,- io\_async\_wake);- if (ret || ipt.error) {- io\_poll\_remove\_double(req);- spin\_unlock\_irq(&ctx->completion\_lock);- kfree(apoll->double\_poll);- kfree(apoll);- return false;- }- spin\_unlock\_irq(&ctx->completion\_lock);- trace\_io\_uring\_poll\_arm(ctx, req->opcode, req->user\_data, mask,- apoll->poll.events);- return true;-}--static bool \_\_io\_poll\_remove\_one(struct io\_kiocb \*req,- struct io\_poll\_iocb \*poll)-{- bool do\_complete = false;+ ret = \_\_io\_arm\_poll\_handler(req, &apoll->poll, &ipt, mask);+ if (ret || ipt.error)+ return ret ? IO\_APOLL\_READY : IO\_APOLL\_ABORTED; - spin\_lock(&poll->head->lock);- WRITE\_ONCE(poll->canceled, true);- if (!list\_empty(&poll->wait.entry)) {- list\_del\_init(&poll->wait.entry);- do\_complete = true;- }- spin\_unlock(&poll->head->lock);- hash\_del(&req->hash\_node);- return do\_complete;-}--static bool io\_poll\_remove\_one(struct io\_kiocb \*req)-{- bool do\_complete;-- io\_poll\_remove\_double(req);-- if (req->opcode == IORING\_OP\_POLL\_ADD) {- do\_complete = \_\_io\_poll\_remove\_one(req, &req->poll);- } else {- struct async\_poll \*apoll = req->apoll;-- /\* non-poll requests have submit ref still \*/- do\_complete = \_\_io\_poll\_remove\_one(req, &apoll->poll);- if (do\_complete) {- io\_put\_req(req);- kfree(apoll->double\_poll);- kfree(apoll);- }- }-- if (do\_complete) {- io\_cqring\_fill\_event(req, -ECANCELED);- io\_commit\_cqring(req->ctx);- req\_set\_fail\_links(req);- io\_put\_req\_deferred(req, 1);- }-- return do\_complete;+ trace\_io\_uring\_poll\_arm(ctx, req, req->opcode, req->user\_data,+ mask, apoll->poll.events);+ return IO\_APOLL\_OK; }  /\* \* Returns true if we found and killed one or more poll requests \*/ static bool io\_poll\_remove\_all(struct io\_ring\_ctx \*ctx, struct task\_struct \*tsk,- struct files\_struct \*files)+ bool cancel\_all) { struct hlist\_node \*tmp; struct io\_kiocb \*req;- int posted = 0, i;+ bool found = false;+ int i; - spin\_lock\_irq(&ctx->completion\_lock);+ spin\_lock(&ctx->completion\_lock); for (i = 0; i < (1U << ctx->cancel\_hash\_bits); i++) { struct hlist\_head \*list;  list = &ctx->cancel\_hash[i]; hlist\_for\_each\_entry\_safe(req, tmp, list, hash\_node) {- if (io\_match\_task(req, tsk, files))- posted += io\_poll\_remove\_one(req);+ if (io\_match\_task\_safe(req, tsk, cancel\_all)) {+ hlist\_del\_init(&req->hash\_node);+ io\_poll\_cancel\_req(req);+ found = true;+ } } }- spin\_unlock\_irq(&ctx->completion\_lock);-- if (posted)- io\_cqring\_ev\_posted(ctx);-- return posted != 0;+ spin\_unlock(&ctx->completion\_lock);+ return found; } -static int io\_poll\_cancel(struct io\_ring\_ctx \*ctx, \_\_u64 sqe\_addr)+static struct io\_kiocb \*io\_poll\_find(struct io\_ring\_ctx \*ctx, \_\_u64 sqe\_addr,+ bool poll\_only)+ \_\_must\_hold(&ctx->completion\_lock) { struct hlist\_head \*list; struct io\_kiocb \*req;@@ -5385,107 +5691,161 @@ static int io\_poll\_cancel(struct io\_ring\_ctx \*ctx, \_\_u64 sqe\_addr) hlist\_for\_each\_entry(req, list, hash\_node) { if (sqe\_addr != req->user\_data) continue;- if (io\_poll\_remove\_one(req))- return 0;- return -EALREADY;+ if (poll\_only && req->opcode != IORING\_OP\_POLL\_ADD)+ continue;+ return req; }-- return -ENOENT;+ return NULL; } -static int io\_poll\_remove\_prep(struct io\_kiocb \*req,- const struct io\_uring\_sqe \*sqe)+static bool io\_poll\_disarm(struct io\_kiocb \*req)+ \_\_must\_hold(&ctx->completion\_lock) {- if (unlikely(req->ctx->flags & IORING\_SETUP\_IOPOLL))- return -EINVAL;- if (sqe->ioprio || sqe->off || sqe->len || sqe->buf\_index ||- sqe->poll\_events)- return -EINVAL;-- req->poll.addr = READ\_ONCE(sqe->addr);- return 0;+ if (!io\_poll\_get\_ownership(req))+ return false;+ io\_poll\_remove\_entries(req);+ hash\_del(&req->hash\_node);+ return true; } -/\*- \* Find a running poll command that matches one specified in sqe->addr,- \* and remove it if found.- \*/-static int io\_poll\_remove(struct io\_kiocb \*req)+static int io\_poll\_cancel(struct io\_ring\_ctx \*ctx, \_\_u64 sqe\_addr,+ bool poll\_only)+ \_\_must\_hold(&ctx->completion\_lock) {- struct io\_ring\_ctx \*ctx = req->ctx;- u64 addr;- int ret;+ struct io\_kiocb \*req = io\_poll\_find(ctx, sqe\_addr, poll\_only); - addr = req->poll.addr;- spin\_lock\_irq(&ctx->completion\_lock);- ret = io\_poll\_cancel(ctx, addr);- spin\_unlock\_irq(&ctx->completion\_lock);-- if (ret < 0)- req\_set\_fail\_links(req);- io\_req\_complete(req, ret);+ if (!req)+ return -ENOENT;+ io\_poll\_cancel\_req(req); return 0; } -static int io\_poll\_wake(struct wait\_queue\_entry \*wait, unsigned mode, int sync,- void \*key)+static \_\_poll\_t io\_poll\_parse\_events(const struct io\_uring\_sqe \*sqe,+ unsigned int flags) {- struct io\_kiocb \*req = wait->private;- struct io\_poll\_iocb \*poll = &req->poll;+ u32 events; - return \_\_io\_async\_wake(req, poll, key\_to\_poll(key), io\_poll\_task\_func);+ events = READ\_ONCE(sqe->poll32\_events);+#ifdef \_\_BIG\_ENDIAN+ events = swahw32(events);+#endif+ if (!(flags & IORING\_POLL\_ADD\_MULTI))+ events |= EPOLLONESHOT;+ return demangle\_poll(events) | (events & (EPOLLEXCLUSIVE|EPOLLONESHOT)); } -static void io\_poll\_queue\_proc(struct file \*file, struct wait\_queue\_head \*head,- struct poll\_table\_struct \*p)+static int io\_poll\_update\_prep(struct io\_kiocb \*req,+ const struct io\_uring\_sqe \*sqe) {- struct io\_poll\_table \*pt = container\_of(p, struct io\_poll\_table, pt);+ struct io\_poll\_update \*upd = &req->poll\_update;+ u32 flags;++ if (unlikely(req->ctx->flags & IORING\_SETUP\_IOPOLL))+ return -EINVAL;+ if (sqe->ioprio || sqe->buf\_index || sqe->splice\_fd\_in)+ return -EINVAL;+ flags = READ\_ONCE(sqe->len);+ if (flags & ~(IORING\_POLL\_UPDATE\_EVENTS | IORING\_POLL\_UPDATE\_USER\_DATA |+ IORING\_POLL\_ADD\_MULTI))+ return -EINVAL;+ /\* meaningless without update \*/+ if (flags == IORING\_POLL\_ADD\_MULTI)+ return -EINVAL;++ upd->old\_user\_data = READ\_ONCE(sqe->addr);+ upd->update\_events = flags & IORING\_POLL\_UPDATE\_EVENTS;+ upd->update\_user\_data = flags & IORING\_POLL\_UPDATE\_USER\_DATA; - \_\_io\_queue\_proc(&pt->req->poll, pt, head, (struct io\_poll\_iocb \*\*) &pt->req->async\_data);+ upd->new\_user\_data = READ\_ONCE(sqe->off);+ if (!upd->update\_user\_data && upd->new\_user\_data)+ return -EINVAL;+ if (upd->update\_events)+ upd->events = io\_poll\_parse\_events(sqe, flags);+ else if (sqe->poll32\_events)+ return -EINVAL;++ return 0; }  static int io\_poll\_add\_prep(struct io\_kiocb \*req, const struct io\_uring\_sqe \*sqe) { struct io\_poll\_iocb \*poll = &req->poll;- u32 events;+ u32 flags;  if (unlikely(req->ctx->flags & IORING\_SETUP\_IOPOLL)) return -EINVAL;- if (sqe->addr || sqe->ioprio || sqe->off || sqe->len || sqe->buf\_index)+ if (sqe->ioprio || sqe->buf\_index || sqe->off || sqe->addr)+ return -EINVAL;+ flags = READ\_ONCE(sqe->len);+ if (flags & ~IORING\_POLL\_ADD\_MULTI) return -EINVAL; - events = READ\_ONCE(sqe->poll32\_events);-#ifdef \_\_BIG\_ENDIAN- events = swahw32(events);-#endif- poll->events = demangle\_poll(events) | EPOLLERR | EPOLLHUP |- (events & EPOLLEXCLUSIVE);+ io\_req\_set\_refcount(req);+ poll->events = io\_poll\_parse\_events(sqe, flags); return 0; } -static int io\_poll\_add(struct io\_kiocb \*req)+static int io\_poll\_add(struct io\_kiocb \*req, unsigned int issue\_flags) { struct io\_poll\_iocb \*poll = &req->poll;- struct io\_ring\_ctx \*ctx = req->ctx; struct io\_poll\_table ipt;- \_\_poll\_t mask;+ int ret;  ipt.pt.\_qproc = io\_poll\_queue\_proc; - mask = \_\_io\_arm\_poll\_handler(req, &req->poll, &ipt, poll->events,- io\_poll\_wake);+ ret = \_\_io\_arm\_poll\_handler(req, &req->poll, &ipt, poll->events);+ if (!ret && ipt.error)+ req\_set\_fail(req);+ ret = ret ?: ipt.error;+ if (ret)+ \_\_io\_req\_complete(req, issue\_flags, ret, 0);+ return 0;+} - if (mask) { /\* no async, we'd stolen it \*/- ipt.error = 0;- io\_poll\_complete(req, mask, 0);+static int io\_poll\_update(struct io\_kiocb \*req, unsigned int issue\_flags)+{+ struct io\_ring\_ctx \*ctx = req->ctx;+ struct io\_kiocb \*preq;+ int ret2, ret = 0;++ spin\_lock(&ctx->completion\_lock);+ preq = io\_poll\_find(ctx, req->poll\_update.old\_user\_data, true);+ if (!preq || !io\_poll\_disarm(preq)) {+ spin\_unlock(&ctx->completion\_lock);+ ret = preq ? -EALREADY : -ENOENT;+ goto out; }- spin\_unlock\_irq(&ctx->completion\_lock);+ spin\_unlock(&ctx->completion\_lock); - if (mask) {- io\_cqring\_ev\_posted(ctx);- io\_put\_req(req);+ if (req->poll\_update.update\_events || req->poll\_update.update\_user\_data) {+ /\* only mask one event flags, keep behavior flags \*/+ if (req->poll\_update.update\_events) {+ preq->poll.events &= ~0xffff;+ preq->poll.events |= req->poll\_update.events & 0xffff;+ preq->poll.events |= IO\_POLL\_UNMASK;+ }+ if (req->poll\_update.update\_user\_data)+ preq->user\_data = req->poll\_update.new\_user\_data;++ ret2 = io\_poll\_add(preq, issue\_flags);+ /\* successfully updated, don't complete poll request \*/+ if (!ret2)+ goto out; }- return ipt.error;+ req\_set\_fail(preq);+ io\_req\_complete(preq, -ECANCELED);+out:+ if (ret < 0)+ req\_set\_fail(req);+ /\* complete update request, we're done with it \*/+ io\_req\_complete(req, ret);+ return 0;+}++static void io\_req\_task\_timeout(struct io\_kiocb \*req, bool \*locked)+{+ req\_set\_fail(req);+ io\_req\_complete\_post(req, -ETIME, 0); }  static enum hrtimer\_restart io\_timeout\_fn(struct hrtimer \*timer)@@ -5496,88 +5856,182 @@ static enum hrtimer\_restart io\_timeout\_fn(struct hrtimer \*timer) struct io\_ring\_ctx \*ctx = req->ctx; unsigned long flags; - spin\_lock\_irqsave(&ctx->completion\_lock, flags);+ spin\_lock\_irqsave(&ctx->timeout\_lock, flags); list\_del\_init(&req->timeout.list); atomic\_set(&req->ctx->cq\_timeouts, atomic\_read(&req->ctx->cq\_timeouts) + 1);+ spin\_unlock\_irqrestore(&ctx->timeout\_lock, flags); - io\_cqring\_fill\_event(req, -ETIME);- io\_commit\_cqring(ctx);- spin\_unlock\_irqrestore(&ctx->completion\_lock, flags);-- io\_cqring\_ev\_posted(ctx);- req\_set\_fail\_links(req);- io\_put\_req(req);+ req->io\_task\_work.func = io\_req\_task\_timeout;+ io\_req\_task\_work\_add(req); return HRTIMER\_NORESTART; } -static int \_\_io\_timeout\_cancel(struct io\_kiocb \*req)+static struct io\_kiocb \*io\_timeout\_extract(struct io\_ring\_ctx \*ctx,+ \_\_u64 user\_data)+ \_\_must\_hold(&ctx->timeout\_lock) {- struct io\_timeout\_data \*io = req->async\_data;- int ret;+ struct io\_timeout\_data \*io;+ struct io\_kiocb \*req;+ bool found = false; - ret = hrtimer\_try\_to\_cancel(&io->timer);- if (ret == -1)- return -EALREADY;+ list\_for\_each\_entry(req, &ctx->timeout\_list, timeout.list) {+ found = user\_data == req->user\_data;+ if (found)+ break;+ }+ if (!found)+ return ERR\_PTR(-ENOENT);++ io = req->async\_data;+ if (hrtimer\_try\_to\_cancel(&io->timer) == -1)+ return ERR\_PTR(-EALREADY); list\_del\_init(&req->timeout.list);+ return req;+}++static int io\_timeout\_cancel(struct io\_ring\_ctx \*ctx, \_\_u64 user\_data)+ \_\_must\_hold(&ctx->completion\_lock)+ \_\_must\_hold(&ctx->timeout\_lock)+{+ struct io\_kiocb \*req = io\_timeout\_extract(ctx, user\_data);++ if (IS\_ERR(req))+ return PTR\_ERR(req); - req\_set\_fail\_links(req);- io\_cqring\_fill\_event(req, -ECANCELED);- io\_put\_req\_deferred(req, 1);+ req\_set\_fail(req);+ io\_fill\_cqe\_req(req, -ECANCELED, 0);+ io\_put\_req\_deferred(req); return 0; } -static int io\_timeout\_cancel(struct io\_ring\_ctx \*ctx, \_\_u64 user\_data)+static clockid\_t io\_timeout\_get\_clock(struct io\_timeout\_data \*data)+{+ switch (data->flags & IORING\_TIMEOUT\_CLOCK\_MASK) {+ case IORING\_TIMEOUT\_BOOTTIME:+ return CLOCK\_BOOTTIME;+ case IORING\_TIMEOUT\_REALTIME:+ return CLOCK\_REALTIME;+ default:+ /\* can't happen, vetted at prep time \*/+ WARN\_ON\_ONCE(1);+ fallthrough;+ case 0:+ return CLOCK\_MONOTONIC;+ }+}++static int io\_linked\_timeout\_update(struct io\_ring\_ctx \*ctx, \_\_u64 user\_data,+ struct timespec64 \*ts, enum hrtimer\_mode mode)+ \_\_must\_hold(&ctx->timeout\_lock) {+ struct io\_timeout\_data \*io; struct io\_kiocb \*req;- int ret = -ENOENT;+ bool found = false; - list\_for\_each\_entry(req, &ctx->timeout\_list, timeout.list) {- if (user\_data == req->user\_data) {- ret = 0;+ list\_for\_each\_entry(req, &ctx->ltimeout\_list, timeout.list) {+ found = user\_data == req->user\_data;+ if (found) break;- } }+ if (!found)+ return -ENOENT; - if (ret == -ENOENT)- return ret;+ io = req->async\_data;+ if (hrtimer\_try\_to\_cancel(&io->timer) == -1)+ return -EALREADY;+ hrtimer\_init(&io->timer, io\_timeout\_get\_clock(io), mode);+ io->timer.function = io\_link\_timeout\_fn;+ hrtimer\_start(&io->timer, timespec64\_to\_ktime(\*ts), mode);+ return 0;+} - return \_\_io\_timeout\_cancel(req);+static int io\_timeout\_update(struct io\_ring\_ctx \*ctx, \_\_u64 user\_data,+ struct timespec64 \*ts, enum hrtimer\_mode mode)+ \_\_must\_hold(&ctx->timeout\_lock)+{+ struct io\_kiocb \*req = io\_timeout\_extract(ctx, user\_data);+ struct io\_timeout\_data \*data;++ if (IS\_ERR(req))+ return PTR\_ERR(req);++ req->timeout.off = 0; /\* noseq \*/+ data = req->async\_data;+ list\_add\_tail(&req->timeout.list, &ctx->timeout\_list);+ hrtimer\_init(&data->timer, io\_timeout\_get\_clock(data), mode);+ data->timer.function = io\_timeout\_fn;+ hrtimer\_start(&data->timer, timespec64\_to\_ktime(\*ts), mode);+ return 0; }  static int io\_timeout\_remove\_prep(struct io\_kiocb \*req, const struct io\_uring\_sqe \*sqe) {+ struct io\_timeout\_rem \*tr = &req->timeout\_rem;+ if (unlikely(req->ctx->flags & IORING\_SETUP\_IOPOLL)) return -EINVAL; if (unlikely(req->flags & (REQ\_F\_FIXED\_FILE | REQ\_F\_BUFFER\_SELECT))) return -EINVAL;- if (sqe->ioprio || sqe->buf\_index || sqe->len || sqe->timeout\_flags ||- sqe->splice\_fd\_in)+ if (sqe->ioprio || sqe->buf\_index || sqe->len || sqe->splice\_fd\_in) return -EINVAL; - req->timeout\_rem.addr = READ\_ONCE(sqe->addr);+ tr->ltimeout = false;+ tr->addr = READ\_ONCE(sqe->addr);+ tr->flags = READ\_ONCE(sqe->timeout\_flags);+ if (tr->flags & IORING\_TIMEOUT\_UPDATE\_MASK) {+ if (hweight32(tr->flags & IORING\_TIMEOUT\_CLOCK\_MASK) > 1)+ return -EINVAL;+ if (tr->flags & IORING\_LINK\_TIMEOUT\_UPDATE)+ tr->ltimeout = true;+ if (tr->flags & ~(IORING\_TIMEOUT\_UPDATE\_MASK|IORING\_TIMEOUT\_ABS))+ return -EINVAL;+ if (get\_timespec64(&tr->ts, u64\_to\_user\_ptr(sqe->addr2)))+ return -EFAULT;+ } else if (tr->flags) {+ /\* timeout removal doesn't support flags \*/+ return -EINVAL;+ }+ return 0; } +static inline enum hrtimer\_mode io\_translate\_timeout\_mode(unsigned int flags)+{+ return (flags & IORING\_TIMEOUT\_ABS) ? HRTIMER\_MODE\_ABS+ : HRTIMER\_MODE\_REL;+}+ /\* \* Remove or update an existing timeout command \*/-static int io\_timeout\_remove(struct io\_kiocb \*req)+static int io\_timeout\_remove(struct io\_kiocb \*req, unsigned int issue\_flags) {+ struct io\_timeout\_rem \*tr = &req->timeout\_rem; struct io\_ring\_ctx \*ctx = req->ctx; int ret; - spin\_lock\_irq(&ctx->completion\_lock);- ret = io\_timeout\_cancel(ctx, req->timeout\_rem.addr);+ if (!(req->timeout\_rem.flags & IORING\_TIMEOUT\_UPDATE)) {+ spin\_lock(&ctx->completion\_lock);+ spin\_lock\_irq(&ctx->timeout\_lock);+ ret = io\_timeout\_cancel(ctx, tr->addr);+ spin\_unlock\_irq(&ctx->timeout\_lock);+ spin\_unlock(&ctx->completion\_lock);+ } else {+ enum hrtimer\_mode mode = io\_translate\_timeout\_mode(tr->flags);++ spin\_lock\_irq(&ctx->timeout\_lock);+ if (tr->ltimeout)+ ret = io\_linked\_timeout\_update(ctx, tr->addr, &tr->ts, mode);+ else+ ret = io\_timeout\_update(ctx, tr->addr, &tr->ts, mode);+ spin\_unlock\_irq(&ctx->timeout\_lock);+ } - io\_cqring\_fill\_event(req, ret);- io\_commit\_cqring(ctx);- spin\_unlock\_irq(&ctx->completion\_lock);- io\_cqring\_ev\_posted(ctx); if (ret < 0)- req\_set\_fail\_links(req);- io\_put\_req(req);+ req\_set\_fail(req);+ io\_req\_complete\_post(req, ret, 0); return 0; } @@ -5596,38 +6050,52 @@ static int io\_timeout\_prep(struct io\_kiocb \*req, const struct io\_uring\_sqe \*sqe, if (off && is\_timeout\_link) return -EINVAL; flags = READ\_ONCE(sqe->timeout\_flags);- if (flags & ~IORING\_TIMEOUT\_ABS)+ if (flags & ~(IORING\_TIMEOUT\_ABS | IORING\_TIMEOUT\_CLOCK\_MASK))+ return -EINVAL;+ /\* more than one clock specified is invalid, obviously \*/+ if (hweight32(flags & IORING\_TIMEOUT\_CLOCK\_MASK) > 1) return -EINVAL; + INIT\_LIST\_HEAD(&req->timeout.list); req->timeout.off = off;+ if (unlikely(off && !req->ctx->off\_timeout\_used))+ req->ctx->off\_timeout\_used = true;  if (!req->async\_data && io\_alloc\_async\_data(req)) return -ENOMEM;  data = req->async\_data; data->req = req;+ data->flags = flags;  if (get\_timespec64(&data->ts, u64\_to\_user\_ptr(sqe->addr))) return -EFAULT; - if (flags & IORING\_TIMEOUT\_ABS)- data->mode = HRTIMER\_MODE\_ABS;- else- data->mode = HRTIMER\_MODE\_REL;- INIT\_LIST\_HEAD(&req->timeout.list);- hrtimer\_init(&data->timer, CLOCK\_MONOTONIC, data->mode);+ data->mode = io\_translate\_timeout\_mode(flags);+ hrtimer\_init(&data->timer, io\_timeout\_get\_clock(data), data->mode);++ if (is\_timeout\_link) {+ struct io\_submit\_link \*link = &req->ctx->submit\_state.link;++ if (!link->head)+ return -EINVAL;+ if (link->last->opcode == IORING\_OP\_LINK\_TIMEOUT)+ return -EINVAL;+ req->timeout.head = link->last;+ link->last->flags |= REQ\_F\_ARM\_LTIMEOUT;+ } return 0; } -static int io\_timeout(struct io\_kiocb \*req)+static int io\_timeout(struct io\_kiocb \*req, unsigned int issue\_flags) { struct io\_ring\_ctx \*ctx = req->ctx; struct io\_timeout\_data \*data = req->async\_data; struct list\_head \*entry; u32 tail, off = req->timeout.off; - spin\_lock\_irq(&ctx->completion\_lock);+ spin\_lock\_irq(&ctx->timeout\_lock);  /\* \* sqe->off holds how many events that need to occur for this@@ -5666,23 +6134,34 @@ add: list\_add(&req->timeout.list, entry); data->timer.function = io\_timeout\_fn; hrtimer\_start(&data->timer, timespec64\_to\_ktime(data->ts), data->mode);- spin\_unlock\_irq(&ctx->completion\_lock);+ spin\_unlock\_irq(&ctx->timeout\_lock); return 0; } +struct io\_cancel\_data {+ struct io\_ring\_ctx \*ctx;+ u64 user\_data;+};+ static bool io\_cancel\_cb(struct io\_wq\_work \*work, void \*data) { struct io\_kiocb \*req = container\_of(work, struct io\_kiocb, work);+ struct io\_cancel\_data \*cd = data; - return req->user\_data == (unsigned long) data;+ return req->ctx == cd->ctx && req->user\_data == cd->user\_data; } -static int io\_async\_cancel\_one(struct io\_ring\_ctx \*ctx, void \*sqe\_addr)+static int io\_async\_cancel\_one(struct io\_uring\_task \*tctx, u64 user\_data,+ struct io\_ring\_ctx \*ctx) {+ struct io\_cancel\_data data = { .ctx = ctx, .user\_data = user\_data, }; enum io\_wq\_cancel cancel\_ret; int ret = 0; - cancel\_ret = io\_wq\_cancel\_cb(ctx->io\_wq, io\_cancel\_cb, sqe\_addr, false);+ if (!tctx || !tctx->io\_wq)+ return -ENOENT;++ cancel\_ret = io\_wq\_cancel\_cb(tctx->io\_wq, io\_cancel\_cb, &data, false); switch (cancel\_ret) { case IO\_WQ\_CANCEL\_OK: ret = 0;@@ -5698,35 +6177,27 @@ static int io\_async\_cancel\_one(struct io\_ring\_ctx \*ctx, void \*sqe\_addr) return ret; } -static void io\_async\_find\_and\_cancel(struct io\_ring\_ctx \*ctx,- struct io\_kiocb \*req, \_\_u64 sqe\_addr,- int success\_ret)+static int io\_try\_cancel\_userdata(struct io\_kiocb \*req, u64 sqe\_addr) {- unsigned long flags;+ struct io\_ring\_ctx \*ctx = req->ctx; int ret; - ret = io\_async\_cancel\_one(ctx, (void \*) (unsigned long) sqe\_addr);- if (ret != -ENOENT) {- spin\_lock\_irqsave(&ctx->completion\_lock, flags);- goto done;- }+ WARN\_ON\_ONCE(!io\_wq\_current\_is\_worker() && req->task != current); - spin\_lock\_irqsave(&ctx->completion\_lock, flags);- ret = io\_timeout\_cancel(ctx, sqe\_addr);+ ret = io\_async\_cancel\_one(req->task->io\_uring, sqe\_addr, ctx); if (ret != -ENOENT)- goto done;- ret = io\_poll\_cancel(ctx, sqe\_addr);-done:- if (!ret)- ret = success\_ret;- io\_cqring\_fill\_event(req, ret);- io\_commit\_cqring(ctx);- spin\_unlock\_irqrestore(&ctx->completion\_lock, flags);- io\_cqring\_ev\_posted(ctx);+ return ret; - if (ret < 0)- req\_set\_fail\_links(req);- io\_put\_req(req);+ spin\_lock(&ctx->completion\_lock);+ spin\_lock\_irq(&ctx->timeout\_lock);+ ret = io\_timeout\_cancel(ctx, sqe\_addr);+ spin\_unlock\_irq(&ctx->timeout\_lock);+ if (ret != -ENOENT)+ goto out;+ ret = io\_poll\_cancel(ctx, sqe\_addr, false);+out:+ spin\_unlock(&ctx->completion\_lock);+ return ret; }  static int io\_async\_cancel\_prep(struct io\_kiocb \*req,@@ -5744,52 +6215,72 @@ static int io\_async\_cancel\_prep(struct io\_kiocb \*req, return 0; } -static int io\_async\_cancel(struct io\_kiocb \*req)+static int io\_async\_cancel(struct io\_kiocb \*req, unsigned int issue\_flags) { struct io\_ring\_ctx \*ctx = req->ctx;+ u64 sqe\_addr = req->cancel.addr;+ struct io\_tctx\_node \*node;+ int ret; - io\_async\_find\_and\_cancel(ctx, req, req->cancel.addr, 0);+ ret = io\_try\_cancel\_userdata(req, sqe\_addr);+ if (ret != -ENOENT)+ goto done;++ /\* slow path, try all io-wq's \*/+ io\_ring\_submit\_lock(ctx, !(issue\_flags & IO\_URING\_F\_NONBLOCK));+ ret = -ENOENT;+ list\_for\_each\_entry(node, &ctx->tctx\_list, ctx\_node) {+ struct io\_uring\_task \*tctx = node->task->io\_uring;++ ret = io\_async\_cancel\_one(tctx, req->cancel.addr, ctx);+ if (ret != -ENOENT)+ break;+ }+ io\_ring\_submit\_unlock(ctx, !(issue\_flags & IO\_URING\_F\_NONBLOCK));+done:+ if (ret < 0)+ req\_set\_fail(req);+ io\_req\_complete\_post(req, ret, 0); return 0; } -static int io\_files\_update\_prep(struct io\_kiocb \*req,+static int io\_rsrc\_update\_prep(struct io\_kiocb \*req, const struct io\_uring\_sqe \*sqe) {- if (unlikely(req->ctx->flags & IORING\_SETUP\_SQPOLL))- return -EINVAL; if (unlikely(req->flags & (REQ\_F\_FIXED\_FILE | REQ\_F\_BUFFER\_SELECT))) return -EINVAL;- if (sqe->ioprio || sqe->rw\_flags)+ if (sqe->ioprio || sqe->rw\_flags || sqe->splice\_fd\_in) return -EINVAL; - req->files\_update.offset = READ\_ONCE(sqe->off);- req->files\_update.nr\_args = READ\_ONCE(sqe->len);- if (!req->files\_update.nr\_args)+ req->rsrc\_update.offset = READ\_ONCE(sqe->off);+ req->rsrc\_update.nr\_args = READ\_ONCE(sqe->len);+ if (!req->rsrc\_update.nr\_args) return -EINVAL;- req->files\_update.arg = READ\_ONCE(sqe->addr);+ req->rsrc\_update.arg = READ\_ONCE(sqe->addr); return 0; } -static int io\_files\_update(struct io\_kiocb \*req, bool force\_nonblock,- struct io\_comp\_state \*cs)+static int io\_files\_update(struct io\_kiocb \*req, unsigned int issue\_flags) { struct io\_ring\_ctx \*ctx = req->ctx;- struct io\_uring\_files\_update up;+ struct io\_uring\_rsrc\_update2 up; int ret; - if (force\_nonblock)- return -EAGAIN;+ up.offset = req->rsrc\_update.offset;+ up.data = req->rsrc\_update.arg;+ up.nr = 0;+ up.tags = 0;+ up.resv = 0;+ up.resv2 = 0; - up.offset = req->files\_update.offset;- up.fds = req->files\_update.arg;-- mutex\_lock(&ctx->uring\_lock);- ret = \_\_io\_sqe\_files\_update(ctx, &up, req->files\_update.nr\_args);- mutex\_unlock(&ctx->uring\_lock);+ io\_ring\_submit\_lock(ctx, !(issue\_flags & IO\_URING\_F\_NONBLOCK));+ ret = \_\_io\_register\_rsrc\_update(ctx, IORING\_RSRC\_FILE,+ &up, req->rsrc\_update.nr\_args);+ io\_ring\_submit\_unlock(ctx, !(issue\_flags & IO\_URING\_F\_NONBLOCK));  if (ret < 0)- req\_set\_fail\_links(req);- \_\_io\_req\_complete(req, ret, 0, cs);+ req\_set\_fail(req);+ \_\_io\_req\_complete(req, issue\_flags, ret, 0); return 0; } @@ -5809,11 +6300,11 @@ static int io\_req\_prep(struct io\_kiocb \*req, const struct io\_uring\_sqe \*sqe) case IORING\_OP\_POLL\_ADD: return io\_poll\_add\_prep(req, sqe); case IORING\_OP\_POLL\_REMOVE:- return io\_poll\_remove\_prep(req, sqe);+ return io\_poll\_update\_prep(req, sqe); case IORING\_OP\_FSYNC:- return io\_prep\_fsync(req, sqe);+ return io\_fsync\_prep(req, sqe); case IORING\_OP\_SYNC\_FILE\_RANGE:- return io\_prep\_sfr(req, sqe);+ return io\_sfr\_prep(req, sqe); case IORING\_OP\_SENDMSG: case IORING\_OP\_SEND: return io\_sendmsg\_prep(req, sqe);@@ -5839,7 +6330,7 @@ static int io\_req\_prep(struct io\_kiocb \*req, const struct io\_uring\_sqe \*sqe) case IORING\_OP\_CLOSE: return io\_close\_prep(req, sqe); case IORING\_OP\_FILES\_UPDATE:- return io\_files\_update\_prep(req, sqe);+ return io\_rsrc\_update\_prep(req, sqe); case IORING\_OP\_STATX: return io\_statx\_prep(req, sqe); case IORING\_OP\_FADVISE:@@ -5858,100 +6349,131 @@ static int io\_req\_prep(struct io\_kiocb \*req, const struct io\_uring\_sqe \*sqe) return io\_remove\_buffers\_prep(req, sqe); case IORING\_OP\_TEE: return io\_tee\_prep(req, sqe);+ case IORING\_OP\_SHUTDOWN:+ return io\_shutdown\_prep(req, sqe);+ case IORING\_OP\_RENAMEAT:+ return io\_renameat\_prep(req, sqe);+ case IORING\_OP\_UNLINKAT:+ return io\_unlinkat\_prep(req, sqe); }  printk\_once(KERN\_WARNING "io\_uring: unhandled opcode %d\n", req->opcode);- return-EINVAL;+ return -EINVAL; } -static int io\_req\_defer\_prep(struct io\_kiocb \*req,- const struct io\_uring\_sqe \*sqe)+static int io\_req\_prep\_async(struct io\_kiocb \*req) {- if (!sqe)+ if (!io\_op\_defs[req->opcode].needs\_async\_setup) return 0;+ if (WARN\_ON\_ONCE(req->async\_data))+ return -EFAULT; if (io\_alloc\_async\_data(req)) return -EAGAIN;- return io\_req\_prep(req, sqe);++ switch (req->opcode) {+ case IORING\_OP\_READV:+ return io\_rw\_prep\_async(req, READ);+ case IORING\_OP\_WRITEV:+ return io\_rw\_prep\_async(req, WRITE);+ case IORING\_OP\_SENDMSG:+ return io\_sendmsg\_prep\_async(req);+ case IORING\_OP\_RECVMSG:+ return io\_recvmsg\_prep\_async(req);+ case IORING\_OP\_CONNECT:+ return io\_connect\_prep\_async(req);+ }+ printk\_once(KERN\_WARNING "io\_uring: prep\_async() bad opcode %d\n",+ req->opcode);+ return -EFAULT; }  static u32 io\_get\_sequence(struct io\_kiocb \*req) {- struct io\_kiocb \*pos;- struct io\_ring\_ctx \*ctx = req->ctx;- u32 total\_submitted, nr\_reqs = 1;+ u32 seq = req->ctx->cached\_sq\_head; - if (req->flags & REQ\_F\_LINK\_HEAD)- list\_for\_each\_entry(pos, &req->link\_list, link\_list)- nr\_reqs++;-- total\_submitted = ctx->cached\_sq\_head - ctx->cached\_sq\_dropped;- return total\_submitted - nr\_reqs;+ /\* need original cached\_sq\_head, but it was increased for each req \*/+ io\_for\_each\_link(req, req)+ seq--;+ return seq; } -static int io\_req\_defer(struct io\_kiocb \*req, const struct io\_uring\_sqe \*sqe)+static bool io\_drain\_req(struct io\_kiocb \*req) {+ struct io\_kiocb \*pos; struct io\_ring\_ctx \*ctx = req->ctx; struct io\_defer\_entry \*de; int ret; u32 seq; + if (req->flags & REQ\_F\_FAIL) {+ io\_req\_complete\_fail\_submit(req);+ return true;+ }++ /\*+ \* If we need to drain a request in the middle of a link, drain the+ \* head request and the next request/link after the current link.+ \* Considering sequential execution of links, IOSQE\_IO\_DRAIN will be+ \* maintained for every request of our link.+ \*/+ if (ctx->drain\_next) {+ req->flags |= REQ\_F\_IO\_DRAIN;+ ctx->drain\_next = false;+ }+ /\* not interested in head, start from the first linked \*/+ io\_for\_each\_link(pos, req->link) {+ if (pos->flags & REQ\_F\_IO\_DRAIN) {+ ctx->drain\_next = true;+ req->flags |= REQ\_F\_IO\_DRAIN;+ break;+ }+ }+ /\* Still need defer if there is pending req in defer list. \*/+ spin\_lock(&ctx->completion\_lock); if (likely(list\_empty\_careful(&ctx->defer\_list) &&- !(req->flags & REQ\_F\_IO\_DRAIN)))- return 0;+ !(req->flags & REQ\_F\_IO\_DRAIN))) {+ spin\_unlock(&ctx->completion\_lock);+ ctx->drain\_active = false;+ return false;+ }+ spin\_unlock(&ctx->completion\_lock);  seq = io\_get\_sequence(req); /\* Still a chance to pass the sequence check \*/ if (!req\_need\_defer(req, seq) && list\_empty\_careful(&ctx->defer\_list))- return 0;+ return false; - if (!req->async\_data) {- ret = io\_req\_defer\_prep(req, sqe);- if (ret)- return ret;- }+ ret = io\_req\_prep\_async(req);+ if (ret)+ goto fail; io\_prep\_async\_link(req); de = kmalloc(sizeof(\*de), GFP\_KERNEL);- if (!de)- return -ENOMEM;+ if (!de) {+ ret = -ENOMEM;+fail:+ io\_req\_complete\_failed(req, ret);+ return true;+ } - spin\_lock\_irq(&ctx->completion\_lock);+ spin\_lock(&ctx->completion\_lock); if (!req\_need\_defer(req, seq) && list\_empty(&ctx->defer\_list)) {- spin\_unlock\_irq(&ctx->completion\_lock);+ spin\_unlock(&ctx->completion\_lock); kfree(de);- io\_queue\_async\_work(req);- return -EIOCBQUEUED;+ io\_queue\_async\_work(req, NULL);+ return true; }  trace\_io\_uring\_defer(ctx, req, req->user\_data); de->req = req; de->seq = seq; list\_add\_tail(&de->list, &ctx->defer\_list);- spin\_unlock\_irq(&ctx->completion\_lock);- return -EIOCBQUEUED;-}--static void io\_req\_drop\_files(struct io\_kiocb \*req)-{- struct io\_ring\_ctx \*ctx = req->ctx;- struct io\_uring\_task \*tctx = req->task->io\_uring;- unsigned long flags;-- if (req->work.flags & IO\_WQ\_WORK\_FILES) {- put\_files\_struct(req->work.identity->files);- put\_nsproxy(req->work.identity->nsproxy);- }- spin\_lock\_irqsave(&ctx->inflight\_lock, flags);- list\_del(&req->inflight\_entry);- spin\_unlock\_irqrestore(&ctx->inflight\_lock, flags);- req->flags &= ~REQ\_F\_INFLIGHT;- req->work.flags &= ~IO\_WQ\_WORK\_FILES;- if (atomic\_read(&tctx->in\_idle))- wake\_up(&tctx->wait);+ spin\_unlock(&ctx->completion\_lock);+ return true; } -static void \_\_io\_clean\_op(struct io\_kiocb \*req)+static void io\_clean\_op(struct io\_kiocb \*req) { if (req->flags & REQ\_F\_BUFFER\_SELECTED) { switch (req->opcode) {@@ -5965,7 +6487,6 @@ static void \_\_io\_clean\_op(struct io\_kiocb \*req) kfree(req->sr\_msg.kbuf); break; }- req->flags &= ~REQ\_F\_BUFFER\_SELECTED; }  if (req->flags & REQ\_F\_NEED\_CLEANUP) {@@ -5977,458 +6498,556 @@ static void \_\_io\_clean\_op(struct io\_kiocb \*req) case IORING\_OP\_WRITE\_FIXED: case IORING\_OP\_WRITE: { struct io\_async\_rw \*io = req->async\_data;- if (io->free\_iovec)- kfree(io->free\_iovec);++ kfree(io->free\_iovec); break; } case IORING\_OP\_RECVMSG: case IORING\_OP\_SENDMSG: { struct io\_async\_msghdr \*io = req->async\_data;- if (io->iov != io->fast\_iov)- kfree(io->iov);++ kfree(io->free\_iov); break; }- case IORING\_OP\_SPLICE:- case IORING\_OP\_TEE:- io\_put\_file(req, req->splice.file\_in,- (req->splice.flags & SPLICE\_F\_FD\_IN\_FIXED));- break; case IORING\_OP\_OPENAT: case IORING\_OP\_OPENAT2: if (req->open.filename) putname(req->open.filename); break;+ case IORING\_OP\_RENAMEAT:+ putname(req->rename.oldpath);+ putname(req->rename.newpath);+ break;+ case IORING\_OP\_UNLINKAT:+ putname(req->unlink.filename);+ break; }- req->flags &= ~REQ\_F\_NEED\_CLEANUP; }+ if ((req->flags & REQ\_F\_POLLED) && req->apoll) {+ kfree(req->apoll->double\_poll);+ kfree(req->apoll);+ req->apoll = NULL;+ }+ if (req->flags & REQ\_F\_INFLIGHT) {+ struct io\_uring\_task \*tctx = req->task->io\_uring;++ atomic\_dec(&tctx->inflight\_tracked);+ }+ if (req->flags & REQ\_F\_CREDS)+ put\_cred(req->creds);++ req->flags &= ~IO\_REQ\_CLEAN\_FLAGS; } -static int io\_issue\_sqe(struct io\_kiocb \*req, bool force\_nonblock,- struct io\_comp\_state \*cs)+static int io\_issue\_sqe(struct io\_kiocb \*req, unsigned int issue\_flags) { struct io\_ring\_ctx \*ctx = req->ctx;+ const struct cred \*creds = NULL; int ret; + if ((req->flags & REQ\_F\_CREDS) && req->creds != current\_cred())+ creds = override\_creds(req->creds);+ switch (req->opcode) { case IORING\_OP\_NOP:- ret = io\_nop(req, cs);+ ret = io\_nop(req, issue\_flags); break; case IORING\_OP\_READV: case IORING\_OP\_READ\_FIXED: case IORING\_OP\_READ:- ret = io\_read(req, force\_nonblock, cs);+ ret = io\_read(req, issue\_flags); break; case IORING\_OP\_WRITEV: case IORING\_OP\_WRITE\_FIXED: case IORING\_OP\_WRITE:- ret = io\_write(req, force\_nonblock, cs);+ ret = io\_write(req, issue\_flags); break; case IORING\_OP\_FSYNC:- ret = io\_fsync(req, force\_nonblock);+ ret = io\_fsync(req, issue\_flags); break; case IORING\_OP\_POLL\_ADD:- ret = io\_poll\_add(req);+ ret = io\_poll\_add(req, issue\_flags); break; case IORING\_OP\_POLL\_REMOVE:- ret = io\_poll\_remove(req);+ ret = io\_poll\_update(req, issue\_flags); break; case IORING\_OP\_SYNC\_FILE\_RANGE:- ret = io\_sync\_file\_range(req, force\_nonblock);+ ret = io\_sync\_file\_range(req, issue\_flags); break; case IORING\_OP\_SENDMSG:- ret = io\_sendmsg(req, force\_nonblock, cs);+ ret = io\_sendmsg(req, issue\_flags); break; case IORING\_OP\_SEND:- ret = io\_send(req, force\_nonblock, cs);+ ret = io\_send(req, issue\_flags); break; case IORING\_OP\_RECVMSG:- ret = io\_recvmsg(req, force\_nonblock, cs);+ ret = io\_recvmsg(req, issue\_flags); break; case IORING\_OP\_RECV:- ret = io\_recv(req, force\_nonblock, cs);+ ret = io\_recv(req, issue\_flags); break; case IORING\_OP\_TIMEOUT:- ret = io\_timeout(req);+ ret = io\_timeout(req, issue\_flags); break; case IORING\_OP\_TIMEOUT\_REMOVE:- ret = io\_timeout\_remove(req);+ ret = io\_timeout\_remove(req, issue\_flags); break; case IORING\_OP\_ACCEPT:- ret = io\_accept(req, force\_nonblock, cs);+ ret = io\_accept(req, issue\_flags); break; case IORING\_OP\_CONNECT:- ret = io\_connect(req, force\_nonblock, cs);+ ret = io\_connect(req, issue\_flags); break; case IORING\_OP\_ASYNC\_CANCEL:- ret = io\_async\_cancel(req);+ ret = io\_async\_cancel(req, issue\_flags); break; case IORING\_OP\_FALLOCATE:- ret = io\_fallocate(req, force\_nonblock);+ ret = io\_fallocate(req, issue\_flags); break; case IORING\_OP\_OPENAT:- ret = io\_openat(req, force\_nonblock);+ ret = io\_openat(req, issue\_flags); break; case IORING\_OP\_CLOSE:- ret = io\_close(req, force\_nonblock, cs);+ ret = io\_close(req, issue\_flags); break; case IORING\_OP\_FILES\_UPDATE:- ret = io\_files\_update(req, force\_nonblock, cs);+ ret = io\_files\_update(req, issue\_flags); break; case IORING\_OP\_STATX:- ret = io\_statx(req, force\_nonblock);+ ret = io\_statx(req, issue\_flags); break; case IORING\_OP\_FADVISE:- ret = io\_fadvise(req, force\_nonblock);+ ret = io\_fadvise(req, issue\_flags); break; case IORING\_OP\_MADVISE:- ret = io\_madvise(req, force\_nonblock);+ ret = io\_madvise(req, issue\_flags); break; case IORING\_OP\_OPENAT2:- ret = io\_openat2(req, force\_nonblock);+ ret = io\_openat2(req, issue\_flags); break; case IORING\_OP\_EPOLL\_CTL:- ret = io\_epoll\_ctl(req, force\_nonblock, cs);+ ret = io\_epoll\_ctl(req, issue\_flags); break; case IORING\_OP\_SPLICE:- ret = io\_splice(req, force\_nonblock);+ ret = io\_splice(req, issue\_flags); break; case IORING\_OP\_PROVIDE\_BUFFERS:- ret = io\_provide\_buffers(req, force\_nonblock, cs);+ ret = io\_provide\_buffers(req, issue\_flags); break; case IORING\_OP\_REMOVE\_BUFFERS:- ret = io\_remove\_buffers(req, force\_nonblock, cs);+ ret = io\_remove\_buffers(req, issue\_flags); break; case IORING\_OP\_TEE:- ret = io\_tee(req, force\_nonblock);+ ret = io\_tee(req, issue\_flags);+ break;+ case IORING\_OP\_SHUTDOWN:+ ret = io\_shutdown(req, issue\_flags);+ break;+ case IORING\_OP\_RENAMEAT:+ ret = io\_renameat(req, issue\_flags);+ break;+ case IORING\_OP\_UNLINKAT:+ ret = io\_unlinkat(req, issue\_flags); break; default: ret = -EINVAL; break; } + if (creds)+ revert\_creds(creds); if (ret) return ret;- /\* If the op doesn't have a file, we're not polling for it \*/- if ((ctx->flags & IORING\_SETUP\_IOPOLL) && req->file) {- const bool in\_async = io\_wq\_current\_is\_worker();-- /\* workqueue context doesn't hold uring\_lock, grab it now \*/- if (in\_async)- mutex\_lock(&ctx->uring\_lock);-+ if ((ctx->flags & IORING\_SETUP\_IOPOLL) && req->file) io\_iopoll\_req\_issued(req); - if (in\_async)- mutex\_unlock(&ctx->uring\_lock);- }- return 0; } -static struct io\_wq\_work \*io\_wq\_submit\_work(struct io\_wq\_work \*work)+static struct io\_wq\_work \*io\_wq\_free\_work(struct io\_wq\_work \*work)+{+ struct io\_kiocb \*req = container\_of(work, struct io\_kiocb, work);++ req = io\_put\_req\_find\_next(req);+ return req ? &req->work : NULL;+}++static void io\_wq\_submit\_work(struct io\_wq\_work \*work) { struct io\_kiocb \*req = container\_of(work, struct io\_kiocb, work); struct io\_kiocb \*timeout; int ret = 0; + /\* one will be dropped by ->io\_free\_work() after returning to io-wq \*/+ if (!(req->flags & REQ\_F\_REFCOUNT))+ \_\_io\_req\_set\_refcount(req, 2);+ else+ req\_ref\_get(req);+ timeout = io\_prep\_linked\_timeout(req); if (timeout) io\_queue\_linked\_timeout(timeout); - /\* if NO\_CANCEL is set, we must still run the work \*/- if ((work->flags & (IO\_WQ\_WORK\_CANCEL|IO\_WQ\_WORK\_NO\_CANCEL)) ==- IO\_WQ\_WORK\_CANCEL) {+ /\* either cancelled or io-wq is dying, so don't touch tctx->iowq \*/+ if (work->flags & IO\_WQ\_WORK\_CANCEL) ret = -ECANCELED;- }  if (!ret) { do {- ret = io\_issue\_sqe(req, false, NULL);+ ret = io\_issue\_sqe(req, 0); /\* \* We can get EAGAIN for polled IO even though we're \* forcing a sync submission from here, since we can't \* wait for request slots on the block side. \*/- if (ret != -EAGAIN)+ if (ret != -EAGAIN || !(req->ctx->flags & IORING\_SETUP\_IOPOLL)) break; cond\_resched(); } while (1); } - if (ret) {- struct io\_ring\_ctx \*lock\_ctx = NULL;-- if (req->ctx->flags & IORING\_SETUP\_IOPOLL)- lock\_ctx = req->ctx;-- /\*- \* io\_iopoll\_complete() does not hold completion\_lock to- \* complete polled io, so here for polled io, we can not call- \* io\_req\_complete() directly, otherwise there maybe concurrent- \* access to cqring, defer\_list, etc, which is not safe. Given- \* that io\_iopoll\_complete() is always called under uring\_lock,- \* so here for polled io, we also get uring\_lock to complete- \* it.- \*/- if (lock\_ctx)- mutex\_lock(&lock\_ctx->uring\_lock);-- req\_set\_fail\_links(req);- io\_req\_complete(req, ret);-- if (lock\_ctx)- mutex\_unlock(&lock\_ctx->uring\_lock);- }+ /\* avoid locking problems by failing it from a clean context \*/+ if (ret)+ io\_req\_task\_queue\_fail(req, ret);+} - return io\_steal\_work(req);+static inline struct io\_fixed\_file \*io\_fixed\_file\_slot(struct io\_file\_table \*table,+ unsigned i)+{+ return &table->files[i]; }  static inline struct file \*io\_file\_from\_index(struct io\_ring\_ctx \*ctx, int index) {- struct fixed\_file\_table \*table;+ struct io\_fixed\_file \*slot = io\_fixed\_file\_slot(&ctx->file\_table, index); - table = &ctx->file\_data->table[index >> IORING\_FILE\_TABLE\_SHIFT];- return table->files[index & IORING\_FILE\_TABLE\_MASK];+ return (struct file \*) (slot->file\_ptr & FFS\_MASK); } -static struct file \*io\_file\_get(struct io\_submit\_state \*state,- struct io\_kiocb \*req, int fd, bool fixed)+static void io\_fixed\_file\_set(struct io\_fixed\_file \*file\_slot, struct file \*file)+{+ unsigned long file\_ptr = (unsigned long) file;++ if (\_\_io\_file\_supports\_nowait(file, READ))+ file\_ptr |= FFS\_ASYNC\_READ;+ if (\_\_io\_file\_supports\_nowait(file, WRITE))+ file\_ptr |= FFS\_ASYNC\_WRITE;+ if (S\_ISREG(file\_inode(file)->i\_mode))+ file\_ptr |= FFS\_ISREG;+ file\_slot->file\_ptr = file\_ptr;+}++static inline struct file \*io\_file\_get\_fixed(struct io\_ring\_ctx \*ctx,+ struct io\_kiocb \*req, int fd) {- struct io\_ring\_ctx \*ctx = req->ctx; struct file \*file;+ unsigned long file\_ptr; - if (fixed) {- if (unlikely((unsigned int)fd >= ctx->nr\_user\_files))- return NULL;- fd = array\_index\_nospec(fd, ctx->nr\_user\_files);- file = io\_file\_from\_index(ctx, fd);- if (file) {- req->fixed\_file\_refs = &ctx->file\_data->node->refs;- percpu\_ref\_get(req->fixed\_file\_refs);- }- } else {- trace\_io\_uring\_file\_get(ctx, fd);- file = \_\_io\_file\_get(state, fd);- }+ if (unlikely((unsigned int)fd >= ctx->nr\_user\_files))+ return NULL;+ fd = array\_index\_nospec(fd, ctx->nr\_user\_files);+ file\_ptr = io\_fixed\_file\_slot(&ctx->file\_table, fd)->file\_ptr;+ file = (struct file \*) (file\_ptr & FFS\_MASK);+ file\_ptr &= ~FFS\_MASK;+ /\* mask in overlapping REQ\_F and FFS bits \*/+ req->flags |= (file\_ptr << REQ\_F\_NOWAIT\_READ\_BIT);+ io\_req\_set\_rsrc\_node(req);+ return file;+} - if (file && file->f\_op == &io\_uring\_fops &&- !(req->flags & REQ\_F\_INFLIGHT)) {- io\_req\_init\_async(req);- req->flags |= REQ\_F\_INFLIGHT;+static struct file \*io\_file\_get\_normal(struct io\_ring\_ctx \*ctx,+ struct io\_kiocb \*req, int fd)+{+ struct file \*file = fget(fd); - spin\_lock\_irq(&ctx->inflight\_lock);- list\_add(&req->inflight\_entry, &ctx->inflight\_list);- spin\_unlock\_irq(&ctx->inflight\_lock);- }+ trace\_io\_uring\_file\_get(ctx, fd); + /\* we don't allow fixed io\_uring files \*/+ if (file && unlikely(file->f\_op == &io\_uring\_fops))+ io\_req\_track\_inflight(req); return file; } -static int io\_req\_set\_file(struct io\_submit\_state \*state, struct io\_kiocb \*req,- int fd)+static inline struct file \*io\_file\_get(struct io\_ring\_ctx \*ctx,+ struct io\_kiocb \*req, int fd, bool fixed) {- bool fixed;+ if (fixed)+ return io\_file\_get\_fixed(ctx, req, fd);+ else+ return io\_file\_get\_normal(ctx, req, fd);+} - fixed = (req->flags & REQ\_F\_FIXED\_FILE) != 0;- if (unlikely(!fixed && io\_async\_submit(req->ctx)))- return -EBADF;+static void io\_req\_task\_link\_timeout(struct io\_kiocb \*req, bool \*locked)+{+ struct io\_kiocb \*prev = req->timeout.prev;+ int ret = -ENOENT; - req->file = io\_file\_get(state, req, fd, fixed);- if (req->file || io\_op\_defs[req->opcode].needs\_file\_no\_error)- return 0;- return -EBADF;+ if (prev) {+ if (!(req->task->flags & PF\_EXITING))+ ret = io\_try\_cancel\_userdata(req, prev->user\_data);+ io\_req\_complete\_post(req, ret ?: -ETIME, 0);+ io\_put\_req(prev);+ } else {+ io\_req\_complete\_post(req, -ETIME, 0);+ } }  static enum hrtimer\_restart io\_link\_timeout\_fn(struct hrtimer \*timer) { struct io\_timeout\_data \*data = container\_of(timer, struct io\_timeout\_data, timer);- struct io\_kiocb \*req = data->req;+ struct io\_kiocb \*prev, \*req = data->req; struct io\_ring\_ctx \*ctx = req->ctx;- struct io\_kiocb \*prev = NULL; unsigned long flags; - spin\_lock\_irqsave(&ctx->completion\_lock, flags);+ spin\_lock\_irqsave(&ctx->timeout\_lock, flags);+ prev = req->timeout.head;+ req->timeout.head = NULL;  /\* \* We don't expect the list to be empty, that will only happen if we \* race with the completion of the linked work. \*/- if (!list\_empty(&req->link\_list)) {- prev = list\_entry(req->link\_list.prev, struct io\_kiocb,- link\_list);- list\_del\_init(&req->link\_list);- if (!refcount\_inc\_not\_zero(&prev->refs))+ if (prev) {+ io\_remove\_next\_linked(prev);+ if (!req\_ref\_inc\_not\_zero(prev)) prev = NULL; }- list\_del(&req->timeout.list);- spin\_unlock\_irqrestore(&ctx->completion\_lock, flags);+ req->timeout.prev = prev;+ spin\_unlock\_irqrestore(&ctx->timeout\_lock, flags); - if (prev) {- io\_async\_find\_and\_cancel(ctx, req, prev->user\_data, -ETIME);- io\_put\_req\_deferred(prev, 1);- } else {- io\_cqring\_add\_event(req, -ETIME, 0);- io\_put\_req\_deferred(req, 1);- }+ req->io\_task\_work.func = io\_req\_task\_link\_timeout;+ io\_req\_task\_work\_add(req); return HRTIMER\_NORESTART; } -static void \_\_io\_queue\_linked\_timeout(struct io\_kiocb \*req)+static void io\_queue\_linked\_timeout(struct io\_kiocb \*req) {+ struct io\_ring\_ctx \*ctx = req->ctx;++ spin\_lock\_irq(&ctx->timeout\_lock); /\*- \* If the list is now empty, then our linked request finished before- \* we got a chance to setup the timer+ \* If the back reference is NULL, then our linked request finished+ \* before we got a chance to setup the timer \*/- if (!list\_empty(&req->link\_list)) {+ if (req->timeout.head) { struct io\_timeout\_data \*data = req->async\_data;  data->timer.function = io\_link\_timeout\_fn; hrtimer\_start(&data->timer, timespec64\_to\_ktime(data->ts), data->mode);+ list\_add\_tail(&req->timeout.list, &ctx->ltimeout\_list); }-}--static void io\_queue\_linked\_timeout(struct io\_kiocb \*req)-{- struct io\_ring\_ctx \*ctx = req->ctx;-- spin\_lock\_irq(&ctx->completion\_lock);- \_\_io\_queue\_linked\_timeout(req);- spin\_unlock\_irq(&ctx->completion\_lock);-+ spin\_unlock\_irq(&ctx->timeout\_lock); /\* drop submission reference \*/ io\_put\_req(req); } -static struct io\_kiocb \*io\_prep\_linked\_timeout(struct io\_kiocb \*req)-{- struct io\_kiocb \*nxt;-- if (!(req->flags & REQ\_F\_LINK\_HEAD))- return NULL;- if (req->flags & REQ\_F\_LINK\_TIMEOUT)- return NULL;-- nxt = list\_first\_entry\_or\_null(&req->link\_list, struct io\_kiocb,- link\_list);- if (!nxt || nxt->opcode != IORING\_OP\_LINK\_TIMEOUT)- return NULL;-- nxt->flags |= REQ\_F\_LTIMEOUT\_ACTIVE;- req->flags |= REQ\_F\_LINK\_TIMEOUT;- return nxt;-}--static void \_\_io\_queue\_sqe(struct io\_kiocb \*req, struct io\_comp\_state \*cs)+static void \_\_io\_queue\_sqe(struct io\_kiocb \*req)+ \_\_must\_hold(&req->ctx->uring\_lock) { struct io\_kiocb \*linked\_timeout;- const struct cred \*old\_creds = NULL; int ret; -again:- linked\_timeout = io\_prep\_linked\_timeout(req);-- if ((req->flags & REQ\_F\_WORK\_INITIALIZED) &&- (req->work.flags & IO\_WQ\_WORK\_CREDS) &&- req->work.identity->creds != current\_cred()) {- if (old\_creds)- revert\_creds(old\_creds);- if (old\_creds == req->work.identity->creds)- old\_creds = NULL; /\* restored original creds \*/- else- old\_creds = override\_creds(req->work.identity->creds);- }-- ret = io\_issue\_sqe(req, true, cs);+issue\_sqe:+ ret = io\_issue\_sqe(req, IO\_URING\_F\_NONBLOCK|IO\_URING\_F\_COMPLETE\_DEFER);  /\* \* We async punt it if the file wasn't marked NOWAIT, or if the file \* doesn't support non-blocking read/write attempts \*/- if (ret == -EAGAIN && !(req->flags & REQ\_F\_NOWAIT)) {- if (!io\_arm\_poll\_handler(req)) {+ if (likely(!ret)) {+ if (req->flags & REQ\_F\_COMPLETE\_INLINE) {+ struct io\_ring\_ctx \*ctx = req->ctx;+ struct io\_submit\_state \*state = &ctx->submit\_state;++ state->compl\_reqs[state->compl\_nr++] = req;+ if (state->compl\_nr == ARRAY\_SIZE(state->compl\_reqs))+ io\_submit\_flush\_completions(ctx);+ return;+ }++ linked\_timeout = io\_prep\_linked\_timeout(req);+ if (linked\_timeout)+ io\_queue\_linked\_timeout(linked\_timeout);+ } else if (ret == -EAGAIN && !(req->flags & REQ\_F\_NOWAIT)) {+ linked\_timeout = io\_prep\_linked\_timeout(req);++ switch (io\_arm\_poll\_handler(req)) {+ case IO\_APOLL\_READY:+ if (linked\_timeout)+ io\_queue\_linked\_timeout(linked\_timeout);+ goto issue\_sqe;+ case IO\_APOLL\_ABORTED: /\* \* Queued up for async execution, worker will release \* submit reference when the iocb is actually submitted. \*/- io\_queue\_async\_work(req);+ io\_queue\_async\_work(req, NULL);+ break; }  if (linked\_timeout) io\_queue\_linked\_timeout(linked\_timeout);- } else if (likely(!ret)) {- /\* drop submission reference \*/- req = io\_put\_req\_find\_next(req);- if (linked\_timeout)- io\_queue\_linked\_timeout(linked\_timeout);-- if (req) {- if (!(req->flags & REQ\_F\_FORCE\_ASYNC))- goto again;- io\_queue\_async\_work(req);- } } else {- /\* un-prep timeout, so it'll be killed as any other linked \*/- req->flags &= ~REQ\_F\_LINK\_TIMEOUT;- req\_set\_fail\_links(req);- io\_put\_req(req);- io\_req\_complete(req, ret);+ io\_req\_complete\_failed(req, ret); }-- if (old\_creds)- revert\_creds(old\_creds); } -static void io\_queue\_sqe(struct io\_kiocb \*req, const struct io\_uring\_sqe \*sqe,- struct io\_comp\_state \*cs)+static inline void io\_queue\_sqe(struct io\_kiocb \*req)+ \_\_must\_hold(&req->ctx->uring\_lock) {- int ret;+ if (unlikely(req->ctx->drain\_active) && io\_drain\_req(req))+ return; - ret = io\_req\_defer(req, sqe);- if (ret) {- if (ret != -EIOCBQUEUED) {-fail\_req:- req\_set\_fail\_links(req);- io\_put\_req(req);- io\_req\_complete(req, ret);- }- } else if (req->flags & REQ\_F\_FORCE\_ASYNC) {- if (!req->async\_data) {- ret = io\_req\_defer\_prep(req, sqe);- if (unlikely(ret))- goto fail\_req;- }- io\_queue\_async\_work(req);+ if (likely(!(req->flags & (REQ\_F\_FORCE\_ASYNC | REQ\_F\_FAIL)))) {+ \_\_io\_queue\_sqe(req);+ } else if (req->flags & REQ\_F\_FAIL) {+ io\_req\_complete\_fail\_submit(req); } else {- if (sqe) {- ret = io\_req\_prep(req, sqe);- if (unlikely(ret))- goto fail\_req;- }- \_\_io\_queue\_sqe(req, cs);+ int ret = io\_req\_prep\_async(req);++ if (unlikely(ret))+ io\_req\_complete\_failed(req, ret);+ else+ io\_queue\_async\_work(req, NULL); } } -static inline void io\_queue\_link\_head(struct io\_kiocb \*req,- struct io\_comp\_state \*cs)+/\*+ \* Check SQE restrictions (opcode and flags).+ \*+ \* Returns 'true' if SQE is allowed, 'false' otherwise.+ \*/+static inline bool io\_check\_restriction(struct io\_ring\_ctx \*ctx,+ struct io\_kiocb \*req,+ unsigned int sqe\_flags) {- if (unlikely(req->flags & REQ\_F\_FAIL\_LINK)) {- io\_put\_req(req);- io\_req\_complete(req, -ECANCELED);- } else- io\_queue\_sqe(req, NULL, cs);+ if (likely(!ctx->restricted))+ return true;++ if (!test\_bit(req->opcode, ctx->restrictions.sqe\_op))+ return false;++ if ((sqe\_flags & ctx->restrictions.sqe\_flags\_required) !=+ ctx->restrictions.sqe\_flags\_required)+ return false;++ if (sqe\_flags & ~(ctx->restrictions.sqe\_flags\_allowed |+ ctx->restrictions.sqe\_flags\_required))+ return false;++ return true; } -static int io\_submit\_sqe(struct io\_kiocb \*req, const struct io\_uring\_sqe \*sqe,- struct io\_kiocb \*\*link, struct io\_comp\_state \*cs)+static int io\_init\_req(struct io\_ring\_ctx \*ctx, struct io\_kiocb \*req,+ const struct io\_uring\_sqe \*sqe)+ \_\_must\_hold(&ctx->uring\_lock) {- struct io\_ring\_ctx \*ctx = req->ctx;+ struct io\_submit\_state \*state;+ unsigned int sqe\_flags;+ int personality, ret = 0;++ /\* req is partially pre-initialised, see io\_preinit\_req() \*/+ req->opcode = READ\_ONCE(sqe->opcode);+ /\* same numerical values with corresponding REQ\_F\_\*, safe to copy \*/+ req->flags = sqe\_flags = READ\_ONCE(sqe->flags);+ req->user\_data = READ\_ONCE(sqe->user\_data);+ req->file = NULL;+ req->fixed\_rsrc\_refs = NULL;+ req->task = current;++ /\* enforce forwards compatibility on users \*/+ if (unlikely(sqe\_flags & ~SQE\_VALID\_FLAGS))+ return -EINVAL;+ if (unlikely(req->opcode >= IORING\_OP\_LAST))+ return -EINVAL;+ if (!io\_check\_restriction(ctx, req, sqe\_flags))+ return -EACCES;++ if ((sqe\_flags & IOSQE\_BUFFER\_SELECT) &&+ !io\_op\_defs[req->opcode].buffer\_select)+ return -EOPNOTSUPP;+ if (unlikely(sqe\_flags & IOSQE\_IO\_DRAIN))+ ctx->drain\_active = true;++ personality = READ\_ONCE(sqe->personality);+ if (personality) {+ req->creds = xa\_load(&ctx->personalities, personality);+ if (!req->creds)+ return -EINVAL;+ get\_cred(req->creds);+ req->flags |= REQ\_F\_CREDS;+ }+ state = &ctx->submit\_state;++ /\*+ \* Plug now if we have more than 1 IO left after this, and the target+ \* is potentially a read/write to block based storage.+ \*/+ if (!state->plug\_started && state->ios\_left > 1 &&+ io\_op\_defs[req->opcode].plug) {+ blk\_start\_plug(&state->plug);+ state->plug\_started = true;+ }++ if (io\_op\_defs[req->opcode].needs\_file) {+ req->file = io\_file\_get(ctx, req, READ\_ONCE(sqe->fd),+ (sqe\_flags & IOSQE\_FIXED\_FILE));+ if (unlikely(!req->file))+ ret = -EBADF;+ }++ state->ios\_left--;+ return ret;+}++static int io\_submit\_sqe(struct io\_ring\_ctx \*ctx, struct io\_kiocb \*req,+ const struct io\_uring\_sqe \*sqe)+ \_\_must\_hold(&ctx->uring\_lock)+{+ struct io\_submit\_link \*link = &ctx->submit\_state.link; int ret; + ret = io\_init\_req(ctx, req, sqe);+ if (unlikely(ret)) {+fail\_req:+ /\* fail even hard links since we don't submit \*/+ if (link->head) {+ /\*+ \* we can judge a link req is failed or cancelled by if+ \* REQ\_F\_FAIL is set, but the head is an exception since+ \* it may be set REQ\_F\_FAIL because of other req's failure+ \* so let's leverage req->result to distinguish if a head+ \* is set REQ\_F\_FAIL because of its failure or other req's+ \* failure so that we can set the correct ret code for it.+ \* init result here to avoid affecting the normal path.+ \*/+ if (!(link->head->flags & REQ\_F\_FAIL))+ req\_fail\_link\_node(link->head, -ECANCELED);+ } else if (!(req->flags & (REQ\_F\_LINK | REQ\_F\_HARDLINK))) {+ /\*+ \* the current req is a normal req, we should return+ \* error and thus break the submittion loop.+ \*/+ io\_req\_complete\_failed(req, ret);+ return ret;+ }+ req\_fail\_link\_node(req, ret);+ } else {+ ret = io\_req\_prep(req, sqe);+ if (unlikely(ret))+ goto fail\_req;+ }++ /\* don't need @sqe from now on \*/+ trace\_io\_uring\_submit\_sqe(ctx, req, req->opcode, req->user\_data,+ req->flags, true,+ ctx->flags & IORING\_SETUP\_SQPOLL);+ /\* \* If we already have a head request, queue this one for async \* submittal once the head completes. If we don't have a head but@@ -6436,49 +7055,32 @@ static int io\_submit\_sqe(struct io\_kiocb \*req, const struct io\_uring\_sqe \*sqe, \* submitted sync once the chain is complete. If none of those \* conditions are true (normal request), then just queue it. \*/- if (\*link) {- struct io\_kiocb \*head = \*link;-- /\*- \* Taking sequential execution of a link, draining both sides- \* of the link also fullfils IOSQE\_IO\_DRAIN semantics for all- \* requests in the link. So, it drains the head and the- \* next after the link request. The last one is done via- \* drain\_next flag to persist the effect across calls.- \*/- if (req->flags & REQ\_F\_IO\_DRAIN) {- head->flags |= REQ\_F\_IO\_DRAIN;- ctx->drain\_next = 1;- }- ret = io\_req\_defer\_prep(req, sqe);- if (unlikely(ret)) {- /\* fail even hard links since we don't submit \*/- head->flags |= REQ\_F\_FAIL\_LINK;- return ret;+ if (link->head) {+ struct io\_kiocb \*head = link->head;++ if (!(req->flags & REQ\_F\_FAIL)) {+ ret = io\_req\_prep\_async(req);+ if (unlikely(ret)) {+ req\_fail\_link\_node(req, ret);+ if (!(head->flags & REQ\_F\_FAIL))+ req\_fail\_link\_node(head, -ECANCELED);+ } } trace\_io\_uring\_link(ctx, req, head);- list\_add\_tail(&req->link\_list, &head->link\_list);+ link->last->link = req;+ link->last = req;  /\* last request of a link, enqueue the link \*/ if (!(req->flags & (REQ\_F\_LINK | REQ\_F\_HARDLINK))) {- io\_queue\_link\_head(head, cs);- \*link = NULL;+ link->head = NULL;+ io\_queue\_sqe(head); } } else {- if (unlikely(ctx->drain\_next)) {- req->flags |= REQ\_F\_IO\_DRAIN;- ctx->drain\_next = 0;- } if (req->flags & (REQ\_F\_LINK | REQ\_F\_HARDLINK)) {- req->flags |= REQ\_F\_LINK\_HEAD;- INIT\_LIST\_HEAD(&req->link\_list);-- ret = io\_req\_defer\_prep(req, sqe);- if (unlikely(ret))- req->flags |= REQ\_F\_FAIL\_LINK;- \*link = req;+ link->head = req;+ link->last = req; } else {- io\_queue\_sqe(req, sqe, cs);+ io\_queue\_sqe(req); } } @@ -6488,29 +7090,27 @@ static int io\_submit\_sqe(struct io\_kiocb \*req, const struct io\_uring\_sqe \*sqe, /\* \* Batched submission is done, ensure local IO is flushed out. \*/-static void io\_submit\_state\_end(struct io\_submit\_state \*state)+static void io\_submit\_state\_end(struct io\_submit\_state \*state,+ struct io\_ring\_ctx \*ctx) {- if (!list\_empty(&state->comp.list))- io\_submit\_flush\_completions(&state->comp);- blk\_finish\_plug(&state->plug);- io\_state\_file\_put(state);- if (state->free\_reqs)- kmem\_cache\_free\_bulk(req\_cachep, state->free\_reqs, state->reqs);+ if (state->link.head)+ io\_queue\_sqe(state->link.head);+ if (state->compl\_nr)+ io\_submit\_flush\_completions(ctx);+ if (state->plug\_started)+ blk\_finish\_plug(&state->plug); }  /\* \* Start submission side cache. \*/ static void io\_submit\_state\_start(struct io\_submit\_state \*state,- struct io\_ring\_ctx \*ctx, unsigned int max\_ios)-{- blk\_start\_plug(&state->plug);- state->comp.nr = 0;- INIT\_LIST\_HEAD(&state->comp.list);- state->comp.ctx = ctx;- state->free\_reqs = 0;- state->file = NULL;+ unsigned int max\_ios)+{+ state->plug\_started = false; state->ios\_left = max\_ios;+ /\* set only head, no need to init link\_last in advance \*/+ state->link.head = NULL; }  static void io\_commit\_sqring(struct io\_ring\_ctx \*ctx)@@ -6526,7 +7126,7 @@ static void io\_commit\_sqring(struct io\_ring\_ctx \*ctx) }  /\*- \* Fetch an sqe, if one is available. Note that sqe\_ptr will point to memory+ \* Fetch an sqe, if one is available. Note this returns a pointer to memory \* that is mapped by userspace. This means that care needs to be taken to \* ensure that reads are stable, as we cannot rely on userspace always \* being a good citizen. If members of the sqe are validated and then later@@ -6535,8 +7135,8 @@ static void io\_commit\_sqring(struct io\_ring\_ctx \*ctx) \*/ static const struct io\_uring\_sqe \*io\_get\_sqe(struct io\_ring\_ctx \*ctx) {- u32 \*sq\_array = ctx->sq\_array;- unsigned head;+ unsigned head, mask = ctx->sq\_entries - 1;+ unsigned sq\_idx = ctx->cached\_sq\_head++ & mask;  /\* \* The cached sq head (or cq tail) serves two purposes:@@ -6546,423 +7146,256 @@ static const struct io\_uring\_sqe \*io\_get\_sqe(struct io\_ring\_ctx \*ctx) \* 2) allows the kernel side to track the head on its own, even \* though the application is the one updating it. \*/- head = READ\_ONCE(sq\_array[ctx->cached\_sq\_head & ctx->sq\_mask]);+ head = READ\_ONCE(ctx->sq\_array[sq\_idx]); if (likely(head < ctx->sq\_entries)) return &ctx->sq\_sqes[head];  /\* drop invalid entries \*/- ctx->cached\_sq\_dropped++;- WRITE\_ONCE(ctx->rings->sq\_dropped, ctx->cached\_sq\_dropped);+ ctx->cq\_extra--;+ WRITE\_ONCE(ctx->rings->sq\_dropped,+ READ\_ONCE(ctx->rings->sq\_dropped) + 1); return NULL; } -static inline void io\_consume\_sqe(struct io\_ring\_ctx \*ctx)-{- ctx->cached\_sq\_head++;-}--/\*- \* Check SQE restrictions (opcode and flags).- \*- \* Returns 'true' if SQE is allowed, 'false' otherwise.- \*/-static inline bool io\_check\_restriction(struct io\_ring\_ctx \*ctx,- struct io\_kiocb \*req,- unsigned int sqe\_flags)-{- if (!ctx->restricted)- return true;-- if (!test\_bit(req->opcode, ctx->restrictions.sqe\_op))- return false;-- if ((sqe\_flags & ctx->restrictions.sqe\_flags\_required) !=- ctx->restrictions.sqe\_flags\_required)- return false;-- if (sqe\_flags & ~(ctx->restrictions.sqe\_flags\_allowed |- ctx->restrictions.sqe\_flags\_required))- return false;-- return true;-}--#define SQE\_VALID\_FLAGS (IOSQE\_FIXED\_FILE|IOSQE\_IO\_DRAIN|IOSQE\_IO\_LINK| \- IOSQE\_IO\_HARDLINK | IOSQE\_ASYNC | \- IOSQE\_BUFFER\_SELECT)--static int io\_init\_req(struct io\_ring\_ctx \*ctx, struct io\_kiocb \*req,- const struct io\_uring\_sqe \*sqe,- struct io\_submit\_state \*state)-{- unsigned int sqe\_flags;- int id, ret;-- req->opcode = READ\_ONCE(sqe->opcode);- req->user\_data = READ\_ONCE(sqe->user\_data);- req->async\_data = NULL;- req->file = NULL;- req->ctx = ctx;- req->flags = 0;- /\* one is dropped after submission, the other at completion \*/- refcount\_set(&req->refs, 2);- req->task = current;- req->result = 0;-- if (unlikely(req->opcode >= IORING\_OP\_LAST))- return -EINVAL;-- if (unlikely(io\_sq\_thread\_acquire\_mm(ctx, req)))- return -EFAULT;-- sqe\_flags = READ\_ONCE(sqe->flags);- /\* enforce forwards compatibility on users \*/- if (unlikely(sqe\_flags & ~SQE\_VALID\_FLAGS))- return -EINVAL;-- if (unlikely(!io\_check\_restriction(ctx, req, sqe\_flags)))- return -EACCES;-- if ((sqe\_flags & IOSQE\_BUFFER\_SELECT) &&- !io\_op\_defs[req->opcode].buffer\_select)- return -EOPNOTSUPP;-- id = READ\_ONCE(sqe->personality);- if (id) {- struct io\_identity \*iod;-- iod = xa\_load(&ctx->personalities, id);- if (unlikely(!iod))- return -EINVAL;- refcount\_inc(&iod->count);-- \_\_io\_req\_init\_async(req);- get\_cred(iod->creds);- req->work.identity = iod;- req->work.flags |= IO\_WQ\_WORK\_CREDS;- }-- /\* same numerical values with corresponding REQ\_F\_\*, safe to copy \*/- req->flags |= sqe\_flags;-- if (!io\_op\_defs[req->opcode].needs\_file)- return 0;-- ret = io\_req\_set\_file(state, req, READ\_ONCE(sqe->fd));- state->ios\_left--;- return ret;-}- static int io\_submit\_sqes(struct io\_ring\_ctx \*ctx, unsigned int nr)+ \_\_must\_hold(&ctx->uring\_lock) {- struct io\_submit\_state state;- struct io\_kiocb \*link = NULL;- int i, submitted = 0;-- /\* if we have a backlog and couldn't flush it all, return BUSY \*/- if (test\_bit(0, &ctx->sq\_check\_overflow)) {- if (!\_\_io\_cqring\_overflow\_flush(ctx, false, NULL, NULL))- return -EBUSY;- }+ int submitted = 0;  /\* make sure SQ entry isn't read before tail \*/ nr = min3(nr, ctx->sq\_entries, io\_sqring\_entries(ctx));- if (!percpu\_ref\_tryget\_many(&ctx->refs, nr)) return -EAGAIN;+ io\_get\_task\_refs(nr); - percpu\_counter\_add(&current->io\_uring->inflight, nr);- refcount\_add(nr, &current->usage);-- io\_submit\_state\_start(&state, ctx, nr);-- for (i = 0; i < nr; i++) {+ io\_submit\_state\_start(&ctx->submit\_state, nr);+ while (submitted < nr) { const struct io\_uring\_sqe \*sqe; struct io\_kiocb \*req;- int err; - sqe = io\_get\_sqe(ctx);- if (unlikely(!sqe)) {- io\_consume\_sqe(ctx);- break;- }- req = io\_alloc\_req(ctx, &state);+ req = io\_alloc\_req(ctx); if (unlikely(!req)) { if (!submitted) submitted = -EAGAIN; break; }- io\_consume\_sqe(ctx);+ sqe = io\_get\_sqe(ctx);+ if (unlikely(!sqe)) {+ list\_add(&req->inflight\_entry, &ctx->submit\_state.free\_list);+ break;+ } /\* will complete beyond this point, count as submitted \*/ submitted++;-- err = io\_init\_req(ctx, req, sqe, &state);- if (unlikely(err)) {-fail\_req:- io\_put\_req(req);- io\_req\_complete(req, err);+ if (io\_submit\_sqe(ctx, req, sqe)) break;- }-- trace\_io\_uring\_submit\_sqe(ctx, req->opcode, req->user\_data,- true, io\_async\_submit(ctx));- err = io\_submit\_sqe(req, sqe, &link, &state.comp);- if (err)- goto fail\_req; }  if (unlikely(submitted != nr)) { int ref\_used = (submitted == -EAGAIN) ? 0 : submitted;- struct io\_uring\_task \*tctx = current->io\_uring; int unused = nr - ref\_used; + current->io\_uring->cached\_refs += unused; percpu\_ref\_put\_many(&ctx->refs, unused);- percpu\_counter\_sub(&tctx->inflight, unused);- put\_task\_struct\_many(current, unused); }- if (link)- io\_queue\_link\_head(link, &state.comp);- io\_submit\_state\_end(&state); + io\_submit\_state\_end(&ctx->submit\_state, ctx); /\* Commit SQ ring head once we've consumed and submitted all SQEs \*/ io\_commit\_sqring(ctx);  return submitted; } -static inline void io\_ring\_set\_wakeup\_flag(struct io\_ring\_ctx \*ctx)+static inline bool io\_sqd\_events\_pending(struct io\_sq\_data \*sqd) {- /\* Tell userspace we may need a wakeup call \*/- spin\_lock\_irq(&ctx->completion\_lock);- ctx->rings->sq\_flags |= IORING\_SQ\_NEED\_WAKEUP;- spin\_unlock\_irq(&ctx->completion\_lock);+ return READ\_ONCE(sqd->state); } -static inline void io\_ring\_clear\_wakeup\_flag(struct io\_ring\_ctx \*ctx)+static inline void io\_ring\_set\_wakeup\_flag(struct io\_ring\_ctx \*ctx) {- spin\_lock\_irq(&ctx->completion\_lock);- ctx->rings->sq\_flags &= ~IORING\_SQ\_NEED\_WAKEUP;- spin\_unlock\_irq(&ctx->completion\_lock);+ /\* Tell userspace we may need a wakeup call \*/+ spin\_lock(&ctx->completion\_lock);+ WRITE\_ONCE(ctx->rings->sq\_flags,+ ctx->rings->sq\_flags | IORING\_SQ\_NEED\_WAKEUP);+ spin\_unlock(&ctx->completion\_lock); } -static int io\_sq\_wake\_function(struct wait\_queue\_entry \*wqe, unsigned mode,- int sync, void \*key)+static inline void io\_ring\_clear\_wakeup\_flag(struct io\_ring\_ctx \*ctx) {- struct io\_ring\_ctx \*ctx = container\_of(wqe, struct io\_ring\_ctx, sqo\_wait\_entry);- int ret;-- ret = autoremove\_wake\_function(wqe, mode, sync, key);- if (ret) {- unsigned long flags;-- spin\_lock\_irqsave(&ctx->completion\_lock, flags);- ctx->rings->sq\_flags &= ~IORING\_SQ\_NEED\_WAKEUP;- spin\_unlock\_irqrestore(&ctx->completion\_lock, flags);- }- return ret;+ spin\_lock(&ctx->completion\_lock);+ WRITE\_ONCE(ctx->rings->sq\_flags,+ ctx->rings->sq\_flags & ~IORING\_SQ\_NEED\_WAKEUP);+ spin\_unlock(&ctx->completion\_lock); } -enum sq\_ret {- SQT\_IDLE = 1,- SQT\_SPIN = 2,- SQT\_DID\_WORK = 4,-};--static enum sq\_ret \_\_io\_sq\_thread(struct io\_ring\_ctx \*ctx,- unsigned long start\_jiffies, bool cap\_entries)+static int \_\_io\_sq\_thread(struct io\_ring\_ctx \*ctx, bool cap\_entries) {- unsigned long timeout = start\_jiffies + ctx->sq\_thread\_idle;- struct io\_sq\_data \*sqd = ctx->sq\_data; unsigned int to\_submit; int ret = 0; -again:- if (!list\_empty(&ctx->iopoll\_list)) {- unsigned nr\_events = 0;-- mutex\_lock(&ctx->uring\_lock);- if (!list\_empty(&ctx->iopoll\_list) && !need\_resched())- io\_do\_iopoll(ctx, &nr\_events, 0);- mutex\_unlock(&ctx->uring\_lock);- }- to\_submit = io\_sqring\_entries(ctx);+ /\* if we're handling multiple rings, cap submit size for fairness \*/+ if (cap\_entries && to\_submit > IORING\_SQPOLL\_CAP\_ENTRIES\_VALUE)+ to\_submit = IORING\_SQPOLL\_CAP\_ENTRIES\_VALUE; - /\*- \* If submit got -EBUSY, flag us as needing the application- \* to enter the kernel to reap and flush events.- \*/- if (!to\_submit || ret == -EBUSY || need\_resched()) {- /\*- \* Drop cur\_mm before scheduling, we can't hold it for- \* long periods (or over schedule()). Do this before- \* adding ourselves to the waitqueue, as the unuse/drop- \* may sleep.- \*/- io\_sq\_thread\_drop\_mm();+ if (!list\_empty(&ctx->iopoll\_list) || to\_submit) {+ unsigned nr\_events = 0;+ const struct cred \*creds = NULL; - /\*- \* We're polling. If we're within the defined idle- \* period, then let us spin without work before going- \* to sleep. The exception is if we got EBUSY doing- \* more IO, we should wait for the application to- \* reap events and wake us up.- \*/- if (!list\_empty(&ctx->iopoll\_list) || need\_resched() ||- (!time\_after(jiffies, timeout) && ret != -EBUSY &&- !percpu\_ref\_is\_dying(&ctx->refs)))- return SQT\_SPIN;+ if (ctx->sq\_creds != current\_cred())+ creds = override\_creds(ctx->sq\_creds); - prepare\_to\_wait(&sqd->wait, &ctx->sqo\_wait\_entry,- TASK\_INTERRUPTIBLE);+ mutex\_lock(&ctx->uring\_lock);+ if (!list\_empty(&ctx->iopoll\_list))+ io\_do\_iopoll(ctx, &nr\_events, 0);  /\*- \* While doing polled IO, before going to sleep, we need- \* to check if there are new reqs added to iopoll\_list,- \* it is because reqs may have been punted to io worker- \* and will be added to iopoll\_list later, hence check- \* the iopoll\_list again.+ \* Don't submit if refs are dying, good for io\_uring\_register(),+ \* but also it is relied upon by io\_ring\_exit\_work() \*/- if ((ctx->flags & IORING\_SETUP\_IOPOLL) &&- !list\_empty\_careful(&ctx->iopoll\_list)) {- finish\_wait(&sqd->wait, &ctx->sqo\_wait\_entry);- goto again;- }+ if (to\_submit && likely(!percpu\_ref\_is\_dying(&ctx->refs)) &&+ !(ctx->flags & IORING\_SETUP\_R\_DISABLED))+ ret = io\_submit\_sqes(ctx, to\_submit);+ mutex\_unlock(&ctx->uring\_lock); - to\_submit = io\_sqring\_entries(ctx);- if (!to\_submit || ret == -EBUSY)- return SQT\_IDLE;+ if (to\_submit && wq\_has\_sleeper(&ctx->sqo\_sq\_wait))+ wake\_up(&ctx->sqo\_sq\_wait);+ if (creds)+ revert\_creds(creds); } - finish\_wait(&sqd->wait, &ctx->sqo\_wait\_entry);- io\_ring\_clear\_wakeup\_flag(ctx);-- /\* if we're handling multiple rings, cap submit size for fairness \*/- if (cap\_entries && to\_submit > 8)- to\_submit = 8;-- mutex\_lock(&ctx->uring\_lock);- if (likely(!percpu\_ref\_is\_dying(&ctx->refs) && !ctx->sqo\_dead))- ret = io\_submit\_sqes(ctx, to\_submit);- mutex\_unlock(&ctx->uring\_lock);+ return ret;+} - if (!io\_sqring\_full(ctx) && wq\_has\_sleeper(&ctx->sqo\_sq\_wait))- wake\_up(&ctx->sqo\_sq\_wait);+static void io\_sqd\_update\_thread\_idle(struct io\_sq\_data \*sqd)+{+ struct io\_ring\_ctx \*ctx;+ unsigned sq\_thread\_idle = 0; - return SQT\_DID\_WORK;+ list\_for\_each\_entry(ctx, &sqd->ctx\_list, sqd\_list)+ sq\_thread\_idle = max(sq\_thread\_idle, ctx->sq\_thread\_idle);+ sqd->sq\_thread\_idle = sq\_thread\_idle; } -static void io\_sqd\_init\_new(struct io\_sq\_data \*sqd)+static bool io\_sqd\_handle\_event(struct io\_sq\_data \*sqd) {- struct io\_ring\_ctx \*ctx;+ bool did\_sig = false;+ struct ksignal ksig; - while (!list\_empty(&sqd->ctx\_new\_list)) {- ctx = list\_first\_entry(&sqd->ctx\_new\_list, struct io\_ring\_ctx, sqd\_list);- init\_wait(&ctx->sqo\_wait\_entry);- ctx->sqo\_wait\_entry.func = io\_sq\_wake\_function;- list\_move\_tail(&ctx->sqd\_list, &sqd->ctx\_list);- complete(&ctx->sq\_thread\_comp);+ if (test\_bit(IO\_SQ\_THREAD\_SHOULD\_PARK, &sqd->state) ||+ signal\_pending(current)) {+ mutex\_unlock(&sqd->lock);+ if (signal\_pending(current))+ did\_sig = get\_signal(&ksig);+ cond\_resched();+ mutex\_lock(&sqd->lock); }+ return did\_sig || test\_bit(IO\_SQ\_THREAD\_SHOULD\_STOP, &sqd->state); }  static int io\_sq\_thread(void \*data) {- struct cgroup\_subsys\_state \*cur\_css = NULL;- const struct cred \*old\_cred = NULL; struct io\_sq\_data \*sqd = data; struct io\_ring\_ctx \*ctx;- unsigned long start\_jiffies;+ unsigned long timeout = 0;+ char buf[TASK\_COMM\_LEN];+ DEFINE\_WAIT(wait); - start\_jiffies = jiffies;- while (!kthread\_should\_stop()) {- enum sq\_ret ret = 0;- bool cap\_entries;+ snprintf(buf, sizeof(buf), "iou-sqp-%d", sqd->task\_pid);+ set\_task\_comm(current, buf); - /\*- \* Any changes to the sqd lists are synchronized through the- \* kthread parking. This synchronizes the thread vs users,- \* the users are synchronized on the sqd->ctx\_lock.- \*/- if (kthread\_should\_park()) {- kthread\_parkme();- /\*- \* When sq thread is unparked, in case the previous park operation- \* comes from io\_put\_sq\_data(), which means that sq thread is going- \* to be stopped, so here needs to have a check.- \*/- if (kthread\_should\_stop())+ if (sqd->sq\_cpu != -1)+ set\_cpus\_allowed\_ptr(current, cpumask\_of(sqd->sq\_cpu));+ else+ set\_cpus\_allowed\_ptr(current, cpu\_online\_mask);+ current->flags |= PF\_NO\_SETAFFINITY;++ mutex\_lock(&sqd->lock);+ while (1) {+ bool cap\_entries, sqt\_spin = false;++ if (io\_sqd\_events\_pending(sqd) || signal\_pending(current)) {+ if (io\_sqd\_handle\_event(sqd)) break;+ timeout = jiffies + sqd->sq\_thread\_idle; } - if (unlikely(!list\_empty(&sqd->ctx\_new\_list)))- io\_sqd\_init\_new(sqd);- cap\_entries = !list\_is\_singular(&sqd->ctx\_list);- list\_for\_each\_entry(ctx, &sqd->ctx\_list, sqd\_list) {- if (current->cred != ctx->creds) {- if (old\_cred)- revert\_creds(old\_cred);- old\_cred = override\_creds(ctx->creds);- }- io\_sq\_thread\_associate\_blkcg(ctx, &cur\_css);-#ifdef CONFIG\_AUDIT- current->loginuid = ctx->loginuid;- current->sessionid = ctx->sessionid;-#endif+ int ret = \_\_io\_sq\_thread(ctx, cap\_entries); - ret |= \_\_io\_sq\_thread(ctx, start\_jiffies, cap\_entries);-- io\_sq\_thread\_drop\_mm();+ if (!sqt\_spin && (ret > 0 || !list\_empty(&ctx->iopoll\_list)))+ sqt\_spin = true; }+ if (io\_run\_task\_work())+ sqt\_spin = true; - if (ret & SQT\_SPIN) {- io\_run\_task\_work();- io\_sq\_thread\_drop\_mm();+ if (sqt\_spin || !time\_after(jiffies, timeout)) { cond\_resched();- } else if (ret == SQT\_IDLE) {- if (kthread\_should\_park())- continue;- list\_for\_each\_entry(ctx, &sqd->ctx\_list, sqd\_list)+ if (sqt\_spin)+ timeout = jiffies + sqd->sq\_thread\_idle;+ continue;+ }++ prepare\_to\_wait(&sqd->wait, &wait, TASK\_INTERRUPTIBLE);+ if (!io\_sqd\_events\_pending(sqd) && !current->task\_works) {+ bool needs\_sched = true;++ list\_for\_each\_entry(ctx, &sqd->ctx\_list, sqd\_list) { io\_ring\_set\_wakeup\_flag(ctx);- schedule();- start\_jiffies = jiffies;++ if ((ctx->flags & IORING\_SETUP\_IOPOLL) &&+ !list\_empty\_careful(&ctx->iopoll\_list)) {+ needs\_sched = false;+ break;+ }+ if (io\_sqring\_entries(ctx)) {+ needs\_sched = false;+ break;+ }+ }++ if (needs\_sched) {+ mutex\_unlock(&sqd->lock);+ schedule();+ mutex\_lock(&sqd->lock);+ } list\_for\_each\_entry(ctx, &sqd->ctx\_list, sqd\_list) io\_ring\_clear\_wakeup\_flag(ctx); }++ finish\_wait(&sqd->wait, &wait);+ timeout = jiffies + sqd->sq\_thread\_idle; } + io\_uring\_cancel\_generic(true, sqd);+ sqd->thread = NULL;+ list\_for\_each\_entry(ctx, &sqd->ctx\_list, sqd\_list)+ io\_ring\_set\_wakeup\_flag(ctx); io\_run\_task\_work();- io\_sq\_thread\_drop\_mm();-- if (cur\_css)- io\_sq\_thread\_unassociate\_blkcg();- if (old\_cred)- revert\_creds(old\_cred);-- kthread\_parkme();+ mutex\_unlock(&sqd->lock); - return 0;+ complete(&sqd->exited);+ do\_exit(0); }  struct io\_wait\_queue { struct wait\_queue\_entry wq; struct io\_ring\_ctx \*ctx;- unsigned to\_wait;+ unsigned cq\_tail; unsigned nr\_timeouts; };  static inline bool io\_should\_wake(struct io\_wait\_queue \*iowq) { struct io\_ring\_ctx \*ctx = iowq->ctx;+ int dist = ctx->cached\_cq\_tail - (int) iowq->cq\_tail;  /\* \* Wake up if we have enough events, or if a timeout occurred since we \* started waiting. For timeouts, we always want to return to userspace, \* regardless of event count. \*/- return io\_cqring\_events(ctx) >= iowq->to\_wait ||- atomic\_read(&ctx->cq\_timeouts) != iowq->nr\_timeouts;+ return dist >= 0 || atomic\_read(&ctx->cq\_timeouts) != iowq->nr\_timeouts; }  static int io\_wake\_function(struct wait\_queue\_entry \*curr, unsigned int mode,@@ -6975,7 +7408,7 @@ static int io\_wake\_function(struct wait\_queue\_entry \*curr, unsigned int mode, \* Cannot safely flush overflowed CQEs from here, ensure we wake up \* the task, and the next invocation will do it. \*/- if (io\_should\_wake(iowq) || test\_bit(0, &iowq->ctx->cq\_check\_overflow))+ if (io\_should\_wake(iowq) || test\_bit(0, &iowq->ctx->check\_cq\_overflow)) return autoremove\_wake\_function(curr, mode, wake\_flags, key); return -1; }@@ -6986,43 +7419,60 @@ static int io\_run\_task\_work\_sig(void) return 1; if (!signal\_pending(current)) return 0;- if (current->jobctl & JOBCTL\_TASK\_WORK) {- spin\_lock\_irq(&current->sighand->siglock);- current->jobctl &= ~JOBCTL\_TASK\_WORK;- recalc\_sigpending();- spin\_unlock\_irq(&current->sighand->siglock);- return 1;- }+ if (test\_thread\_flag(TIF\_NOTIFY\_SIGNAL))+ return -ERESTARTSYS; return -EINTR; } +/\* when returns >0, the caller should retry \*/+static inline int io\_cqring\_wait\_schedule(struct io\_ring\_ctx \*ctx,+ struct io\_wait\_queue \*iowq,+ ktime\_t timeout)+{+ int ret;++ /\* make sure we run task\_work before checking for signals \*/+ ret = io\_run\_task\_work\_sig();+ if (ret || io\_should\_wake(iowq))+ return ret;+ /\* let the caller flush overflows, retry \*/+ if (test\_bit(0, &ctx->check\_cq\_overflow))+ return 1;++ if (!schedule\_hrtimeout(&timeout, HRTIMER\_MODE\_ABS))+ return -ETIME;+ return 1;+}+ /\* \* Wait until events become available, if we don't already have some. The \* application must reap them itself, as they reside on the shared cq ring. \*/ static int io\_cqring\_wait(struct io\_ring\_ctx \*ctx, int min\_events,- const sigset\_t \_\_user \*sig, size\_t sigsz)-{- struct io\_wait\_queue iowq = {- .wq = {- .private = current,- .func = io\_wake\_function,- .entry = LIST\_HEAD\_INIT(iowq.wq.entry),- },- .ctx = ctx,- .to\_wait = min\_events,- };+ const sigset\_t \_\_user \*sig, size\_t sigsz,+ struct \_\_kernel\_timespec \_\_user \*uts)+{+ struct io\_wait\_queue iowq; struct io\_rings \*rings = ctx->rings;- int ret = 0;+ ktime\_t timeout = KTIME\_MAX;+ int ret;  do {- io\_cqring\_overflow\_flush(ctx, false, NULL, NULL);+ io\_cqring\_overflow\_flush(ctx); if (io\_cqring\_events(ctx) >= min\_events) return 0; if (!io\_run\_task\_work()) break; } while (1); + if (uts) {+ struct timespec64 ts;++ if (get\_timespec64(&ts, uts))+ return -EFAULT;+ timeout = ktime\_add\_ns(timespec64\_to\_ktime(ts), ktime\_get\_ns());+ }+ if (sig) { #ifdef CONFIG\_COMPAT if (in\_compat\_syscall())@@ -7036,179 +7486,381 @@ static int io\_cqring\_wait(struct io\_ring\_ctx \*ctx, int min\_events, return ret; } + init\_waitqueue\_func\_entry(&iowq.wq, io\_wake\_function);+ iowq.wq.private = current;+ INIT\_LIST\_HEAD(&iowq.wq.entry);+ iowq.ctx = ctx; iowq.nr\_timeouts = atomic\_read(&ctx->cq\_timeouts);+ iowq.cq\_tail = READ\_ONCE(ctx->rings->cq.head) + min\_events;+ trace\_io\_uring\_cqring\_wait(ctx, min\_events); do {- io\_cqring\_overflow\_flush(ctx, false, NULL, NULL);- prepare\_to\_wait\_exclusive(&ctx->wait, &iowq.wq,- TASK\_INTERRUPTIBLE);- /\* make sure we run task\_work before checking for signals \*/- ret = io\_run\_task\_work\_sig();- if (ret > 0) {- finish\_wait(&ctx->wait, &iowq.wq);- continue;- }- else if (ret < 0)+ /\* if we can't even flush overflow, don't wait for more \*/+ if (!io\_cqring\_overflow\_flush(ctx)) {+ ret = -EBUSY; break;- if (io\_should\_wake(&iowq))- break;- if (test\_bit(0, &ctx->cq\_check\_overflow)) {- finish\_wait(&ctx->wait, &iowq.wq);- continue; }- schedule();- } while (1);- finish\_wait(&ctx->wait, &iowq.wq);+ prepare\_to\_wait\_exclusive(&ctx->cq\_wait, &iowq.wq,+ TASK\_INTERRUPTIBLE);+ ret = io\_cqring\_wait\_schedule(ctx, &iowq, timeout);+ finish\_wait(&ctx->cq\_wait, &iowq.wq);+ cond\_resched();+ } while (ret > 0);  restore\_saved\_sigmask\_unless(ret == -EINTR);  return READ\_ONCE(rings->cq.head) == READ\_ONCE(rings->cq.tail) ? ret : 0; } -static void \_\_io\_sqe\_files\_unregister(struct io\_ring\_ctx \*ctx)+static void io\_free\_page\_table(void \*\*table, size\_t size) {-#if defined(CONFIG\_UNIX)- if (ctx->ring\_sock) {- struct sock \*sock = ctx->ring\_sock->sk;- struct sk\_buff \*skb;+ unsigned i, nr\_tables = DIV\_ROUND\_UP(size, PAGE\_SIZE); - while ((skb = skb\_dequeue(&sock->sk\_receive\_queue)) != NULL)- kfree\_skb(skb);- }-#else- int i;+ for (i = 0; i < nr\_tables; i++)+ kfree(table[i]);+ kfree(table);+} - for (i = 0; i < ctx->nr\_user\_files; i++) {- struct file \*file;+static void \*\*io\_alloc\_page\_table(size\_t size)+{+ unsigned i, nr\_tables = DIV\_ROUND\_UP(size, PAGE\_SIZE);+ size\_t init\_size = size;+ void \*\*table; - file = io\_file\_from\_index(ctx, i);- if (file)- fput(file);+ table = kcalloc(nr\_tables, sizeof(\*table), GFP\_KERNEL\_ACCOUNT);+ if (!table)+ return NULL;++ for (i = 0; i < nr\_tables; i++) {+ unsigned int this\_size = min\_t(size\_t, size, PAGE\_SIZE);++ table[i] = kzalloc(this\_size, GFP\_KERNEL\_ACCOUNT);+ if (!table[i]) {+ io\_free\_page\_table(table, init\_size);+ return NULL;+ }+ size -= this\_size; }-#endif+ return table; } -static void io\_file\_ref\_kill(struct percpu\_ref \*ref)+static void io\_rsrc\_node\_destroy(struct io\_rsrc\_node \*ref\_node) {- struct fixed\_file\_data \*data;-- data = container\_of(ref, struct fixed\_file\_data, refs);- complete(&data->done);+ percpu\_ref\_exit(&ref\_node->refs);+ kfree(ref\_node); } -static void io\_sqe\_files\_set\_node(struct fixed\_file\_data \*file\_data,- struct fixed\_file\_ref\_node \*ref\_node)+static void io\_rsrc\_node\_ref\_zero(struct percpu\_ref \*ref) {- spin\_lock\_bh(&file\_data->lock);- file\_data->node = ref\_node;- list\_add\_tail(&ref\_node->node, &file\_data->ref\_list);- spin\_unlock\_bh(&file\_data->lock);- percpu\_ref\_get(&file\_data->refs);+ struct io\_rsrc\_node \*node = container\_of(ref, struct io\_rsrc\_node, refs);+ struct io\_ring\_ctx \*ctx = node->rsrc\_data->ctx;+ unsigned long flags;+ bool first\_add = false;+ unsigned long delay = HZ;++ spin\_lock\_irqsave(&ctx->rsrc\_ref\_lock, flags);+ node->done = true;++ /\* if we are mid-quiesce then do not delay \*/+ if (node->rsrc\_data->quiesce)+ delay = 0;++ while (!list\_empty(&ctx->rsrc\_ref\_list)) {+ node = list\_first\_entry(&ctx->rsrc\_ref\_list,+ struct io\_rsrc\_node, node);+ /\* recycle ref nodes in order \*/+ if (!node->done)+ break;+ list\_del(&node->node);+ first\_add |= llist\_add(&node->llist, &ctx->rsrc\_put\_llist);+ }+ spin\_unlock\_irqrestore(&ctx->rsrc\_ref\_lock, flags);++ if (first\_add)+ mod\_delayed\_work(system\_wq, &ctx->rsrc\_put\_work, delay); } +static struct io\_rsrc\_node \*io\_rsrc\_node\_alloc(struct io\_ring\_ctx \*ctx)+{+ struct io\_rsrc\_node \*ref\_node;++ ref\_node = kzalloc(sizeof(\*ref\_node), GFP\_KERNEL);+ if (!ref\_node)+ return NULL; -static void io\_sqe\_files\_kill\_node(struct fixed\_file\_data \*data)+ if (percpu\_ref\_init(&ref\_node->refs, io\_rsrc\_node\_ref\_zero,+ 0, GFP\_KERNEL)) {+ kfree(ref\_node);+ return NULL;+ }+ INIT\_LIST\_HEAD(&ref\_node->node);+ INIT\_LIST\_HEAD(&ref\_node->rsrc\_list);+ ref\_node->done = false;+ return ref\_node;+}++static void io\_rsrc\_node\_switch(struct io\_ring\_ctx \*ctx,+ struct io\_rsrc\_data \*data\_to\_kill) {- struct fixed\_file\_ref\_node \*ref\_node = NULL;+ WARN\_ON\_ONCE(!ctx->rsrc\_backup\_node);+ WARN\_ON\_ONCE(data\_to\_kill && !ctx->rsrc\_node);++ if (data\_to\_kill) {+ struct io\_rsrc\_node \*rsrc\_node = ctx->rsrc\_node;++ rsrc\_node->rsrc\_data = data\_to\_kill;+ spin\_lock\_irq(&ctx->rsrc\_ref\_lock);+ list\_add\_tail(&rsrc\_node->node, &ctx->rsrc\_ref\_list);+ spin\_unlock\_irq(&ctx->rsrc\_ref\_lock); - spin\_lock\_bh(&data->lock);- ref\_node = data->node;- spin\_unlock\_bh(&data->lock);- if (ref\_node)- percpu\_ref\_kill(&ref\_node->refs);+ atomic\_inc(&data\_to\_kill->refs);+ percpu\_ref\_kill(&rsrc\_node->refs);+ ctx->rsrc\_node = NULL;+ }++ if (!ctx->rsrc\_node) {+ ctx->rsrc\_node = ctx->rsrc\_backup\_node;+ ctx->rsrc\_backup\_node = NULL;+ } } -static int io\_file\_ref\_quiesce(struct fixed\_file\_data \*data,- struct io\_ring\_ctx \*ctx)+static int io\_rsrc\_node\_switch\_start(struct io\_ring\_ctx \*ctx)+{+ if (ctx->rsrc\_backup\_node)+ return 0;+ ctx->rsrc\_backup\_node = io\_rsrc\_node\_alloc(ctx);+ return ctx->rsrc\_backup\_node ? 0 : -ENOMEM;+}++static int io\_rsrc\_ref\_quiesce(struct io\_rsrc\_data \*data, struct io\_ring\_ctx \*ctx) { int ret;- struct fixed\_file\_ref\_node \*backup\_node; + /\* As we may drop ->uring\_lock, other task may have started quiesce \*/ if (data->quiesce) return -ENXIO;  data->quiesce = true; do {- backup\_node = alloc\_fixed\_file\_ref\_node(ctx);- if (!backup\_node)+ ret = io\_rsrc\_node\_switch\_start(ctx);+ if (ret) break;+ io\_rsrc\_node\_switch(ctx, data); - io\_sqe\_files\_kill\_node(data);- percpu\_ref\_kill(&data->refs);- flush\_delayed\_work(&ctx->file\_put\_work);-- ret = wait\_for\_completion\_interruptible(&data->done);- if (!ret)+ /\* kill initial ref, already quiesced if zero \*/+ if (atomic\_dec\_and\_test(&data->refs)) break;+ mutex\_unlock(&ctx->uring\_lock);+ flush\_delayed\_work(&ctx->rsrc\_put\_work);+ ret = wait\_for\_completion\_interruptible(&data->done);+ if (!ret) {+ mutex\_lock(&ctx->uring\_lock);+ if (atomic\_read(&data->refs) > 0) {+ /\*+ \* it has been revived by another thread while+ \* we were unlocked+ \*/+ mutex\_unlock(&ctx->uring\_lock);+ } else {+ break;+ }+ } - percpu\_ref\_resurrect(&data->refs);- io\_sqe\_files\_set\_node(data, backup\_node);- backup\_node = NULL;+ atomic\_inc(&data->refs);+ /\* wait for all works potentially completing data->done \*/+ flush\_delayed\_work(&ctx->rsrc\_put\_work); reinit\_completion(&data->done);- mutex\_unlock(&ctx->uring\_lock);+ ret = io\_run\_task\_work\_sig(); mutex\_lock(&ctx->uring\_lock);-- if (ret < 0)- break;- backup\_node = alloc\_fixed\_file\_ref\_node(ctx);- ret = -ENOMEM;- if (!backup\_node)- break;- } while (1);+ } while (ret >= 0); data->quiesce = false; - if (backup\_node)- destroy\_fixed\_file\_ref\_node(backup\_node); return ret; } +static u64 \*io\_get\_tag\_slot(struct io\_rsrc\_data \*data, unsigned int idx)+{+ unsigned int off = idx & IO\_RSRC\_TAG\_TABLE\_MASK;+ unsigned int table\_idx = idx >> IO\_RSRC\_TAG\_TABLE\_SHIFT;++ return &data->tags[table\_idx][off];+}++static void io\_rsrc\_data\_free(struct io\_rsrc\_data \*data)+{+ size\_t size = data->nr \* sizeof(data->tags[0][0]);++ if (data->tags)+ io\_free\_page\_table((void \*\*)data->tags, size);+ kfree(data);+}++static int io\_rsrc\_data\_alloc(struct io\_ring\_ctx \*ctx, rsrc\_put\_fn \*do\_put,+ u64 \_\_user \*utags, unsigned nr,+ struct io\_rsrc\_data \*\*pdata)+{+ struct io\_rsrc\_data \*data;+ int ret = -ENOMEM;+ unsigned i;++ data = kzalloc(sizeof(\*data), GFP\_KERNEL);+ if (!data)+ return -ENOMEM;+ data->tags = (u64 \*\*)io\_alloc\_page\_table(nr \* sizeof(data->tags[0][0]));+ if (!data->tags) {+ kfree(data);+ return -ENOMEM;+ }++ data->nr = nr;+ data->ctx = ctx;+ data->do\_put = do\_put;+ if (utags) {+ ret = -EFAULT;+ for (i = 0; i < nr; i++) {+ u64 \*tag\_slot = io\_get\_tag\_slot(data, i);++ if (copy\_from\_user(tag\_slot, &utags[i],+ sizeof(\*tag\_slot)))+ goto fail;+ }+ }++ atomic\_set(&data->refs, 1);+ init\_completion(&data->done);+ \*pdata = data;+ return 0;+fail:+ io\_rsrc\_data\_free(data);+ return ret;+}++static bool io\_alloc\_file\_tables(struct io\_file\_table \*table, unsigned nr\_files)+{+ table->files = kvcalloc(nr\_files, sizeof(table->files[0]),+ GFP\_KERNEL\_ACCOUNT);+ return !!table->files;+}++static void io\_free\_file\_tables(struct io\_file\_table \*table)+{+ kvfree(table->files);+ table->files = NULL;+}++static void \_\_io\_sqe\_files\_unregister(struct io\_ring\_ctx \*ctx)+{+#if defined(CONFIG\_UNIX)+ if (ctx->ring\_sock) {+ struct sock \*sock = ctx->ring\_sock->sk;+ struct sk\_buff \*skb;++ while ((skb = skb\_dequeue(&sock->sk\_receive\_queue)) != NULL)+ kfree\_skb(skb);+ }+#else+ int i;++ for (i = 0; i < ctx->nr\_user\_files; i++) {+ struct file \*file;++ file = io\_file\_from\_index(ctx, i);+ if (file)+ fput(file);+ }+#endif+ io\_free\_file\_tables(&ctx->file\_table);+ io\_rsrc\_data\_free(ctx->file\_data);+ ctx->file\_data = NULL;+ ctx->nr\_user\_files = 0;+}+ static int io\_sqe\_files\_unregister(struct io\_ring\_ctx \*ctx) {- struct fixed\_file\_data \*data = ctx->file\_data;- unsigned nr\_tables, i;+ unsigned nr = ctx->nr\_user\_files; int ret; - /\*- \* percpu\_ref\_is\_dying() is to stop parallel files unregister- \* Since we possibly drop uring lock later in this function to- \* run task work.- \*/- if (!data || percpu\_ref\_is\_dying(&data->refs))+ if (!ctx->file\_data) return -ENXIO;- ret = io\_file\_ref\_quiesce(data, ctx);- if (ret)- return ret; - \_\_io\_sqe\_files\_unregister(ctx);- nr\_tables = DIV\_ROUND\_UP(ctx->nr\_user\_files, IORING\_MAX\_FILES\_TABLE);- for (i = 0; i < nr\_tables; i++)- kfree(data->table[i].files);- kfree(data->table);- percpu\_ref\_exit(&data->refs);- kfree(data);- ctx->file\_data = NULL;+ /\*+ \* Quiesce may unlock ->uring\_lock, and while it's not held+ \* prevent new requests using the table.+ \*/ ctx->nr\_user\_files = 0;- return 0;+ ret = io\_rsrc\_ref\_quiesce(ctx->file\_data, ctx);+ ctx->nr\_user\_files = nr;+ if (!ret)+ \_\_io\_sqe\_files\_unregister(ctx);+ return ret;+}++static void io\_sq\_thread\_unpark(struct io\_sq\_data \*sqd)+ \_\_releases(&sqd->lock)+{+ WARN\_ON\_ONCE(sqd->thread == current);++ /\*+ \* Do the dance but not conditional clear\_bit() because it'd race with+ \* other threads incrementing park\_pending and setting the bit.+ \*/+ clear\_bit(IO\_SQ\_THREAD\_SHOULD\_PARK, &sqd->state);+ if (atomic\_dec\_return(&sqd->park\_pending))+ set\_bit(IO\_SQ\_THREAD\_SHOULD\_PARK, &sqd->state);+ mutex\_unlock(&sqd->lock);+}++static void io\_sq\_thread\_park(struct io\_sq\_data \*sqd)+ \_\_acquires(&sqd->lock)+{+ WARN\_ON\_ONCE(sqd->thread == current);++ atomic\_inc(&sqd->park\_pending);+ set\_bit(IO\_SQ\_THREAD\_SHOULD\_PARK, &sqd->state);+ mutex\_lock(&sqd->lock);+ if (sqd->thread)+ wake\_up\_process(sqd->thread);+}++static void io\_sq\_thread\_stop(struct io\_sq\_data \*sqd)+{+ WARN\_ON\_ONCE(sqd->thread == current);+ WARN\_ON\_ONCE(test\_bit(IO\_SQ\_THREAD\_SHOULD\_STOP, &sqd->state));++ set\_bit(IO\_SQ\_THREAD\_SHOULD\_STOP, &sqd->state);+ mutex\_lock(&sqd->lock);+ if (sqd->thread)+ wake\_up\_process(sqd->thread);+ mutex\_unlock(&sqd->lock);+ wait\_for\_completion(&sqd->exited); }  static void io\_put\_sq\_data(struct io\_sq\_data \*sqd) { if (refcount\_dec\_and\_test(&sqd->refs)) {- /\*- \* The park is a bit of a work-around, without it we get- \* warning spews on shutdown with SQPOLL set and affinity- \* set to a single CPU.- \*/- if (sqd->thread) {- kthread\_park(sqd->thread);- kthread\_stop(sqd->thread);- }+ WARN\_ON\_ONCE(atomic\_read(&sqd->park\_pending)); + io\_sq\_thread\_stop(sqd); kfree(sqd); } } +static void io\_sq\_thread\_finish(struct io\_ring\_ctx \*ctx)+{+ struct io\_sq\_data \*sqd = ctx->sq\_data;++ if (sqd) {+ io\_sq\_thread\_park(sqd);+ list\_del\_init(&ctx->sqd\_list);+ io\_sqd\_update\_thread\_idle(sqd);+ io\_sq\_thread\_unpark(sqd);++ io\_put\_sq\_data(sqd);+ ctx->sq\_data = NULL;+ }+}+ static struct io\_sq\_data \*io\_attach\_sq\_data(struct io\_uring\_params \*p) { struct io\_ring\_ctx \*ctx\_attach;@@ -7229,92 +7881,46 @@ static struct io\_sq\_data \*io\_attach\_sq\_data(struct io\_uring\_params \*p) fdput(f); return ERR\_PTR(-EINVAL); }+ if (sqd->task\_tgid != current->tgid) {+ fdput(f);+ return ERR\_PTR(-EPERM);+ }  refcount\_inc(&sqd->refs); fdput(f); return sqd; } -static struct io\_sq\_data \*io\_get\_sq\_data(struct io\_uring\_params \*p)+static struct io\_sq\_data \*io\_get\_sq\_data(struct io\_uring\_params \*p,+ bool \*attached) { struct io\_sq\_data \*sqd; - if (p->flags & IORING\_SETUP\_ATTACH\_WQ)- return io\_attach\_sq\_data(p);+ \*attached = false;+ if (p->flags & IORING\_SETUP\_ATTACH\_WQ) {+ sqd = io\_attach\_sq\_data(p);+ if (!IS\_ERR(sqd)) {+ \*attached = true;+ return sqd;+ }+ /\* fall through for EPERM case, setup new sqd/task \*/+ if (PTR\_ERR(sqd) != -EPERM)+ return sqd;+ }  sqd = kzalloc(sizeof(\*sqd), GFP\_KERNEL); if (!sqd) return ERR\_PTR(-ENOMEM); + atomic\_set(&sqd->park\_pending, 0); refcount\_set(&sqd->refs, 1); INIT\_LIST\_HEAD(&sqd->ctx\_list);- INIT\_LIST\_HEAD(&sqd->ctx\_new\_list);- mutex\_init(&sqd->ctx\_lock); mutex\_init(&sqd->lock); init\_waitqueue\_head(&sqd->wait);+ init\_completion(&sqd->exited); return sqd; } -static void io\_sq\_thread\_unpark(struct io\_sq\_data \*sqd)- \_\_releases(&sqd->lock)-{- if (!sqd->thread)- return;- kthread\_unpark(sqd->thread);- mutex\_unlock(&sqd->lock);-}--static void io\_sq\_thread\_park(struct io\_sq\_data \*sqd)- \_\_acquires(&sqd->lock)-{- if (!sqd->thread)- return;- mutex\_lock(&sqd->lock);- kthread\_park(sqd->thread);-}--static void io\_sq\_thread\_stop(struct io\_ring\_ctx \*ctx)-{- struct io\_sq\_data \*sqd = ctx->sq\_data;-- if (sqd) {- if (sqd->thread) {- /\*- \* We may arrive here from the error branch in- \* io\_sq\_offload\_create() where the kthread is created- \* without being waked up, thus wake it up now to make- \* sure the wait will complete.- \*/- wake\_up\_process(sqd->thread);- wait\_for\_completion(&ctx->sq\_thread\_comp);-- io\_sq\_thread\_park(sqd);- }-- mutex\_lock(&sqd->ctx\_lock);- list\_del(&ctx->sqd\_list);- mutex\_unlock(&sqd->ctx\_lock);-- if (sqd->thread) {- finish\_wait(&sqd->wait, &ctx->sqo\_wait\_entry);- io\_sq\_thread\_unpark(sqd);- }-- io\_put\_sq\_data(sqd);- ctx->sq\_data = NULL;- }-}--static void io\_finish\_async(struct io\_ring\_ctx \*ctx)-{- io\_sq\_thread\_stop(ctx);-- if (ctx->io\_wq) {- io\_wq\_destroy(ctx->io\_wq);- ctx->io\_wq = NULL;- }-}- #if defined(CONFIG\_UNIX) /\* \* Ensure the UNIX gc is aware of our file set, so we are certain that@@ -7342,7 +7948,7 @@ static int \_\_io\_sqe\_files\_scm(struct io\_ring\_ctx \*ctx, int nr, int offset) skb->scm\_io\_uring = 1;  nr\_files = 0;- fpl->user = get\_uid(ctx->user);+ fpl->user = get\_uid(current\_user()); for (i = 0; i < nr; i++) { struct file \*file = io\_file\_from\_index(ctx, i + offset); @@ -7418,35 +8024,9 @@ static int io\_sqe\_files\_scm(struct io\_ring\_ctx \*ctx) } #endif -static int io\_sqe\_alloc\_file\_tables(struct fixed\_file\_data \*file\_data,- unsigned nr\_tables, unsigned nr\_files)-{- int i;-- for (i = 0; i < nr\_tables; i++) {- struct fixed\_file\_table \*table = &file\_data->table[i];- unsigned this\_files;-- this\_files = min(nr\_files, IORING\_MAX\_FILES\_TABLE);- table->files = kcalloc(this\_files, sizeof(struct file \*),- GFP\_KERNEL\_ACCOUNT);- if (!table->files)- break;- nr\_files -= this\_files;- }-- if (i == nr\_tables)- return 0;-- for (i = 0; i < nr\_tables; i++) {- struct fixed\_file\_table \*table = &file\_data->table[i];- kfree(table->files);- }- return 1;-}--static void io\_ring\_file\_put(struct io\_ring\_ctx \*ctx, struct file \*file)+static void io\_rsrc\_file\_put(struct io\_ring\_ctx \*ctx, struct io\_rsrc\_put \*prsrc) {+ struct file \*file = prsrc->file; #if defined(CONFIG\_UNIX) struct sock \*sock = ctx->ring\_sock->sk; struct sk\_buff\_head list, \*head = &sock->sk\_receive\_queue;@@ -7507,117 +8087,61 @@ static void io\_ring\_file\_put(struct io\_ring\_ctx \*ctx, struct file \*file) #endif } -struct io\_file\_put {- struct list\_head list;- struct file \*file;-};--static void \_\_io\_file\_put\_work(struct fixed\_file\_ref\_node \*ref\_node)+static void \_\_io\_rsrc\_put\_work(struct io\_rsrc\_node \*ref\_node) {- struct fixed\_file\_data \*file\_data = ref\_node->file\_data;- struct io\_ring\_ctx \*ctx = file\_data->ctx;- struct io\_file\_put \*pfile, \*tmp;+ struct io\_rsrc\_data \*rsrc\_data = ref\_node->rsrc\_data;+ struct io\_ring\_ctx \*ctx = rsrc\_data->ctx;+ struct io\_rsrc\_put \*prsrc, \*tmp;++ list\_for\_each\_entry\_safe(prsrc, tmp, &ref\_node->rsrc\_list, list) {+ list\_del(&prsrc->list);++ if (prsrc->tag) {+ bool lock\_ring = ctx->flags & IORING\_SETUP\_IOPOLL; - list\_for\_each\_entry\_safe(pfile, tmp, &ref\_node->file\_list, list) {- list\_del(&pfile->list);- io\_ring\_file\_put(ctx, pfile->file);- kfree(pfile);+ io\_ring\_submit\_lock(ctx, lock\_ring);+ spin\_lock(&ctx->completion\_lock);+ io\_fill\_cqe\_aux(ctx, prsrc->tag, 0, 0);+ io\_commit\_cqring(ctx);+ spin\_unlock(&ctx->completion\_lock);+ io\_cqring\_ev\_posted(ctx);+ io\_ring\_submit\_unlock(ctx, lock\_ring);+ }++ rsrc\_data->do\_put(ctx, prsrc);+ kfree(prsrc); } - percpu\_ref\_exit(&ref\_node->refs);- kfree(ref\_node);- percpu\_ref\_put(&file\_data->refs);+ io\_rsrc\_node\_destroy(ref\_node);+ if (atomic\_dec\_and\_test(&rsrc\_data->refs))+ complete(&rsrc\_data->done); } -static void io\_file\_put\_work(struct work\_struct \*work)+static void io\_rsrc\_put\_work(struct work\_struct \*work) { struct io\_ring\_ctx \*ctx; struct llist\_node \*node; - ctx = container\_of(work, struct io\_ring\_ctx, file\_put\_work.work);- node = llist\_del\_all(&ctx->file\_put\_llist);+ ctx = container\_of(work, struct io\_ring\_ctx, rsrc\_put\_work.work);+ node = llist\_del\_all(&ctx->rsrc\_put\_llist);  while (node) {- struct fixed\_file\_ref\_node \*ref\_node;+ struct io\_rsrc\_node \*ref\_node; struct llist\_node \*next = node->next; - ref\_node = llist\_entry(node, struct fixed\_file\_ref\_node, llist);- \_\_io\_file\_put\_work(ref\_node);+ ref\_node = llist\_entry(node, struct io\_rsrc\_node, llist);+ \_\_io\_rsrc\_put\_work(ref\_node); node = next; } } -static void io\_file\_data\_ref\_zero(struct percpu\_ref \*ref)-{- struct fixed\_file\_ref\_node \*ref\_node;- struct fixed\_file\_data \*data;- struct io\_ring\_ctx \*ctx;- bool first\_add = false;- int delay = HZ;-- ref\_node = container\_of(ref, struct fixed\_file\_ref\_node, refs);- data = ref\_node->file\_data;- ctx = data->ctx;-- spin\_lock\_bh(&data->lock);- ref\_node->done = true;-- while (!list\_empty(&data->ref\_list)) {- ref\_node = list\_first\_entry(&data->ref\_list,- struct fixed\_file\_ref\_node, node);- /\* recycle ref nodes in order \*/- if (!ref\_node->done)- break;- list\_del(&ref\_node->node);- first\_add |= llist\_add(&ref\_node->llist, &ctx->file\_put\_llist);- }- spin\_unlock\_bh(&data->lock);-- if (percpu\_ref\_is\_dying(&data->refs))- delay = 0;-- if (!delay)- mod\_delayed\_work(system\_wq, &ctx->file\_put\_work, 0);- else if (first\_add)- queue\_delayed\_work(system\_wq, &ctx->file\_put\_work, delay);-}--static struct fixed\_file\_ref\_node \*alloc\_fixed\_file\_ref\_node(- struct io\_ring\_ctx \*ctx)-{- struct fixed\_file\_ref\_node \*ref\_node;-- ref\_node = kzalloc(sizeof(\*ref\_node), GFP\_KERNEL);- if (!ref\_node)- return NULL;-- if (percpu\_ref\_init(&ref\_node->refs, io\_file\_data\_ref\_zero,- 0, GFP\_KERNEL)) {- kfree(ref\_node);- return NULL;- }- INIT\_LIST\_HEAD(&ref\_node->node);- INIT\_LIST\_HEAD(&ref\_node->file\_list);- ref\_node->file\_data = ctx->file\_data;- ref\_node->done = false;- return ref\_node;-}--static void destroy\_fixed\_file\_ref\_node(struct fixed\_file\_ref\_node \*ref\_node)-{- percpu\_ref\_exit(&ref\_node->refs);- kfree(ref\_node);-}- static int io\_sqe\_files\_register(struct io\_ring\_ctx \*ctx, void \_\_user \*arg,- unsigned nr\_args)+ unsigned nr\_args, u64 \_\_user \*tags) { \_\_s32 \_\_user \*fds = (\_\_s32 \_\_user \*) arg;- unsigned nr\_tables, i; struct file \*file;- int fd, ret = -ENOMEM;- struct fixed\_file\_ref\_node \*ref\_node;- struct fixed\_file\_data \*file\_data;+ int fd, ret;+ unsigned i;  if (ctx->file\_data) return -EBUSY;@@ -7627,44 +8151,34 @@ static int io\_sqe\_files\_register(struct io\_ring\_ctx \*ctx, void \_\_user \*arg, return -EMFILE; if (nr\_args > rlimit(RLIMIT\_NOFILE)) return -EMFILE;+ ret = io\_rsrc\_node\_switch\_start(ctx);+ if (ret)+ return ret;+ ret = io\_rsrc\_data\_alloc(ctx, io\_rsrc\_file\_put, tags, nr\_args,+ &ctx->file\_data);+ if (ret)+ return ret; - file\_data = kzalloc(sizeof(\*ctx->file\_data), GFP\_KERNEL\_ACCOUNT);- if (!file\_data)- return -ENOMEM;- file\_data->ctx = ctx;- init\_completion(&file\_data->done);- INIT\_LIST\_HEAD(&file\_data->ref\_list);- spin\_lock\_init(&file\_data->lock);-- nr\_tables = DIV\_ROUND\_UP(nr\_args, IORING\_MAX\_FILES\_TABLE);- file\_data->table = kcalloc(nr\_tables, sizeof(\*file\_data->table),- GFP\_KERNEL\_ACCOUNT);- if (!file\_data->table)- goto out\_free;-- if (percpu\_ref\_init(&file\_data->refs, io\_file\_ref\_kill,- PERCPU\_REF\_ALLOW\_REINIT, GFP\_KERNEL))+ ret = -ENOMEM;+ if (!io\_alloc\_file\_tables(&ctx->file\_table, nr\_args)) goto out\_free; - if (io\_sqe\_alloc\_file\_tables(file\_data, nr\_tables, nr\_args))- goto out\_ref;- ctx->file\_data = file\_data;- for (i = 0; i < nr\_args; i++, ctx->nr\_user\_files++) {- struct fixed\_file\_table \*table;- unsigned index;- if (copy\_from\_user(&fd, &fds[i], sizeof(fd))) { ret = -EFAULT; goto out\_fput; } /\* allow sparse sets \*/- if (fd == -1)+ if (fd == -1) {+ ret = -EINVAL;+ if (unlikely(\*io\_get\_tag\_slot(ctx->file\_data, i)))+ goto out\_fput; continue;+ }  file = fget(fd); ret = -EBADF;- if (!file)+ if (unlikely(!file)) goto out\_fput;  /\*@@ -7678,24 +8192,16 @@ static int io\_sqe\_files\_register(struct io\_ring\_ctx \*ctx, void \_\_user \*arg, fput(file); goto out\_fput; }- table = &file\_data->table[i >> IORING\_FILE\_TABLE\_SHIFT];- index = i & IORING\_FILE\_TABLE\_MASK;- table->files[index] = file;+ io\_fixed\_file\_set(io\_fixed\_file\_slot(&ctx->file\_table, i), file); }  ret = io\_sqe\_files\_scm(ctx); if (ret) {- io\_sqe\_files\_unregister(ctx);+ \_\_io\_sqe\_files\_unregister(ctx); return ret; } - ref\_node = alloc\_fixed\_file\_ref\_node(ctx);- if (!ref\_node) {- io\_sqe\_files\_unregister(ctx);- return -ENOMEM;- }-- io\_sqe\_files\_set\_node(file\_data, ref\_node);+ io\_rsrc\_node\_switch(ctx, NULL); return ret; out\_fput: for (i = 0; i < ctx->nr\_user\_files; i++) {@@ -7703,14 +8209,10 @@ out\_fput: if (file) fput(file); }- for (i = 0; i < nr\_tables; i++)- kfree(file\_data->table[i].files);+ io\_free\_file\_tables(&ctx->file\_table); ctx->nr\_user\_files = 0;-out\_ref:- percpu\_ref\_exit(&file\_data->refs); out\_free:- kfree(file\_data->table);- kfree(file\_data);+ io\_rsrc\_data\_free(ctx->file\_data); ctx->file\_data = NULL; return ret; }@@ -7758,63 +8260,159 @@ static int io\_sqe\_file\_register(struct io\_ring\_ctx \*ctx, struct file \*file, #endif } -static int io\_queue\_file\_removal(struct fixed\_file\_data \*data,- struct file \*file)+static int io\_queue\_rsrc\_removal(struct io\_rsrc\_data \*data, unsigned idx,+ struct io\_rsrc\_node \*node, void \*rsrc) {- struct io\_file\_put \*pfile;- struct fixed\_file\_ref\_node \*ref\_node = data->node;+ u64 \*tag\_slot = io\_get\_tag\_slot(data, idx);+ struct io\_rsrc\_put \*prsrc; - pfile = kzalloc(sizeof(\*pfile), GFP\_KERNEL);- if (!pfile)+ prsrc = kzalloc(sizeof(\*prsrc), GFP\_KERNEL);+ if (!prsrc) return -ENOMEM; - pfile->file = file;- list\_add(&pfile->list, &ref\_node->file\_list);-+ prsrc->tag = \*tag\_slot;+ \*tag\_slot = 0;+ prsrc->rsrc = rsrc;+ list\_add(&prsrc->list, &node->rsrc\_list); return 0; } +static int io\_install\_fixed\_file(struct io\_kiocb \*req, struct file \*file,+ unsigned int issue\_flags, u32 slot\_index)+{+ struct io\_ring\_ctx \*ctx = req->ctx;+ bool force\_nonblock = issue\_flags & IO\_URING\_F\_NONBLOCK;+ bool needs\_switch = false;+ struct io\_fixed\_file \*file\_slot;+ int ret = -EBADF;++ io\_ring\_submit\_lock(ctx, !force\_nonblock);+ if (file->f\_op == &io\_uring\_fops)+ goto err;+ ret = -ENXIO;+ if (!ctx->file\_data)+ goto err;+ ret = -EINVAL;+ if (slot\_index >= ctx->nr\_user\_files)+ goto err;++ slot\_index = array\_index\_nospec(slot\_index, ctx->nr\_user\_files);+ file\_slot = io\_fixed\_file\_slot(&ctx->file\_table, slot\_index);++ if (file\_slot->file\_ptr) {+ struct file \*old\_file;++ ret = io\_rsrc\_node\_switch\_start(ctx);+ if (ret)+ goto err;++ old\_file = (struct file \*)(file\_slot->file\_ptr & FFS\_MASK);+ ret = io\_queue\_rsrc\_removal(ctx->file\_data, slot\_index,+ ctx->rsrc\_node, old\_file);+ if (ret)+ goto err;+ file\_slot->file\_ptr = 0;+ needs\_switch = true;+ }++ \*io\_get\_tag\_slot(ctx->file\_data, slot\_index) = 0;+ io\_fixed\_file\_set(file\_slot, file);+ ret = io\_sqe\_file\_register(ctx, file, slot\_index);+ if (ret) {+ file\_slot->file\_ptr = 0;+ goto err;+ }++ ret = 0;+err:+ if (needs\_switch)+ io\_rsrc\_node\_switch(ctx, ctx->file\_data);+ io\_ring\_submit\_unlock(ctx, !force\_nonblock);+ if (ret)+ fput(file);+ return ret;+}++static int io\_close\_fixed(struct io\_kiocb \*req, unsigned int issue\_flags)+{+ unsigned int offset = req->close.file\_slot - 1;+ struct io\_ring\_ctx \*ctx = req->ctx;+ struct io\_fixed\_file \*file\_slot;+ struct file \*file;+ int ret;++ io\_ring\_submit\_lock(ctx, !(issue\_flags & IO\_URING\_F\_NONBLOCK));+ ret = -ENXIO;+ if (unlikely(!ctx->file\_data))+ goto out;+ ret = -EINVAL;+ if (offset >= ctx->nr\_user\_files)+ goto out;+ ret = io\_rsrc\_node\_switch\_start(ctx);+ if (ret)+ goto out;++ offset = array\_index\_nospec(offset, ctx->nr\_user\_files);+ file\_slot = io\_fixed\_file\_slot(&ctx->file\_table, offset);+ ret = -EBADF;+ if (!file\_slot->file\_ptr)+ goto out;++ file = (struct file \*)(file\_slot->file\_ptr & FFS\_MASK);+ ret = io\_queue\_rsrc\_removal(ctx->file\_data, offset, ctx->rsrc\_node, file);+ if (ret)+ goto out;++ file\_slot->file\_ptr = 0;+ io\_rsrc\_node\_switch(ctx, ctx->file\_data);+ ret = 0;+out:+ io\_ring\_submit\_unlock(ctx, !(issue\_flags & IO\_URING\_F\_NONBLOCK));+ return ret;+}+ static int \_\_io\_sqe\_files\_update(struct io\_ring\_ctx \*ctx,- struct io\_uring\_files\_update \*up,+ struct io\_uring\_rsrc\_update2 \*up, unsigned nr\_args) {- struct fixed\_file\_data \*data = ctx->file\_data;- struct fixed\_file\_ref\_node \*ref\_node;+ u64 \_\_user \*tags = u64\_to\_user\_ptr(up->tags);+ \_\_s32 \_\_user \*fds = u64\_to\_user\_ptr(up->data);+ struct io\_rsrc\_data \*data = ctx->file\_data;+ struct io\_fixed\_file \*file\_slot; struct file \*file;- \_\_s32 \_\_user \*fds;- int fd, i, err;- \_\_u32 done;+ int fd, i, err = 0;+ unsigned int done; bool needs\_switch = false; - if (check\_add\_overflow(up->offset, nr\_args, &done))- return -EOVERFLOW;- if (done > ctx->nr\_user\_files)+ if (!ctx->file\_data)+ return -ENXIO;+ if (up->offset + nr\_args > ctx->nr\_user\_files) return -EINVAL; - ref\_node = alloc\_fixed\_file\_ref\_node(ctx);- if (!ref\_node)- return -ENOMEM;+ for (done = 0; done < nr\_args; done++) {+ u64 tag = 0; - done = 0;- fds = u64\_to\_user\_ptr(up->fds);- while (nr\_args) {- struct fixed\_file\_table \*table;- unsigned index;-- err = 0;- if (copy\_from\_user(&fd, &fds[done], sizeof(fd))) {+ if ((tags && copy\_from\_user(&tag, &tags[done], sizeof(tag))) ||+ copy\_from\_user(&fd, &fds[done], sizeof(fd))) { err = -EFAULT; break; }- i = array\_index\_nospec(up->offset, ctx->nr\_user\_files);- table = &ctx->file\_data->table[i >> IORING\_FILE\_TABLE\_SHIFT];- index = i & IORING\_FILE\_TABLE\_MASK;- if (table->files[index]) {- file = table->files[index];- err = io\_queue\_file\_removal(data, file);+ if ((fd == IORING\_REGISTER\_FILES\_SKIP || fd == -1) && tag) {+ err = -EINVAL;+ break;+ }+ if (fd == IORING\_REGISTER\_FILES\_SKIP)+ continue;++ i = array\_index\_nospec(up->offset + done, ctx->nr\_user\_files);+ file\_slot = io\_fixed\_file\_slot(&ctx->file\_table, i);++ if (file\_slot->file\_ptr) {+ file = (struct file \*)(file\_slot->file\_ptr & FFS\_MASK);+ err = io\_queue\_rsrc\_removal(data, i, ctx->rsrc\_node, file); if (err) break;- table->files[index] = NULL;+ file\_slot->file\_ptr = 0; needs\_switch = true; } if (fd != -1) {@@ -7836,106 +8434,61 @@ static int \_\_io\_sqe\_files\_update(struct io\_ring\_ctx \*ctx, err = -EBADF; break; }- table->files[index] = file;+ \*io\_get\_tag\_slot(data, i) = tag;+ io\_fixed\_file\_set(file\_slot, file); err = io\_sqe\_file\_register(ctx, file, i); if (err) {- table->files[index] = NULL;+ file\_slot->file\_ptr = 0; fput(file); break; } }- nr\_args--;- done++;- up->offset++; } - if (needs\_switch) {- percpu\_ref\_kill(&data->node->refs);- io\_sqe\_files\_set\_node(data, ref\_node);- } else- destroy\_fixed\_file\_ref\_node(ref\_node);-+ if (needs\_switch)+ io\_rsrc\_node\_switch(ctx, data); return done ? done : err; } -static int io\_sqe\_files\_update(struct io\_ring\_ctx \*ctx, void \_\_user \*arg,- unsigned nr\_args)-{- struct io\_uring\_files\_update up;-- if (!ctx->file\_data)- return -ENXIO;- if (!nr\_args)- return -EINVAL;- if (copy\_from\_user(&up, arg, sizeof(up)))- return -EFAULT;- if (up.resv)- return -EINVAL;-- return \_\_io\_sqe\_files\_update(ctx, &up, nr\_args);-}--static void io\_free\_work(struct io\_wq\_work \*work)-{- struct io\_kiocb \*req = container\_of(work, struct io\_kiocb, work);-- /\* Consider that io\_steal\_work() relies on this ref \*/- io\_put\_req(req);-}--static int io\_init\_wq\_offload(struct io\_ring\_ctx \*ctx,- struct io\_uring\_params \*p)+static struct io\_wq \*io\_init\_wq\_offload(struct io\_ring\_ctx \*ctx,+ struct task\_struct \*task) {+ struct io\_wq\_hash \*hash; struct io\_wq\_data data;- struct fd f;- struct io\_ring\_ctx \*ctx\_attach; unsigned int concurrency;- int ret = 0; - data.user = ctx->user;- data.free\_work = io\_free\_work;- data.do\_work = io\_wq\_submit\_work;-- if (!(p->flags & IORING\_SETUP\_ATTACH\_WQ)) {- /\* Do QD, or 4 \* CPUS, whatever is smallest \*/- concurrency = min(ctx->sq\_entries, 4 \* num\_online\_cpus());-- ctx->io\_wq = io\_wq\_create(concurrency, &data);- if (IS\_ERR(ctx->io\_wq)) {- ret = PTR\_ERR(ctx->io\_wq);- ctx->io\_wq = NULL;+ mutex\_lock(&ctx->uring\_lock);+ hash = ctx->hash\_map;+ if (!hash) {+ hash = kzalloc(sizeof(\*hash), GFP\_KERNEL);+ if (!hash) {+ mutex\_unlock(&ctx->uring\_lock);+ return ERR\_PTR(-ENOMEM); }- return ret;+ refcount\_set(&hash->refs, 1);+ init\_waitqueue\_head(&hash->wait);+ ctx->hash\_map = hash; }+ mutex\_unlock(&ctx->uring\_lock); - f = fdget(p->wq\_fd);- if (!f.file)- return -EBADF;-- if (f.file->f\_op != &io\_uring\_fops) {- ret = -EINVAL;- goto out\_fput;- }+ data.hash = hash;+ data.task = task;+ data.free\_work = io\_wq\_free\_work;+ data.do\_work = io\_wq\_submit\_work; - ctx\_attach = f.file->private\_data;- /\* @io\_wq is protected by holding the fd \*/- if (!io\_wq\_get(ctx\_attach->io\_wq, &data)) {- ret = -EINVAL;- goto out\_fput;- }+ /\* Do QD, or 4 \* CPUS, whatever is smallest \*/+ concurrency = min(ctx->sq\_entries, 4 \* num\_online\_cpus()); - ctx->io\_wq = ctx\_attach->io\_wq;-out\_fput:- fdput(f);- return ret;+ return io\_wq\_create(concurrency, &data); } -static int io\_uring\_alloc\_task\_context(struct task\_struct \*task)+static int io\_uring\_alloc\_task\_context(struct task\_struct \*task,+ struct io\_ring\_ctx \*ctx) { struct io\_uring\_task \*tctx; int ret; - tctx = kmalloc(sizeof(\*tctx), GFP\_KERNEL);+ tctx = kzalloc(sizeof(\*tctx), GFP\_KERNEL); if (unlikely(!tctx)) return -ENOMEM; @@ -7945,14 +8498,22 @@ static int io\_uring\_alloc\_task\_context(struct task\_struct \*task) return ret; } + tctx->io\_wq = io\_init\_wq\_offload(ctx, task);+ if (IS\_ERR(tctx->io\_wq)) {+ ret = PTR\_ERR(tctx->io\_wq);+ percpu\_counter\_destroy(&tctx->inflight);+ kfree(tctx);+ return ret;+ }+ xa\_init(&tctx->xa); init\_waitqueue\_head(&tctx->wait);- tctx->last = NULL; atomic\_set(&tctx->in\_idle, 0);- tctx->sqpoll = false;- io\_init\_identity(&tctx->\_\_identity);- tctx->identity = &tctx->\_\_identity;+ atomic\_set(&tctx->inflight\_tracked, 0); task->io\_uring = tctx;+ spin\_lock\_init(&tctx->task\_lock);+ INIT\_WQ\_LIST(&tctx->task\_list);+ init\_task\_work(&tctx->task\_work, tctx\_task\_work); return 0; } @@ -7961,9 +8522,9 @@ void \_\_io\_uring\_free(struct task\_struct \*tsk) struct io\_uring\_task \*tctx = tsk->io\_uring;  WARN\_ON\_ONCE(!xa\_empty(&tctx->xa));- WARN\_ON\_ONCE(refcount\_read(&tctx->identity->count) != 1);- if (tctx->identity != &tctx->\_\_identity)- kfree(tctx->identity);+ WARN\_ON\_ONCE(tctx->io\_wq);+ WARN\_ON\_ONCE(tctx->cached\_refs);+ percpu\_counter\_destroy(&tctx->inflight); kfree(tctx); tsk->io\_uring = NULL;@@ -7974,54 +8535,71 @@ static int io\_sq\_offload\_create(struct io\_ring\_ctx \*ctx, { int ret; + /\* Retain compatibility with failing for an invalid attach attempt \*/+ if ((ctx->flags & (IORING\_SETUP\_ATTACH\_WQ | IORING\_SETUP\_SQPOLL)) ==+ IORING\_SETUP\_ATTACH\_WQ) {+ struct fd f;++ f = fdget(p->wq\_fd);+ if (!f.file)+ return -ENXIO;+ if (f.file->f\_op != &io\_uring\_fops) {+ fdput(f);+ return -EINVAL;+ }+ fdput(f);+ } if (ctx->flags & IORING\_SETUP\_SQPOLL) {+ struct task\_struct \*tsk; struct io\_sq\_data \*sqd;+ bool attached; - ret = -EPERM;- if (!capable(CAP\_SYS\_ADMIN))- goto err;-- sqd = io\_get\_sq\_data(p);+ sqd = io\_get\_sq\_data(p, &attached); if (IS\_ERR(sqd)) { ret = PTR\_ERR(sqd); goto err; } + ctx->sq\_creds = get\_current\_cred(); ctx->sq\_data = sqd;- io\_sq\_thread\_park(sqd);- mutex\_lock(&sqd->ctx\_lock);- list\_add(&ctx->sqd\_list, &sqd->ctx\_new\_list);- mutex\_unlock(&sqd->ctx\_lock);- io\_sq\_thread\_unpark(sqd);- ctx->sq\_thread\_idle = msecs\_to\_jiffies(p->sq\_thread\_idle); if (!ctx->sq\_thread\_idle) ctx->sq\_thread\_idle = HZ; - if (sqd->thread)- goto done;+ io\_sq\_thread\_park(sqd);+ list\_add(&ctx->sqd\_list, &sqd->ctx\_list);+ io\_sqd\_update\_thread\_idle(sqd);+ /\* don't attach to a dying SQPOLL thread, would be racy \*/+ ret = (attached && !sqd->thread) ? -ENXIO : 0;+ io\_sq\_thread\_unpark(sqd);++ if (ret < 0)+ goto err;+ if (attached)+ return 0;  if (p->flags & IORING\_SETUP\_SQ\_AFF) { int cpu = p->sq\_thread\_cpu;  ret = -EINVAL;- if (cpu >= nr\_cpu\_ids)- goto err;- if (!cpu\_online(cpu))- goto err;-- sqd->thread = kthread\_create\_on\_cpu(io\_sq\_thread, sqd,- cpu, "io\_uring-sq");+ if (cpu >= nr\_cpu\_ids || !cpu\_online(cpu))+ goto err\_sqpoll;+ sqd->sq\_cpu = cpu; } else {- sqd->thread = kthread\_create(io\_sq\_thread, sqd,- "io\_uring-sq");+ sqd->sq\_cpu = -1; }- if (IS\_ERR(sqd->thread)) {- ret = PTR\_ERR(sqd->thread);- sqd->thread = NULL;- goto err;++ sqd->task\_pid = current->pid;+ sqd->task\_tgid = current->tgid;+ tsk = create\_io\_thread(io\_sq\_thread, sqd, NUMA\_NO\_NODE);+ if (IS\_ERR(tsk)) {+ ret = PTR\_ERR(tsk);+ goto err\_sqpoll; }- ret = io\_uring\_alloc\_task\_context(sqd->thread);++ sqd->thread = tsk;+ ret = io\_uring\_alloc\_task\_context(tsk, ctx);+ wake\_up\_new\_task(tsk); if (ret) goto err; } else if (p->flags & IORING\_SETUP\_SQ\_AFF) {@@ -8030,26 +8608,14 @@ static int io\_sq\_offload\_create(struct io\_ring\_ctx \*ctx, goto err; } -done:- ret = io\_init\_wq\_offload(ctx, p);- if (ret)- goto err;- return 0;+err\_sqpoll:+ complete(&ctx->sq\_data->exited); err:- io\_finish\_async(ctx);+ io\_sq\_thread\_finish(ctx); return ret; } -static void io\_sq\_offload\_start(struct io\_ring\_ctx \*ctx)-{- struct io\_sq\_data \*sqd = ctx->sq\_data;-- ctx->flags &= ~IORING\_SETUP\_R\_DISABLED;- if ((ctx->flags & IORING\_SETUP\_SQPOLL) && sqd && sqd->thread)- wake\_up\_process(sqd->thread);-}- static inline void \_\_io\_unaccount\_mem(struct user\_struct \*user, unsigned long nr\_pages) {@@ -8075,37 +8641,27 @@ static inline int \_\_io\_account\_mem(struct user\_struct \*user, return 0; } -static void io\_unaccount\_mem(struct io\_ring\_ctx \*ctx, unsigned long nr\_pages,- enum io\_mem\_account acct)+static void io\_unaccount\_mem(struct io\_ring\_ctx \*ctx, unsigned long nr\_pages) {- if (ctx->limit\_mem)+ if (ctx->user) \_\_io\_unaccount\_mem(ctx->user, nr\_pages); - if (ctx->mm\_account) {- if (acct == ACCT\_LOCKED)- ctx->mm\_account->locked\_vm -= nr\_pages;- else if (acct == ACCT\_PINNED)- atomic64\_sub(nr\_pages, &ctx->mm\_account->pinned\_vm);- }+ if (ctx->mm\_account)+ atomic64\_sub(nr\_pages, &ctx->mm\_account->pinned\_vm); } -static int io\_account\_mem(struct io\_ring\_ctx \*ctx, unsigned long nr\_pages,- enum io\_mem\_account acct)+static int io\_account\_mem(struct io\_ring\_ctx \*ctx, unsigned long nr\_pages) { int ret; - if (ctx->limit\_mem) {+ if (ctx->user) { ret = \_\_io\_account\_mem(ctx->user, nr\_pages); if (ret) return ret; } - if (ctx->mm\_account) {- if (acct == ACCT\_LOCKED)- ctx->mm\_account->locked\_vm += nr\_pages;- else if (acct == ACCT\_PINNED)- atomic64\_add(nr\_pages, &ctx->mm\_account->pinned\_vm);- }+ if (ctx->mm\_account)+ atomic64\_add(nr\_pages, &ctx->mm\_account->pinned\_vm);  return 0; }@@ -8124,10 +8680,9 @@ static void io\_mem\_free(void \*ptr)  static void \*io\_mem\_alloc(size\_t size) {- gfp\_t gfp\_flags = GFP\_KERNEL | \_\_GFP\_ZERO | \_\_GFP\_NOWARN | \_\_GFP\_COMP |- \_\_GFP\_NORETRY;+ gfp\_t gfp = GFP\_KERNEL\_ACCOUNT | \_\_GFP\_ZERO | \_\_GFP\_NOWARN | \_\_GFP\_COMP; - return (void \*) \_\_get\_free\_pages(gfp\_flags, get\_order(size));+ return (void \*) \_\_get\_free\_pages(gfp, get\_order(size)); }  static unsigned long rings\_size(unsigned sq\_entries, unsigned cq\_entries,@@ -8159,41 +8714,58 @@ static unsigned long rings\_size(unsigned sq\_entries, unsigned cq\_entries, return off; } -static unsigned long ring\_pages(unsigned sq\_entries, unsigned cq\_entries)+static void io\_buffer\_unmap(struct io\_ring\_ctx \*ctx, struct io\_mapped\_ubuf \*\*slot) {- size\_t pages;+ struct io\_mapped\_ubuf \*imu = \*slot;+ unsigned int i; - pages = (size\_t)1 << get\_order(- rings\_size(sq\_entries, cq\_entries, NULL));- pages += (size\_t)1 << get\_order(- array\_size(sizeof(struct io\_uring\_sqe), sq\_entries));-- return pages;+ if (imu != ctx->dummy\_ubuf) {+ for (i = 0; i < imu->nr\_bvecs; i++)+ unpin\_user\_page(imu->bvec[i].bv\_page);+ if (imu->acct\_pages)+ io\_unaccount\_mem(ctx, imu->acct\_pages);+ kvfree(imu);+ }+ \*slot = NULL; } -static int io\_sqe\_buffer\_unregister(struct io\_ring\_ctx \*ctx)+static void io\_rsrc\_buf\_put(struct io\_ring\_ctx \*ctx, struct io\_rsrc\_put \*prsrc) {- int i, j;+ io\_buffer\_unmap(ctx, &prsrc->buf);+ prsrc->buf = NULL;+} - if (!ctx->user\_bufs)- return -ENXIO;+static void \_\_io\_sqe\_buffers\_unregister(struct io\_ring\_ctx \*ctx)+{+ unsigned int i; - for (i = 0; i < ctx->nr\_user\_bufs; i++) {- struct io\_mapped\_ubuf \*imu = &ctx->user\_bufs[i];+ for (i = 0; i < ctx->nr\_user\_bufs; i++)+ io\_buffer\_unmap(ctx, &ctx->user\_bufs[i]);+ kfree(ctx->user\_bufs);+ io\_rsrc\_data\_free(ctx->buf\_data);+ ctx->user\_bufs = NULL;+ ctx->buf\_data = NULL;+ ctx->nr\_user\_bufs = 0;+} - for (j = 0; j < imu->nr\_bvecs; j++)- unpin\_user\_page(imu->bvec[j].bv\_page);+static int io\_sqe\_buffers\_unregister(struct io\_ring\_ctx \*ctx)+{+ unsigned nr = ctx->nr\_user\_bufs;+ int ret; - if (imu->acct\_pages)- io\_unaccount\_mem(ctx, imu->acct\_pages, ACCT\_PINNED);- kvfree(imu->bvec);- imu->nr\_bvecs = 0;- }+ if (!ctx->buf\_data)+ return -ENXIO; - kfree(ctx->user\_bufs);- ctx->user\_bufs = NULL;+ /\*+ \* Quiesce may unlock ->uring\_lock, and while it's not held+ \* prevent new requests using the table.+ \*/ ctx->nr\_user\_bufs = 0;- return 0;+ ret = io\_rsrc\_ref\_quiesce(ctx->buf\_data, ctx);+ ctx->nr\_user\_bufs = nr;+ if (!ret)+ \_\_io\_sqe\_buffers\_unregister(ctx);+ return ret; }  static int io\_copy\_iov(struct io\_ring\_ctx \*ctx, struct iovec \*dst,@@ -8245,7 +8817,7 @@ static bool headpage\_already\_acct(struct io\_ring\_ctx \*ctx, struct page \*\*pages,  /\* check previously registered pages \*/ for (i = 0; i < ctx->nr\_user\_bufs; i++) {- struct io\_mapped\_ubuf \*imu = &ctx->user\_bufs[i];+ struct io\_mapped\_ubuf \*imu = ctx->user\_bufs[i];  for (j = 0; j < imu->nr\_bvecs; j++) { if (!PageCompound(imu->bvec[j].bv\_page))@@ -8264,6 +8836,7 @@ static int io\_buffer\_account\_pin(struct io\_ring\_ctx \*ctx, struct page \*\*pages, { int i, ret; + imu->acct\_pages = 0; for (i = 0; i < nr\_pages; i++) { if (!PageCompound(pages[i])) { imu->acct\_pages++;@@ -8283,147 +8856,252 @@ static int io\_buffer\_account\_pin(struct io\_ring\_ctx \*ctx, struct page \*\*pages, if (!imu->acct\_pages) return 0; - ret = io\_account\_mem(ctx, imu->acct\_pages, ACCT\_PINNED);+ ret = io\_account\_mem(ctx, imu->acct\_pages); if (ret) imu->acct\_pages = 0; return ret; } -static int io\_sqe\_buffer\_register(struct io\_ring\_ctx \*ctx, void \_\_user \*arg,- unsigned nr\_args)+static int io\_sqe\_buffer\_register(struct io\_ring\_ctx \*ctx, struct iovec \*iov,+ struct io\_mapped\_ubuf \*\*pimu,+ struct page \*\*last\_hpage) {+ struct io\_mapped\_ubuf \*imu = NULL; struct vm\_area\_struct \*\*vmas = NULL; struct page \*\*pages = NULL;+ unsigned long off, start, end, ubuf;+ size\_t size;+ int ret, pret, nr\_pages, i;++ if (!iov->iov\_base) {+ \*pimu = ctx->dummy\_ubuf;+ return 0;+ }++ ubuf = (unsigned long) iov->iov\_base;+ end = (ubuf + iov->iov\_len + PAGE\_SIZE - 1) >> PAGE\_SHIFT;+ start = ubuf >> PAGE\_SHIFT;+ nr\_pages = end - start;++ \*pimu = NULL;+ ret = -ENOMEM;++ pages = kvmalloc\_array(nr\_pages, sizeof(struct page \*), GFP\_KERNEL);+ if (!pages)+ goto done;++ vmas = kvmalloc\_array(nr\_pages, sizeof(struct vm\_area\_struct \*),+ GFP\_KERNEL);+ if (!vmas)+ goto done;++ imu = kvmalloc(struct\_size(imu, bvec, nr\_pages), GFP\_KERNEL);+ if (!imu)+ goto done;++ ret = 0;+ mmap\_read\_lock(current->mm);+ pret = pin\_user\_pages(ubuf, nr\_pages, FOLL\_WRITE | FOLL\_LONGTERM,+ pages, vmas);+ if (pret == nr\_pages) {+ /\* don't support file backed memory \*/+ for (i = 0; i < nr\_pages; i++) {+ struct vm\_area\_struct \*vma = vmas[i];++ if (vma\_is\_shmem(vma))+ continue;+ if (vma->vm\_file &&+ !is\_file\_hugepages(vma->vm\_file)) {+ ret = -EOPNOTSUPP;+ break;+ }+ }+ } else {+ ret = pret < 0 ? pret : -EFAULT;+ }+ mmap\_read\_unlock(current->mm);+ if (ret) {+ /\*+ \* if we did partial map, or found file backed vmas,+ \* release any pages we did get+ \*/+ if (pret > 0)+ unpin\_user\_pages(pages, pret);+ goto done;+ }++ ret = io\_buffer\_account\_pin(ctx, pages, pret, imu, last\_hpage);+ if (ret) {+ unpin\_user\_pages(pages, pret);+ goto done;+ }++ off = ubuf & ~PAGE\_MASK;+ size = iov->iov\_len;+ for (i = 0; i < nr\_pages; i++) {+ size\_t vec\_len;++ vec\_len = min\_t(size\_t, size, PAGE\_SIZE - off);+ imu->bvec[i].bv\_page = pages[i];+ imu->bvec[i].bv\_len = vec\_len;+ imu->bvec[i].bv\_offset = off;+ off = 0;+ size -= vec\_len;+ }+ /\* store original address for later verification \*/+ imu->ubuf = ubuf;+ imu->ubuf\_end = ubuf + iov->iov\_len;+ imu->nr\_bvecs = nr\_pages;+ \*pimu = imu;+ ret = 0;+done:+ if (ret)+ kvfree(imu);+ kvfree(pages);+ kvfree(vmas);+ return ret;+}++static int io\_buffers\_map\_alloc(struct io\_ring\_ctx \*ctx, unsigned int nr\_args)+{+ ctx->user\_bufs = kcalloc(nr\_args, sizeof(\*ctx->user\_bufs), GFP\_KERNEL);+ return ctx->user\_bufs ? 0 : -ENOMEM;+}++static int io\_buffer\_validate(struct iovec \*iov)+{+ unsigned long tmp, acct\_len = iov->iov\_len + (PAGE\_SIZE - 1);++ /\*+ \* Don't impose further limits on the size and buffer+ \* constraints here, we'll -EINVAL later when IO is+ \* submitted if they are wrong.+ \*/+ if (!iov->iov\_base)+ return iov->iov\_len ? -EFAULT : 0;+ if (!iov->iov\_len)+ return -EFAULT;++ /\* arbitrary limit, but we need something \*/+ if (iov->iov\_len > SZ\_1G)+ return -EFAULT;++ if (check\_add\_overflow((unsigned long)iov->iov\_base, acct\_len, &tmp))+ return -EOVERFLOW;++ return 0;+}++static int io\_sqe\_buffers\_register(struct io\_ring\_ctx \*ctx, void \_\_user \*arg,+ unsigned int nr\_args, u64 \_\_user \*tags)+{ struct page \*last\_hpage = NULL;- int i, j, got\_pages = 0;- int ret = -EINVAL;+ struct io\_rsrc\_data \*data;+ int i, ret;+ struct iovec iov;  if (ctx->user\_bufs) return -EBUSY;- if (!nr\_args || nr\_args > UIO\_MAXIOV)+ if (!nr\_args || nr\_args > IORING\_MAX\_REG\_BUFFERS) return -EINVAL;+ ret = io\_rsrc\_node\_switch\_start(ctx);+ if (ret)+ return ret;+ ret = io\_rsrc\_data\_alloc(ctx, io\_rsrc\_buf\_put, tags, nr\_args, &data);+ if (ret)+ return ret;+ ret = io\_buffers\_map\_alloc(ctx, nr\_args);+ if (ret) {+ io\_rsrc\_data\_free(data);+ return ret;+ } - ctx->user\_bufs = kcalloc(nr\_args, sizeof(struct io\_mapped\_ubuf),- GFP\_KERNEL);- if (!ctx->user\_bufs)- return -ENOMEM;-- for (i = 0; i < nr\_args; i++) {- struct io\_mapped\_ubuf \*imu = &ctx->user\_bufs[i];- unsigned long off, start, end, ubuf;- int pret, nr\_pages;- struct iovec iov;- size\_t size;-+ for (i = 0; i < nr\_args; i++, ctx->nr\_user\_bufs++) { ret = io\_copy\_iov(ctx, &iov, arg, i); if (ret)- goto err;+ break;+ ret = io\_buffer\_validate(&iov);+ if (ret)+ break;+ if (!iov.iov\_base && \*io\_get\_tag\_slot(data, i)) {+ ret = -EINVAL;+ break;+ } - /\*- \* Don't impose further limits on the size and buffer- \* constraints here, we'll -EINVAL later when IO is- \* submitted if they are wrong.- \*/- ret = -EFAULT;- if (!iov.iov\_base || !iov.iov\_len)- goto err;+ ret = io\_sqe\_buffer\_register(ctx, &iov, &ctx->user\_bufs[i],+ &last\_hpage);+ if (ret)+ break;+ } - /\* arbitrary limit, but we need something \*/- if (iov.iov\_len > SZ\_1G)- goto err;+ WARN\_ON\_ONCE(ctx->buf\_data); - ubuf = (unsigned long) iov.iov\_base;- end = (ubuf + iov.iov\_len + PAGE\_SIZE - 1) >> PAGE\_SHIFT;- start = ubuf >> PAGE\_SHIFT;- nr\_pages = end - start;+ ctx->buf\_data = data;+ if (ret)+ \_\_io\_sqe\_buffers\_unregister(ctx);+ else+ io\_rsrc\_node\_switch(ctx, NULL);+ return ret;+} - ret = 0;- if (!pages || nr\_pages > got\_pages) {- kvfree(vmas);- kvfree(pages);- pages = kvmalloc\_array(nr\_pages, sizeof(struct page \*),- GFP\_KERNEL);- vmas = kvmalloc\_array(nr\_pages,- sizeof(struct vm\_area\_struct \*),- GFP\_KERNEL);- if (!pages || !vmas) {- ret = -ENOMEM;- goto err;- }- got\_pages = nr\_pages;- }+static int \_\_io\_sqe\_buffers\_update(struct io\_ring\_ctx \*ctx,+ struct io\_uring\_rsrc\_update2 \*up,+ unsigned int nr\_args)+{+ u64 \_\_user \*tags = u64\_to\_user\_ptr(up->tags);+ struct iovec iov, \_\_user \*iovs = u64\_to\_user\_ptr(up->data);+ struct page \*last\_hpage = NULL;+ bool needs\_switch = false;+ \_\_u32 done;+ int i, err; - imu->bvec = kvmalloc\_array(nr\_pages, sizeof(struct bio\_vec),- GFP\_KERNEL);- ret = -ENOMEM;- if (!imu->bvec)- goto err;+ if (!ctx->buf\_data)+ return -ENXIO;+ if (up->offset + nr\_args > ctx->nr\_user\_bufs)+ return -EINVAL; - ret = 0;- mmap\_read\_lock(current->mm);- pret = pin\_user\_pages(ubuf, nr\_pages,- FOLL\_WRITE | FOLL\_LONGTERM,- pages, vmas);- if (pret == nr\_pages) {- /\* don't support file backed memory \*/- for (j = 0; j < nr\_pages; j++) {- struct vm\_area\_struct \*vma = vmas[j];-- if (vma->vm\_file &&- !is\_file\_hugepages(vma->vm\_file)) {- ret = -EOPNOTSUPP;- break;- }- }- } else {- ret = pret < 0 ? pret : -EFAULT;- }- mmap\_read\_unlock(current->mm);- if (ret) {- /\*- \* if we did partial map, or found file backed vmas,- \* release any pages we did get- \*/- if (pret > 0)- unpin\_user\_pages(pages, pret);- kvfree(imu->bvec);- goto err;- }+ for (done = 0; done < nr\_args; done++) {+ struct io\_mapped\_ubuf \*imu;+ int offset = up->offset + done;+ u64 tag = 0; - ret = io\_buffer\_account\_pin(ctx, pages, pret, imu, &last\_hpage);- if (ret) {- unpin\_user\_pages(pages, pret);- kvfree(imu->bvec);- goto err;+ err = io\_copy\_iov(ctx, &iov, iovs, done);+ if (err)+ break;+ if (tags && copy\_from\_user(&tag, &tags[done], sizeof(tag))) {+ err = -EFAULT;+ break; }+ err = io\_buffer\_validate(&iov);+ if (err)+ break;+ if (!iov.iov\_base && tag) {+ err = -EINVAL;+ break;+ }+ err = io\_sqe\_buffer\_register(ctx, &iov, &imu, &last\_hpage);+ if (err)+ break; - off = ubuf & ~PAGE\_MASK;- size = iov.iov\_len;- for (j = 0; j < nr\_pages; j++) {- size\_t vec\_len;-- vec\_len = min\_t(size\_t, size, PAGE\_SIZE - off);- imu->bvec[j].bv\_page = pages[j];- imu->bvec[j].bv\_len = vec\_len;- imu->bvec[j].bv\_offset = off;- off = 0;- size -= vec\_len;+ i = array\_index\_nospec(offset, ctx->nr\_user\_bufs);+ if (ctx->user\_bufs[i] != ctx->dummy\_ubuf) {+ err = io\_queue\_rsrc\_removal(ctx->buf\_data, i,+ ctx->rsrc\_node, ctx->user\_bufs[i]);+ if (unlikely(err)) {+ io\_buffer\_unmap(ctx, &imu);+ break;+ }+ ctx->user\_bufs[i] = NULL;+ needs\_switch = true; }- /\* store original address for later verification \*/- imu->ubuf = ubuf;- imu->len = iov.iov\_len;- imu->nr\_bvecs = nr\_pages; - ctx->nr\_user\_bufs++;+ ctx->user\_bufs[i] = imu;+ \*io\_get\_tag\_slot(ctx->buf\_data, offset) = tag; }- kvfree(pages);- kvfree(vmas);- return 0;-err:- kvfree(pages);- kvfree(vmas);- io\_sqe\_buffer\_unregister(ctx);- return ret;++ if (needs\_switch)+ io\_rsrc\_node\_switch(ctx, ctx->buf\_data);+ return done ? done : err; }  static int io\_eventfd\_register(struct io\_ring\_ctx \*ctx, void \_\_user \*arg)@@ -8440,6 +9118,7 @@ static int io\_eventfd\_register(struct io\_ring\_ctx \*ctx, void \_\_user \*arg) ctx->cq\_ev\_fd = eventfd\_ctx\_fdget(fd); if (IS\_ERR(ctx->cq\_ev\_fd)) { int ret = PTR\_ERR(ctx->cq\_ev\_fd);+ ctx->cq\_ev\_fd = NULL; return ret; }@@ -8467,26 +9146,68 @@ static void io\_destroy\_buffers(struct io\_ring\_ctx \*ctx) \_\_io\_remove\_buffers(ctx, buf, index, -1U); } -static void io\_ring\_ctx\_free(struct io\_ring\_ctx \*ctx)+static void io\_req\_cache\_free(struct list\_head \*list) {- io\_finish\_async(ctx);- io\_sqe\_buffer\_unregister(ctx);+ struct io\_kiocb \*req, \*nxt; - if (ctx->sqo\_task) {- put\_task\_struct(ctx->sqo\_task);- ctx->sqo\_task = NULL;+ list\_for\_each\_entry\_safe(req, nxt, list, inflight\_entry) {+ list\_del(&req->inflight\_entry);+ kmem\_cache\_free(req\_cachep, req); }+} -#ifdef CONFIG\_BLK\_CGROUP- if (ctx->sqo\_blkcg\_css)- css\_put(ctx->sqo\_blkcg\_css);-#endif+static void io\_req\_caches\_free(struct io\_ring\_ctx \*ctx)+{+ struct io\_submit\_state \*state = &ctx->submit\_state;++ mutex\_lock(&ctx->uring\_lock);++ if (state->free\_reqs) {+ kmem\_cache\_free\_bulk(req\_cachep, state->free\_reqs, state->reqs);+ state->free\_reqs = 0;+ }++ io\_flush\_cached\_locked\_reqs(ctx, state);+ io\_req\_cache\_free(&state->free\_list);+ mutex\_unlock(&ctx->uring\_lock);+}++static void io\_wait\_rsrc\_data(struct io\_rsrc\_data \*data)+{+ if (data && !atomic\_dec\_and\_test(&data->refs))+ wait\_for\_completion(&data->done);+}++static void io\_ring\_ctx\_free(struct io\_ring\_ctx \*ctx)+{+ io\_sq\_thread\_finish(ctx);++ /\* \_\_io\_rsrc\_put\_work() may need uring\_lock to progress, wait w/o it \*/+ io\_wait\_rsrc\_data(ctx->buf\_data);+ io\_wait\_rsrc\_data(ctx->file\_data);  mutex\_lock(&ctx->uring\_lock);- io\_sqe\_files\_unregister(ctx);+ if (ctx->buf\_data)+ \_\_io\_sqe\_buffers\_unregister(ctx);+ if (ctx->file\_data)+ \_\_io\_sqe\_files\_unregister(ctx);+ if (ctx->rings)+ \_\_io\_cqring\_overflow\_flush(ctx, true); mutex\_unlock(&ctx->uring\_lock); io\_eventfd\_unregister(ctx); io\_destroy\_buffers(ctx);+ if (ctx->sq\_creds)+ put\_cred(ctx->sq\_creds);++ /\* there are no registered resources left, nobody uses it \*/+ if (ctx->rsrc\_node)+ io\_rsrc\_node\_destroy(ctx->rsrc\_node);+ if (ctx->rsrc\_backup\_node)+ io\_rsrc\_node\_destroy(ctx->rsrc\_backup\_node);+ flush\_delayed\_work(&ctx->rsrc\_put\_work);++ WARN\_ON\_ONCE(!list\_empty(&ctx->rsrc\_ref\_list));+ WARN\_ON\_ONCE(!llist\_empty(&ctx->rsrc\_put\_llist));  #if defined(CONFIG\_UNIX) if (ctx->ring\_sock) {@@ -8494,6 +9215,7 @@ static void io\_ring\_ctx\_free(struct io\_ring\_ctx \*ctx) sock\_release(ctx->ring\_sock); } #endif+ WARN\_ON\_ONCE(!list\_empty(&ctx->ltimeout\_list));  if (ctx->mm\_account) { mmdrop(ctx->mm\_account);@@ -8505,9 +9227,11 @@ static void io\_ring\_ctx\_free(struct io\_ring\_ctx \*ctx)  percpu\_ref\_exit(&ctx->refs); free\_uid(ctx->user);- put\_cred(ctx->creds);+ io\_req\_caches\_free(ctx);+ if (ctx->hash\_map)+ io\_wq\_put\_hash(ctx->hash\_map); kfree(ctx->cancel\_hash);- kmem\_cache\_free(req\_cachep, ctx->fallback\_req);+ kfree(ctx->dummy\_ubuf); kfree(ctx); } @@ -8516,7 +9240,7 @@ static \_\_poll\_t io\_uring\_poll(struct file \*file, poll\_table \*wait) struct io\_ring\_ctx \*ctx = file->private\_data; \_\_poll\_t mask = 0; - poll\_wait(file, &ctx->cq\_wait, wait);+ poll\_wait(file, &ctx->poll\_wait, wait); /\* \* synchronizes with barrier from wq\_has\_sleeper call in \* io\_commit\_cqring@@ -8538,38 +9262,63 @@ static \_\_poll\_t io\_uring\_poll(struct file \*file, poll\_table \*wait) \* Users may get EPOLLIN meanwhile seeing nothing in cqring, this \* pushs them to do the flush. \*/- if (io\_cqring\_events(ctx) || test\_bit(0, &ctx->cq\_check\_overflow))+ if (io\_cqring\_events(ctx) || test\_bit(0, &ctx->check\_cq\_overflow)) mask |= EPOLLIN | EPOLLRDNORM;  return mask; } -static int io\_uring\_fasync(int fd, struct file \*file, int on)-{- struct io\_ring\_ctx \*ctx = file->private\_data;-- return fasync\_helper(fd, file, on, &ctx->cq\_fasync);-}- static int io\_unregister\_personality(struct io\_ring\_ctx \*ctx, unsigned id) {- struct io\_identity \*iod;+ const struct cred \*creds; - iod = xa\_erase(&ctx->personalities, id);- if (iod) {- put\_cred(iod->creds);- if (refcount\_dec\_and\_test(&iod->count))- kfree(iod);+ creds = xa\_erase(&ctx->personalities, id);+ if (creds) {+ put\_cred(creds); return 0; }  return -EINVAL; } +struct io\_tctx\_exit {+ struct callback\_head task\_work;+ struct completion completion;+ struct io\_ring\_ctx \*ctx;+};++static void io\_tctx\_exit\_cb(struct callback\_head \*cb)+{+ struct io\_uring\_task \*tctx = current->io\_uring;+ struct io\_tctx\_exit \*work;++ work = container\_of(cb, struct io\_tctx\_exit, task\_work);+ /\*+ \* When @in\_idle, we're in cancellation and it's racy to remove the+ \* node. It'll be removed by the end of cancellation, just ignore it.+ \* tctx can be NULL if the queueing of this task\_work raced with+ \* work cancelation off the exec path.+ \*/+ if (tctx && !atomic\_read(&tctx->in\_idle))+ io\_uring\_del\_tctx\_node((unsigned long)work->ctx);+ complete(&work->completion);+}++static bool io\_cancel\_ctx\_cb(struct io\_wq\_work \*work, void \*data)+{+ struct io\_kiocb \*req = container\_of(work, struct io\_kiocb, work);++ return req->ctx == data;+}+ static void io\_ring\_exit\_work(struct work\_struct \*work) {- struct io\_ring\_ctx \*ctx = container\_of(work, struct io\_ring\_ctx,- exit\_work);+ struct io\_ring\_ctx \*ctx = container\_of(work, struct io\_ring\_ctx, exit\_work);+ unsigned long timeout = jiffies + HZ \* 60 \* 5;+ unsigned long interval = HZ / 20;+ struct io\_tctx\_exit exit;+ struct io\_tctx\_node \*node;+ int ret;  /\* \* If we're doing polled IO and end up having requests being@@ -8578,53 +9327,100 @@ static void io\_ring\_exit\_work(struct work\_struct \*work) \* as nobody else will be looking for them. \*/ do {- io\_iopoll\_try\_reap\_events(ctx);- } while (!wait\_for\_completion\_timeout(&ctx->ref\_comp, HZ/20));+ io\_uring\_try\_cancel\_requests(ctx, NULL, true);+ if (ctx->sq\_data) {+ struct io\_sq\_data \*sqd = ctx->sq\_data;+ struct task\_struct \*tsk;++ io\_sq\_thread\_park(sqd);+ tsk = sqd->thread;+ if (tsk && tsk->io\_uring && tsk->io\_uring->io\_wq)+ io\_wq\_cancel\_cb(tsk->io\_uring->io\_wq,+ io\_cancel\_ctx\_cb, ctx, true);+ io\_sq\_thread\_unpark(sqd);+ }++ if (WARN\_ON\_ONCE(time\_after(jiffies, timeout))) {+ /\* there is little hope left, don't run it too often \*/+ interval = HZ \* 60;+ }+ } while (!wait\_for\_completion\_timeout(&ctx->ref\_comp, interval));++ init\_completion(&exit.completion);+ init\_task\_work(&exit.task\_work, io\_tctx\_exit\_cb);+ exit.ctx = ctx;+ /\*+ \* Some may use context even when all refs and requests have been put,+ \* and they are free to do so while still holding uring\_lock or+ \* completion\_lock, see io\_req\_task\_submit(). Apart from other work,+ \* this lock/unlock section also waits them to finish.+ \*/+ mutex\_lock(&ctx->uring\_lock);+ while (!list\_empty(&ctx->tctx\_list)) {+ WARN\_ON\_ONCE(time\_after(jiffies, timeout));++ node = list\_first\_entry(&ctx->tctx\_list, struct io\_tctx\_node,+ ctx\_node);+ /\* don't spin on a single task if cancellation failed \*/+ list\_rotate\_left(&ctx->tctx\_list);+ ret = task\_work\_add(node->task, &exit.task\_work, TWA\_SIGNAL);+ if (WARN\_ON\_ONCE(ret))+ continue;+ wake\_up\_process(node->task);++ mutex\_unlock(&ctx->uring\_lock);+ wait\_for\_completion(&exit.completion);+ mutex\_lock(&ctx->uring\_lock);+ }+ mutex\_unlock(&ctx->uring\_lock);+ spin\_lock(&ctx->completion\_lock);+ spin\_unlock(&ctx->completion\_lock);+ io\_ring\_ctx\_free(ctx); } -static bool io\_cancel\_ctx\_cb(struct io\_wq\_work \*work, void \*data)+/\* Returns true if we found and killed one or more timeouts \*/+static bool io\_kill\_timeouts(struct io\_ring\_ctx \*ctx, struct task\_struct \*tsk,+ bool cancel\_all) {- struct io\_kiocb \*req = container\_of(work, struct io\_kiocb, work);+ struct io\_kiocb \*req, \*tmp;+ int canceled = 0; - return req->ctx == data;+ spin\_lock(&ctx->completion\_lock);+ spin\_lock\_irq(&ctx->timeout\_lock);+ list\_for\_each\_entry\_safe(req, tmp, &ctx->timeout\_list, timeout.list) {+ if (io\_match\_task(req, tsk, cancel\_all)) {+ io\_kill\_timeout(req, -ECANCELED);+ canceled++;+ }+ }+ spin\_unlock\_irq(&ctx->timeout\_lock);+ if (canceled != 0)+ io\_commit\_cqring(ctx);+ spin\_unlock(&ctx->completion\_lock);+ if (canceled != 0)+ io\_cqring\_ev\_posted(ctx);+ return canceled != 0; }  static void io\_ring\_ctx\_wait\_and\_kill(struct io\_ring\_ctx \*ctx) { unsigned long index;- struct io\_identify \*iod;+ struct creds \*creds;  mutex\_lock(&ctx->uring\_lock); percpu\_ref\_kill(&ctx->refs);- /\* if force is set, the ring is going away. always drop after that \*/-- if (WARN\_ON\_ONCE((ctx->flags & IORING\_SETUP\_SQPOLL) && !ctx->sqo\_dead))- ctx->sqo\_dead = 1;-- ctx->cq\_overflow\_flushed = 1; if (ctx->rings)- \_\_io\_cqring\_overflow\_flush(ctx, true, NULL, NULL);+ \_\_io\_cqring\_overflow\_flush(ctx, true);+ xa\_for\_each(&ctx->personalities, index, creds)+ io\_unregister\_personality(ctx, index); mutex\_unlock(&ctx->uring\_lock); - io\_kill\_timeouts(ctx, NULL, NULL);- io\_poll\_remove\_all(ctx, NULL, NULL);-- if (ctx->io\_wq)- io\_wq\_cancel\_cb(ctx->io\_wq, io\_cancel\_ctx\_cb, ctx, true);+ io\_kill\_timeouts(ctx, NULL, true);+ io\_poll\_remove\_all(ctx, NULL, true);  /\* if we failed setting up the ctx, we might not have any rings \*/ io\_iopoll\_try\_reap\_events(ctx);- xa\_for\_each(&ctx->personalities, index, iod)- io\_unregister\_personality(ctx, index);-- /\*- \* Do this upfront, so we won't have a grace period where the ring- \* is closed but resources aren't reaped yet. This can cause- \* spurious failure in setting up a new ring.- \*/- io\_unaccount\_mem(ctx, ring\_pages(ctx->sq\_entries, ctx->cq\_entries),- ACCT\_LOCKED);  INIT\_WORK(&ctx->exit\_work, io\_ring\_exit\_work); /\*@@ -8647,352 +9443,290 @@ static int io\_uring\_release(struct inode \*inode, struct file \*file)  struct io\_task\_cancel { struct task\_struct \*task;- struct files\_struct \*files;+ bool all; };  static bool io\_cancel\_task\_cb(struct io\_wq\_work \*work, void \*data) { struct io\_kiocb \*req = container\_of(work, struct io\_kiocb, work); struct io\_task\_cancel \*cancel = data;- bool ret;-- if (cancel->files && (req->flags & REQ\_F\_LINK\_TIMEOUT)) {- unsigned long flags;- struct io\_ring\_ctx \*ctx = req->ctx; - /\* protect against races with linked timeouts \*/- spin\_lock\_irqsave(&ctx->completion\_lock, flags);- ret = io\_match\_task(req, cancel->task, cancel->files);- spin\_unlock\_irqrestore(&ctx->completion\_lock, flags);- } else {- ret = io\_match\_task(req, cancel->task, cancel->files);- }- return ret;+ return io\_match\_task\_safe(req, cancel->task, cancel->all); } -static void io\_cancel\_defer\_files(struct io\_ring\_ctx \*ctx,- struct task\_struct \*task,- struct files\_struct \*files)+static bool io\_cancel\_defer\_files(struct io\_ring\_ctx \*ctx,+ struct task\_struct \*task, bool cancel\_all) {- struct io\_defer\_entry \*de = NULL;+ struct io\_defer\_entry \*de; LIST\_HEAD(list); - spin\_lock\_irq(&ctx->completion\_lock);+ spin\_lock(&ctx->completion\_lock); list\_for\_each\_entry\_reverse(de, &ctx->defer\_list, list) {- if (io\_match\_task(de->req, task, files)) {+ if (io\_match\_task\_safe(de->req, task, cancel\_all)) { list\_cut\_position(&list, &ctx->defer\_list, &de->list); break; } }- spin\_unlock\_irq(&ctx->completion\_lock);+ spin\_unlock(&ctx->completion\_lock);+ if (list\_empty(&list))+ return false;  while (!list\_empty(&list)) { de = list\_first\_entry(&list, struct io\_defer\_entry, list); list\_del\_init(&de->list);- req\_set\_fail\_links(de->req);- io\_put\_req(de->req);- io\_req\_complete(de->req, -ECANCELED);+ io\_req\_complete\_failed(de->req, -ECANCELED); kfree(de); }+ return true; } -static int io\_uring\_count\_inflight(struct io\_ring\_ctx \*ctx,- struct task\_struct \*task,- struct files\_struct \*files)-{- struct io\_kiocb \*req;- int cnt = 0;-- spin\_lock\_irq(&ctx->inflight\_lock);- list\_for\_each\_entry(req, &ctx->inflight\_list, inflight\_entry)- cnt += io\_match\_task(req, task, files);- spin\_unlock\_irq(&ctx->inflight\_lock);- return cnt;-}--static void io\_uring\_cancel\_files(struct io\_ring\_ctx \*ctx,- struct task\_struct \*task,- struct files\_struct \*files)+static bool io\_uring\_try\_cancel\_iowq(struct io\_ring\_ctx \*ctx) {- while (!list\_empty\_careful(&ctx->inflight\_list)) {- struct io\_task\_cancel cancel = { .task = task, .files = files };- DEFINE\_WAIT(wait);- int inflight;+ struct io\_tctx\_node \*node;+ enum io\_wq\_cancel cret;+ bool ret = false; - inflight = io\_uring\_count\_inflight(ctx, task, files);- if (!inflight)- break;-- io\_wq\_cancel\_cb(ctx->io\_wq, io\_cancel\_task\_cb, &cancel, true);- io\_poll\_remove\_all(ctx, task, files);- io\_kill\_timeouts(ctx, task, files);- /\* cancellations \_may\_ trigger task work \*/- io\_run\_task\_work();+ mutex\_lock(&ctx->uring\_lock);+ list\_for\_each\_entry(node, &ctx->tctx\_list, ctx\_node) {+ struct io\_uring\_task \*tctx = node->task->io\_uring; - prepare\_to\_wait(&task->io\_uring->wait, &wait,- TASK\_UNINTERRUPTIBLE);- if (inflight == io\_uring\_count\_inflight(ctx, task, files))- schedule();- finish\_wait(&task->io\_uring->wait, &wait);+ /\*+ \* io\_wq will stay alive while we hold uring\_lock, because it's+ \* killed after ctx nodes, which requires to take the lock.+ \*/+ if (!tctx || !tctx->io\_wq)+ continue;+ cret = io\_wq\_cancel\_cb(tctx->io\_wq, io\_cancel\_ctx\_cb, ctx, true);+ ret |= (cret != IO\_WQ\_CANCEL\_NOTFOUND); }+ mutex\_unlock(&ctx->uring\_lock);++ return ret; } -static void \_\_io\_uring\_cancel\_task\_requests(struct io\_ring\_ctx \*ctx,- struct task\_struct \*task)+static void io\_uring\_try\_cancel\_requests(struct io\_ring\_ctx \*ctx,+ struct task\_struct \*task,+ bool cancel\_all) {+ struct io\_task\_cancel cancel = { .task = task, .all = cancel\_all, };+ struct io\_uring\_task \*tctx = task ? task->io\_uring : NULL;+ while (1) {- struct io\_task\_cancel cancel = { .task = task, .files = NULL, }; enum io\_wq\_cancel cret; bool ret = false; - cret = io\_wq\_cancel\_cb(ctx->io\_wq, io\_cancel\_task\_cb, &cancel, true);- if (cret != IO\_WQ\_CANCEL\_NOTFOUND)- ret = true;+ if (!task) {+ ret |= io\_uring\_try\_cancel\_iowq(ctx);+ } else if (tctx && tctx->io\_wq) {+ /\*+ \* Cancels requests of all rings, not only @ctx, but+ \* it's fine as the task is in exit/exec.+ \*/+ cret = io\_wq\_cancel\_cb(tctx->io\_wq, io\_cancel\_task\_cb,+ &cancel, true);+ ret |= (cret != IO\_WQ\_CANCEL\_NOTFOUND);+ }  /\* SQPOLL thread does its own polling \*/- if (!(ctx->flags & IORING\_SETUP\_SQPOLL)) {+ if ((!(ctx->flags & IORING\_SETUP\_SQPOLL) && cancel\_all) ||+ (ctx->sq\_data && ctx->sq\_data->thread == current)) { while (!list\_empty\_careful(&ctx->iopoll\_list)) { io\_iopoll\_try\_reap\_events(ctx); ret = true; } } - ret |= io\_poll\_remove\_all(ctx, task, NULL);- ret |= io\_kill\_timeouts(ctx, task, NULL);+ ret |= io\_cancel\_defer\_files(ctx, task, cancel\_all);+ ret |= io\_poll\_remove\_all(ctx, task, cancel\_all);+ ret |= io\_kill\_timeouts(ctx, task, cancel\_all);+ if (task)+ ret |= io\_run\_task\_work(); if (!ret) break;- io\_run\_task\_work(); cond\_resched(); } } -static void io\_disable\_sqo\_submit(struct io\_ring\_ctx \*ctx)-{- mutex\_lock(&ctx->uring\_lock);- ctx->sqo\_dead = 1;- if (ctx->flags & IORING\_SETUP\_R\_DISABLED)- io\_sq\_offload\_start(ctx);- mutex\_unlock(&ctx->uring\_lock);-- /\* make sure callers enter the ring to get error \*/- if (ctx->rings)- io\_ring\_set\_wakeup\_flag(ctx);-}--/\*- \* We need to iteratively cancel requests, in case a request has dependent- \* hard links. These persist even for failure of cancelations, hence keep- \* looping until none are found.- \*/-static void io\_uring\_cancel\_task\_requests(struct io\_ring\_ctx \*ctx,- struct files\_struct \*files)-{- struct task\_struct \*task = current;-- if ((ctx->flags & IORING\_SETUP\_SQPOLL) && ctx->sq\_data) {- io\_disable\_sqo\_submit(ctx);- task = ctx->sq\_data->thread;- atomic\_inc(&task->io\_uring->in\_idle);- io\_sq\_thread\_park(ctx->sq\_data);- }-- io\_cancel\_defer\_files(ctx, task, files);- io\_cqring\_overflow\_flush(ctx, true, task, files);-- if (!files)- \_\_io\_uring\_cancel\_task\_requests(ctx, task);- else- io\_uring\_cancel\_files(ctx, task, files);-- if ((ctx->flags & IORING\_SETUP\_SQPOLL) && ctx->sq\_data) {- atomic\_dec(&task->io\_uring->in\_idle);- io\_sq\_thread\_unpark(ctx->sq\_data);- }-}--/\*- \* Note that this task has used io\_uring. We use it for cancelation purposes.- \*/-static int io\_uring\_add\_task\_file(struct io\_ring\_ctx \*ctx, struct file \*file)+static int \_\_io\_uring\_add\_tctx\_node(struct io\_ring\_ctx \*ctx) { struct io\_uring\_task \*tctx = current->io\_uring;+ struct io\_tctx\_node \*node; int ret;  if (unlikely(!tctx)) {- ret = io\_uring\_alloc\_task\_context(current);+ ret = io\_uring\_alloc\_task\_context(current, ctx); if (unlikely(ret)) return ret;+ tctx = current->io\_uring;- }- if (tctx->last != file) {- void \*old = xa\_load(&tctx->xa, (unsigned long)file);+ if (ctx->iowq\_limits\_set) {+ unsigned int limits[2] = { ctx->iowq\_limits[0],+ ctx->iowq\_limits[1], }; - if (!old) {- get\_file(file);- ret = xa\_err(xa\_store(&tctx->xa, (unsigned long)file,- file, GFP\_KERNEL));- if (ret) {- fput(file);+ ret = io\_wq\_max\_workers(tctx->io\_wq, limits);+ if (ret) return ret;- } }- tctx->last = file; }+ if (!xa\_load(&tctx->xa, (unsigned long)ctx)) {+ node = kmalloc(sizeof(\*node), GFP\_KERNEL);+ if (!node)+ return -ENOMEM;+ node->ctx = ctx;+ node->task = current; - /\*- \* This is race safe in that the task itself is doing this, hence it- \* cannot be going through the exit/cancel paths at the same time.- \* This cannot be modified while exit/cancel is running.- \*/- if (!tctx->sqpoll && (ctx->flags & IORING\_SETUP\_SQPOLL))- tctx->sqpoll = true;+ ret = xa\_err(xa\_store(&tctx->xa, (unsigned long)ctx,+ node, GFP\_KERNEL));+ if (ret) {+ kfree(node);+ return ret;+ } + mutex\_lock(&ctx->uring\_lock);+ list\_add(&node->ctx\_node, &ctx->tctx\_list);+ mutex\_unlock(&ctx->uring\_lock);+ }+ tctx->last = ctx; return 0; }  /\*- \* Remove this io\_uring\_file -> task mapping.+ \* Note that this task has used io\_uring. We use it for cancelation purposes. \*/-static void io\_uring\_del\_task\_file(struct file \*file)+static inline int io\_uring\_add\_tctx\_node(struct io\_ring\_ctx \*ctx) { struct io\_uring\_task \*tctx = current->io\_uring; - if (tctx->last == file)- tctx->last = NULL;- file = xa\_erase(&tctx->xa, (unsigned long)file);- if (file)- fput(file);+ if (likely(tctx && tctx->last == ctx))+ return 0;+ return \_\_io\_uring\_add\_tctx\_node(ctx); } -static void io\_uring\_remove\_task\_files(struct io\_uring\_task \*tctx)+/\*+ \* Remove this io\_uring\_file -> task mapping.+ \*/+static void io\_uring\_del\_tctx\_node(unsigned long index) {- struct file \*file;- unsigned long index;+ struct io\_uring\_task \*tctx = current->io\_uring;+ struct io\_tctx\_node \*node; - xa\_for\_each(&tctx->xa, index, file)- io\_uring\_del\_task\_file(file);-}+ if (!tctx)+ return;+ node = xa\_erase(&tctx->xa, index);+ if (!node)+ return; -void \_\_io\_uring\_files\_cancel(struct files\_struct \*files)-{- struct io\_uring\_task \*tctx = current->io\_uring;- struct file \*file;- unsigned long index;+ WARN\_ON\_ONCE(current != node->task);+ WARN\_ON\_ONCE(list\_empty(&node->ctx\_node)); - /\* make sure overflow events are dropped \*/- atomic\_inc(&tctx->in\_idle);- xa\_for\_each(&tctx->xa, index, file)- io\_uring\_cancel\_task\_requests(file->private\_data, files);- atomic\_dec(&tctx->in\_idle);+ mutex\_lock(&node->ctx->uring\_lock);+ list\_del(&node->ctx\_node);+ mutex\_unlock(&node->ctx->uring\_lock); - if (files)- io\_uring\_remove\_task\_files(tctx);+ if (tctx->last == node->ctx)+ tctx->last = NULL;+ kfree(node); } -static s64 tctx\_inflight(struct io\_uring\_task \*tctx)+static void io\_uring\_clean\_tctx(struct io\_uring\_task \*tctx) {+ struct io\_wq \*wq = tctx->io\_wq;+ struct io\_tctx\_node \*node; unsigned long index;- struct file \*file;- s64 inflight;-- inflight = percpu\_counter\_sum(&tctx->inflight);- if (!tctx->sqpoll)- return inflight; - /\*- \* If we have SQPOLL rings, then we need to iterate and find them, and- \* add the pending count for those.- \*/- xa\_for\_each(&tctx->xa, index, file) {- struct io\_ring\_ctx \*ctx = file->private\_data;-- if (ctx->flags & IORING\_SETUP\_SQPOLL) {- struct io\_uring\_task \*\_\_tctx = ctx->sqo\_task->io\_uring;-- inflight += percpu\_counter\_sum(&\_\_tctx->inflight);- }+ xa\_for\_each(&tctx->xa, index, node) {+ io\_uring\_del\_tctx\_node(index);+ cond\_resched(); }+ if (wq) {+ /\*+ \* Must be after io\_uring\_del\_task\_file() (removes nodes under+ \* uring\_lock) to avoid race with io\_uring\_try\_cancel\_iowq().+ \*/+ io\_wq\_put\_and\_exit(wq);+ tctx->io\_wq = NULL;+ }+} - return inflight;+static s64 tctx\_inflight(struct io\_uring\_task \*tctx, bool tracked)+{+ if (tracked)+ return atomic\_read(&tctx->inflight\_tracked);+ return percpu\_counter\_sum(&tctx->inflight); }  /\*- \* Find any io\_uring fd that this task has registered or done IO on, and cancel- \* requests.+ \* Find any io\_uring ctx that this task has registered or done IO on, and cancel+ \* requests. @sqd should be not-null IFF it's an SQPOLL thread cancellation. \*/-void \_\_io\_uring\_task\_cancel(void)+static void io\_uring\_cancel\_generic(bool cancel\_all, struct io\_sq\_data \*sqd) { struct io\_uring\_task \*tctx = current->io\_uring;- DEFINE\_WAIT(wait);+ struct io\_ring\_ctx \*ctx; s64 inflight;+ DEFINE\_WAIT(wait); - /\* make sure overflow events are dropped \*/- atomic\_inc(&tctx->in\_idle);+ WARN\_ON\_ONCE(sqd && sqd->thread != current); - /\* trigger io\_disable\_sqo\_submit() \*/- if (tctx->sqpoll)- \_\_io\_uring\_files\_cancel(NULL);+ if (!current->io\_uring)+ return;+ if (tctx->io\_wq)+ io\_wq\_exit\_start(tctx->io\_wq); + atomic\_inc(&tctx->in\_idle); do {+ io\_uring\_drop\_tctx\_refs(current); /\* read completions before cancelations \*/- inflight = tctx\_inflight(tctx);+ inflight = tctx\_inflight(tctx, !cancel\_all); if (!inflight) break;- \_\_io\_uring\_files\_cancel(NULL); - prepare\_to\_wait(&tctx->wait, &wait, TASK\_UNINTERRUPTIBLE);+ if (!sqd) {+ struct io\_tctx\_node \*node;+ unsigned long index;++ xa\_for\_each(&tctx->xa, index, node) {+ /\* sqpoll task will cancel all its requests \*/+ if (node->ctx->sq\_data)+ continue;+ io\_uring\_try\_cancel\_requests(node->ctx, current,+ cancel\_all);+ }+ } else {+ list\_for\_each\_entry(ctx, &sqd->ctx\_list, sqd\_list)+ io\_uring\_try\_cancel\_requests(ctx, current,+ cancel\_all);+ }++ prepare\_to\_wait(&tctx->wait, &wait, TASK\_INTERRUPTIBLE);+ io\_run\_task\_work();+ io\_uring\_drop\_tctx\_refs(current);  /\* \* If we've seen completions, retry without waiting. This \* avoids a race where a completion comes in before we did \* prepare\_to\_wait(). \*/- if (inflight == tctx\_inflight(tctx))+ if (inflight == tctx\_inflight(tctx, !cancel\_all)) schedule(); finish\_wait(&tctx->wait, &wait); } while (1); - atomic\_dec(&tctx->in\_idle);-- io\_uring\_remove\_task\_files(tctx);+ io\_uring\_clean\_tctx(tctx);+ if (cancel\_all) {+ /\*+ \* We shouldn't run task\_works after cancel, so just leave+ \* ->in\_idle set for normal exit.+ \*/+ atomic\_dec(&tctx->in\_idle);+ /\* for exec all current's requests should be gone, kill tctx \*/+ \_\_io\_uring\_free(current);+ } } -static int io\_uring\_flush(struct file \*file, void \*data)+void \_\_io\_uring\_cancel(bool cancel\_all) {- struct io\_uring\_task \*tctx = current->io\_uring;- struct io\_ring\_ctx \*ctx = file->private\_data;-- if (fatal\_signal\_pending(current) || (current->flags & PF\_EXITING))- io\_uring\_cancel\_task\_requests(ctx, NULL);-- if (!tctx)- return 0;-- /\* we should have cancelled and erased it before PF\_EXITING \*/- WARN\_ON\_ONCE((current->flags & PF\_EXITING) &&- xa\_load(&tctx->xa, (unsigned long)file));-- /\*- \* fput() is pending, will be 2 if the only other ref is our potential- \* task file note. If the task is exiting, drop regardless of count.- \*/- if (atomic\_long\_read(&file->f\_count) != 2)- return 0;-- if (ctx->flags & IORING\_SETUP\_SQPOLL) {- /\* there is only one file note, which is owned by sqo\_task \*/- WARN\_ON\_ONCE(ctx->sqo\_task != current &&- xa\_load(&tctx->xa, (unsigned long)file));- /\* sqo\_dead check is for when this happens after cancellation \*/- WARN\_ON\_ONCE(ctx->sqo\_task == current && !ctx->sqo\_dead &&- !xa\_load(&tctx->xa, (unsigned long)file));-- io\_disable\_sqo\_submit(ctx);- }-- if (!(ctx->flags & IORING\_SETUP\_SQPOLL) || ctx->sqo\_task == current)- io\_uring\_del\_task\_file(file);- return 0;+ io\_uring\_cancel\_generic(cancel\_all, NULL); }  static void \*io\_uring\_validate\_mmap\_request(struct file \*file,@@ -9067,60 +9801,84 @@ static unsigned long io\_uring\_nommu\_get\_unmapped\_area(struct file \*file,  static int io\_sqpoll\_wait\_sq(struct io\_ring\_ctx \*ctx) {- int ret = 0; DEFINE\_WAIT(wait);  do { if (!io\_sqring\_full(ctx)) break;- prepare\_to\_wait(&ctx->sqo\_sq\_wait, &wait, TASK\_INTERRUPTIBLE); - if (unlikely(ctx->sqo\_dead)) {- ret = -EOWNERDEAD;- break;- }- if (!io\_sqring\_full(ctx)) break;- schedule(); } while (!signal\_pending(current));  finish\_wait(&ctx->sqo\_sq\_wait, &wait);- return ret;+ return 0;+}++static int io\_get\_ext\_arg(unsigned flags, const void \_\_user \*argp, size\_t \*argsz,+ struct \_\_kernel\_timespec \_\_user \*\*ts,+ const sigset\_t \_\_user \*\*sig)+{+ struct io\_uring\_getevents\_arg arg;++ /\*+ \* If EXT\_ARG isn't set, then we have no timespec and the argp pointer+ \* is just a pointer to the sigset\_t.+ \*/+ if (!(flags & IORING\_ENTER\_EXT\_ARG)) {+ \*sig = (const sigset\_t \_\_user \*) argp;+ \*ts = NULL;+ return 0;+ }++ /\*+ \* EXT\_ARG is set - ensure we agree on the size of it and copy in our+ \* timespec and sigset\_t pointers if good.+ \*/+ if (\*argsz != sizeof(arg))+ return -EINVAL;+ if (copy\_from\_user(&arg, argp, sizeof(arg)))+ return -EFAULT;+ if (arg.pad)+ return -EINVAL;+ \*sig = u64\_to\_user\_ptr(arg.sigmask);+ \*argsz = arg.sigmask\_sz;+ \*ts = u64\_to\_user\_ptr(arg.ts);+ return 0; }  SYSCALL\_DEFINE6(io\_uring\_enter, unsigned int, fd, u32, to\_submit,- u32, min\_complete, u32, flags, const sigset\_t \_\_user \*, sig,- size\_t, sigsz)+ u32, min\_complete, u32, flags, const void \_\_user \*, argp,+ size\_t, argsz) { struct io\_ring\_ctx \*ctx;- long ret = -EBADF; int submitted = 0; struct fd f;+ long ret;  io\_run\_task\_work(); - if (flags & ~(IORING\_ENTER\_GETEVENTS | IORING\_ENTER\_SQ\_WAKEUP |- IORING\_ENTER\_SQ\_WAIT))+ if (unlikely(flags & ~(IORING\_ENTER\_GETEVENTS | IORING\_ENTER\_SQ\_WAKEUP |+ IORING\_ENTER\_SQ\_WAIT | IORING\_ENTER\_EXT\_ARG))) return -EINVAL;  f = fdget(fd);- if (!f.file)+ if (unlikely(!f.file)) return -EBADF;  ret = -EOPNOTSUPP;- if (f.file->f\_op != &io\_uring\_fops)+ if (unlikely(f.file->f\_op != &io\_uring\_fops)) goto out\_fput;  ret = -ENXIO; ctx = f.file->private\_data;- if (!percpu\_ref\_tryget(&ctx->refs))+ if (unlikely(!percpu\_ref\_tryget(&ctx->refs))) goto out\_fput;  ret = -EBADFD;- if (ctx->flags & IORING\_SETUP\_R\_DISABLED)+ if (unlikely(ctx->flags & IORING\_SETUP\_R\_DISABLED)) goto out;  /\*@@ -9130,9 +9888,9 @@ SYSCALL\_DEFINE6(io\_uring\_enter, unsigned int, fd, u32, to\_submit, \*/ ret = 0; if (ctx->flags & IORING\_SETUP\_SQPOLL) {- io\_cqring\_overflow\_flush(ctx, false, NULL, NULL);+ io\_cqring\_overflow\_flush(ctx); - if (unlikely(ctx->sqo\_dead)) {+ if (unlikely(ctx->sq\_data->thread == NULL)) { ret = -EOWNERDEAD; goto out; }@@ -9145,7 +9903,7 @@ SYSCALL\_DEFINE6(io\_uring\_enter, unsigned int, fd, u32, to\_submit, } submitted = to\_submit; } else if (to\_submit) {- ret = io\_uring\_add\_task\_file(ctx, f.file);+ ret = io\_uring\_add\_tctx\_node(ctx); if (unlikely(ret)) goto out; mutex\_lock(&ctx->uring\_lock);@@ -9156,6 +9914,13 @@ SYSCALL\_DEFINE6(io\_uring\_enter, unsigned int, fd, u32, to\_submit, goto out; } if (flags & IORING\_ENTER\_GETEVENTS) {+ const sigset\_t \_\_user \*sig;+ struct \_\_kernel\_timespec \_\_user \*ts;++ ret = io\_get\_ext\_arg(flags, argp, &argsz, &ts, &sig);+ if (unlikely(ret))+ goto out;+ min\_complete = min(min\_complete, ctx->cq\_entries);  /\*@@ -9168,7 +9933,7 @@ SYSCALL\_DEFINE6(io\_uring\_enter, unsigned int, fd, u32, to\_submit, !(ctx->flags & IORING\_SETUP\_SQPOLL)) { ret = io\_iopoll\_check(ctx, min\_complete); } else {- ret = io\_cqring\_wait(ctx, min\_complete, sig, sigsz);+ ret = io\_cqring\_wait(ctx, min\_complete, sig, argsz, ts); } } @@ -9181,9 +9946,8 @@ out\_fput:  #ifdef CONFIG\_PROC\_FS static int io\_uring\_show\_cred(struct seq\_file \*m, unsigned int id,- const struct io\_identity \*iod)+ const struct cred \*cred) {- const struct cred \*cred = iod->creds; struct user\_namespace \*uns = seq\_user\_ns(m); struct group\_info \*gi; kernel\_cap\_t cap;@@ -9227,18 +9991,18 @@ static void \_\_io\_uring\_show\_fdinfo(struct io\_ring\_ctx \*ctx, struct seq\_file \*m) \*/ has\_lock = mutex\_trylock(&ctx->uring\_lock); - if (has\_lock && (ctx->flags & IORING\_SETUP\_SQPOLL))+ if (has\_lock && (ctx->flags & IORING\_SETUP\_SQPOLL)) { sq = ctx->sq\_data;+ if (!sq->thread)+ sq = NULL;+ }  seq\_printf(m, "SqThread:\t%d\n", sq ? task\_pid\_nr(sq->thread) : -1); seq\_printf(m, "SqThreadCpu:\t%d\n", sq ? task\_cpu(sq->thread) : -1); seq\_printf(m, "UserFiles:\t%u\n", ctx->nr\_user\_files); for (i = 0; has\_lock && i < ctx->nr\_user\_files; i++) {- struct fixed\_file\_table \*table;- struct file \*f;+ struct file \*f = io\_file\_from\_index(ctx, i); - table = &ctx->file\_data->table[i >> IORING\_FILE\_TABLE\_SHIFT];- f = table->files[i & IORING\_FILE\_TABLE\_MASK]; if (f) seq\_printf(m, "%5u: %s\n", i, file\_dentry(f)->d\_iname); else@@ -9246,21 +10010,21 @@ static void \_\_io\_uring\_show\_fdinfo(struct io\_ring\_ctx \*ctx, struct seq\_file \*m) } seq\_printf(m, "UserBufs:\t%u\n", ctx->nr\_user\_bufs); for (i = 0; has\_lock && i < ctx->nr\_user\_bufs; i++) {- struct io\_mapped\_ubuf \*buf = &ctx->user\_bufs[i];+ struct io\_mapped\_ubuf \*buf = ctx->user\_bufs[i];+ unsigned int len = buf->ubuf\_end - buf->ubuf; - seq\_printf(m, "%5u: 0x%llx/%u\n", i, buf->ubuf,- (unsigned int) buf->len);+ seq\_printf(m, "%5u: 0x%llx/%u\n", i, buf->ubuf, len); } if (has\_lock && !xa\_empty(&ctx->personalities)) { unsigned long index;- const struct io\_identity \*iod;+ const struct cred \*cred;  seq\_printf(m, "Personalities:\n");- xa\_for\_each(&ctx->personalities, index, iod)- io\_uring\_show\_cred(m, index, iod);+ xa\_for\_each(&ctx->personalities, index, cred)+ io\_uring\_show\_cred(m, index, cred); } seq\_printf(m, "PollList:\n");- spin\_lock\_irq(&ctx->completion\_lock);+ spin\_lock(&ctx->completion\_lock); for (i = 0; i < (1U << ctx->cancel\_hash\_bits); i++) { struct hlist\_head \*list = &ctx->cancel\_hash[i]; struct io\_kiocb \*req;@@ -9269,7 +10033,7 @@ static void \_\_io\_uring\_show\_fdinfo(struct io\_ring\_ctx \*ctx, struct seq\_file \*m) seq\_printf(m, " op=%d, task\_works=%d\n", req->opcode, req->task->task\_works != NULL); }- spin\_unlock\_irq(&ctx->completion\_lock);+ spin\_unlock(&ctx->completion\_lock); if (has\_lock) mutex\_unlock(&ctx->uring\_lock); }@@ -9287,14 +10051,12 @@ static void io\_uring\_show\_fdinfo(struct seq\_file \*m, struct file \*f)  static const struct file\_operations io\_uring\_fops = { .release = io\_uring\_release,- .flush = io\_uring\_flush, .mmap = io\_uring\_mmap, #ifndef CONFIG\_MMU .get\_unmapped\_area = io\_uring\_nommu\_get\_unmapped\_area, .mmap\_capabilities = io\_uring\_nommu\_mmap\_capabilities, #endif .poll = io\_uring\_poll,- .fasync = io\_uring\_fasync, #ifdef CONFIG\_PROC\_FS .show\_fdinfo = io\_uring\_show\_fdinfo, #endif@@ -9324,8 +10086,6 @@ static int io\_allocate\_scq\_urings(struct io\_ring\_ctx \*ctx, rings->cq\_ring\_mask = p->cq\_entries - 1; rings->sq\_ring\_entries = p->sq\_entries; rings->cq\_ring\_entries = p->cq\_entries;- ctx->sq\_mask = rings->sq\_ring\_mask;- ctx->cq\_mask = rings->cq\_ring\_mask;  size = array\_size(sizeof(struct io\_uring\_sqe), p->sq\_entries); if (size == SIZE\_MAX) {@@ -9352,7 +10112,7 @@ static int io\_uring\_install\_fd(struct io\_ring\_ctx \*ctx, struct file \*file) if (fd < 0) return fd; - ret = io\_uring\_add\_task\_file(ctx, file);+ ret = io\_uring\_add\_tctx\_node(ctx); if (ret) { put\_unused\_fd(fd); return ret;@@ -9395,10 +10155,8 @@ static struct file \*io\_uring\_get\_file(struct io\_ring\_ctx \*ctx) static int io\_uring\_create(unsigned entries, struct io\_uring\_params \*p, struct io\_uring\_params \_\_user \*params) {- struct user\_struct \*user = NULL; struct io\_ring\_ctx \*ctx; struct file \*file;- bool limit\_mem; int ret;  if (!entries)@@ -9438,34 +10196,12 @@ static int io\_uring\_create(unsigned entries, struct io\_uring\_params \*p, p->cq\_entries = 2 \* p->sq\_entries; } - user = get\_uid(current\_user());- limit\_mem = !capable(CAP\_IPC\_LOCK);-- if (limit\_mem) {- ret = \_\_io\_account\_mem(user,- ring\_pages(p->sq\_entries, p->cq\_entries));- if (ret) {- free\_uid(user);- return ret;- }- }- ctx = io\_ring\_ctx\_alloc(p);- if (!ctx) {- if (limit\_mem)- \_\_io\_unaccount\_mem(user, ring\_pages(p->sq\_entries,- p->cq\_entries));- free\_uid(user);+ if (!ctx) return -ENOMEM;- } ctx->compat = in\_compat\_syscall();- ctx->user = user;- ctx->creds = get\_current\_cred();-#ifdef CONFIG\_AUDIT- ctx->loginuid = current->loginuid;- ctx->sessionid = current->sessionid;-#endif- ctx->sqo\_task = get\_task\_struct(current);+ if (!capable(CAP\_IPC\_LOCK))+ ctx->user = get\_uid(current\_user());  /\* \* This is just grabbed for accounting purposes. When a process exits,@@ -9476,35 +10212,6 @@ static int io\_uring\_create(unsigned entries, struct io\_uring\_params \*p, mmgrab(current->mm); ctx->mm\_account = current->mm; -#ifdef CONFIG\_BLK\_CGROUP- /\*- \* The sq thread will belong to the original cgroup it was inited in.- \* If the cgroup goes offline (e.g. disabling the io controller), then- \* issued bios will be associated with the closest cgroup later in the- \* block layer.- \*/- rcu\_read\_lock();- ctx->sqo\_blkcg\_css = blkcg\_css();- ret = css\_tryget\_online(ctx->sqo\_blkcg\_css);- rcu\_read\_unlock();- if (!ret) {- /\* don't init against a dying cgroup, have the user try again \*/- ctx->sqo\_blkcg\_css = NULL;- ret = -ENODEV;- goto err;- }-#endif-- /\*- \* Account memory \_before\_ installing the file descriptor. Once- \* the descriptor is installed, it can get closed at any time. Also- \* do this before hitting the general error path, as ring freeing- \* will un-account as well.- \*/- io\_account\_mem(ctx, ring\_pages(p->sq\_entries, p->cq\_entries),- ACCT\_LOCKED);- ctx->limit\_mem = limit\_mem;- ret = io\_allocate\_scq\_urings(ctx, p); if (ret) goto err;@@ -9512,9 +10219,11 @@ static int io\_uring\_create(unsigned entries, struct io\_uring\_params \*p, ret = io\_sq\_offload\_create(ctx, p); if (ret) goto err;-- if (!(p->flags & IORING\_SETUP\_R\_DISABLED))- io\_sq\_offload\_start(ctx);+ /\* always set a rsrc node \*/+ ret = io\_rsrc\_node\_switch\_start(ctx);+ if (ret)+ goto err;+ io\_rsrc\_node\_switch(ctx, NULL);  memset(&p->sq\_off, 0, sizeof(p->sq\_off)); p->sq\_off.head = offsetof(struct io\_rings, sq.head);@@ -9537,7 +10246,9 @@ static int io\_uring\_create(unsigned entries, struct io\_uring\_params \*p, p->features = IORING\_FEAT\_SINGLE\_MMAP | IORING\_FEAT\_NODROP | IORING\_FEAT\_SUBMIT\_STABLE | IORING\_FEAT\_RW\_CUR\_POS | IORING\_FEAT\_CUR\_PERSONALITY | IORING\_FEAT\_FAST\_POLL |- IORING\_FEAT\_POLL\_32BITS;+ IORING\_FEAT\_POLL\_32BITS | IORING\_FEAT\_SQPOLL\_NONFIXED |+ IORING\_FEAT\_EXT\_ARG | IORING\_FEAT\_NATIVE\_WORKERS |+ IORING\_FEAT\_RSRC\_TAGS;  if (copy\_to\_user(params, p, sizeof(\*p))) { ret = -EFAULT;@@ -9556,7 +10267,6 @@ static int io\_uring\_create(unsigned entries, struct io\_uring\_params \*p, \*/ ret = io\_uring\_install\_fd(ctx, file); if (ret < 0) {- io\_disable\_sqo\_submit(ctx); /\* fput will clean it up \*/ fput(file); return ret;@@ -9565,7 +10275,6 @@ static int io\_uring\_create(unsigned entries, struct io\_uring\_params \*p, trace\_io\_uring\_create(ret, ctx, p->sq\_entries, p->cq\_entries, p->flags); return ret; err:- io\_disable\_sqo\_submit(ctx); io\_ring\_ctx\_wait\_and\_kill(ctx); return ret; }@@ -9643,22 +10352,16 @@ out:  static int io\_register\_personality(struct io\_ring\_ctx \*ctx) {- struct io\_identity \*iod;+ const struct cred \*creds; u32 id; int ret; - iod = kmalloc(sizeof(\*iod), GFP\_KERNEL);- if (unlikely(!iod))- return -ENOMEM;+ creds = get\_current\_cred(); - io\_init\_identity(iod);- iod->creds = get\_current\_cred();-- ret = xa\_alloc\_cyclic(&ctx->personalities, &id, (void \*)iod,+ ret = xa\_alloc\_cyclic(&ctx->personalities, &id, (void \*)creds, XA\_LIMIT(0, USHRT\_MAX), &ctx->pers\_next, GFP\_KERNEL); if (ret < 0) {- put\_cred(iod->creds);- kfree(iod);+ put\_cred(creds); return ret; } return id;@@ -9742,24 +10445,273 @@ static int io\_register\_enable\_rings(struct io\_ring\_ctx \*ctx) if (ctx->restrictions.registered) ctx->restricted = 1; - io\_sq\_offload\_start(ctx);+ ctx->flags &= ~IORING\_SETUP\_R\_DISABLED;+ if (ctx->sq\_data && wq\_has\_sleeper(&ctx->sq\_data->wait))+ wake\_up(&ctx->sq\_data->wait);+ return 0;+}++static int \_\_io\_register\_rsrc\_update(struct io\_ring\_ctx \*ctx, unsigned type,+ struct io\_uring\_rsrc\_update2 \*up,+ unsigned nr\_args)+{+ \_\_u32 tmp;+ int err;++ if (check\_add\_overflow(up->offset, nr\_args, &tmp))+ return -EOVERFLOW;+ err = io\_rsrc\_node\_switch\_start(ctx);+ if (err)+ return err;++ switch (type) {+ case IORING\_RSRC\_FILE:+ return \_\_io\_sqe\_files\_update(ctx, up, nr\_args);+ case IORING\_RSRC\_BUFFER:+ return \_\_io\_sqe\_buffers\_update(ctx, up, nr\_args);+ }+ return -EINVAL;+}++static int io\_register\_files\_update(struct io\_ring\_ctx \*ctx, void \_\_user \*arg,+ unsigned nr\_args)+{+ struct io\_uring\_rsrc\_update2 up;++ if (!nr\_args)+ return -EINVAL;+ memset(&up, 0, sizeof(up));+ if (copy\_from\_user(&up, arg, sizeof(struct io\_uring\_rsrc\_update)))+ return -EFAULT;+ if (up.resv || up.resv2)+ return -EINVAL;+ return \_\_io\_register\_rsrc\_update(ctx, IORING\_RSRC\_FILE, &up, nr\_args);+}++static int io\_register\_rsrc\_update(struct io\_ring\_ctx \*ctx, void \_\_user \*arg,+ unsigned size, unsigned type)+{+ struct io\_uring\_rsrc\_update2 up;++ if (size != sizeof(up))+ return -EINVAL;+ if (copy\_from\_user(&up, arg, sizeof(up)))+ return -EFAULT;+ if (!up.nr || up.resv || up.resv2)+ return -EINVAL;+ return \_\_io\_register\_rsrc\_update(ctx, type, &up, up.nr);+}++static int io\_register\_rsrc(struct io\_ring\_ctx \*ctx, void \_\_user \*arg,+ unsigned int size, unsigned int type)+{+ struct io\_uring\_rsrc\_register rr;++ /\* keep it extendible \*/+ if (size != sizeof(rr))+ return -EINVAL;++ memset(&rr, 0, sizeof(rr));+ if (copy\_from\_user(&rr, arg, size))+ return -EFAULT;+ if (!rr.nr || rr.resv || rr.resv2)+ return -EINVAL;++ switch (type) {+ case IORING\_RSRC\_FILE:+ return io\_sqe\_files\_register(ctx, u64\_to\_user\_ptr(rr.data),+ rr.nr, u64\_to\_user\_ptr(rr.tags));+ case IORING\_RSRC\_BUFFER:+ return io\_sqe\_buffers\_register(ctx, u64\_to\_user\_ptr(rr.data),+ rr.nr, u64\_to\_user\_ptr(rr.tags));+ }+ return -EINVAL;+}++static int io\_register\_iowq\_aff(struct io\_ring\_ctx \*ctx, void \_\_user \*arg,+ unsigned len)+{+ struct io\_uring\_task \*tctx = current->io\_uring;+ cpumask\_var\_t new\_mask;+ int ret;++ if (!tctx || !tctx->io\_wq)+ return -EINVAL;++ if (!alloc\_cpumask\_var(&new\_mask, GFP\_KERNEL))+ return -ENOMEM;++ cpumask\_clear(new\_mask);+ if (len > cpumask\_size())+ len = cpumask\_size();++#ifdef CONFIG\_COMPAT+ if (in\_compat\_syscall()) {+ ret = compat\_get\_bitmap(cpumask\_bits(new\_mask),+ (const compat\_ulong\_t \_\_user \*)arg,+ len \* 8 /\* CHAR\_BIT \*/);+ } else {+ ret = copy\_from\_user(new\_mask, arg, len);+ }+#else+ ret = copy\_from\_user(new\_mask, arg, len);+#endif++ if (ret) {+ free\_cpumask\_var(new\_mask);+ return -EFAULT;+ }++ ret = io\_wq\_cpu\_affinity(tctx->io\_wq, new\_mask);+ free\_cpumask\_var(new\_mask);+ return ret;+}++static int io\_unregister\_iowq\_aff(struct io\_ring\_ctx \*ctx)+{+ struct io\_uring\_task \*tctx = current->io\_uring;++ if (!tctx || !tctx->io\_wq)+ return -EINVAL;++ return io\_wq\_cpu\_affinity(tctx->io\_wq, NULL);+}++static int io\_register\_iowq\_max\_workers(struct io\_ring\_ctx \*ctx,+ void \_\_user \*arg)+ \_\_must\_hold(&ctx->uring\_lock)+{+ struct io\_tctx\_node \*node;+ struct io\_uring\_task \*tctx = NULL;+ struct io\_sq\_data \*sqd = NULL;+ \_\_u32 new\_count[2];+ int i, ret;++ if (copy\_from\_user(new\_count, arg, sizeof(new\_count)))+ return -EFAULT;+ for (i = 0; i < ARRAY\_SIZE(new\_count); i++)+ if (new\_count[i] > INT\_MAX)+ return -EINVAL;++ if (ctx->flags & IORING\_SETUP\_SQPOLL) {+ sqd = ctx->sq\_data;+ if (sqd) {+ /\*+ \* Observe the correct sqd->lock -> ctx->uring\_lock+ \* ordering. Fine to drop uring\_lock here, we hold+ \* a ref to the ctx.+ \*/+ refcount\_inc(&sqd->refs);+ mutex\_unlock(&ctx->uring\_lock);+ mutex\_lock(&sqd->lock);+ mutex\_lock(&ctx->uring\_lock);+ if (sqd->thread)+ tctx = sqd->thread->io\_uring;+ }+ } else {+ tctx = current->io\_uring;+ }++ BUILD\_BUG\_ON(sizeof(new\_count) != sizeof(ctx->iowq\_limits));++ for (i = 0; i < ARRAY\_SIZE(new\_count); i++)+ if (new\_count[i])+ ctx->iowq\_limits[i] = new\_count[i];+ ctx->iowq\_limits\_set = true;++ ret = -EINVAL;+ if (tctx && tctx->io\_wq) {+ ret = io\_wq\_max\_workers(tctx->io\_wq, new\_count);+ if (ret)+ goto err;+ } else {+ memset(new\_count, 0, sizeof(new\_count));+ }++ if (sqd) {+ mutex\_unlock(&sqd->lock);+ io\_put\_sq\_data(sqd);+ }++ if (copy\_to\_user(arg, new\_count, sizeof(new\_count)))+ return -EFAULT;++ /\* that's it for SQPOLL, only the SQPOLL task creates requests \*/+ if (sqd)+ return 0;++ /\* now propagate the restriction to all registered users \*/+ list\_for\_each\_entry(node, &ctx->tctx\_list, ctx\_node) {+ struct io\_uring\_task \*tctx = node->task->io\_uring;++ if (WARN\_ON\_ONCE(!tctx->io\_wq))+ continue;++ for (i = 0; i < ARRAY\_SIZE(new\_count); i++)+ new\_count[i] = ctx->iowq\_limits[i];+ /\* ignore errors, it always returns zero anyway \*/+ (void)io\_wq\_max\_workers(tctx->io\_wq, new\_count);+ } return 0;+err:+ if (sqd) {+ mutex\_unlock(&sqd->lock);+ io\_put\_sq\_data(sqd);+ }+ return ret; }  static bool io\_register\_op\_must\_quiesce(int op) { switch (op) {+ case IORING\_REGISTER\_BUFFERS:+ case IORING\_UNREGISTER\_BUFFERS:+ case IORING\_REGISTER\_FILES: case IORING\_UNREGISTER\_FILES: case IORING\_REGISTER\_FILES\_UPDATE: case IORING\_REGISTER\_PROBE: case IORING\_REGISTER\_PERSONALITY: case IORING\_UNREGISTER\_PERSONALITY:+ case IORING\_REGISTER\_FILES2:+ case IORING\_REGISTER\_FILES\_UPDATE2:+ case IORING\_REGISTER\_BUFFERS2:+ case IORING\_REGISTER\_BUFFERS\_UPDATE:+ case IORING\_REGISTER\_IOWQ\_AFF:+ case IORING\_UNREGISTER\_IOWQ\_AFF:+ case IORING\_REGISTER\_IOWQ\_MAX\_WORKERS: return false; default: return true; } } +static int io\_ctx\_quiesce(struct io\_ring\_ctx \*ctx)+{+ long ret;++ percpu\_ref\_kill(&ctx->refs);++ /\*+ \* Drop uring mutex before waiting for references to exit. If another+ \* thread is currently inside io\_uring\_enter() it might need to grab the+ \* uring\_lock to make progress. If we hold it here across the drain+ \* wait, then we can deadlock. It's safe to drop the mutex here, since+ \* no new references will come in after we've killed the percpu ref.+ \*/+ mutex\_unlock(&ctx->uring\_lock);+ do {+ ret = wait\_for\_completion\_interruptible(&ctx->ref\_comp);+ if (!ret)+ break;+ ret = io\_run\_task\_work\_sig();+ } while (ret >= 0);+ mutex\_lock(&ctx->uring\_lock);++ if (ret)+ io\_refs\_resurrect(&ctx->refs, &ctx->ref\_comp);+ return ret;+}+ static int \_\_io\_uring\_register(struct io\_ring\_ctx \*ctx, unsigned opcode, void \_\_user \*arg, unsigned nr\_args) \_\_releases(ctx->uring\_lock)@@ -9775,58 +10727,32 @@ static int \_\_io\_uring\_register(struct io\_ring\_ctx \*ctx, unsigned opcode, if (percpu\_ref\_is\_dying(&ctx->refs)) return -ENXIO; - if (io\_register\_op\_must\_quiesce(opcode)) {- percpu\_ref\_kill(&ctx->refs);-- /\*- \* Drop uring mutex before waiting for references to exit. If- \* another thread is currently inside io\_uring\_enter() it might- \* need to grab the uring\_lock to make progress. If we hold it- \* here across the drain wait, then we can deadlock. It's safe- \* to drop the mutex here, since no new references will come in- \* after we've killed the percpu ref.- \*/- mutex\_unlock(&ctx->uring\_lock);- do {- ret = wait\_for\_completion\_interruptible(&ctx->ref\_comp);- if (!ret)- break;- ret = io\_run\_task\_work\_sig();- if (ret < 0)- break;- } while (1);- mutex\_lock(&ctx->uring\_lock);-- if (ret) {- io\_refs\_resurrect(&ctx->refs, &ctx->ref\_comp);- return ret;- }- }- if (ctx->restricted) {- if (opcode >= IORING\_REGISTER\_LAST) {- ret = -EINVAL;- goto out;- }+ if (opcode >= IORING\_REGISTER\_LAST)+ return -EINVAL;+ opcode = array\_index\_nospec(opcode, IORING\_REGISTER\_LAST);+ if (!test\_bit(opcode, ctx->restrictions.register\_op))+ return -EACCES;+ } - if (!test\_bit(opcode, ctx->restrictions.register\_op)) {- ret = -EACCES;- goto out;- }+ if (io\_register\_op\_must\_quiesce(opcode)) {+ ret = io\_ctx\_quiesce(ctx);+ if (ret)+ return ret; }  switch (opcode) { case IORING\_REGISTER\_BUFFERS:- ret = io\_sqe\_buffer\_register(ctx, arg, nr\_args);+ ret = io\_sqe\_buffers\_register(ctx, arg, nr\_args, NULL); break; case IORING\_UNREGISTER\_BUFFERS: ret = -EINVAL; if (arg || nr\_args) break;- ret = io\_sqe\_buffer\_unregister(ctx);+ ret = io\_sqe\_buffers\_unregister(ctx); break; case IORING\_REGISTER\_FILES:- ret = io\_sqe\_files\_register(ctx, arg, nr\_args);+ ret = io\_sqe\_files\_register(ctx, arg, nr\_args, NULL); break; case IORING\_UNREGISTER\_FILES: ret = -EINVAL;@@ -9835,7 +10761,7 @@ static int \_\_io\_uring\_register(struct io\_ring\_ctx \*ctx, unsigned opcode, ret = io\_sqe\_files\_unregister(ctx); break; case IORING\_REGISTER\_FILES\_UPDATE:- ret = io\_sqe\_files\_update(ctx, arg, nr\_args);+ ret = io\_register\_files\_update(ctx, arg, nr\_args); break; case IORING\_REGISTER\_EVENTFD: case IORING\_REGISTER\_EVENTFD\_ASYNC:@@ -9883,12 +10809,43 @@ static int \_\_io\_uring\_register(struct io\_ring\_ctx \*ctx, unsigned opcode, case IORING\_REGISTER\_RESTRICTIONS: ret = io\_register\_restrictions(ctx, arg, nr\_args); break;+ case IORING\_REGISTER\_FILES2:+ ret = io\_register\_rsrc(ctx, arg, nr\_args, IORING\_RSRC\_FILE);+ break;+ case IORING\_REGISTER\_FILES\_UPDATE2:+ ret = io\_register\_rsrc\_update(ctx, arg, nr\_args,+ IORING\_RSRC\_FILE);+ break;+ case IORING\_REGISTER\_BUFFERS2:+ ret = io\_register\_rsrc(ctx, arg, nr\_args, IORING\_RSRC\_BUFFER);+ break;+ case IORING\_REGISTER\_BUFFERS\_UPDATE:+ ret = io\_register\_rsrc\_update(ctx, arg, nr\_args,+ IORING\_RSRC\_BUFFER);+ break;+ case IORING\_REGISTER\_IOWQ\_AFF:+ ret = -EINVAL;+ if (!arg || !nr\_args)+ break;+ ret = io\_register\_iowq\_aff(ctx, arg, nr\_args);+ break;+ case IORING\_UNREGISTER\_IOWQ\_AFF:+ ret = -EINVAL;+ if (arg || nr\_args)+ break;+ ret = io\_unregister\_iowq\_aff(ctx);+ break;+ case IORING\_REGISTER\_IOWQ\_MAX\_WORKERS:+ ret = -EINVAL;+ if (!arg || nr\_args != 2)+ break;+ ret = io\_register\_iowq\_max\_workers(ctx, arg);+ break; default: ret = -EINVAL; break; } -out: if (io\_register\_op\_must\_quiesce(opcode)) { /\* bring the ctx back to life \*/ percpu\_ref\_reinit(&ctx->refs);@@ -9914,6 +10871,8 @@ SYSCALL\_DEFINE4(io\_uring\_register, unsigned int, fd, unsigned int, opcode,  ctx = f.file->private\_data; + io\_run\_task\_work();+ mutex\_lock(&ctx->uring\_lock); ret = \_\_io\_uring\_register(ctx, opcode, arg, nr\_args); mutex\_unlock(&ctx->uring\_lock);@@ -9960,12 +10919,27 @@ static int \_\_init io\_uring\_init(void) BUILD\_BUG\_SQE\_ELEM(28, \_\_u32, splice\_flags); BUILD\_BUG\_SQE\_ELEM(32, \_\_u64, user\_data); BUILD\_BUG\_SQE\_ELEM(40, \_\_u16, buf\_index);+ BUILD\_BUG\_SQE\_ELEM(40, \_\_u16, buf\_group); BUILD\_BUG\_SQE\_ELEM(42, \_\_u16, personality); BUILD\_BUG\_SQE\_ELEM(44, \_\_s32, splice\_fd\_in);+ BUILD\_BUG\_SQE\_ELEM(44, \_\_u32, file\_index);++ BUILD\_BUG\_ON(sizeof(struct io\_uring\_files\_update) !=+ sizeof(struct io\_uring\_rsrc\_update));+ BUILD\_BUG\_ON(sizeof(struct io\_uring\_rsrc\_update) >+ sizeof(struct io\_uring\_rsrc\_update2));++ /\* ->buf\_index is u16 \*/+ BUILD\_BUG\_ON(IORING\_MAX\_REG\_BUFFERS >= (1u << 16));++ /\* should fit into one byte \*/+ BUILD\_BUG\_ON(SQE\_VALID\_FLAGS >= (1 << 8));  BUILD\_BUG\_ON(ARRAY\_SIZE(io\_op\_defs) != IORING\_OP\_LAST);- BUILD\_BUG\_ON(\_\_REQ\_F\_LAST\_BIT >= 8 \* sizeof(int));- req\_cachep = KMEM\_CACHE(io\_kiocb, SLAB\_HWCACHE\_ALIGN | SLAB\_PANIC);+ BUILD\_BUG\_ON(\_\_REQ\_F\_LAST\_BIT > 8 \* sizeof(int));++ req\_cachep = KMEM\_CACHE(io\_kiocb, SLAB\_HWCACHE\_ALIGN | SLAB\_PANIC |+ SLAB\_ACCOUNT); return 0; }; \_\_initcall(io\_uring\_init);diff --git a/kernel/exit.c b/kernel/exit.cindex ab900b661867f6..8989e1d1f79b78 100644--- a/[kernel/exit.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/kernel/exit.c?id=ed3005032993da7a3fe2e6095436e0bc2e83d011)+++ b/[kernel/exit.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/kernel/exit.c?id=788d0824269bef539fe31a785b1517882eafed93)@@ -763,7 +763,7 @@ void \_\_noreturn do\_exit(long code) schedule(); } - io\_uring\_files\_cancel(tsk->files);+ io\_uring\_files\_cancel(); exit\_signals(tsk); /\* sets PF\_EXITING \*/  /\* sync mm's RSS info before statistics gathering \*/diff --git a/kernel/fork.c b/kernel/fork.cindex 55c1a880a281ae..68efe2a0b4fbc8 100644--- a/[kernel/fork.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/kernel/fork.c?id=ed3005032993da7a3fe2e6095436e0bc2e83d011)+++ b/[kernel/fork.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/kernel/fork.c?id=788d0824269bef539fe31a785b1517882eafed93)@@ -926,6 +926,7 @@ static struct task\_struct \*dup\_task\_struct(struct task\_struct \*orig, int node) tsk->splice\_pipe = NULL; tsk->task\_frag.page = NULL; tsk->wake\_q.next = NULL;+ tsk->pf\_io\_worker = NULL;  account\_kernel\_stack(tsk, 1); diff --git a/kernel/sched/core.c b/kernel/sched/core.cindex da96a309eefed7..a875bc59804eb7 100644--- a/[kernel/sched/core.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/kernel/sched/core.c?id=ed3005032993da7a3fe2e6095436e0bc2e83d011)+++ b/[kernel/sched/core.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/kernel/sched/core.c?id=788d0824269bef539fe31a785b1517882eafed93)@@ -21,7 +21,7 @@ #include <asm/tlb.h>  #include "../workqueue\_internal.h"-#include "../../fs/io-wq.h"+#include "../../io\_uring/io-wq.h" #include "../smpboot.h"  #include "pelt.h" |
| --- |

generated by [cgit 1.2.3-korg](https://git.zx2c4.com/cgit/about/) ([git 2.43.0](https://git-scm.com/)) at 2025-01-14 20:01:06 +0000



=== Content from security.netapp.com_a024aa58_20250115_115550.html ===

[Skip to main content](#n-main-content)

* [NetApp.com](https://www.netapp.com/)
* [Support](https://mysupport.netapp.com)
* [Community](https://community.netapp.com)
* [Training](https://www.netapp.com/support-and-training/netapp-learning-services/)

* [Contact Us](https://www.netapp.com/company/contact-us/)

English
æ¥æ¬èª

[netapp-mark

NetApp

## Product Security](https://security.netapp.com)

Search

Search

* Search

Search

Search

* [Home](https://security.netapp.com/en)
* [Advisories](https://security.netapp.com/advisory/)
* [Bulletins](https://security.netapp.com/bulletins/)
* [Contact](https://security.netapp.com/contact/)
* [Policy](https://security.netapp.com/policy/)
* [Resources](https://security.netapp.com/resources/)
* [Certifications](https://security.netapp.com/certs/)

* [Home](https://security.netapp.com/en)
* [Advisory](https://security.netapp.com/advisory)
* [CVE-2023-1295 Linux Kernel Vulnerability in NetApp Products](https://security.netapp.com/advisory/ntap-20230731-0006)

## CVE-2023-1295 Linux Kernel Vulnerability in NetApp Products

circle-info

NetApp will continue to update this advisory as additional information becomes available.

This advisory should be considered the single source of current, up-to-date, authorized and accurate information from NetApp regarding Full Support products and versions.

close ×

#### Subscribe to NTAP-20230731-0006 updates

Email

Yes, please send me emails when NetApp Security Advisories are posted or updated.

 By filling and submitting this form, I understand and agree with the [NetApp privacy policy](https://www.netapp.com/company/legal/privacy-policy/ "Privacy Policy") and understand that I can unsubscribe from NetApp Security Advisory communications at any time.

Subscribe

#### Subscribe to NTAP-20230731-0006 advisory updates

OTP

Confirm

ionicons-v5-e

Confirmed your subscription to advisory alerts

close ×

#### Unsubscribe from NTAP-20230731-0006 advisory updates

Email

Unsubscribe

#### Unsubscribe from NTAP-20230731-0006 advisory updates

Email

Confirm

ionicons-v5-e

Unsubscribed successfully from advisory alerts

Subscribe to receive email updates

**Advisory ID:** NTAP-20230731-0006
**Version:**
9.0

**Last updated:**
10/18/2024

**Status:**
Interim.

**CVEs:** CVE-2023-1295

Overview
#### Summary

Multiple NetApp products incorporate Linux kernel. Linux kernel versions prior to 5.12-rc2 are susceptible to a vulnerability which when successfully exploited could lead to disclosure of sensitive information, addition or modification of data, or Denial of Service (DoS).

#### Impact

Successful exploitation of this vulnerability could lead to disclosure of sensitive information, addition or modification of data, or Denial of Service (DoS).

#### Vulnerability Scoring Details

| **CVE** | **Score** | **Vector** |
| --- | --- | --- |
| [CVE-2023-1295](https://nvd.nist.gov/vuln/detail/CVE-2023-1295) | 7.0 (HIGH) | CVSS:3.1/AV:L/AC:H/PR:L/UI:N/S:U/C:H/I:H/A:H |

#### Exploitation and Public Announcements

NetApp is aware of public discussion of this vulnerability.

Affected Products
#### Affected Products

* NetApp HCI Baseboard Management Controller (BMC) - H300S/H500S/H700S/H410S
* NetApp HCI Baseboard Management Controller (BMC) - H410C

#### Products Under Investigation

* NetApp HCI Compute Node (Bootstrap OS)

#### Products Not Affected

* AFF BIOS - A700s
* AFF Baseboard Management Controller (BMC) - A700s
* ATTO FibreBridge - 7600N
* Active IQ Unified Manager for Linux
* Active IQ Unified Manager for Microsoft Windows
* Active IQ Unified Manager for VMware vSphere
* Active IQ mobile app
* Astra Control Center
* Astra Control Center - NetApp Kubernetes Monitoring Operator
* Astra Trident
* Astra Trident Autosupport
* BlueXP Classification
* Brocade Fabric Operating System Firmware
* Brocade SAN Navigator (SANnav)
* Cloud Volumes ONTAP Mediator
* Data Infrastructure Insights Acquisition Unit (formerly Cloud Insights Acquisition Unit)
* Data Infrastructure Insights Storage Workload Security Agent (formerly Cloud Insights Storage Workload Security Agent)
* Data Infrastructure Insights Telegraf Agent (formerly Cloud Insights Telegraf Agent)
* E-Series BIOS
* E-Series SANtricity OS Controller Software 11.x
* E-Series SANtricity Unified Manager and Web Services Proxy
* Element .NET SDK
* Element HealthTools
* Element JAVA SDK
* Element Plug-in for vCenter Server
* Element Powershell Tools
* Element Python SDK
* FAS/AFF BIOS - 2820
* FAS/AFF BIOS - 8300/8700/A400/C400
* FAS/AFF BIOS - A250/500f/C250
* FAS/AFF BIOS - A300/8200/C190/A220/2720/2750/A150
* FAS/AFF BIOS - A320
* FAS/AFF BIOS - A700/9000
* FAS/AFF BIOS - A800/C800
* FAS/AFF BIOS - A900/9500
* FAS/AFF Baseboard Management Controller (BMC) - 8300/8700/A400/C400
* FAS/AFF Baseboard Management Controller (BMC) - A250/500f/C250
* FAS/AFF Baseboard Management Controller (BMC) - A900/9500
* FAS/AFF Baseboard Management Controller (BMC) - C190/A150/A220/FAS2720/FAS2750
* FAS/AFF Baseboard Management Controller (BMC) - FAS2820
* FAS/AFF Service Processor - A300/8200
* FAS/AFF Service Processor - A700/9000
* Global File Cache
* Host Utilities - SAN for Linux
* Host Utilities - SAN for Windows
* IOM12 SAS Disk Shelf Firmware
* IOM12B SAS Disk Shelf Firmware
* IOM12E SAS Disk Shelf Firmware
* IOM12F SAS Disk Shelf Firmware
* IOM12G SAS Disk Shelf Firmware
* IOM6 SAS Disk Shelf Firmware
* IOM6E SAS Disk Shelf Firmware
* Management Services for Element Software and NetApp HCI
* MetroCluster DataCollector
* MetroCluster Tiebreaker
* Multipath I/O (SANtricity DSM for Windows MPIO)
* NetApp BlueXP
* NetApp Converged Systems Advisor Agent
* NetApp E-Series Host Collection
* NetApp E-Series SANtricity Collection
* NetApp HCI Baseboard Management Controller (BMC) - H610C
* NetApp HCI Baseboard Management Controller (BMC) - H610S
* NetApp HCI Baseboard Management Controller (BMC) - H615C
* NetApp HCI Compute Node BIOS
* NetApp HCI Storage Node BIOS
* NetApp Kubernetes Monitoring Operator
* NetApp Manageability SDK
* NetApp NFS Plug-in for VMware VAAI
* NetApp ONTAP PowerShell Toolkit (PSTK)
* NetApp SolidFire & HCI Management Node
* NetApp SolidFire & HCI Storage Node (Element Software)
* NetApp SolidFire Plug-in for vRealize Orchestrator (SolidFire vRO)
* NetApp XCP NFS
* NetApp XCP SMB
* ONTAP 9 (formerly Clustered Data ONTAP)
* ONTAP Antivirus Connector
* ONTAP Mediator
* ONTAP Select Deploy administration utility
* ONTAP tools for VMware vSphere 9
* OnCommand Insight
* OnCommand Workflow Automation
* RcfFileGenerator for MetroCluster IP
* SANtricity Storage Plugin for vCenter
* SRA Plugin for Linux
* SRA Plugin for Windows
* Single Mailbox Recovery
* Snap Creator Framework
* SnapCenter
* SnapCenter Plug-in for VMware vSphere/BlueXP backup and Recovery for Virtual Machine
* SnapManager for Hyper-V
* SolidFire Storage Replication Adapter
* StorageGRID (formerly StorageGRID Webscale)
* StorageGRID BIOS SG1000/SG100/SG1100/SG110
* StorageGRID BIOS SG5660/SG5612/SG5760/SG5712
* StorageGRID BIOS SG6060/SGF6024/SGF6112/SG6160
* StorageGRID Baseboard Management Controller (BMC) - SG6060/SG6160/SGF6024/SGF6112/SG100/SG110/SG1000/SG1100
* System Manager 9.x

Remediation
#### Software Versions and Fixes

NetApp's currently available patches are listed below.

| **Product** | **First Fixed in Release** |
| --- | --- |
| **NetApp HCI Baseboard Management Controller (BMC) - H300S/H500S/H700S/H410S** | NetApp HCI Baseboard Management Controller (BMC) - H300S/H500S/H700S/H410S has no plans to address this vulnerability. See the [EOA announcement](https://mysupport.netapp.com/info/communications/ECMLP2876227.html) for more information. |
| **NetApp HCI Baseboard Management Controller (BMC) - H410C** | NetApp HCI Baseboard Management Controller (BMC) - H410C has no plans to address this vulnerability. See the [EOA announcement](https://mysupport.netapp.com/info/communications/ECMLP2876227.html) for more information. |

#### Workarounds

None at this time.

#### Obtaining Software Fixes

Software fixes will be made available through the NetApp Support website in the Software Download section.

<https://mysupport.netapp.com/site/downloads/>

Customers who do not have access to the Support website should contact Technical Support at the number below to obtain the patches.

#### Contact Information

Check <http://mysupport.netapp.com> for further
updates.

**Technical Support**

Revision History
#### Status of This Notice

**Interim.**

NetApp will continue to update this advisory as additional information becomes available.

This advisory should be considered the single source of current, up-to-date, authorized and accurate information from NetApp regarding Full Support products and versions.

This advisory is posted at the following link:

<https://security.netapp.com/advisory/NTAP-20230731-0006>
#### Revision History

| **Revision #** | **Date** | **Comments** |
| --- | --- | --- |
| 1.0 | 20230731 | Initial Public Release |
| 2.0 | 20230801 | E-Series SANtricity OS Controller Software 11.x moved to Products Not Affected |
| 3.0 | 20230808 | Brocade Fabric Operating System Firmware, FAS/AFF Baseboard Management Controller (BMC) - 8300/8700/A400/C400, FAS/AFF Baseboard Management Controller (BMC) - A900/9500, FAS/AFF Baseboard Management Controller (BMC) - A320/C190/A220/FAS2720/FAS2750/A800/C800/A150, FAS/AFF Baseboard Management Controller (BMC) - FAS2820, FAS/AFF Service Processor - A300/8200/A200/2650/2620, FAS/AFF Service Processor - A700/9000, FAS/AFF Baseboard Management Controller (BMC) - A250/500f/C250 moved to Products Not Affected |
| 4.0 | 20230810 | ONTAP tools for VMware vSphere moved to Products Not Affected |
| 5.0 | 20230821 | NetApp SolidFire & HCI Storage Node (Element Software) and NetApp SolidFire & HCI Management Node moved to Products Not Affected |
| 6.0 | 20230828 | SnapCenter Plug-in for VMware vSphere moved to Products Not Affected |
| 7.0 | 20231101 | NetApp HCI Baseboard Management Controller (BMC) - H300S/H500S/H700S/H410S and NetApp HCI Baseboard Management Controller (BMC) - H410C moved to Won't Fix status |
| 8.0 | 20231130 | NetApp HCI Baseboard Management Controller (BMC) - H615C, NetApp HCI Baseboard Management Controller (BMC) - H610C, and NetApp HCI Baseboard Management Controller (BMC) - H610S moved to Products Not Affected |
| 9.0 | 20241018 | AFF Baseboard Management Controller (BMC) - A700s moved to Products Not Affected |

This document is provided solely for informational purposes. All information is based upon NetAppâs current knowledge and understanding of the hardware and software products tested by NetApp, and the methodology and assumptions used by NetApp. NetApp is not responsible for any errors or omissions that may be contained herein, and no warranty, representation, or other legal commitment or obligation is being provided by NetApp. Â© 2025 NetApp, Inc. All rights reserved. No portions of this document may be reproduced without prior written consent of NetApp, Inc.

 ©  NetApp

Have feedback for our website?
[Let us know](https://www.netapp.com/forms/site-feedback/)



=== Content from security.netapp.com_c69dfadc_20250114_200236.html ===

[Skip to main content](#n-main-content)

* [NetApp.com](https://www.netapp.com/)
* [Support](https://mysupport.netapp.com)
* [Community](https://community.netapp.com)
* [Training](https://www.netapp.com/support-and-training/netapp-learning-services/)

* [Contact Us](https://www.netapp.com/company/contact-us/)

English
æ¥æ¬èª

[netapp-mark

NetApp

## Product Security](https://security.netapp.com)

Search

Search

* Search

Search

Search

* [Home](https://security.netapp.com/en)
* [Advisories](https://security.netapp.com/advisory/)
* [Bulletins](https://security.netapp.com/bulletins/)
* [Contact](https://security.netapp.com/contact/)
* [Policy](https://security.netapp.com/policy/)
* [Resources](https://security.netapp.com/resources/)
* [Certifications](https://security.netapp.com/certs/)

* [Home](https://security.netapp.com/en)
* [Advisory](https://security.netapp.com/advisory)
* [CVE-2023-1295 Linux Kernel Vulnerability in NetApp Products](https://security.netapp.com/advisory/ntap-20230731-0006)

## CVE-2023-1295 Linux Kernel Vulnerability in NetApp Products

circle-info

NetApp will continue to update this advisory as additional information becomes available.

This advisory should be considered the single source of current, up-to-date, authorized and accurate information from NetApp regarding Full Support products and versions.

close ×

#### Subscribe to NTAP-20230731-0006 updates

Email

Yes, please send me emails when NetApp Security Advisories are posted or updated.

 By filling and submitting this form, I understand and agree with the [NetApp privacy policy](https://www.netapp.com/company/legal/privacy-policy/ "Privacy Policy") and understand that I can unsubscribe from NetApp Security Advisory communications at any time.

Subscribe

#### Subscribe to NTAP-20230731-0006 advisory updates

OTP

Confirm

ionicons-v5-e

Confirmed your subscription to advisory alerts

close ×

#### Unsubscribe from NTAP-20230731-0006 advisory updates

Email

Unsubscribe

#### Unsubscribe from NTAP-20230731-0006 advisory updates

Email

Confirm

ionicons-v5-e

Unsubscribed successfully from advisory alerts

Subscribe to receive email updates

**Advisory ID:** NTAP-20230731-0006
**Version:**
9.0

**Last updated:**
10/18/2024

**Status:**
Interim.

**CVEs:** CVE-2023-1295

Overview
#### Summary

Multiple NetApp products incorporate Linux kernel. Linux kernel versions prior to 5.12-rc2 are susceptible to a vulnerability which when successfully exploited could lead to disclosure of sensitive information, addition or modification of data, or Denial of Service (DoS).

#### Impact

Successful exploitation of this vulnerability could lead to disclosure of sensitive information, addition or modification of data, or Denial of Service (DoS).

#### Vulnerability Scoring Details

| **CVE** | **Score** | **Vector** |
| --- | --- | --- |
| [CVE-2023-1295](https://nvd.nist.gov/vuln/detail/CVE-2023-1295) | 7.0 (HIGH) | CVSS:3.1/AV:L/AC:H/PR:L/UI:N/S:U/C:H/I:H/A:H |

#### Exploitation and Public Announcements

NetApp is aware of public discussion of this vulnerability.

Affected Products
#### Affected Products

* NetApp HCI Baseboard Management Controller (BMC) - H300S/H500S/H700S/H410S
* NetApp HCI Baseboard Management Controller (BMC) - H410C

#### Products Under Investigation

* NetApp HCI Compute Node (Bootstrap OS)

#### Products Not Affected

* AFF BIOS - A700s
* AFF Baseboard Management Controller (BMC) - A700s
* ATTO FibreBridge - 7600N
* Active IQ Unified Manager for Linux
* Active IQ Unified Manager for Microsoft Windows
* Active IQ Unified Manager for VMware vSphere
* Active IQ mobile app
* Astra Control Center
* Astra Control Center - NetApp Kubernetes Monitoring Operator
* Astra Trident
* Astra Trident Autosupport
* BlueXP Classification
* Brocade Fabric Operating System Firmware
* Brocade SAN Navigator (SANnav)
* Cloud Volumes ONTAP Mediator
* Data Infrastructure Insights Acquisition Unit (formerly Cloud Insights Acquisition Unit)
* Data Infrastructure Insights Storage Workload Security Agent (formerly Cloud Insights Storage Workload Security Agent)
* Data Infrastructure Insights Telegraf Agent (formerly Cloud Insights Telegraf Agent)
* E-Series BIOS
* E-Series SANtricity OS Controller Software 11.x
* E-Series SANtricity Unified Manager and Web Services Proxy
* Element .NET SDK
* Element HealthTools
* Element JAVA SDK
* Element Plug-in for vCenter Server
* Element Powershell Tools
* Element Python SDK
* FAS/AFF BIOS - 2820
* FAS/AFF BIOS - 8300/8700/A400/C400
* FAS/AFF BIOS - A250/500f/C250
* FAS/AFF BIOS - A300/8200/C190/A220/2720/2750/A150
* FAS/AFF BIOS - A320
* FAS/AFF BIOS - A700/9000
* FAS/AFF BIOS - A800/C800
* FAS/AFF BIOS - A900/9500
* FAS/AFF Baseboard Management Controller (BMC) - 8300/8700/A400/C400
* FAS/AFF Baseboard Management Controller (BMC) - A250/500f/C250
* FAS/AFF Baseboard Management Controller (BMC) - A900/9500
* FAS/AFF Baseboard Management Controller (BMC) - C190/A150/A220/FAS2720/FAS2750
* FAS/AFF Baseboard Management Controller (BMC) - FAS2820
* FAS/AFF Service Processor - A300/8200
* FAS/AFF Service Processor - A700/9000
* Global File Cache
* Host Utilities - SAN for Linux
* Host Utilities - SAN for Windows
* IOM12 SAS Disk Shelf Firmware
* IOM12B SAS Disk Shelf Firmware
* IOM12E SAS Disk Shelf Firmware
* IOM12F SAS Disk Shelf Firmware
* IOM12G SAS Disk Shelf Firmware
* IOM6 SAS Disk Shelf Firmware
* IOM6E SAS Disk Shelf Firmware
* Management Services for Element Software and NetApp HCI
* MetroCluster DataCollector
* MetroCluster Tiebreaker
* Multipath I/O (SANtricity DSM for Windows MPIO)
* NetApp BlueXP
* NetApp Converged Systems Advisor Agent
* NetApp E-Series Host Collection
* NetApp E-Series SANtricity Collection
* NetApp HCI Baseboard Management Controller (BMC) - H610C
* NetApp HCI Baseboard Management Controller (BMC) - H610S
* NetApp HCI Baseboard Management Controller (BMC) - H615C
* NetApp HCI Compute Node BIOS
* NetApp HCI Storage Node BIOS
* NetApp Kubernetes Monitoring Operator
* NetApp Manageability SDK
* NetApp NFS Plug-in for VMware VAAI
* NetApp ONTAP PowerShell Toolkit (PSTK)
* NetApp SolidFire & HCI Management Node
* NetApp SolidFire & HCI Storage Node (Element Software)
* NetApp SolidFire Plug-in for vRealize Orchestrator (SolidFire vRO)
* NetApp XCP NFS
* NetApp XCP SMB
* ONTAP 9 (formerly Clustered Data ONTAP)
* ONTAP Antivirus Connector
* ONTAP Mediator
* ONTAP Select Deploy administration utility
* ONTAP tools for VMware vSphere 9
* OnCommand Insight
* OnCommand Workflow Automation
* RcfFileGenerator for MetroCluster IP
* SANtricity Storage Plugin for vCenter
* SRA Plugin for Linux
* SRA Plugin for Windows
* Single Mailbox Recovery
* Snap Creator Framework
* SnapCenter
* SnapCenter Plug-in for VMware vSphere/BlueXP backup and Recovery for Virtual Machine
* SnapManager for Hyper-V
* SolidFire Storage Replication Adapter
* StorageGRID (formerly StorageGRID Webscale)
* StorageGRID BIOS SG1000/SG100/SG1100/SG110
* StorageGRID BIOS SG5660/SG5612/SG5760/SG5712
* StorageGRID BIOS SG6060/SGF6024/SGF6112/SG6160
* StorageGRID Baseboard Management Controller (BMC) - SG6060/SG6160/SGF6024/SGF6112/SG100/SG110/SG1000/SG1100
* System Manager 9.x

Remediation
#### Software Versions and Fixes

NetApp's currently available patches are listed below.

| **Product** | **First Fixed in Release** |
| --- | --- |
| **NetApp HCI Baseboard Management Controller (BMC) - H300S/H500S/H700S/H410S** | NetApp HCI Baseboard Management Controller (BMC) - H300S/H500S/H700S/H410S has no plans to address this vulnerability. See the [EOA announcement](https://mysupport.netapp.com/info/communications/ECMLP2876227.html) for more information. |
| **NetApp HCI Baseboard Management Controller (BMC) - H410C** | NetApp HCI Baseboard Management Controller (BMC) - H410C has no plans to address this vulnerability. See the [EOA announcement](https://mysupport.netapp.com/info/communications/ECMLP2876227.html) for more information. |

#### Workarounds

None at this time.

#### Obtaining Software Fixes

Software fixes will be made available through the NetApp Support website in the Software Download section.

<https://mysupport.netapp.com/site/downloads/>

Customers who do not have access to the Support website should contact Technical Support at the number below to obtain the patches.

#### Contact Information

Check <http://mysupport.netapp.com> for further
updates.

**Technical Support**

Revision History
#### Status of This Notice

**Interim.**

NetApp will continue to update this advisory as additional information becomes available.

This advisory should be considered the single source of current, up-to-date, authorized and accurate information from NetApp regarding Full Support products and versions.

This advisory is posted at the following link:

<https://security.netapp.com/advisory/NTAP-20230731-0006>
#### Revision History

| **Revision #** | **Date** | **Comments** |
| --- | --- | --- |
| 1.0 | 20230731 | Initial Public Release |
| 2.0 | 20230801 | E-Series SANtricity OS Controller Software 11.x moved to Products Not Affected |
| 3.0 | 20230808 | Brocade Fabric Operating System Firmware, FAS/AFF Baseboard Management Controller (BMC) - 8300/8700/A400/C400, FAS/AFF Baseboard Management Controller (BMC) - A900/9500, FAS/AFF Baseboard Management Controller (BMC) - A320/C190/A220/FAS2720/FAS2750/A800/C800/A150, FAS/AFF Baseboard Management Controller (BMC) - FAS2820, FAS/AFF Service Processor - A300/8200/A200/2650/2620, FAS/AFF Service Processor - A700/9000, FAS/AFF Baseboard Management Controller (BMC) - A250/500f/C250 moved to Products Not Affected |
| 4.0 | 20230810 | ONTAP tools for VMware vSphere moved to Products Not Affected |
| 5.0 | 20230821 | NetApp SolidFire & HCI Storage Node (Element Software) and NetApp SolidFire & HCI Management Node moved to Products Not Affected |
| 6.0 | 20230828 | SnapCenter Plug-in for VMware vSphere moved to Products Not Affected |
| 7.0 | 20231101 | NetApp HCI Baseboard Management Controller (BMC) - H300S/H500S/H700S/H410S and NetApp HCI Baseboard Management Controller (BMC) - H410C moved to Won't Fix status |
| 8.0 | 20231130 | NetApp HCI Baseboard Management Controller (BMC) - H615C, NetApp HCI Baseboard Management Controller (BMC) - H610C, and NetApp HCI Baseboard Management Controller (BMC) - H610S moved to Products Not Affected |
| 9.0 | 20241018 | AFF Baseboard Management Controller (BMC) - A700s moved to Products Not Affected |

This document is provided solely for informational purposes. All information is based upon NetAppâs current knowledge and understanding of the hardware and software products tested by NetApp, and the methodology and assumptions used by NetApp. NetApp is not responsible for any errors or omissions that may be contained herein, and no warranty, representation, or other legal commitment or obligation is being provided by NetApp. Â© 2025 NetApp, Inc. All rights reserved. No portions of this document may be reproduced without prior written consent of NetApp, Inc.

 ©  NetApp

Have feedback for our website?
[Let us know](https://www.netapp.com/forms/site-feedback/)


