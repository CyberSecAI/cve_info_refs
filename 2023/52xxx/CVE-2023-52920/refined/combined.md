=== Content from git.kernel.org_12fb2890_20250115_080214.html ===


| [cgit logo](/) | [index](/) : [kernel/git/stable/linux.git](/pub/scm/linux/kernel/git/stable/linux.git/) | linux-2.6.11.y linux-2.6.12.y linux-2.6.13.y linux-2.6.14.y linux-2.6.15.y linux-2.6.16.y linux-2.6.17.y linux-2.6.18.y linux-2.6.19.y linux-2.6.20.y linux-2.6.21.y linux-2.6.22.y linux-2.6.23.y linux-2.6.24.y linux-2.6.25.y linux-2.6.26.y linux-2.6.27.y linux-2.6.28.y linux-2.6.29.y linux-2.6.30.y linux-2.6.31.y linux-2.6.32.y linux-2.6.33.y linux-2.6.34.y linux-2.6.35.y linux-2.6.36.y linux-2.6.37.y linux-2.6.38.y linux-2.6.39.y linux-3.0.y linux-3.1.y linux-3.10.y linux-3.11.y linux-3.12.y linux-3.13.y linux-3.14.y linux-3.15.y linux-3.16.y linux-3.17.y linux-3.18.y linux-3.19.y linux-3.2.y linux-3.3.y linux-3.4.y linux-3.5.y linux-3.6.y linux-3.7.y linux-3.8.y linux-3.9.y linux-4.0.y linux-4.1.y linux-4.10.y linux-4.11.y linux-4.12.y linux-4.13.y linux-4.14.y linux-4.15.y linux-4.16.y linux-4.17.y linux-4.18.y linux-4.19.y linux-4.2.y linux-4.20.y linux-4.3.y linux-4.4.y linux-4.5.y linux-4.6.y linux-4.7.y linux-4.8.y linux-4.9.y linux-5.0.y linux-5.1.y linux-5.10.y linux-5.11.y linux-5.12.y linux-5.13.y linux-5.14.y linux-5.15.y linux-5.16.y linux-5.17.y linux-5.18.y linux-5.19.y linux-5.2.y linux-5.3.y linux-5.4.y linux-5.5.y linux-5.6.y linux-5.7.y linux-5.8.y linux-5.9.y linux-6.0.y linux-6.1.y linux-6.10.y linux-6.11.y linux-6.12.y linux-6.2.y linux-6.3.y linux-6.4.y linux-6.5.y linux-6.6.y linux-6.7.y linux-6.8.y linux-6.9.y linux-rolling-lts linux-rolling-stable master |
| --- | --- | --- |
| Linux kernel stable tree | Stable Group |

| [about](/pub/scm/linux/kernel/git/stable/linux.git/about/)[summary](/pub/scm/linux/kernel/git/stable/linux.git/)[refs](/pub/scm/linux/kernel/git/stable/linux.git/refs/?id=ecc2aeeaa08a355d84d3ca9c3d2512399a194f29)[log](/pub/scm/linux/kernel/git/stable/linux.git/log/)[tree](/pub/scm/linux/kernel/git/stable/linux.git/tree/?id=ecc2aeeaa08a355d84d3ca9c3d2512399a194f29)[commit](/pub/scm/linux/kernel/git/stable/linux.git/commit/?id=ecc2aeeaa08a355d84d3ca9c3d2512399a194f29)[diff](/pub/scm/linux/kernel/git/stable/linux.git/diff/?id=ecc2aeeaa08a355d84d3ca9c3d2512399a194f29)[stats](/pub/scm/linux/kernel/git/stable/linux.git/stats/) | log msg author committer range |
| --- | --- |

**diff options**

|  | |
| --- | --- |
| context: | 12345678910152025303540 |
| space: | includeignore |
| mode: | unifiedssdiffstat only |
|  |  |

| author | Andrii Nakryiko <andrii@kernel.org> | 2024-11-26 15:37:07 +0800 |
| --- | --- | --- |
| committer | Greg Kroah-Hartman <gregkh@linuxfoundation.org> | 2024-12-09 10:31:42 +0100 |
| commit | [ecc2aeeaa08a355d84d3ca9c3d2512399a194f29](/pub/scm/linux/kernel/git/stable/linux.git/commit/?id=ecc2aeeaa08a355d84d3ca9c3d2512399a194f29) ([patch](/pub/scm/linux/kernel/git/stable/linux.git/patch/?id=ecc2aeeaa08a355d84d3ca9c3d2512399a194f29)) | |
| tree | [2a93750f6892aff2c465b0b6b150989267f058af](/pub/scm/linux/kernel/git/stable/linux.git/tree/?id=ecc2aeeaa08a355d84d3ca9c3d2512399a194f29) | |
| parent | [44b1bfb5bd97973ccb8d1d105a08470fc36812d8](/pub/scm/linux/kernel/git/stable/linux.git/commit/?id=44b1bfb5bd97973ccb8d1d105a08470fc36812d8) ([diff](/pub/scm/linux/kernel/git/stable/linux.git/diff/?id=ecc2aeeaa08a355d84d3ca9c3d2512399a194f29&id2=44b1bfb5bd97973ccb8d1d105a08470fc36812d8)) | |
| download | [linux-ecc2aeeaa08a355d84d3ca9c3d2512399a194f29.tar.gz](/pub/scm/linux/kernel/git/stable/linux.git/snapshot/linux-ecc2aeeaa08a355d84d3ca9c3d2512399a194f29.tar.gz) | |

bpf: support non-r10 register spill/fill to/from stack in precision tracking[ Upstream commit 41f6f64e6999a837048b1bd13a2f8742964eca6b ]
Use instruction (jump) history to record instructions that performed
register spill/fill to/from stack, regardless if this was done through
read-only r10 register, or any other register after copying r10 into it
\*and\* potentially adjusting offset.
To make this work reliably, we push extra per-instruction flags into
instruction history, encoding stack slot index (spi) and stack frame
number in extra 10 bit flags we take away from prev\_idx in instruction
history. We don't touch idx field for maximum performance, as it's
checked most frequently during backtracking.
This change removes basically the last remaining practical limitation of
precision backtracking logic in BPF verifier. It fixes known
deficiencies, but also opens up new opportunities to reduce number of
verified states, explored in the subsequent patches.
There are only three differences in selftests' BPF object files
according to veristat, all in the positive direction (less states).
File Program Insns (A) Insns (B) Insns (DIFF) States (A) States (B) States (DIFF)
-------------------------------------- ------------- --------- --------- ------------- ---------- ---------- -------------
test\_cls\_redirect\_dynptr.bpf.linked3.o cls\_redirect 2987 2864 -123 (-4.12%) 240 231 -9 (-3.75%)
xdp\_synproxy\_kern.bpf.linked3.o syncookie\_tc 82848 82661 -187 (-0.23%) 5107 5073 -34 (-0.67%)
xdp\_synproxy\_kern.bpf.linked3.o syncookie\_xdp 85116 84964 -152 (-0.18%) 5162 5130 -32 (-0.62%)
Note, I avoided renaming jmp\_history to more generic insn\_hist to
minimize number of lines changed and potential merge conflicts between
bpf and bpf-next trees.
Notice also cur\_hist\_entry pointer reset to NULL at the beginning of
instruction verification loop. This pointer avoids the problem of
relying on last jump history entry's insn\_idx to determine whether we
already have entry for current instruction or not. It can happen that we
added jump history entry because current instruction is\_jmp\_point(), but
also we need to add instruction flags for stack access. In this case, we
don't want to entries, so we need to reuse last added entry, if it is
present.
Relying on insn\_idx comparison has the same ambiguity problem as the one
that was fixed recently in [0], so we avoid that.
[0] https://patchwork.kernel.org/project/netdevbpf/patch/20231110002638.4168352-3-andrii@kernel.org/
Acked-by: Eduard Zingerman <eddyz87@gmail.com>
Reported-by: Tao Lyu <tao.lyu@epfl.ch>
Signed-off-by: Andrii Nakryiko <andrii@kernel.org>
Link: [https://lore.kernel.org/r/20231205184248.1502704-2-andrii@kernel.org](https://lore.kernel.org/r/20231205184248.1502704-2-andrii%40kernel.org)
Signed-off-by: Alexei Starovoitov <ast@kernel.org>
Signed-off-by: Shung-Hsi Yu <shung-hsi.yu@suse.com>
Signed-off-by: Sasha Levin <sashal@kernel.org>
[Diffstat](/pub/scm/linux/kernel/git/stable/linux.git/diff/?id=ecc2aeeaa08a355d84d3ca9c3d2512399a194f29)

| -rw-r--r-- | [include/linux/bpf\_verifier.h](/pub/scm/linux/kernel/git/stable/linux.git/diff/include/linux/bpf_verifier.h?id=ecc2aeeaa08a355d84d3ca9c3d2512399a194f29) | 31 | |  |  |  | | --- | --- | --- | |
| --- | --- | --- | --- | --- | --- | --- |
| -rw-r--r-- | [kernel/bpf/verifier.c](/pub/scm/linux/kernel/git/stable/linux.git/diff/kernel/bpf/verifier.c?id=ecc2aeeaa08a355d84d3ca9c3d2512399a194f29) | 175 | |  |  |  | | --- | --- | --- | |
| -rw-r--r-- | [tools/testing/selftests/bpf/progs/verifier\_subprog\_precision.c](/pub/scm/linux/kernel/git/stable/linux.git/diff/tools/testing/selftests/bpf/progs/verifier_subprog_precision.c?id=ecc2aeeaa08a355d84d3ca9c3d2512399a194f29) | 23 | |  |  |  | | --- | --- | --- | |
| -rw-r--r-- | [tools/testing/selftests/bpf/verifier/precise.c](/pub/scm/linux/kernel/git/stable/linux.git/diff/tools/testing/selftests/bpf/verifier/precise.c?id=ecc2aeeaa08a355d84d3ca9c3d2512399a194f29) | 38 | |  |  |  | | --- | --- | --- | |

4 files changed, 169 insertions, 98 deletions

| diff --git a/include/linux/bpf\_verifier.h b/include/linux/bpf\_verifier.hindex 92919d52f7e1b2..cb8e97665eaa59 100644--- a/[include/linux/bpf\_verifier.h](/pub/scm/linux/kernel/git/stable/linux.git/tree/include/linux/bpf_verifier.h?id=44b1bfb5bd97973ccb8d1d105a08470fc36812d8)+++ b/[include/linux/bpf\_verifier.h](/pub/scm/linux/kernel/git/stable/linux.git/tree/include/linux/bpf_verifier.h?id=ecc2aeeaa08a355d84d3ca9c3d2512399a194f29)@@ -319,12 +319,34 @@ struct bpf\_func\_state { struct bpf\_stack\_state \*stack; }; -struct bpf\_idx\_pair {- u32 prev\_idx;+#define MAX\_CALL\_FRAMES 8++/\* instruction history flags, used in bpf\_jmp\_history\_entry.flags field \*/+enum {+ /\* instruction references stack slot through PTR\_TO\_STACK register;+ \* we also store stack's frame number in lower 3 bits (MAX\_CALL\_FRAMES is 8)+ \* and accessed stack slot's index in next 6 bits (MAX\_BPF\_STACK is 512,+ \* 8 bytes per slot, so slot index (spi) is [0, 63])+ \*/+ INSN\_F\_FRAMENO\_MASK = 0x7, /\* 3 bits \*/++ INSN\_F\_SPI\_MASK = 0x3f, /\* 6 bits \*/+ INSN\_F\_SPI\_SHIFT = 3, /\* shifted 3 bits to the left \*/++ INSN\_F\_STACK\_ACCESS = BIT(9), /\* we need 10 bits total \*/+};++static\_assert(INSN\_F\_FRAMENO\_MASK + 1 >= MAX\_CALL\_FRAMES);+static\_assert(INSN\_F\_SPI\_MASK + 1 >= MAX\_BPF\_STACK / 8);++struct bpf\_jmp\_history\_entry { u32 idx;+ /\* insn idx can't be bigger than 1 million \*/+ u32 prev\_idx : 22;+ /\* special flags, e.g., whether insn is doing register stack spill/load \*/+ u32 flags : 10; }; -#define MAX\_CALL\_FRAMES 8 /\* Maximum number of register states that can exist at once \*/ #define BPF\_ID\_MAP\_SIZE ((MAX\_BPF\_REG + MAX\_BPF\_STACK / BPF\_REG\_SIZE) \* MAX\_CALL\_FRAMES) struct bpf\_verifier\_state {@@ -407,7 +429,7 @@ struct bpf\_verifier\_state { \* For most states jmp\_history\_cnt is [0-3]. \* For loops can go up to ~40. \*/- struct bpf\_idx\_pair \*jmp\_history;+ struct bpf\_jmp\_history\_entry \*jmp\_history; u32 jmp\_history\_cnt; u32 dfs\_depth; u32 callback\_unroll\_depth;@@ -640,6 +662,7 @@ struct bpf\_verifier\_env { int cur\_stack; } cfg; struct backtrack\_state bt;+ struct bpf\_jmp\_history\_entry \*cur\_hist\_ent; u32 pass\_cnt; /\* number of times do\_check() was called \*/ u32 subprog\_cnt; /\* number of instructions analyzed by the verifier \*/diff --git a/kernel/bpf/verifier.c b/kernel/bpf/verifier.cindex 4f19a091571bb6..5ca02af3a87280 100644--- a/[kernel/bpf/verifier.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/kernel/bpf/verifier.c?id=44b1bfb5bd97973ccb8d1d105a08470fc36812d8)+++ b/[kernel/bpf/verifier.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/kernel/bpf/verifier.c?id=ecc2aeeaa08a355d84d3ca9c3d2512399a194f29)@@ -1762,8 +1762,8 @@ static int copy\_verifier\_state(struct bpf\_verifier\_state \*dst\_state, int i, err;  dst\_state->jmp\_history = copy\_array(dst\_state->jmp\_history, src->jmp\_history,- src->jmp\_history\_cnt, sizeof(struct bpf\_idx\_pair),- GFP\_USER);+ src->jmp\_history\_cnt, sizeof(\*dst\_state->jmp\_history),+ GFP\_USER); if (!dst\_state->jmp\_history) return -ENOMEM; dst\_state->jmp\_history\_cnt = src->jmp\_history\_cnt;@@ -3397,6 +3397,21 @@ static int check\_reg\_arg(struct bpf\_verifier\_env \*env, u32 regno, return \_\_check\_reg\_arg(env, state->regs, regno, t); } +static int insn\_stack\_access\_flags(int frameno, int spi)+{+ return INSN\_F\_STACK\_ACCESS | (spi << INSN\_F\_SPI\_SHIFT) | frameno;+}++static int insn\_stack\_access\_spi(int insn\_flags)+{+ return (insn\_flags >> INSN\_F\_SPI\_SHIFT) & INSN\_F\_SPI\_MASK;+}++static int insn\_stack\_access\_frameno(int insn\_flags)+{+ return insn\_flags & INSN\_F\_FRAMENO\_MASK;+}+ static void mark\_jmp\_point(struct bpf\_verifier\_env \*env, int idx) { env->insn\_aux\_data[idx].jmp\_point = true;@@ -3408,28 +3423,51 @@ static bool is\_jmp\_point(struct bpf\_verifier\_env \*env, int insn\_idx) }  /\* for any branch, call, exit record the history of jmps in the given state \*/-static int push\_jmp\_history(struct bpf\_verifier\_env \*env,- struct bpf\_verifier\_state \*cur)+static int push\_jmp\_history(struct bpf\_verifier\_env \*env, struct bpf\_verifier\_state \*cur,+ int insn\_flags) { u32 cnt = cur->jmp\_history\_cnt;- struct bpf\_idx\_pair \*p;+ struct bpf\_jmp\_history\_entry \*p; size\_t alloc\_size; - if (!is\_jmp\_point(env, env->insn\_idx))+ /\* combine instruction flags if we already recorded this instruction \*/+ if (env->cur\_hist\_ent) {+ /\* atomic instructions push insn\_flags twice, for READ and+ \* WRITE sides, but they should agree on stack slot+ \*/+ WARN\_ONCE((env->cur\_hist\_ent->flags & insn\_flags) &&+ (env->cur\_hist\_ent->flags & insn\_flags) != insn\_flags,+ "verifier insn history bug: insn\_idx %d cur flags %x new flags %x\n",+ env->insn\_idx, env->cur\_hist\_ent->flags, insn\_flags);+ env->cur\_hist\_ent->flags |= insn\_flags; return 0;+ }  cnt++; alloc\_size = kmalloc\_size\_roundup(size\_mul(cnt, sizeof(\*p))); p = krealloc(cur->jmp\_history, alloc\_size, GFP\_USER); if (!p) return -ENOMEM;- p[cnt - 1].idx = env->insn\_idx;- p[cnt - 1].prev\_idx = env->prev\_insn\_idx; cur->jmp\_history = p;++ p = &cur->jmp\_history[cnt - 1];+ p->idx = env->insn\_idx;+ p->prev\_idx = env->prev\_insn\_idx;+ p->flags = insn\_flags; cur->jmp\_history\_cnt = cnt;+ env->cur\_hist\_ent = p;+ return 0; } +static struct bpf\_jmp\_history\_entry \*get\_jmp\_hist\_entry(struct bpf\_verifier\_state \*st,+ u32 hist\_end, int insn\_idx)+{+ if (hist\_end > 0 && st->jmp\_history[hist\_end - 1].idx == insn\_idx)+ return &st->jmp\_history[hist\_end - 1];+ return NULL;+}+ /\* Backtrack one insn at a time. If idx is not at the top of recorded \* history then previous instruction came from straight line execution. \* Return -ENOENT if we exhausted all instructions within given state.@@ -3591,9 +3629,14 @@ static inline bool bt\_is\_reg\_set(struct backtrack\_state \*bt, u32 reg) return bt->reg\_masks[bt->frame] & (1 << reg); } +static inline bool bt\_is\_frame\_slot\_set(struct backtrack\_state \*bt, u32 frame, u32 slot)+{+ return bt->stack\_masks[frame] & (1ull << slot);+}+ static inline bool bt\_is\_slot\_set(struct backtrack\_state \*bt, u32 slot) {- return bt->stack\_masks[bt->frame] & (1ull << slot);+ return bt\_is\_frame\_slot\_set(bt, bt->frame, slot); }  /\* format registers bitmask, e.g., "r0,r2,r4" for 0x15 mask \*/@@ -3647,7 +3690,7 @@ static bool calls\_callback(struct bpf\_verifier\_env \*env, int insn\_idx); \* - \*was\* processed previously during backtracking. \*/ static int backtrack\_insn(struct bpf\_verifier\_env \*env, int idx, int subseq\_idx,- struct backtrack\_state \*bt)+ struct bpf\_jmp\_history\_entry \*hist, struct backtrack\_state \*bt) { const struct bpf\_insn\_cbs cbs = { .cb\_call = disasm\_kfunc\_name,@@ -3660,7 +3703,7 @@ static int backtrack\_insn(struct bpf\_verifier\_env \*env, int idx, int subseq\_idx, u8 mode = BPF\_MODE(insn->code); u32 dreg = insn->dst\_reg; u32 sreg = insn->src\_reg;- u32 spi, i;+ u32 spi, i, fr;  if (insn->code == 0) return 0;@@ -3723,20 +3766,15 @@ static int backtrack\_insn(struct bpf\_verifier\_env \*env, int idx, int subseq\_idx, \* by 'precise' mark in corresponding register of this state. \* No further tracking necessary. \*/- if (insn->src\_reg != BPF\_REG\_FP)+ if (!hist || !(hist->flags & INSN\_F\_STACK\_ACCESS)) return 0;- /\* dreg = \*(u64 \*)[fp - off] was a fill from the stack. \* that [fp - off] slot contains scalar that needs to be \* tracked with precision \*/- spi = (-insn->off - 1) / BPF\_REG\_SIZE;- if (spi >= 64) {- verbose(env, "BUG spi %d\n", spi);- WARN\_ONCE(1, "verifier backtracking bug");- return -EFAULT;- }- bt\_set\_slot(bt, spi);+ spi = insn\_stack\_access\_spi(hist->flags);+ fr = insn\_stack\_access\_frameno(hist->flags);+ bt\_set\_frame\_slot(bt, fr, spi); } else if (class == BPF\_STX || class == BPF\_ST) { if (bt\_is\_reg\_set(bt, dreg)) /\* stx & st shouldn't be using \_scalar\_ dst\_reg@@ -3745,17 +3783,13 @@ static int backtrack\_insn(struct bpf\_verifier\_env \*env, int idx, int subseq\_idx, \*/ return -ENOTSUPP; /\* scalars can only be spilled into stack \*/- if (insn->dst\_reg != BPF\_REG\_FP)+ if (!hist || !(hist->flags & INSN\_F\_STACK\_ACCESS)) return 0;- spi = (-insn->off - 1) / BPF\_REG\_SIZE;- if (spi >= 64) {- verbose(env, "BUG spi %d\n", spi);- WARN\_ONCE(1, "verifier backtracking bug");- return -EFAULT;- }- if (!bt\_is\_slot\_set(bt, spi))+ spi = insn\_stack\_access\_spi(hist->flags);+ fr = insn\_stack\_access\_frameno(hist->flags);+ if (!bt\_is\_frame\_slot\_set(bt, fr, spi)) return 0;- bt\_clear\_slot(bt, spi);+ bt\_clear\_frame\_slot(bt, fr, spi); if (class == BPF\_STX) bt\_set\_reg(bt, sreg); } else if (class == BPF\_JMP || class == BPF\_JMP32) {@@ -3799,10 +3833,14 @@ static int backtrack\_insn(struct bpf\_verifier\_env \*env, int idx, int subseq\_idx, WARN\_ONCE(1, "verifier backtracking bug"); return -EFAULT; }- /\* we don't track register spills perfectly,- \* so fallback to force-precise instead of failing \*/- if (bt\_stack\_mask(bt) != 0)- return -ENOTSUPP;+ /\* we are now tracking register spills correctly,+ \* so any instance of leftover slots is a bug+ \*/+ if (bt\_stack\_mask(bt) != 0) {+ verbose(env, "BUG stack slots %llx\n", bt\_stack\_mask(bt));+ WARN\_ONCE(1, "verifier backtracking bug (subprog leftover stack slots)");+ return -EFAULT;+ } /\* propagate r1-r5 to the caller \*/ for (i = BPF\_REG\_1; i <= BPF\_REG\_5; i++) { if (bt\_is\_reg\_set(bt, i)) {@@ -3827,8 +3865,11 @@ static int backtrack\_insn(struct bpf\_verifier\_env \*env, int idx, int subseq\_idx, WARN\_ONCE(1, "verifier backtracking bug"); return -EFAULT; }- if (bt\_stack\_mask(bt) != 0)- return -ENOTSUPP;+ if (bt\_stack\_mask(bt) != 0) {+ verbose(env, "BUG stack slots %llx\n", bt\_stack\_mask(bt));+ WARN\_ONCE(1, "verifier backtracking bug (callback leftover stack slots)");+ return -EFAULT;+ } /\* clear r1-r5 in callback subprog's mask \*/ for (i = BPF\_REG\_1; i <= BPF\_REG\_5; i++) bt\_clear\_reg(bt, i);@@ -4265,6 +4306,7 @@ static int \_\_mark\_chain\_precision(struct bpf\_verifier\_env \*env, int regno) for (;;) { DECLARE\_BITMAP(mask, 64); u32 history = st->jmp\_history\_cnt;+ struct bpf\_jmp\_history\_entry \*hist;  if (env->log.level & BPF\_LOG\_LEVEL2) { verbose(env, "mark\_precise: frame%d: last\_idx %d first\_idx %d subseq\_idx %d \n",@@ -4328,7 +4370,8 @@ static int \_\_mark\_chain\_precision(struct bpf\_verifier\_env \*env, int regno) err = 0; skip\_first = false; } else {- err = backtrack\_insn(env, i, subseq\_idx, bt);+ hist = get\_jmp\_hist\_entry(st, history, i);+ err = backtrack\_insn(env, i, subseq\_idx, hist, bt); } if (err == -ENOTSUPP) { mark\_all\_scalars\_precise(env, env->cur\_state);@@ -4381,22 +4424,10 @@ static int \_\_mark\_chain\_precision(struct bpf\_verifier\_env \*env, int regno) bitmap\_from\_u64(mask, bt\_frame\_stack\_mask(bt, fr)); for\_each\_set\_bit(i, mask, 64) { if (i >= func->allocated\_stack / BPF\_REG\_SIZE) {- /\* the sequence of instructions:- \* 2: (bf) r3 = r10- \* 3: (7b) \*(u64 \*)(r3 -8) = r0- \* 4: (79) r4 = \*(u64 \*)(r10 -8)- \* doesn't contain jmps. It's backtracked- \* as a single block.- \* During backtracking insn 3 is not recognized as- \* stack access, so at the end of backtracking- \* stack slot fp-8 is still marked in stack\_mask.- \* However the parent state may not have accessed- \* fp-8 and it's "unallocated" stack space.- \* In such case fallback to conservative.- \*/- mark\_all\_scalars\_precise(env, env->cur\_state);- bt\_reset(bt);- return 0;+ verbose(env, "BUG backtracking (stack slot %d, total slots %d)\n",+ i, func->allocated\_stack / BPF\_REG\_SIZE);+ WARN\_ONCE(1, "verifier backtracking bug (stack slot out of bounds)");+ return -EFAULT; }  if (!is\_spilled\_scalar\_reg(&func->stack[i])) {@@ -4561,7 +4592,7 @@ static int check\_stack\_write\_fixed\_off(struct bpf\_verifier\_env \*env, int i, slot = -off - 1, spi = slot / BPF\_REG\_SIZE, err; struct bpf\_insn \*insn = &env->prog->insnsi[insn\_idx]; struct bpf\_reg\_state \*reg = NULL;- u32 dst\_reg = insn->dst\_reg;+ int insn\_flags = insn\_stack\_access\_flags(state->frameno, spi);  /\* caller checked that off % size == 0 and -MAX\_BPF\_STACK <= off < 0, \* so it's aligned access and [off, off + size) are within stack limits@@ -4599,17 +4630,6 @@ static int check\_stack\_write\_fixed\_off(struct bpf\_verifier\_env \*env, mark\_stack\_slot\_scratched(env, spi); if (reg && !(off % BPF\_REG\_SIZE) && register\_is\_bounded(reg) && !register\_is\_null(reg) && env->bpf\_capable) {- if (dst\_reg != BPF\_REG\_FP) {- /\* The backtracking logic can only recognize explicit- \* stack slot address like [fp - 8]. Other spill of- \* scalar via different register has to be conservative.- \* Backtrack from here and mark all registers as precise- \* that contributed into 'reg' being a constant.- \*/- err = mark\_chain\_precision(env, value\_regno);- if (err)- return err;- } save\_register\_state(state, spi, reg, size); /\* Break the relation on a narrowing spill. \*/ if (fls64(reg->umax\_value) > BITS\_PER\_BYTE \* size)@@ -4621,6 +4641,7 @@ static int check\_stack\_write\_fixed\_off(struct bpf\_verifier\_env \*env, \_\_mark\_reg\_known(&fake\_reg, insn->imm); fake\_reg.type = SCALAR\_VALUE; save\_register\_state(state, spi, &fake\_reg, size);+ insn\_flags = 0; /\* not a register spill \*/ } else if (reg && is\_spillable\_regtype(reg->type)) { /\* register containing pointer is being spilled into stack \*/ if (size != BPF\_REG\_SIZE) {@@ -4666,9 +4687,12 @@ static int check\_stack\_write\_fixed\_off(struct bpf\_verifier\_env \*env,  /\* Mark slots affected by this stack write. \*/ for (i = 0; i < size; i++)- state->stack[spi].slot\_type[(slot - i) % BPF\_REG\_SIZE] =- type;+ state->stack[spi].slot\_type[(slot - i) % BPF\_REG\_SIZE] = type;+ insn\_flags = 0; /\* not a register spill \*/ }++ if (insn\_flags)+ return push\_jmp\_history(env, env->cur\_state, insn\_flags); return 0; } @@ -4857,6 +4881,7 @@ static int check\_stack\_read\_fixed\_off(struct bpf\_verifier\_env \*env, int i, slot = -off - 1, spi = slot / BPF\_REG\_SIZE; struct bpf\_reg\_state \*reg; u8 \*stype, type;+ int insn\_flags = insn\_stack\_access\_flags(reg\_state->frameno, spi);  stype = reg\_state->stack[spi].slot\_type; reg = &reg\_state->stack[spi].spilled\_ptr;@@ -4902,12 +4927,10 @@ static int check\_stack\_read\_fixed\_off(struct bpf\_verifier\_env \*env, return -EACCES; } mark\_reg\_unknown(env, state->regs, dst\_regno);+ insn\_flags = 0; /\* not restoring original register state \*/ } state->regs[dst\_regno].live |= REG\_LIVE\_WRITTEN;- return 0;- }-- if (dst\_regno >= 0) {+ } else if (dst\_regno >= 0) { /\* restore register state from stack \*/ copy\_register\_state(&state->regs[dst\_regno], reg); /\* mark reg as written since spilled pointer state likely@@ -4943,7 +4966,10 @@ static int check\_stack\_read\_fixed\_off(struct bpf\_verifier\_env \*env, mark\_reg\_read(env, reg, reg->parent, REG\_LIVE\_READ64); if (dst\_regno >= 0) mark\_reg\_stack\_read(env, reg\_state, off, off + size, dst\_regno);+ insn\_flags = 0; /\* we are not restoring spilled register \*/ }+ if (insn\_flags)+ return push\_jmp\_history(env, env->cur\_state, insn\_flags); return 0; } @@ -7027,7 +7053,6 @@ static int check\_atomic(struct bpf\_verifier\_env \*env, int insn\_idx, struct bpf\_i BPF\_SIZE(insn->code), BPF\_WRITE, -1, true, false); if (err) return err;- return 0; } @@ -16773,7 +16798,8 @@ hit: \* the precision needs to be propagated back in \* the current state. \*/- err = err ? : push\_jmp\_history(env, cur);+ if (is\_jmp\_point(env, env->insn\_idx))+ err = err ? : push\_jmp\_history(env, cur, 0); err = err ? : propagate\_precision(env, &sl->state); if (err) return err;@@ -16997,6 +17023,9 @@ static int do\_check(struct bpf\_verifier\_env \*env) u8 class; int err; + /\* reset current history entry on each new instruction \*/+ env->cur\_hist\_ent = NULL;+ env->prev\_insn\_idx = prev\_insn\_idx; if (env->insn\_idx >= insn\_cnt) { verbose(env, "invalid insn idx %d insn\_cnt %d\n",@@ -17036,7 +17065,7 @@ static int do\_check(struct bpf\_verifier\_env \*env) }  if (is\_jmp\_point(env, env->insn\_idx)) {- err = push\_jmp\_history(env, state);+ err = push\_jmp\_history(env, state, 0); if (err) return err; }diff --git a/tools/testing/selftests/bpf/progs/verifier\_subprog\_precision.c b/tools/testing/selftests/bpf/progs/verifier\_subprog\_precision.cindex f61d623b1ce8df..f87365f7599bf7 100644--- a/[tools/testing/selftests/bpf/progs/verifier\_subprog\_precision.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/tools/testing/selftests/bpf/progs/verifier_subprog_precision.c?id=44b1bfb5bd97973ccb8d1d105a08470fc36812d8)+++ b/[tools/testing/selftests/bpf/progs/verifier\_subprog\_precision.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/tools/testing/selftests/bpf/progs/verifier_subprog_precision.c?id=ecc2aeeaa08a355d84d3ca9c3d2512399a194f29)@@ -541,11 +541,24 @@ static \_\_u64 subprog\_spill\_reg\_precise(void)  SEC("?raw\_tp") \_\_success \_\_log\_level(2)-/\* precision backtracking can't currently handle stack access not through r10,- \* so we won't be able to mark stack slot fp-8 as precise, and so will- \* fallback to forcing all as precise- \*/-\_\_msg("mark\_precise: frame0: falling back to forcing all scalars precise")+\_\_msg("10: (0f) r1 += r7")+\_\_msg("mark\_precise: frame0: last\_idx 10 first\_idx 7 subseq\_idx -1")+\_\_msg("mark\_precise: frame0: regs=r7 stack= before 9: (bf) r1 = r8")+\_\_msg("mark\_precise: frame0: regs=r7 stack= before 8: (27) r7 \*= 4")+\_\_msg("mark\_precise: frame0: regs=r7 stack= before 7: (79) r7 = \*(u64 \*)(r10 -8)")+\_\_msg("mark\_precise: frame0: parent state regs= stack=-8: R0\_w=2 R6\_w=1 R8\_rw=map\_value(map=.data.vals,ks=4,vs=16) R10=fp0 fp-8\_rw=P1")+\_\_msg("mark\_precise: frame0: last\_idx 18 first\_idx 0 subseq\_idx 7")+\_\_msg("mark\_precise: frame0: regs= stack=-8 before 18: (95) exit")+\_\_msg("mark\_precise: frame1: regs= stack= before 17: (0f) r0 += r2")+\_\_msg("mark\_precise: frame1: regs= stack= before 16: (79) r2 = \*(u64 \*)(r1 +0)")+\_\_msg("mark\_precise: frame1: regs= stack= before 15: (79) r0 = \*(u64 \*)(r10 -16)")+\_\_msg("mark\_precise: frame1: regs= stack= before 14: (7b) \*(u64 \*)(r10 -16) = r2")+\_\_msg("mark\_precise: frame1: regs= stack= before 13: (7b) \*(u64 \*)(r1 +0) = r2")+\_\_msg("mark\_precise: frame1: regs=r2 stack= before 6: (85) call pc+6")+\_\_msg("mark\_precise: frame0: regs=r2 stack= before 5: (bf) r2 = r6")+\_\_msg("mark\_precise: frame0: regs=r6 stack= before 4: (07) r1 += -8")+\_\_msg("mark\_precise: frame0: regs=r6 stack= before 3: (bf) r1 = r10")+\_\_msg("mark\_precise: frame0: regs=r6 stack= before 2: (b7) r6 = 1") \_\_naked int subprog\_spill\_into\_parent\_stack\_slot\_precise(void) { asm volatile (diff --git a/tools/testing/selftests/bpf/verifier/precise.c b/tools/testing/selftests/bpf/verifier/precise.cindex 0d84dd1f38b6b0..8a2ff81d835088 100644--- a/[tools/testing/selftests/bpf/verifier/precise.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/tools/testing/selftests/bpf/verifier/precise.c?id=44b1bfb5bd97973ccb8d1d105a08470fc36812d8)+++ b/[tools/testing/selftests/bpf/verifier/precise.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/tools/testing/selftests/bpf/verifier/precise.c?id=ecc2aeeaa08a355d84d3ca9c3d2512399a194f29)@@ -140,10 +140,11 @@ .result = REJECT, }, {- "precise: ST insn causing spi > allocated\_stack",+ "precise: ST zero to stack insn is supported", .insns = { BPF\_MOV64\_REG(BPF\_REG\_3, BPF\_REG\_10), BPF\_JMP\_IMM(BPF\_JNE, BPF\_REG\_3, 123, 0),+ /\* not a register spill, so we stop precision propagation for R4 here \*/ BPF\_ST\_MEM(BPF\_DW, BPF\_REG\_3, -8, 0), BPF\_LDX\_MEM(BPF\_DW, BPF\_REG\_4, BPF\_REG\_10, -8), BPF\_MOV64\_IMM(BPF\_REG\_0, -1),@@ -157,11 +158,11 @@ mark\_precise: frame0: last\_idx 4 first\_idx 2\ mark\_precise: frame0: regs=r4 stack= before 4\ mark\_precise: frame0: regs=r4 stack= before 3\- mark\_precise: frame0: regs= stack=-8 before 2\- mark\_precise: frame0: falling back to forcing all scalars precise\- force\_precise: frame0: forcing r0 to be precise\ mark\_precise: frame0: last\_idx 5 first\_idx 5\- mark\_precise: frame0: parent state regs= stack=:",+ mark\_precise: frame0: parent state regs=r0 stack=:\+ mark\_precise: frame0: last\_idx 4 first\_idx 2\+ mark\_precise: frame0: regs=r0 stack= before 4\+ 5: R0=-1 R4=0", .result = VERBOSE\_ACCEPT, .retval = -1, },@@ -169,6 +170,8 @@ "precise: STX insn causing spi > allocated\_stack", .insns = { BPF\_RAW\_INSN(BPF\_JMP | BPF\_CALL, 0, 0, 0, BPF\_FUNC\_get\_prandom\_u32),+ /\* make later reg spill more interesting by having somewhat known scalar \*/+ BPF\_ALU64\_IMM(BPF\_AND, BPF\_REG\_0, 0xff), BPF\_MOV64\_REG(BPF\_REG\_3, BPF\_REG\_10), BPF\_JMP\_IMM(BPF\_JNE, BPF\_REG\_3, 123, 0), BPF\_STX\_MEM(BPF\_DW, BPF\_REG\_3, BPF\_REG\_0, -8),@@ -179,18 +182,21 @@ }, .prog\_type = BPF\_PROG\_TYPE\_XDP, .flags = BPF\_F\_TEST\_STATE\_FREQ,- .errstr = "mark\_precise: frame0: last\_idx 6 first\_idx 6\+ .errstr = "mark\_precise: frame0: last\_idx 7 first\_idx 7\ mark\_precise: frame0: parent state regs=r4 stack=:\- mark\_precise: frame0: last\_idx 5 first\_idx 3\- mark\_precise: frame0: regs=r4 stack= before 5\- mark\_precise: frame0: regs=r4 stack= before 4\- mark\_precise: frame0: regs= stack=-8 before 3\- mark\_precise: frame0: falling back to forcing all scalars precise\- force\_precise: frame0: forcing r0 to be precise\- force\_precise: frame0: forcing r0 to be precise\- force\_precise: frame0: forcing r0 to be precise\- force\_precise: frame0: forcing r0 to be precise\- mark\_precise: frame0: last\_idx 6 first\_idx 6\+ mark\_precise: frame0: last\_idx 6 first\_idx 4\+ mark\_precise: frame0: regs=r4 stack= before 6: (b7) r0 = -1\+ mark\_precise: frame0: regs=r4 stack= before 5: (79) r4 = \*(u64 \*)(r10 -8)\+ mark\_precise: frame0: regs= stack=-8 before 4: (7b) \*(u64 \*)(r3 -8) = r0\+ mark\_precise: frame0: parent state regs=r0 stack=:\+ mark\_precise: frame0: last\_idx 3 first\_idx 3\+ mark\_precise: frame0: regs=r0 stack= before 3: (55) if r3 != 0x7b goto pc+0\+ mark\_precise: frame0: regs=r0 stack= before 2: (bf) r3 = r10\+ mark\_precise: frame0: regs=r0 stack= before 1: (57) r0 &= 255\+ mark\_precise: frame0: parent state regs=r0 stack=:\+ mark\_precise: frame0: last\_idx 0 first\_idx 0\+ mark\_precise: frame0: regs=r0 stack= before 0: (85) call bpf\_get\_prandom\_u32#7\+ mark\_precise: frame0: last\_idx 7 first\_idx 7\ mark\_precise: frame0: parent state regs= stack=:", .result = VERBOSE\_ACCEPT, .retval = -1, |
| --- |

generated by [cgit 1.2.3-korg](https://git.zx2c4.com/cgit/about/) ([git 2.43.0](https://git-scm.com/)) at 2025-01-15 08:00:51 +0000



=== Content from git.kernel.org_33fb5e32_20250115_080213.html ===


| [cgit logo](/) | [index](/) : [kernel/git/stable/linux.git](/pub/scm/linux/kernel/git/stable/linux.git/) | linux-2.6.11.y linux-2.6.12.y linux-2.6.13.y linux-2.6.14.y linux-2.6.15.y linux-2.6.16.y linux-2.6.17.y linux-2.6.18.y linux-2.6.19.y linux-2.6.20.y linux-2.6.21.y linux-2.6.22.y linux-2.6.23.y linux-2.6.24.y linux-2.6.25.y linux-2.6.26.y linux-2.6.27.y linux-2.6.28.y linux-2.6.29.y linux-2.6.30.y linux-2.6.31.y linux-2.6.32.y linux-2.6.33.y linux-2.6.34.y linux-2.6.35.y linux-2.6.36.y linux-2.6.37.y linux-2.6.38.y linux-2.6.39.y linux-3.0.y linux-3.1.y linux-3.10.y linux-3.11.y linux-3.12.y linux-3.13.y linux-3.14.y linux-3.15.y linux-3.16.y linux-3.17.y linux-3.18.y linux-3.19.y linux-3.2.y linux-3.3.y linux-3.4.y linux-3.5.y linux-3.6.y linux-3.7.y linux-3.8.y linux-3.9.y linux-4.0.y linux-4.1.y linux-4.10.y linux-4.11.y linux-4.12.y linux-4.13.y linux-4.14.y linux-4.15.y linux-4.16.y linux-4.17.y linux-4.18.y linux-4.19.y linux-4.2.y linux-4.20.y linux-4.3.y linux-4.4.y linux-4.5.y linux-4.6.y linux-4.7.y linux-4.8.y linux-4.9.y linux-5.0.y linux-5.1.y linux-5.10.y linux-5.11.y linux-5.12.y linux-5.13.y linux-5.14.y linux-5.15.y linux-5.16.y linux-5.17.y linux-5.18.y linux-5.19.y linux-5.2.y linux-5.3.y linux-5.4.y linux-5.5.y linux-5.6.y linux-5.7.y linux-5.8.y linux-5.9.y linux-6.0.y linux-6.1.y linux-6.10.y linux-6.11.y linux-6.12.y linux-6.2.y linux-6.3.y linux-6.4.y linux-6.5.y linux-6.6.y linux-6.7.y linux-6.8.y linux-6.9.y linux-rolling-lts linux-rolling-stable master |
| --- | --- | --- |
| Linux kernel stable tree | Stable Group |

| [about](/pub/scm/linux/kernel/git/stable/linux.git/about/)[summary](/pub/scm/linux/kernel/git/stable/linux.git/)[refs](/pub/scm/linux/kernel/git/stable/linux.git/refs/?id=41f6f64e6999a837048b1bd13a2f8742964eca6b)[log](/pub/scm/linux/kernel/git/stable/linux.git/log/)[tree](/pub/scm/linux/kernel/git/stable/linux.git/tree/?id=41f6f64e6999a837048b1bd13a2f8742964eca6b)[commit](/pub/scm/linux/kernel/git/stable/linux.git/commit/?id=41f6f64e6999a837048b1bd13a2f8742964eca6b)[diff](/pub/scm/linux/kernel/git/stable/linux.git/diff/?id=41f6f64e6999a837048b1bd13a2f8742964eca6b)[stats](/pub/scm/linux/kernel/git/stable/linux.git/stats/) | log msg author committer range |
| --- | --- |

**diff options**

|  | |
| --- | --- |
| context: | 12345678910152025303540 |
| space: | includeignore |
| mode: | unifiedssdiffstat only |
|  |  |

| author | Andrii Nakryiko <andrii@kernel.org> | 2023-12-05 10:42:39 -0800 |
| --- | --- | --- |
| committer | Alexei Starovoitov <ast@kernel.org> | 2023-12-05 13:40:20 -0800 |
| commit | [41f6f64e6999a837048b1bd13a2f8742964eca6b](/pub/scm/linux/kernel/git/stable/linux.git/commit/?id=41f6f64e6999a837048b1bd13a2f8742964eca6b) ([patch](/pub/scm/linux/kernel/git/stable/linux.git/patch/?id=41f6f64e6999a837048b1bd13a2f8742964eca6b)) | |
| tree | [b0ec4c4bfd2ae8fc83461d3d92e3292bd265f068](/pub/scm/linux/kernel/git/stable/linux.git/tree/?id=41f6f64e6999a837048b1bd13a2f8742964eca6b) | |
| parent | [5ffb260f754bf838507fe0c23d05254b33e2bf3d](/pub/scm/linux/kernel/git/stable/linux.git/commit/?id=5ffb260f754bf838507fe0c23d05254b33e2bf3d) ([diff](/pub/scm/linux/kernel/git/stable/linux.git/diff/?id=41f6f64e6999a837048b1bd13a2f8742964eca6b&id2=5ffb260f754bf838507fe0c23d05254b33e2bf3d)) | |
| download | [linux-41f6f64e6999a837048b1bd13a2f8742964eca6b.tar.gz](/pub/scm/linux/kernel/git/stable/linux.git/snapshot/linux-41f6f64e6999a837048b1bd13a2f8742964eca6b.tar.gz) | |

bpf: support non-r10 register spill/fill to/from stack in precision trackingUse instruction (jump) history to record instructions that performed
register spill/fill to/from stack, regardless if this was done through
read-only r10 register, or any other register after copying r10 into it
\*and\* potentially adjusting offset.
To make this work reliably, we push extra per-instruction flags into
instruction history, encoding stack slot index (spi) and stack frame
number in extra 10 bit flags we take away from prev\_idx in instruction
history. We don't touch idx field for maximum performance, as it's
checked most frequently during backtracking.
This change removes basically the last remaining practical limitation of
precision backtracking logic in BPF verifier. It fixes known
deficiencies, but also opens up new opportunities to reduce number of
verified states, explored in the subsequent patches.
There are only three differences in selftests' BPF object files
according to veristat, all in the positive direction (less states).
File Program Insns (A) Insns (B) Insns (DIFF) States (A) States (B) States (DIFF)
-------------------------------------- ------------- --------- --------- ------------- ---------- ---------- -------------
test\_cls\_redirect\_dynptr.bpf.linked3.o cls\_redirect 2987 2864 -123 (-4.12%) 240 231 -9 (-3.75%)
xdp\_synproxy\_kern.bpf.linked3.o syncookie\_tc 82848 82661 -187 (-0.23%) 5107 5073 -34 (-0.67%)
xdp\_synproxy\_kern.bpf.linked3.o syncookie\_xdp 85116 84964 -152 (-0.18%) 5162 5130 -32 (-0.62%)
Note, I avoided renaming jmp\_history to more generic insn\_hist to
minimize number of lines changed and potential merge conflicts between
bpf and bpf-next trees.
Notice also cur\_hist\_entry pointer reset to NULL at the beginning of
instruction verification loop. This pointer avoids the problem of
relying on last jump history entry's insn\_idx to determine whether we
already have entry for current instruction or not. It can happen that we
added jump history entry because current instruction is\_jmp\_point(), but
also we need to add instruction flags for stack access. In this case, we
don't want to entries, so we need to reuse last added entry, if it is
present.
Relying on insn\_idx comparison has the same ambiguity problem as the one
that was fixed recently in [0], so we avoid that.
[0] https://patchwork.kernel.org/project/netdevbpf/patch/20231110002638.4168352-3-andrii@kernel.org/
Acked-by: Eduard Zingerman <eddyz87@gmail.com>
Reported-by: Tao Lyu <tao.lyu@epfl.ch>
Signed-off-by: Andrii Nakryiko <andrii@kernel.org>
Link: [https://lore.kernel.org/r/20231205184248.1502704-2-andrii@kernel.org](https://lore.kernel.org/r/20231205184248.1502704-2-andrii%40kernel.org)
Signed-off-by: Alexei Starovoitov <ast@kernel.org>
[Diffstat](/pub/scm/linux/kernel/git/stable/linux.git/diff/?id=41f6f64e6999a837048b1bd13a2f8742964eca6b)

| -rw-r--r-- | [include/linux/bpf\_verifier.h](/pub/scm/linux/kernel/git/stable/linux.git/diff/include/linux/bpf_verifier.h?id=41f6f64e6999a837048b1bd13a2f8742964eca6b) | 31 | |  |  |  | | --- | --- | --- | |
| --- | --- | --- | --- | --- | --- | --- |
| -rw-r--r-- | [kernel/bpf/verifier.c](/pub/scm/linux/kernel/git/stable/linux.git/diff/kernel/bpf/verifier.c?id=41f6f64e6999a837048b1bd13a2f8742964eca6b) | 175 | |  |  |  | | --- | --- | --- | |
| -rw-r--r-- | [tools/testing/selftests/bpf/progs/verifier\_subprog\_precision.c](/pub/scm/linux/kernel/git/stable/linux.git/diff/tools/testing/selftests/bpf/progs/verifier_subprog_precision.c?id=41f6f64e6999a837048b1bd13a2f8742964eca6b) | 23 | |  |  |  | | --- | --- | --- | |
| -rw-r--r-- | [tools/testing/selftests/bpf/verifier/precise.c](/pub/scm/linux/kernel/git/stable/linux.git/diff/tools/testing/selftests/bpf/verifier/precise.c?id=41f6f64e6999a837048b1bd13a2f8742964eca6b) | 38 | |  |  |  | | --- | --- | --- | |

4 files changed, 169 insertions, 98 deletions

| diff --git a/include/linux/bpf\_verifier.h b/include/linux/bpf\_verifier.hindex 3378cc753061e6..bada59812e0034 100644--- a/[include/linux/bpf\_verifier.h](/pub/scm/linux/kernel/git/stable/linux.git/tree/include/linux/bpf_verifier.h?id=5ffb260f754bf838507fe0c23d05254b33e2bf3d)+++ b/[include/linux/bpf\_verifier.h](/pub/scm/linux/kernel/git/stable/linux.git/tree/include/linux/bpf_verifier.h?id=41f6f64e6999a837048b1bd13a2f8742964eca6b)@@ -325,12 +325,34 @@ struct bpf\_func\_state { int allocated\_stack; }; -struct bpf\_idx\_pair {- u32 prev\_idx;+#define MAX\_CALL\_FRAMES 8++/\* instruction history flags, used in bpf\_jmp\_history\_entry.flags field \*/+enum {+ /\* instruction references stack slot through PTR\_TO\_STACK register;+ \* we also store stack's frame number in lower 3 bits (MAX\_CALL\_FRAMES is 8)+ \* and accessed stack slot's index in next 6 bits (MAX\_BPF\_STACK is 512,+ \* 8 bytes per slot, so slot index (spi) is [0, 63])+ \*/+ INSN\_F\_FRAMENO\_MASK = 0x7, /\* 3 bits \*/++ INSN\_F\_SPI\_MASK = 0x3f, /\* 6 bits \*/+ INSN\_F\_SPI\_SHIFT = 3, /\* shifted 3 bits to the left \*/++ INSN\_F\_STACK\_ACCESS = BIT(9), /\* we need 10 bits total \*/+};++static\_assert(INSN\_F\_FRAMENO\_MASK + 1 >= MAX\_CALL\_FRAMES);+static\_assert(INSN\_F\_SPI\_MASK + 1 >= MAX\_BPF\_STACK / 8);++struct bpf\_jmp\_history\_entry { u32 idx;+ /\* insn idx can't be bigger than 1 million \*/+ u32 prev\_idx : 22;+ /\* special flags, e.g., whether insn is doing register stack spill/load \*/+ u32 flags : 10; }; -#define MAX\_CALL\_FRAMES 8 /\* Maximum number of register states that can exist at once \*/ #define BPF\_ID\_MAP\_SIZE ((MAX\_BPF\_REG + MAX\_BPF\_STACK / BPF\_REG\_SIZE) \* MAX\_CALL\_FRAMES) struct bpf\_verifier\_state {@@ -413,7 +435,7 @@ struct bpf\_verifier\_state { \* For most states jmp\_history\_cnt is [0-3]. \* For loops can go up to ~40. \*/- struct bpf\_idx\_pair \*jmp\_history;+ struct bpf\_jmp\_history\_entry \*jmp\_history; u32 jmp\_history\_cnt; u32 dfs\_depth; u32 callback\_unroll\_depth;@@ -656,6 +678,7 @@ struct bpf\_verifier\_env { int cur\_stack; } cfg; struct backtrack\_state bt;+ struct bpf\_jmp\_history\_entry \*cur\_hist\_ent; u32 pass\_cnt; /\* number of times do\_check() was called \*/ u32 subprog\_cnt; /\* number of instructions analyzed by the verifier \*/diff --git a/kernel/bpf/verifier.c b/kernel/bpf/verifier.cindex 1ed39665f80219..9bc16dc664659b 100644--- a/[kernel/bpf/verifier.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/kernel/bpf/verifier.c?id=5ffb260f754bf838507fe0c23d05254b33e2bf3d)+++ b/[kernel/bpf/verifier.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/kernel/bpf/verifier.c?id=41f6f64e6999a837048b1bd13a2f8742964eca6b)@@ -1355,8 +1355,8 @@ static int copy\_verifier\_state(struct bpf\_verifier\_state \*dst\_state, int i, err;  dst\_state->jmp\_history = copy\_array(dst\_state->jmp\_history, src->jmp\_history,- src->jmp\_history\_cnt, sizeof(struct bpf\_idx\_pair),- GFP\_USER);+ src->jmp\_history\_cnt, sizeof(\*dst\_state->jmp\_history),+ GFP\_USER); if (!dst\_state->jmp\_history) return -ENOMEM; dst\_state->jmp\_history\_cnt = src->jmp\_history\_cnt;@@ -3221,6 +3221,21 @@ static int check\_reg\_arg(struct bpf\_verifier\_env \*env, u32 regno, return \_\_check\_reg\_arg(env, state->regs, regno, t); } +static int insn\_stack\_access\_flags(int frameno, int spi)+{+ return INSN\_F\_STACK\_ACCESS | (spi << INSN\_F\_SPI\_SHIFT) | frameno;+}++static int insn\_stack\_access\_spi(int insn\_flags)+{+ return (insn\_flags >> INSN\_F\_SPI\_SHIFT) & INSN\_F\_SPI\_MASK;+}++static int insn\_stack\_access\_frameno(int insn\_flags)+{+ return insn\_flags & INSN\_F\_FRAMENO\_MASK;+}+ static void mark\_jmp\_point(struct bpf\_verifier\_env \*env, int idx) { env->insn\_aux\_data[idx].jmp\_point = true;@@ -3232,28 +3247,51 @@ static bool is\_jmp\_point(struct bpf\_verifier\_env \*env, int insn\_idx) }  /\* for any branch, call, exit record the history of jmps in the given state \*/-static int push\_jmp\_history(struct bpf\_verifier\_env \*env,- struct bpf\_verifier\_state \*cur)+static int push\_jmp\_history(struct bpf\_verifier\_env \*env, struct bpf\_verifier\_state \*cur,+ int insn\_flags) { u32 cnt = cur->jmp\_history\_cnt;- struct bpf\_idx\_pair \*p;+ struct bpf\_jmp\_history\_entry \*p; size\_t alloc\_size; - if (!is\_jmp\_point(env, env->insn\_idx))+ /\* combine instruction flags if we already recorded this instruction \*/+ if (env->cur\_hist\_ent) {+ /\* atomic instructions push insn\_flags twice, for READ and+ \* WRITE sides, but they should agree on stack slot+ \*/+ WARN\_ONCE((env->cur\_hist\_ent->flags & insn\_flags) &&+ (env->cur\_hist\_ent->flags & insn\_flags) != insn\_flags,+ "verifier insn history bug: insn\_idx %d cur flags %x new flags %x\n",+ env->insn\_idx, env->cur\_hist\_ent->flags, insn\_flags);+ env->cur\_hist\_ent->flags |= insn\_flags; return 0;+ }  cnt++; alloc\_size = kmalloc\_size\_roundup(size\_mul(cnt, sizeof(\*p))); p = krealloc(cur->jmp\_history, alloc\_size, GFP\_USER); if (!p) return -ENOMEM;- p[cnt - 1].idx = env->insn\_idx;- p[cnt - 1].prev\_idx = env->prev\_insn\_idx; cur->jmp\_history = p;++ p = &cur->jmp\_history[cnt - 1];+ p->idx = env->insn\_idx;+ p->prev\_idx = env->prev\_insn\_idx;+ p->flags = insn\_flags; cur->jmp\_history\_cnt = cnt;+ env->cur\_hist\_ent = p;+ return 0; } +static struct bpf\_jmp\_history\_entry \*get\_jmp\_hist\_entry(struct bpf\_verifier\_state \*st,+ u32 hist\_end, int insn\_idx)+{+ if (hist\_end > 0 && st->jmp\_history[hist\_end - 1].idx == insn\_idx)+ return &st->jmp\_history[hist\_end - 1];+ return NULL;+}+ /\* Backtrack one insn at a time. If idx is not at the top of recorded \* history then previous instruction came from straight line execution. \* Return -ENOENT if we exhausted all instructions within given state.@@ -3415,9 +3453,14 @@ static inline bool bt\_is\_reg\_set(struct backtrack\_state \*bt, u32 reg) return bt->reg\_masks[bt->frame] & (1 << reg); } +static inline bool bt\_is\_frame\_slot\_set(struct backtrack\_state \*bt, u32 frame, u32 slot)+{+ return bt->stack\_masks[frame] & (1ull << slot);+}+ static inline bool bt\_is\_slot\_set(struct backtrack\_state \*bt, u32 slot) {- return bt->stack\_masks[bt->frame] & (1ull << slot);+ return bt\_is\_frame\_slot\_set(bt, bt->frame, slot); }  /\* format registers bitmask, e.g., "r0,r2,r4" for 0x15 mask \*/@@ -3471,7 +3514,7 @@ static bool calls\_callback(struct bpf\_verifier\_env \*env, int insn\_idx); \* - \*was\* processed previously during backtracking. \*/ static int backtrack\_insn(struct bpf\_verifier\_env \*env, int idx, int subseq\_idx,- struct backtrack\_state \*bt)+ struct bpf\_jmp\_history\_entry \*hist, struct backtrack\_state \*bt) { const struct bpf\_insn\_cbs cbs = { .cb\_call = disasm\_kfunc\_name,@@ -3484,7 +3527,7 @@ static int backtrack\_insn(struct bpf\_verifier\_env \*env, int idx, int subseq\_idx, u8 mode = BPF\_MODE(insn->code); u32 dreg = insn->dst\_reg; u32 sreg = insn->src\_reg;- u32 spi, i;+ u32 spi, i, fr;  if (insn->code == 0) return 0;@@ -3545,20 +3588,15 @@ static int backtrack\_insn(struct bpf\_verifier\_env \*env, int idx, int subseq\_idx, \* by 'precise' mark in corresponding register of this state. \* No further tracking necessary. \*/- if (insn->src\_reg != BPF\_REG\_FP)+ if (!hist || !(hist->flags & INSN\_F\_STACK\_ACCESS)) return 0;- /\* dreg = \*(u64 \*)[fp - off] was a fill from the stack. \* that [fp - off] slot contains scalar that needs to be \* tracked with precision \*/- spi = (-insn->off - 1) / BPF\_REG\_SIZE;- if (spi >= 64) {- verbose(env, "BUG spi %d\n", spi);- WARN\_ONCE(1, "verifier backtracking bug");- return -EFAULT;- }- bt\_set\_slot(bt, spi);+ spi = insn\_stack\_access\_spi(hist->flags);+ fr = insn\_stack\_access\_frameno(hist->flags);+ bt\_set\_frame\_slot(bt, fr, spi); } else if (class == BPF\_STX || class == BPF\_ST) { if (bt\_is\_reg\_set(bt, dreg)) /\* stx & st shouldn't be using \_scalar\_ dst\_reg@@ -3567,17 +3605,13 @@ static int backtrack\_insn(struct bpf\_verifier\_env \*env, int idx, int subseq\_idx, \*/ return -ENOTSUPP; /\* scalars can only be spilled into stack \*/- if (insn->dst\_reg != BPF\_REG\_FP)+ if (!hist || !(hist->flags & INSN\_F\_STACK\_ACCESS)) return 0;- spi = (-insn->off - 1) / BPF\_REG\_SIZE;- if (spi >= 64) {- verbose(env, "BUG spi %d\n", spi);- WARN\_ONCE(1, "verifier backtracking bug");- return -EFAULT;- }- if (!bt\_is\_slot\_set(bt, spi))+ spi = insn\_stack\_access\_spi(hist->flags);+ fr = insn\_stack\_access\_frameno(hist->flags);+ if (!bt\_is\_frame\_slot\_set(bt, fr, spi)) return 0;- bt\_clear\_slot(bt, spi);+ bt\_clear\_frame\_slot(bt, fr, spi); if (class == BPF\_STX) bt\_set\_reg(bt, sreg); } else if (class == BPF\_JMP || class == BPF\_JMP32) {@@ -3621,10 +3655,14 @@ static int backtrack\_insn(struct bpf\_verifier\_env \*env, int idx, int subseq\_idx, WARN\_ONCE(1, "verifier backtracking bug"); return -EFAULT; }- /\* we don't track register spills perfectly,- \* so fallback to force-precise instead of failing \*/- if (bt\_stack\_mask(bt) != 0)- return -ENOTSUPP;+ /\* we are now tracking register spills correctly,+ \* so any instance of leftover slots is a bug+ \*/+ if (bt\_stack\_mask(bt) != 0) {+ verbose(env, "BUG stack slots %llx\n", bt\_stack\_mask(bt));+ WARN\_ONCE(1, "verifier backtracking bug (subprog leftover stack slots)");+ return -EFAULT;+ } /\* propagate r1-r5 to the caller \*/ for (i = BPF\_REG\_1; i <= BPF\_REG\_5; i++) { if (bt\_is\_reg\_set(bt, i)) {@@ -3649,8 +3687,11 @@ static int backtrack\_insn(struct bpf\_verifier\_env \*env, int idx, int subseq\_idx, WARN\_ONCE(1, "verifier backtracking bug"); return -EFAULT; }- if (bt\_stack\_mask(bt) != 0)- return -ENOTSUPP;+ if (bt\_stack\_mask(bt) != 0) {+ verbose(env, "BUG stack slots %llx\n", bt\_stack\_mask(bt));+ WARN\_ONCE(1, "verifier backtracking bug (callback leftover stack slots)");+ return -EFAULT;+ } /\* clear r1-r5 in callback subprog's mask \*/ for (i = BPF\_REG\_1; i <= BPF\_REG\_5; i++) bt\_clear\_reg(bt, i);@@ -4087,6 +4128,7 @@ static int \_\_mark\_chain\_precision(struct bpf\_verifier\_env \*env, int regno) for (;;) { DECLARE\_BITMAP(mask, 64); u32 history = st->jmp\_history\_cnt;+ struct bpf\_jmp\_history\_entry \*hist;  if (env->log.level & BPF\_LOG\_LEVEL2) { verbose(env, "mark\_precise: frame%d: last\_idx %d first\_idx %d subseq\_idx %d \n",@@ -4150,7 +4192,8 @@ static int \_\_mark\_chain\_precision(struct bpf\_verifier\_env \*env, int regno) err = 0; skip\_first = false; } else {- err = backtrack\_insn(env, i, subseq\_idx, bt);+ hist = get\_jmp\_hist\_entry(st, history, i);+ err = backtrack\_insn(env, i, subseq\_idx, hist, bt); } if (err == -ENOTSUPP) { mark\_all\_scalars\_precise(env, env->cur\_state);@@ -4203,22 +4246,10 @@ static int \_\_mark\_chain\_precision(struct bpf\_verifier\_env \*env, int regno) bitmap\_from\_u64(mask, bt\_frame\_stack\_mask(bt, fr)); for\_each\_set\_bit(i, mask, 64) { if (i >= func->allocated\_stack / BPF\_REG\_SIZE) {- /\* the sequence of instructions:- \* 2: (bf) r3 = r10- \* 3: (7b) \*(u64 \*)(r3 -8) = r0- \* 4: (79) r4 = \*(u64 \*)(r10 -8)- \* doesn't contain jmps. It's backtracked- \* as a single block.- \* During backtracking insn 3 is not recognized as- \* stack access, so at the end of backtracking- \* stack slot fp-8 is still marked in stack\_mask.- \* However the parent state may not have accessed- \* fp-8 and it's "unallocated" stack space.- \* In such case fallback to conservative.- \*/- mark\_all\_scalars\_precise(env, env->cur\_state);- bt\_reset(bt);- return 0;+ verbose(env, "BUG backtracking (stack slot %d, total slots %d)\n",+ i, func->allocated\_stack / BPF\_REG\_SIZE);+ WARN\_ONCE(1, "verifier backtracking bug (stack slot out of bounds)");+ return -EFAULT; }  if (!is\_spilled\_scalar\_reg(&func->stack[i])) {@@ -4391,7 +4422,7 @@ static int check\_stack\_write\_fixed\_off(struct bpf\_verifier\_env \*env, int i, slot = -off - 1, spi = slot / BPF\_REG\_SIZE, err; struct bpf\_insn \*insn = &env->prog->insnsi[insn\_idx]; struct bpf\_reg\_state \*reg = NULL;- u32 dst\_reg = insn->dst\_reg;+ int insn\_flags = insn\_stack\_access\_flags(state->frameno, spi);  err = grow\_stack\_state(state, round\_up(slot + 1, BPF\_REG\_SIZE)); if (err)@@ -4432,17 +4463,6 @@ static int check\_stack\_write\_fixed\_off(struct bpf\_verifier\_env \*env, mark\_stack\_slot\_scratched(env, spi); if (reg && !(off % BPF\_REG\_SIZE) && register\_is\_bounded(reg) && !register\_is\_null(reg) && env->bpf\_capable) {- if (dst\_reg != BPF\_REG\_FP) {- /\* The backtracking logic can only recognize explicit- \* stack slot address like [fp - 8]. Other spill of- \* scalar via different register has to be conservative.- \* Backtrack from here and mark all registers as precise- \* that contributed into 'reg' being a constant.- \*/- err = mark\_chain\_precision(env, value\_regno);- if (err)- return err;- } save\_register\_state(state, spi, reg, size); /\* Break the relation on a narrowing spill. \*/ if (fls64(reg->umax\_value) > BITS\_PER\_BYTE \* size)@@ -4454,6 +4474,7 @@ static int check\_stack\_write\_fixed\_off(struct bpf\_verifier\_env \*env, \_\_mark\_reg\_known(&fake\_reg, insn->imm); fake\_reg.type = SCALAR\_VALUE; save\_register\_state(state, spi, &fake\_reg, size);+ insn\_flags = 0; /\* not a register spill \*/ } else if (reg && is\_spillable\_regtype(reg->type)) { /\* register containing pointer is being spilled into stack \*/ if (size != BPF\_REG\_SIZE) {@@ -4499,9 +4520,12 @@ static int check\_stack\_write\_fixed\_off(struct bpf\_verifier\_env \*env,  /\* Mark slots affected by this stack write. \*/ for (i = 0; i < size; i++)- state->stack[spi].slot\_type[(slot - i) % BPF\_REG\_SIZE] =- type;+ state->stack[spi].slot\_type[(slot - i) % BPF\_REG\_SIZE] = type;+ insn\_flags = 0; /\* not a register spill \*/ }++ if (insn\_flags)+ return push\_jmp\_history(env, env->cur\_state, insn\_flags); return 0; } @@ -4694,6 +4718,7 @@ static int check\_stack\_read\_fixed\_off(struct bpf\_verifier\_env \*env, int i, slot = -off - 1, spi = slot / BPF\_REG\_SIZE; struct bpf\_reg\_state \*reg; u8 \*stype, type;+ int insn\_flags = insn\_stack\_access\_flags(reg\_state->frameno, spi);  stype = reg\_state->stack[spi].slot\_type; reg = &reg\_state->stack[spi].spilled\_ptr;@@ -4739,12 +4764,10 @@ static int check\_stack\_read\_fixed\_off(struct bpf\_verifier\_env \*env, return -EACCES; } mark\_reg\_unknown(env, state->regs, dst\_regno);+ insn\_flags = 0; /\* not restoring original register state \*/ } state->regs[dst\_regno].live |= REG\_LIVE\_WRITTEN;- return 0;- }-- if (dst\_regno >= 0) {+ } else if (dst\_regno >= 0) { /\* restore register state from stack \*/ copy\_register\_state(&state->regs[dst\_regno], reg); /\* mark reg as written since spilled pointer state likely@@ -4780,7 +4803,10 @@ static int check\_stack\_read\_fixed\_off(struct bpf\_verifier\_env \*env, mark\_reg\_read(env, reg, reg->parent, REG\_LIVE\_READ64); if (dst\_regno >= 0) mark\_reg\_stack\_read(env, reg\_state, off, off + size, dst\_regno);+ insn\_flags = 0; /\* we are not restoring spilled register \*/ }+ if (insn\_flags)+ return push\_jmp\_history(env, env->cur\_state, insn\_flags); return 0; } @@ -6940,7 +6966,6 @@ static int check\_atomic(struct bpf\_verifier\_env \*env, int insn\_idx, struct bpf\_i BPF\_SIZE(insn->code), BPF\_WRITE, -1, true, false); if (err) return err;- return 0; } @@ -16910,7 +16935,8 @@ hit: \* the precision needs to be propagated back in \* the current state. \*/- err = err ? : push\_jmp\_history(env, cur);+ if (is\_jmp\_point(env, env->insn\_idx))+ err = err ? : push\_jmp\_history(env, cur, 0); err = err ? : propagate\_precision(env, &sl->state); if (err) return err;@@ -17135,6 +17161,9 @@ static int do\_check(struct bpf\_verifier\_env \*env) u8 class; int err; + /\* reset current history entry on each new instruction \*/+ env->cur\_hist\_ent = NULL;+ env->prev\_insn\_idx = prev\_insn\_idx; if (env->insn\_idx >= insn\_cnt) { verbose(env, "invalid insn idx %d insn\_cnt %d\n",@@ -17174,7 +17203,7 @@ static int do\_check(struct bpf\_verifier\_env \*env) }  if (is\_jmp\_point(env, env->insn\_idx)) {- err = push\_jmp\_history(env, state);+ err = push\_jmp\_history(env, state, 0); if (err) return err; }diff --git a/tools/testing/selftests/bpf/progs/verifier\_subprog\_precision.c b/tools/testing/selftests/bpf/progs/verifier\_subprog\_precision.cindex 0dfe3f8b69acff..eba98fab2f5451 100644--- a/[tools/testing/selftests/bpf/progs/verifier\_subprog\_precision.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/tools/testing/selftests/bpf/progs/verifier_subprog_precision.c?id=5ffb260f754bf838507fe0c23d05254b33e2bf3d)+++ b/[tools/testing/selftests/bpf/progs/verifier\_subprog\_precision.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/tools/testing/selftests/bpf/progs/verifier_subprog_precision.c?id=41f6f64e6999a837048b1bd13a2f8742964eca6b)@@ -589,11 +589,24 @@ static \_\_u64 subprog\_spill\_reg\_precise(void)  SEC("?raw\_tp") \_\_success \_\_log\_level(2)-/\* precision backtracking can't currently handle stack access not through r10,- \* so we won't be able to mark stack slot fp-8 as precise, and so will- \* fallback to forcing all as precise- \*/-\_\_msg("mark\_precise: frame0: falling back to forcing all scalars precise")+\_\_msg("10: (0f) r1 += r7")+\_\_msg("mark\_precise: frame0: last\_idx 10 first\_idx 7 subseq\_idx -1")+\_\_msg("mark\_precise: frame0: regs=r7 stack= before 9: (bf) r1 = r8")+\_\_msg("mark\_precise: frame0: regs=r7 stack= before 8: (27) r7 \*= 4")+\_\_msg("mark\_precise: frame0: regs=r7 stack= before 7: (79) r7 = \*(u64 \*)(r10 -8)")+\_\_msg("mark\_precise: frame0: parent state regs= stack=-8: R0\_w=2 R6\_w=1 R8\_rw=map\_value(map=.data.vals,ks=4,vs=16) R10=fp0 fp-8\_rw=P1")+\_\_msg("mark\_precise: frame0: last\_idx 18 first\_idx 0 subseq\_idx 7")+\_\_msg("mark\_precise: frame0: regs= stack=-8 before 18: (95) exit")+\_\_msg("mark\_precise: frame1: regs= stack= before 17: (0f) r0 += r2")+\_\_msg("mark\_precise: frame1: regs= stack= before 16: (79) r2 = \*(u64 \*)(r1 +0)")+\_\_msg("mark\_precise: frame1: regs= stack= before 15: (79) r0 = \*(u64 \*)(r10 -16)")+\_\_msg("mark\_precise: frame1: regs= stack= before 14: (7b) \*(u64 \*)(r10 -16) = r2")+\_\_msg("mark\_precise: frame1: regs= stack= before 13: (7b) \*(u64 \*)(r1 +0) = r2")+\_\_msg("mark\_precise: frame1: regs=r2 stack= before 6: (85) call pc+6")+\_\_msg("mark\_precise: frame0: regs=r2 stack= before 5: (bf) r2 = r6")+\_\_msg("mark\_precise: frame0: regs=r6 stack= before 4: (07) r1 += -8")+\_\_msg("mark\_precise: frame0: regs=r6 stack= before 3: (bf) r1 = r10")+\_\_msg("mark\_precise: frame0: regs=r6 stack= before 2: (b7) r6 = 1") \_\_naked int subprog\_spill\_into\_parent\_stack\_slot\_precise(void) { asm volatile (diff --git a/tools/testing/selftests/bpf/verifier/precise.c b/tools/testing/selftests/bpf/verifier/precise.cindex 0d84dd1f38b6b0..8a2ff81d835088 100644--- a/[tools/testing/selftests/bpf/verifier/precise.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/tools/testing/selftests/bpf/verifier/precise.c?id=5ffb260f754bf838507fe0c23d05254b33e2bf3d)+++ b/[tools/testing/selftests/bpf/verifier/precise.c](/pub/scm/linux/kernel/git/stable/linux.git/tree/tools/testing/selftests/bpf/verifier/precise.c?id=41f6f64e6999a837048b1bd13a2f8742964eca6b)@@ -140,10 +140,11 @@ .result = REJECT, }, {- "precise: ST insn causing spi > allocated\_stack",+ "precise: ST zero to stack insn is supported", .insns = { BPF\_MOV64\_REG(BPF\_REG\_3, BPF\_REG\_10), BPF\_JMP\_IMM(BPF\_JNE, BPF\_REG\_3, 123, 0),+ /\* not a register spill, so we stop precision propagation for R4 here \*/ BPF\_ST\_MEM(BPF\_DW, BPF\_REG\_3, -8, 0), BPF\_LDX\_MEM(BPF\_DW, BPF\_REG\_4, BPF\_REG\_10, -8), BPF\_MOV64\_IMM(BPF\_REG\_0, -1),@@ -157,11 +158,11 @@ mark\_precise: frame0: last\_idx 4 first\_idx 2\ mark\_precise: frame0: regs=r4 stack= before 4\ mark\_precise: frame0: regs=r4 stack= before 3\- mark\_precise: frame0: regs= stack=-8 before 2\- mark\_precise: frame0: falling back to forcing all scalars precise\- force\_precise: frame0: forcing r0 to be precise\ mark\_precise: frame0: last\_idx 5 first\_idx 5\- mark\_precise: frame0: parent state regs= stack=:",+ mark\_precise: frame0: parent state regs=r0 stack=:\+ mark\_precise: frame0: last\_idx 4 first\_idx 2\+ mark\_precise: frame0: regs=r0 stack= before 4\+ 5: R0=-1 R4=0", .result = VERBOSE\_ACCEPT, .retval = -1, },@@ -169,6 +170,8 @@ "precise: STX insn causing spi > allocated\_stack", .insns = { BPF\_RAW\_INSN(BPF\_JMP | BPF\_CALL, 0, 0, 0, BPF\_FUNC\_get\_prandom\_u32),+ /\* make later reg spill more interesting by having somewhat known scalar \*/+ BPF\_ALU64\_IMM(BPF\_AND, BPF\_REG\_0, 0xff), BPF\_MOV64\_REG(BPF\_REG\_3, BPF\_REG\_10), BPF\_JMP\_IMM(BPF\_JNE, BPF\_REG\_3, 123, 0), BPF\_STX\_MEM(BPF\_DW, BPF\_REG\_3, BPF\_REG\_0, -8),@@ -179,18 +182,21 @@ }, .prog\_type = BPF\_PROG\_TYPE\_XDP, .flags = BPF\_F\_TEST\_STATE\_FREQ,- .errstr = "mark\_precise: frame0: last\_idx 6 first\_idx 6\+ .errstr = "mark\_precise: frame0: last\_idx 7 first\_idx 7\ mark\_precise: frame0: parent state regs=r4 stack=:\- mark\_precise: frame0: last\_idx 5 first\_idx 3\- mark\_precise: frame0: regs=r4 stack= before 5\- mark\_precise: frame0: regs=r4 stack= before 4\- mark\_precise: frame0: regs= stack=-8 before 3\- mark\_precise: frame0: falling back to forcing all scalars precise\- force\_precise: frame0: forcing r0 to be precise\- force\_precise: frame0: forcing r0 to be precise\- force\_precise: frame0: forcing r0 to be precise\- force\_precise: frame0: forcing r0 to be precise\- mark\_precise: frame0: last\_idx 6 first\_idx 6\+ mark\_precise: frame0: last\_idx 6 first\_idx 4\+ mark\_precise: frame0: regs=r4 stack= before 6: (b7) r0 = -1\+ mark\_precise: frame0: regs=r4 stack= before 5: (79) r4 = \*(u64 \*)(r10 -8)\+ mark\_precise: frame0: regs= stack=-8 before 4: (7b) \*(u64 \*)(r3 -8) = r0\+ mark\_precise: frame0: parent state regs=r0 stack=:\+ mark\_precise: frame0: last\_idx 3 first\_idx 3\+ mark\_precise: frame0: regs=r0 stack= before 3: (55) if r3 != 0x7b goto pc+0\+ mark\_precise: frame0: regs=r0 stack= before 2: (bf) r3 = r10\+ mark\_precise: frame0: regs=r0 stack= before 1: (57) r0 &= 255\+ mark\_precise: frame0: parent state regs=r0 stack=:\+ mark\_precise: frame0: last\_idx 0 first\_idx 0\+ mark\_precise: frame0: regs=r0 stack= before 0: (85) call bpf\_get\_prandom\_u32#7\+ mark\_precise: frame0: last\_idx 7 first\_idx 7\ mark\_precise: frame0: parent state regs= stack=:", .result = VERBOSE\_ACCEPT, .retval = -1, |
| --- |

generated by [cgit 1.2.3-korg](https://git.zx2c4.com/cgit/about/) ([git 2.43.0](https://git-scm.com/)) at 2025-01-15 08:00:51 +0000


