=== Content from github.com_be8dd3ee_20250114_205537.html ===

[Skip to content](#start-of-content)

## Navigation Menu

Toggle navigation

[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fblob%2Fb8cad4c631096a34461ff8a07840d5f4d123ce32%2Ftensorflow%2Fpython%2Fkeras%2FREADME.md)

* Product

  + [GitHub Copilot
    Write better code with AI](https://github.com/features/copilot)
  + [Security
    Find and fix vulnerabilities](https://github.com/features/security)
  + [Actions
    Automate any workflow](https://github.com/features/actions)
  + [Codespaces
    Instant dev environments](https://github.com/features/codespaces)
  + [Issues
    Plan and track work](https://github.com/features/issues)
  + [Code Review
    Manage code changes](https://github.com/features/code-review)
  + [Discussions
    Collaborate outside of code](https://github.com/features/discussions)
  + [Code Search
    Find more, search less](https://github.com/features/code-search)

  Explore
  + [All features](https://github.com/features)
  + [Documentation](https://docs.github.com)
  + [GitHub Skills](https://skills.github.com)
  + [Blog](https://github.blog)
* Solutions

  By company size
  + [Enterprises](https://github.com/enterprise)
  + [Small and medium teams](https://github.com/team)
  + [Startups](https://github.com/enterprise/startups)
  By use case
  + [DevSecOps](/solutions/use-case/devsecops)
  + [DevOps](/solutions/use-case/devops)
  + [CI/CD](/solutions/use-case/ci-cd)
  + [View all use cases](/solutions/use-case)

  By industry
  + [Healthcare](/solutions/industry/healthcare)
  + [Financial services](/solutions/industry/financial-services)
  + [Manufacturing](/solutions/industry/manufacturing)
  + [Government](/solutions/industry/government)
  + [View all industries](/solutions/industry)

  [View all solutions](/solutions)
* Resources

  Topics
  + [AI](/resources/articles/ai)
  + [DevOps](/resources/articles/devops)
  + [Security](/resources/articles/security)
  + [Software Development](/resources/articles/software-development)
  + [View all](/resources/articles)

  Explore
  + [Learning Pathways](https://resources.github.com/learn/pathways)
  + [White papers, Ebooks, Webinars](https://resources.github.com)
  + [Customer Stories](https://github.com/customer-stories)
  + [Partners](https://partner.github.com)
  + [Executive Insights](https://github.com/solutions/executive-insights)
* Open Source

  + [GitHub Sponsors
    Fund open source developers](/sponsors)
  + [The ReadME Project
    GitHub community articles](https://github.com/readme)
  Repositories
  + [Topics](https://github.com/topics)
  + [Trending](https://github.com/trending)
  + [Collections](https://github.com/collections)
* Enterprise

  + [Enterprise platform
    AI-powered developer platform](/enterprise)
  Available add-ons
  + [Advanced Security
    Enterprise-grade security features](https://github.com/enterprise/advanced-security)
  + [GitHub Copilot
    Enterprise-grade AI features](/features/copilot#enterprise)
  + [Premium Support
    Enterprise-grade 24/7 support](/premium-support)
* [Pricing](https://github.com/pricing)

Search or jump to...

# Search code, repositories, users, issues, pull requests...

Search

Clear

[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)

# Provide feedback

We read every piece of feedback, and take your input very seriously.

Include my email address so I can be contacted

  Cancel

 Submit feedback

# Saved searches

## Use saved searches to filter your results more quickly

Name

Query

To see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).

  Cancel

 Create saved search

[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fblob%2Fb8cad4c631096a34461ff8a07840d5f4d123ce32%2Ftensorflow%2Fpython%2Fkeras%2FREADME.md)

[Sign up](/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E%2Fblob%2Fshow&source=header-repo&source_repo=tensorflow%2Ftensorflow)
Reseting focus

You signed in with another tab or window. Reload to refresh your session.
You signed out in another tab or window. Reload to refresh your session.
You switched accounts on another tab or window. Reload to refresh your session.

Dismiss alert

{{ message }}

[tensorflow](/tensorflow)
/
**[tensorflow](/tensorflow/tensorflow)**
Public

* [Notifications](/login?return_to=%2Ftensorflow%2Ftensorflow) You must be signed in to change notification settings
* [Fork
  74.4k](/login?return_to=%2Ftensorflow%2Ftensorflow)
* [Star
   187k](/login?return_to=%2Ftensorflow%2Ftensorflow)

* [Code](/tensorflow/tensorflow)
* [Issues
  827](/tensorflow/tensorflow/issues)
* [Pull requests
  5k+](/tensorflow/tensorflow/pulls)
* [Actions](/tensorflow/tensorflow/actions)
* [Projects
  2](/tensorflow/tensorflow/projects)
* [Security](/tensorflow/tensorflow/security)
* [Insights](/tensorflow/tensorflow/pulse)

Additional navigation options

* [Code](/tensorflow/tensorflow)
* [Issues](/tensorflow/tensorflow/issues)
* [Pull requests](/tensorflow/tensorflow/pulls)
* [Actions](/tensorflow/tensorflow/actions)
* [Projects](/tensorflow/tensorflow/projects)
* [Security](/tensorflow/tensorflow/security)
* [Insights](/tensorflow/tensorflow/pulse)

## Files

 b8cad4c
## Breadcrumbs

1. [tensorflow](/tensorflow/tensorflow/tree/b8cad4c631096a34461ff8a07840d5f4d123ce32)
2. /[tensorflow](/tensorflow/tensorflow/tree/b8cad4c631096a34461ff8a07840d5f4d123ce32/tensorflow)
3. /[python](/tensorflow/tensorflow/tree/b8cad4c631096a34461ff8a07840d5f4d123ce32/tensorflow/python)
4. /[keras](/tensorflow/tensorflow/tree/b8cad4c631096a34461ff8a07840d5f4d123ce32/tensorflow/python/keras)
/
# README.md

Copy path Blame  Blame
## Latest commit

## History

[History](/tensorflow/tensorflow/commits/b8cad4c631096a34461ff8a07840d5f4d123ce32/tensorflow/python/keras/README.md)6 lines (4 loc) · 287 Bytes b8cad4c
## Breadcrumbs

1. [tensorflow](/tensorflow/tensorflow/tree/b8cad4c631096a34461ff8a07840d5f4d123ce32)
2. /[tensorflow](/tensorflow/tensorflow/tree/b8cad4c631096a34461ff8a07840d5f4d123ce32/tensorflow)
3. /[python](/tensorflow/tensorflow/tree/b8cad4c631096a34461ff8a07840d5f4d123ce32/tensorflow/python)
4. /[keras](/tensorflow/tensorflow/tree/b8cad4c631096a34461ff8a07840d5f4d123ce32/tensorflow/python/keras)
/
# README.md

Top
## File metadata and controls

* Preview
* Code
* Blame

6 lines (4 loc) · 287 Bytes[Raw](https://github.com/tensorflow/tensorflow/raw/b8cad4c631096a34461ff8a07840d5f4d123ce32/tensorflow/python/keras/README.md)

Keras is an object-oriented API for defining and training neural networks.

This module contains a pure-TensorFlow implementation of the Keras API,
allowing for deep integration with TensorFlow functionality.

See [keras.io](https://keras.io) for complete documentation and user guides.

## Footer

© 2025 GitHub, Inc.

### Footer navigation

* [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)
* [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)
* [Security](https://github.com/security)
* [Status](https://www.githubstatus.com/)
* [Docs](https://docs.github.com/)
* [Contact](https://support.github.com?tags=dotcom-footer)
* Manage cookies
* Do not share my personal information

You can’t perform that action at this time.



=== Content from docs.python.org_4f181ca5_20250114_205534.html ===


[![Python logo](../_static/py.svg)](https://www.python.org/)

Theme
Auto
Light
Dark

### [Table of Contents](../contents.html)

* `tarfile` â Read and write tar archive files
  + [TarFile Objects](#tarfile-objects)
  + [TarInfo Objects](#tarinfo-objects)
  + [Extraction filters](#extraction-filters)
    - [Default named filters](#default-named-filters)
    - [Filter errors](#filter-errors)
    - [Hints for further verification](#hints-for-further-verification)
    - [Supporting older Python versions](#supporting-older-python-versions)
    - [Stateful extraction filter example](#stateful-extraction-filter-example)
  + [Command-Line Interface](#command-line-interface)
    - [Command-line options](#command-line-options)
  + [Examples](#examples)
  + [Supported tar formats](#supported-tar-formats)
  + [Unicode issues](#unicode-issues)

#### Previous topic

[`zipfile` â Work with ZIP archives](zipfile.html "previous chapter")

#### Next topic

[File Formats](fileformats.html "next chapter")

### This Page

* [Report a Bug](../bugs.html)
* [Show Source](https://github.com/python/cpython/blob/main/Doc/library/tarfile.rst)

### Navigation

* [index](../genindex.html "General Index")
* [modules](../py-modindex.html "Python Module Index") |
* [next](fileformats.html "File Formats") |
* [previous](zipfile.html "zipfile â Work with ZIP archives") |
* ![Python logo](../_static/py.svg)
* [Python](https://www.python.org/) »
* [3.13.1 Documentation](../index.html) »
* [The Python Standard Library](index.html) »
* [Data Compression and Archiving](archiving.html) »
* `tarfile` â Read and write tar archive files
* |
* Theme
  Auto
  Light
  Dark

   |

# `tarfile` â Read and write tar archive files[Â¶](#module-tarfile "Link to this heading")

**Source code:** [Lib/tarfile.py](https://github.com/python/cpython/tree/3.13/Lib/tarfile.py)

---

The [`tarfile`](#module-tarfile "tarfile: Read and write tar-format archive files.") module makes it possible to read and write tar
archives, including those using gzip, bz2 and lzma compression.
Use the [`zipfile`](zipfile.html#module-zipfile "zipfile: Read and write ZIP-format archive files.") module to read or write `.zip` files, or the
higher-level functions in [shutil](shutil.html#archiving-operations).

Some facts and figures:

* reads and writes [`gzip`](gzip.html#module-gzip "gzip: Interfaces for gzip compression and decompression using file objects."), [`bz2`](bz2.html#module-bz2 "bz2: Interfaces for bzip2 compression and decompression.") and [`lzma`](lzma.html#module-lzma "lzma: A Python wrapper for the liblzma compression library.") compressed archives
  if the respective modules are available.
* read/write support for the POSIX.1-1988 (ustar) format.
* read/write support for the GNU tar format including *longname* and *longlink*
  extensions, read-only support for all variants of the *sparse* extension
  including restoration of sparse files.
* read/write support for the POSIX.1-2001 (pax) format.
* handles directories, regular files, hardlinks, symbolic links, fifos,
  character devices and block devices and is able to acquire and restore file
  information like timestamp, access permissions and owner.

Changed in version 3.3: Added support for [`lzma`](lzma.html#module-lzma "lzma: A Python wrapper for the liblzma compression library.") compression.

Changed in version 3.12: Archives are extracted using a [filter](#tarfile-extraction-filter),
which makes it possible to either limit surprising/dangerous features,
or to acknowledge that they are expected and the archive is fully trusted.
By default, archives are fully trusted, but this default is deprecated
and slated to change in Python 3.14.

tarfile.open(*name=None*, *mode='r'*, *fileobj=None*, *bufsize=10240*, *\*\*kwargs*)[Â¶](#tarfile.open "Link to this definition")

Return a [`TarFile`](#tarfile.TarFile "tarfile.TarFile") object for the pathname *name*. For detailed
information on [`TarFile`](#tarfile.TarFile "tarfile.TarFile") objects and the keyword arguments that are
allowed, see [TarFile Objects](#tarfile-objects).

*mode* has to be a string of the form `'filemode[:compression]'`, it defaults
to `'r'`. Here is a full list of mode combinations:

| mode | action |
| --- | --- |
| `'r' or 'r:*'` | Open for reading with transparent compression (recommended). |
| `'r:'` | Open for reading exclusively without compression. |
| `'r:gz'` | Open for reading with gzip compression. |
| `'r:bz2'` | Open for reading with bzip2 compression. |
| `'r:xz'` | Open for reading with lzma compression. |
| `'x'` or `'x:'` | Create a tarfile exclusively without compression. Raise a [`FileExistsError`](exceptions.html#FileExistsError "FileExistsError") exception if it already exists. |
| `'x:gz'` | Create a tarfile with gzip compression. Raise a [`FileExistsError`](exceptions.html#FileExistsError "FileExistsError") exception if it already exists. |
| `'x:bz2'` | Create a tarfile with bzip2 compression. Raise a [`FileExistsError`](exceptions.html#FileExistsError "FileExistsError") exception if it already exists. |
| `'x:xz'` | Create a tarfile with lzma compression. Raise a [`FileExistsError`](exceptions.html#FileExistsError "FileExistsError") exception if it already exists. |
| `'a' or 'a:'` | Open for appending with no compression. The file is created if it does not exist. |
| `'w' or 'w:'` | Open for uncompressed writing. |
| `'w:gz'` | Open for gzip compressed writing. |
| `'w:bz2'` | Open for bzip2 compressed writing. |
| `'w:xz'` | Open for lzma compressed writing. |

Note that `'a:gz'`, `'a:bz2'` or `'a:xz'` is not possible. If *mode*
is not suitable to open a certain (compressed) file for reading,
[`ReadError`](#tarfile.ReadError "tarfile.ReadError") is raised. Use *mode* `'r'` to avoid this. If a
compression method is not supported, [`CompressionError`](#tarfile.CompressionError "tarfile.CompressionError") is raised.

If *fileobj* is specified, it is used as an alternative to a [file object](../glossary.html#term-file-object)
opened in binary mode for *name*. It is supposed to be at position 0.

For modes `'w:gz'`, `'x:gz'`, `'w|gz'`, `'w:bz2'`, `'x:bz2'`,
`'w|bz2'`, [`tarfile.open()`](#tarfile.open "tarfile.open") accepts the keyword argument
*compresslevel* (default `9`) to specify the compression level of the file.

For modes `'w:xz'` and `'x:xz'`, [`tarfile.open()`](#tarfile.open "tarfile.open") accepts the
keyword argument *preset* to specify the compression level of the file.

For special purposes, there is a second format for *mode*:
`'filemode|[compression]'`. [`tarfile.open()`](#tarfile.open "tarfile.open") will return a [`TarFile`](#tarfile.TarFile "tarfile.TarFile")
object that processes its data as a stream of blocks. No random seeking will
be done on the file. If given, *fileobj* may be any object that has a
[`read()`](io.html#io.RawIOBase.read "io.RawIOBase.read") or [`write()`](io.html#io.RawIOBase.write "io.RawIOBase.write") method
(depending on the *mode*) that works with bytes.
*bufsize* specifies the blocksize and defaults to `20 * 512` bytes.
Use this variant in combination with e.g. `sys.stdin.buffer`, a socket
[file object](../glossary.html#term-file-object) or a tape device.
However, such a [`TarFile`](#tarfile.TarFile "tarfile.TarFile") object is limited in that it does
not allow random access, see [Examples](#tar-examples). The currently
possible modes:

| Mode | Action |
| --- | --- |
| `'r|*'` | Open a *stream* of tar blocks for reading with transparent compression. |
| `'r|'` | Open a *stream* of uncompressed tar blocks for reading. |
| `'r|gz'` | Open a gzip compressed *stream* for reading. |
| `'r|bz2'` | Open a bzip2 compressed *stream* for reading. |
| `'r|xz'` | Open an lzma compressed *stream* for reading. |
| `'w|'` | Open an uncompressed *stream* for writing. |
| `'w|gz'` | Open a gzip compressed *stream* for writing. |
| `'w|bz2'` | Open a bzip2 compressed *stream* for writing. |
| `'w|xz'` | Open an lzma compressed *stream* for writing. |

Changed in version 3.5: The `'x'` (exclusive creation) mode was added.

Changed in version 3.6: The *name* parameter accepts a [path-like object](../glossary.html#term-path-like-object).

Changed in version 3.12: The *compresslevel* keyword argument also works for streams.

*class* tarfile.TarFile

Class for reading and writing tar archives. Do not use this class directly:
use [`tarfile.open()`](#tarfile.open "tarfile.open") instead. See [TarFile Objects](#tarfile-objects).

tarfile.is\_tarfile(*name*)[Â¶](#tarfile.is_tarfile "Link to this definition")

Return [`True`](constants.html#True "True") if *name* is a tar archive file, that the [`tarfile`](#module-tarfile "tarfile: Read and write tar-format archive files.")
module can read. *name* may be a [`str`](stdtypes.html#str "str"), file, or file-like object.

Changed in version 3.9: Support for file and file-like objects.

The [`tarfile`](#module-tarfile "tarfile: Read and write tar-format archive files.") module defines the following exceptions:

*exception* tarfile.TarError[Â¶](#tarfile.TarError "Link to this definition")

Base class for all [`tarfile`](#module-tarfile "tarfile: Read and write tar-format archive files.") exceptions.

*exception* tarfile.ReadError[Â¶](#tarfile.ReadError "Link to this definition")

Is raised when a tar archive is opened, that either cannot be handled by the
[`tarfile`](#module-tarfile "tarfile: Read and write tar-format archive files.") module or is somehow invalid.

*exception* tarfile.CompressionError[Â¶](#tarfile.CompressionError "Link to this definition")

Is raised when a compression method is not supported or when the data cannot be
decoded properly.

*exception* tarfile.StreamError[Â¶](#tarfile.StreamError "Link to this definition")

Is raised for the limitations that are typical for stream-like [`TarFile`](#tarfile.TarFile "tarfile.TarFile")
objects.

*exception* tarfile.ExtractError[Â¶](#tarfile.ExtractError "Link to this definition")

Is raised for *non-fatal* errors when using [`TarFile.extract()`](#tarfile.TarFile.extract "tarfile.TarFile.extract"), but only if
[`TarFile.errorlevel`](#tarfile.TarFile.errorlevel "tarfile.TarFile.errorlevel")`== 2`.

*exception* tarfile.HeaderError[Â¶](#tarfile.HeaderError "Link to this definition")

Is raised by [`TarInfo.frombuf()`](#tarfile.TarInfo.frombuf "tarfile.TarInfo.frombuf") if the buffer it gets is invalid.

*exception* tarfile.FilterError[Â¶](#tarfile.FilterError "Link to this definition")

Base class for members [refused](#tarfile-extraction-refuse) by
filters.

tarinfo[Â¶](#tarfile.FilterError.tarinfo "Link to this definition")

Information about the member that the filter refused to extract,
as [TarInfo](#tarinfo-objects).

*exception* tarfile.AbsolutePathError[Â¶](#tarfile.AbsolutePathError "Link to this definition")

Raised to refuse extracting a member with an absolute path.

*exception* tarfile.OutsideDestinationError[Â¶](#tarfile.OutsideDestinationError "Link to this definition")

Raised to refuse extracting a member outside the destination directory.

*exception* tarfile.SpecialFileError[Â¶](#tarfile.SpecialFileError "Link to this definition")

Raised to refuse extracting a special file (e.g. a device or pipe).

*exception* tarfile.AbsoluteLinkError[Â¶](#tarfile.AbsoluteLinkError "Link to this definition")

Raised to refuse extracting a symbolic link with an absolute path.

*exception* tarfile.LinkOutsideDestinationError[Â¶](#tarfile.LinkOutsideDestinationError "Link to this definition")

Raised to refuse extracting a symbolic link pointing outside the destination
directory.

The following constants are available at the module level:

tarfile.ENCODING[Â¶](#tarfile.ENCODING "Link to this definition")

The default character encoding: `'utf-8'` on Windows, the value returned by
[`sys.getfilesystemencoding()`](sys.html#sys.getfilesystemencoding "sys.getfilesystemencoding") otherwise.

tarfile.REGTYPE[Â¶](#tarfile.REGTYPE "Link to this definition")
tarfile.AREGTYPE[Â¶](#tarfile.AREGTYPE "Link to this definition")

A regular file [`type`](#tarfile.TarInfo.type "tarfile.TarInfo.type").

tarfile.LNKTYPE[Â¶](#tarfile.LNKTYPE "Link to this definition")

A link (inside tarfile) [`type`](#tarfile.TarInfo.type "tarfile.TarInfo.type").

tarfile.SYMTYPE[Â¶](#tarfile.SYMTYPE "Link to this definition")

A symbolic link [`type`](#tarfile.TarInfo.type "tarfile.TarInfo.type").

tarfile.CHRTYPE[Â¶](#tarfile.CHRTYPE "Link to this definition")

A character special device [`type`](#tarfile.TarInfo.type "tarfile.TarInfo.type").

tarfile.BLKTYPE[Â¶](#tarfile.BLKTYPE "Link to this definition")

A block special device [`type`](#tarfile.TarInfo.type "tarfile.TarInfo.type").

tarfile.DIRTYPE[Â¶](#tarfile.DIRTYPE "Link to this definition")

A directory [`type`](#tarfile.TarInfo.type "tarfile.TarInfo.type").

tarfile.FIFOTYPE[Â¶](#tarfile.FIFOTYPE "Link to this definition")

A FIFO special device [`type`](#tarfile.TarInfo.type "tarfile.TarInfo.type").

tarfile.CONTTYPE[Â¶](#tarfile.CONTTYPE "Link to this definition")

A contiguous file [`type`](#tarfile.TarInfo.type "tarfile.TarInfo.type").

tarfile.GNUTYPE\_LONGNAME[Â¶](#tarfile.GNUTYPE_LONGNAME "Link to this definition")

A GNU tar longname [`type`](#tarfile.TarInfo.type "tarfile.TarInfo.type").

tarfile.GNUTYPE\_LONGLINK[Â¶](#tarfile.GNUTYPE_LONGLINK "Link to this definition")

A GNU tar longlink [`type`](#tarfile.TarInfo.type "tarfile.TarInfo.type").

tarfile.GNUTYPE\_SPARSE[Â¶](#tarfile.GNUTYPE_SPARSE "Link to this definition")

A GNU tar sparse file [`type`](#tarfile.TarInfo.type "tarfile.TarInfo.type").

Each of the following constants defines a tar archive format that the
[`tarfile`](#module-tarfile "tarfile: Read and write tar-format archive files.") module is able to create. See section [Supported tar formats](#tar-formats) for
details.

tarfile.USTAR\_FORMAT[Â¶](#tarfile.USTAR_FORMAT "Link to this definition")

POSIX.1-1988 (ustar) format.

tarfile.GNU\_FORMAT[Â¶](#tarfile.GNU_FORMAT "Link to this definition")

GNU tar format.

tarfile.PAX\_FORMAT[Â¶](#tarfile.PAX_FORMAT "Link to this definition")

POSIX.1-2001 (pax) format.

tarfile.DEFAULT\_FORMAT[Â¶](#tarfile.DEFAULT_FORMAT "Link to this definition")

The default format for creating archives. This is currently [`PAX_FORMAT`](#tarfile.PAX_FORMAT "tarfile.PAX_FORMAT").

Changed in version 3.8: The default format for new archives was changed to
[`PAX_FORMAT`](#tarfile.PAX_FORMAT "tarfile.PAX_FORMAT") from [`GNU_FORMAT`](#tarfile.GNU_FORMAT "tarfile.GNU_FORMAT").

See also

Module [`zipfile`](zipfile.html#module-zipfile "zipfile: Read and write ZIP-format archive files.")

Documentation of the [`zipfile`](zipfile.html#module-zipfile "zipfile: Read and write ZIP-format archive files.") standard module.

[Archiving operations](shutil.html#archiving-operations)

Documentation of the higher-level archiving facilities provided by the
standard [`shutil`](shutil.html#module-shutil "shutil: High-level file operations, including copying.") module.

[GNU tar manual, Basic Tar Format](https://www.gnu.org/software/tar/manual/html_node/Standard.html)

Documentation for tar archive files, including GNU tar extensions.

## TarFile Objects[Â¶](#tarfile-objects "Link to this heading")

The [`TarFile`](#tarfile.TarFile "tarfile.TarFile") object provides an interface to a tar archive. A tar
archive is a sequence of blocks. An archive member (a stored file) is made up of
a header block followed by data blocks. It is possible to store a file in a tar
archive several times. Each archive member is represented by a [`TarInfo`](#tarfile.TarInfo "tarfile.TarInfo")
object, see [TarInfo Objects](#tarinfo-objects) for details.

A [`TarFile`](#tarfile.TarFile "tarfile.TarFile") object can be used as a context manager in a [`with`](../reference/compound_stmts.html#with)
statement. It will automatically be closed when the block is completed. Please
note that in the event of an exception an archive opened for writing will not
be finalized; only the internally used file object will be closed. See the
[Examples](#tar-examples) section for a use case.

Added in version 3.2: Added support for the context management protocol.

*class* tarfile.TarFile(*name=None*, *mode='r'*, *fileobj=None*, *format=DEFAULT\_FORMAT*, *tarinfo=TarInfo*, *dereference=False*, *ignore\_zeros=False*, *encoding=ENCODING*, *errors='surrogateescape'*, *pax\_headers=None*, *debug=0*, *errorlevel=1*, *stream=False*)[Â¶](#tarfile.TarFile "Link to this definition")

All following arguments are optional and can be accessed as instance attributes
as well.

*name* is the pathname of the archive. *name* may be a [path-like object](../glossary.html#term-path-like-object).
It can be omitted if *fileobj* is given.
In this case, the file objectâs `name` attribute is used if it exists.

*mode* is either `'r'` to read from an existing archive, `'a'` to append
data to an existing file, `'w'` to create a new file overwriting an existing
one, or `'x'` to create a new file only if it does not already exist.

If *fileobj* is given, it is used for reading or writing data. If it can be
determined, *mode* is overridden by *fileobj*âs mode. *fileobj* will be used
from position 0.

Note

*fileobj* is not closed, when [`TarFile`](#tarfile.TarFile "tarfile.TarFile") is closed.

*format* controls the archive format for writing. It must be one of the constants
[`USTAR_FORMAT`](#tarfile.USTAR_FORMAT "tarfile.USTAR_FORMAT"), [`GNU_FORMAT`](#tarfile.GNU_FORMAT "tarfile.GNU_FORMAT") or [`PAX_FORMAT`](#tarfile.PAX_FORMAT "tarfile.PAX_FORMAT") that are
defined at module level. When reading, format will be automatically detected, even
if different formats are present in a single archive.

The *tarinfo* argument can be used to replace the default [`TarInfo`](#tarfile.TarInfo "tarfile.TarInfo") class
with a different one.

If *dereference* is [`False`](constants.html#False "False"), add symbolic and hard links to the archive. If it
is [`True`](constants.html#True "True"), add the content of the target files to the archive. This has no
effect on systems that do not support symbolic links.

If *ignore\_zeros* is [`False`](constants.html#False "False"), treat an empty block as the end of the archive.
If it is [`True`](constants.html#True "True"), skip empty (and invalid) blocks and try to get as many members
as possible. This is only useful for reading concatenated or damaged archives.

*debug* can be set from `0` (no debug messages) up to `3` (all debug
messages). The messages are written to `sys.stderr`.

*errorlevel* controls how extraction errors are handled,
see [`the corresponding attribute`](#tarfile.TarFile.errorlevel "tarfile.TarFile.errorlevel").

The *encoding* and *errors* arguments define the character encoding to be
used for reading or writing the archive and how conversion errors are going
to be handled. The default settings will work for most users.
See section [Unicode issues](#tar-unicode) for in-depth information.

The *pax\_headers* argument is an optional dictionary of strings which
will be added as a pax global header if *format* is [`PAX_FORMAT`](#tarfile.PAX_FORMAT "tarfile.PAX_FORMAT").

If *stream* is set to [`True`](constants.html#True "True") then while reading the archive info about files
in the archive are not cached, saving memory.

Changed in version 3.2: Use `'surrogateescape'` as the default for the *errors* argument.

Changed in version 3.5: The `'x'` (exclusive creation) mode was added.

Changed in version 3.6: The *name* parameter accepts a [path-like object](../glossary.html#term-path-like-object).

Changed in version 3.13: Add the *stream* parameter.

*classmethod* TarFile.open(*...*)[Â¶](#tarfile.TarFile.open "Link to this definition")

Alternative constructor. The [`tarfile.open()`](#tarfile.open "tarfile.open") function is actually a
shortcut to this classmethod.

TarFile.getmember(*name*)[Â¶](#tarfile.TarFile.getmember "Link to this definition")

Return a [`TarInfo`](#tarfile.TarInfo "tarfile.TarInfo") object for member *name*. If *name* can not be found
in the archive, [`KeyError`](exceptions.html#KeyError "KeyError") is raised.

Note

If a member occurs more than once in the archive, its last occurrence is assumed
to be the most up-to-date version.

TarFile.getmembers()[Â¶](#tarfile.TarFile.getmembers "Link to this definition")

Return the members of the archive as a list of [`TarInfo`](#tarfile.TarInfo "tarfile.TarInfo") objects. The
list has the same order as the members in the archive.

TarFile.getnames()[Â¶](#tarfile.TarFile.getnames "Link to this definition")

Return the members as a list of their names. It has the same order as the list
returned by [`getmembers()`](#tarfile.TarFile.getmembers "tarfile.TarFile.getmembers").

TarFile.list(*verbose=True*, *\**, *members=None*)[Â¶](#tarfile.TarFile.list "Link to this definition")

Print a table of contents to `sys.stdout`. If *verbose* is [`False`](constants.html#False "False"),
only the names of the members are printed. If it is [`True`](constants.html#True "True"), output
similar to that of **ls -l** is produced. If optional *members* is
given, it must be a subset of the list returned by [`getmembers()`](#tarfile.TarFile.getmembers "tarfile.TarFile.getmembers").

Changed in version 3.5: Added the *members* parameter.

TarFile.next()[Â¶](#tarfile.TarFile.next "Link to this definition")

Return the next member of the archive as a [`TarInfo`](#tarfile.TarInfo "tarfile.TarInfo") object, when
[`TarFile`](#tarfile.TarFile "tarfile.TarFile") is opened for reading. Return [`None`](constants.html#None "None") if there is no more
available.

TarFile.extractall(*path='.'*, *members=None*, *\**, *numeric\_owner=False*, *filter=None*)[Â¶](#tarfile.TarFile.extractall "Link to this definition")

Extract all members from the archive to the current working directory or
directory *path*. If optional *members* is given, it must be a subset of the
list returned by [`getmembers()`](#tarfile.TarFile.getmembers "tarfile.TarFile.getmembers"). Directory information like owner,
modification time and permissions are set after all members have been extracted.
This is done to work around two problems: A directoryâs modification time is
reset each time a file is created in it. And, if a directoryâs permissions do
not allow writing, extracting files to it will fail.

If *numeric\_owner* is [`True`](constants.html#True "True"), the uid and gid numbers from the tarfile
are used to set the owner/group for the extracted files. Otherwise, the named
values from the tarfile are used.

The *filter* argument specifies how `members` are modified or rejected
before extraction.
See [Extraction filters](#tarfile-extraction-filter) for details.
It is recommended to set this explicitly depending on which *tar* features
you need to support.

Warning

Never extract archives from untrusted sources without prior inspection.
It is possible that files are created outside of *path*, e.g. members
that have absolute filenames starting with `"/"` or filenames with two
dots `".."`.

Set `filter='data'` to prevent the most dangerous security issues,
and read the [Extraction filters](#tarfile-extraction-filter) section for details.

Changed in version 3.5: Added the *numeric\_owner* parameter.

Changed in version 3.6: The *path* parameter accepts a [path-like object](../glossary.html#term-path-like-object).

Changed in version 3.12: Added the *filter* parameter.

TarFile.extract(*member*, *path=''*, *set\_attrs=True*, *\**, *numeric\_owner=False*, *filter=None*)[Â¶](#tarfile.TarFile.extract "Link to this definition")

Extract a member from the archive to the current working directory, using its
full name. Its file information is extracted as accurately as possible. *member*
may be a filename or a [`TarInfo`](#tarfile.TarInfo "tarfile.TarInfo") object. You can specify a different
directory using *path*. *path* may be a [path-like object](../glossary.html#term-path-like-object).
File attributes (owner, mtime, mode) are set unless *set\_attrs* is false.

The *numeric\_owner* and *filter* arguments are the same as
for [`extractall()`](#tarfile.TarFile.extractall "tarfile.TarFile.extractall").

Note

The [`extract()`](#tarfile.TarFile.extract "tarfile.TarFile.extract") method does not take care of several extraction issues.
In most cases you should consider using the [`extractall()`](#tarfile.TarFile.extractall "tarfile.TarFile.extractall") method.

Warning

See the warning for [`extractall()`](#tarfile.TarFile.extractall "tarfile.TarFile.extractall").

Set `filter='data'` to prevent the most dangerous security issues,
and read the [Extraction filters](#tarfile-extraction-filter) section for details.

Changed in version 3.2: Added the *set\_attrs* parameter.

Changed in version 3.5: Added the *numeric\_owner* parameter.

Changed in version 3.6: The *path* parameter accepts a [path-like object](../glossary.html#term-path-like-object).

Changed in version 3.12: Added the *filter* parameter.

TarFile.extractfile(*member*)[Â¶](#tarfile.TarFile.extractfile "Link to this definition")

Extract a member from the archive as a file object. *member* may be
a filename or a [`TarInfo`](#tarfile.TarInfo "tarfile.TarInfo") object. If *member* is a regular file or
a link, an [`io.BufferedReader`](io.html#io.BufferedReader "io.BufferedReader") object is returned. For all other
existing members, [`None`](constants.html#None "None") is returned. If *member* does not appear
in the archive, [`KeyError`](exceptions.html#KeyError "KeyError") is raised.

Changed in version 3.3: Return an [`io.BufferedReader`](io.html#io.BufferedReader "io.BufferedReader") object.

Changed in version 3.13: The returned [`io.BufferedReader`](io.html#io.BufferedReader "io.BufferedReader") object has the `mode`
attribute which is always equal to `'rb'`.

TarFile.errorlevel*: [int](functions.html#int "int")*[Â¶](#tarfile.TarFile.errorlevel "Link to this definition")

If *errorlevel* is `0`, errors are ignored when using [`TarFile.extract()`](#tarfile.TarFile.extract "tarfile.TarFile.extract")
and [`TarFile.extractall()`](#tarfile.TarFile.extractall "tarfile.TarFile.extractall").
Nevertheless, they appear as error messages in the debug output when
*debug* is greater than 0.
If `1` (the default), all *fatal* errors are raised as [`OSError`](exceptions.html#OSError "OSError") or
[`FilterError`](#tarfile.FilterError "tarfile.FilterError") exceptions. If `2`, all *non-fatal* errors are raised
as [`TarError`](#tarfile.TarError "tarfile.TarError") exceptions as well.

Some exceptions, e.g. ones caused by wrong argument types or data
corruption, are always raised.

Custom [extraction filters](#tarfile-extraction-filter)
should raise [`FilterError`](#tarfile.FilterError "tarfile.FilterError") for *fatal* errors
and [`ExtractError`](#tarfile.ExtractError "tarfile.ExtractError") for *non-fatal* ones.

Note that when an exception is raised, the archive may be partially
extracted. It is the userâs responsibility to clean up.

TarFile.extraction\_filter[Â¶](#tarfile.TarFile.extraction_filter "Link to this definition")

Added in version 3.12.

The [extraction filter](#tarfile-extraction-filter) used
as a default for the *filter* argument of [`extract()`](#tarfile.TarFile.extract "tarfile.TarFile.extract")
and [`extractall()`](#tarfile.TarFile.extractall "tarfile.TarFile.extractall").

The attribute may be `None` or a callable.
String names are not allowed for this attribute, unlike the *filter*
argument to [`extract()`](#tarfile.TarFile.extract "tarfile.TarFile.extract").

If `extraction_filter` is `None` (the default),
calling an extraction method without a *filter* argument will raise a
`DeprecationWarning`,
and fall back to the [`fully_trusted`](#tarfile.fully_trusted_filter "tarfile.fully_trusted_filter") filter,
whose dangerous behavior matches previous versions of Python.

In Python 3.14+, leaving `extraction_filter=None` will cause
extraction methods to use the [`data`](#tarfile.data_filter "tarfile.data_filter") filter by default.

The attribute may be set on instances or overridden in subclasses.
It also is possible to set it on the `TarFile` class itself to set a
global default, although, since it affects all uses of *tarfile*,
it is best practice to only do so in top-level applications or
[`site configuration`](site.html#module-site "site: Module responsible for site-specific configuration.").
To set a global default this way, a filter function needs to be wrapped in
[`staticmethod()`](functions.html#staticmethod "staticmethod") to prevent injection of a `self` argument.

TarFile.add(*name*, *arcname=None*, *recursive=True*, *\**, *filter=None*)[Â¶](#tarfile.TarFile.add "Link to this definition")

Add the file *name* to the archive. *name* may be any type of file
(directory, fifo, symbolic link, etc.). If given, *arcname* specifies an
alternative name for the file in the archive. Directories are added
recursively by default. This can be avoided by setting *recursive* to
[`False`](constants.html#False "False"). Recursion adds entries in sorted order.
If *filter* is given, it
should be a function that takes a [`TarInfo`](#tarfile.TarInfo "tarfile.TarInfo") object argument and
returns the changed [`TarInfo`](#tarfile.TarInfo "tarfile.TarInfo") object. If it instead returns
[`None`](constants.html#None "None") the [`TarInfo`](#tarfile.TarInfo "tarfile.TarInfo") object will be excluded from the
archive. See [Examples](#tar-examples) for an example.

Changed in version 3.2: Added the *filter* parameter.

Changed in version 3.7: Recursion adds entries in sorted order.

TarFile.addfile(*tarinfo*, *fileobj=None*)[Â¶](#tarfile.TarFile.addfile "Link to this definition")

Add the [`TarInfo`](#tarfile.TarInfo "tarfile.TarInfo") object *tarinfo* to the archive. If *tarinfo* represents
a non zero-size regular file, the *fileobj* argument should be a [binary file](../glossary.html#term-binary-file),
and `tarinfo.size` bytes are read from it and added to the archive. You can
create [`TarInfo`](#tarfile.TarInfo "tarfile.TarInfo") objects directly, or by using [`gettarinfo()`](#tarfile.TarFile.gettarinfo "tarfile.TarFile.gettarinfo").

Changed in version 3.13: *fileobj* must be given for non-zero-sized regular files.

TarFile.gettarinfo(*name=None*, *arcname=None*, *fileobj=None*)[Â¶](#tarfile.TarFile.gettarinfo "Link to this definition")

Create a [`TarInfo`](#tarfile.TarInfo "tarfile.TarInfo") object from the result of [`os.stat()`](os.html#os.stat "os.stat") or
equivalent on an existing file. The file is either named by *name*, or
specified as a [file object](../glossary.html#term-file-object) *fileobj* with a file descriptor.
*name* may be a [path-like object](../glossary.html#term-path-like-object). If
given, *arcname* specifies an alternative name for the file in the
archive, otherwise, the name is taken from *fileobj*âs
[`name`](io.html#io.FileIO.name "io.FileIO.name") attribute, or the *name* argument. The name
should be a text string.

You can modify
some of the [`TarInfo`](#tarfile.TarInfo "tarfile.TarInfo")âs attributes before you add it using [`addfile()`](#tarfile.TarFile.addfile "tarfile.TarFile.addfile").
If the file object is not an ordinary file object positioned at the
beginning of the file, attributes such as [`size`](#tarfile.TarInfo.size "tarfile.TarInfo.size") may need
modifying. This is the case for objects such as [`GzipFile`](gzip.html#gzip.GzipFile "gzip.GzipFile").
The [`name`](#tarfile.TarInfo.name "tarfile.TarInfo.name") may also be modified, in which case *arcname*
could be a dummy string.

Changed in version 3.6: The *name* parameter accepts a [path-like object](../glossary.html#term-path-like-object).

TarFile.close()[Â¶](#tarfile.TarFile.close "Link to this definition")

Close the [`TarFile`](#tarfile.TarFile "tarfile.TarFile"). In write mode, two finishing zero blocks are
appended to the archive.

TarFile.pax\_headers*: [dict](stdtypes.html#dict "dict")*[Â¶](#tarfile.TarFile.pax_headers "Link to this definition")

A dictionary containing key-value pairs of pax global headers.

## TarInfo Objects[Â¶](#tarinfo-objects "Link to this heading")

A [`TarInfo`](#tarfile.TarInfo "tarfile.TarInfo") object represents one member in a [`TarFile`](#tarfile.TarFile "tarfile.TarFile"). Aside
from storing all required attributes of a file (like file type, size, time,
permissions, owner etc.), it provides some useful methods to determine its type.
It does *not* contain the fileâs data itself.

[`TarInfo`](#tarfile.TarInfo "tarfile.TarInfo") objects are returned by [`TarFile`](#tarfile.TarFile "tarfile.TarFile")âs methods
[`getmember()`](#tarfile.TarFile.getmember "tarfile.TarFile.getmember"), [`getmembers()`](#tarfile.TarFile.getmembers "tarfile.TarFile.getmembers") and
[`gettarinfo()`](#tarfile.TarFile.gettarinfo "tarfile.TarFile.gettarinfo").

Modifying the objects returned by [`getmember()`](#tarfile.TarFile.getmember "tarfile.TarFile.getmember") or
[`getmembers()`](#tarfile.TarFile.getmembers "tarfile.TarFile.getmembers") will affect all subsequent
operations on the archive.
For cases where this is unwanted, you can use [`copy.copy()`](copy.html#module-copy "copy: Shallow and deep copy operations.") or
call the [`replace()`](#tarfile.TarInfo.replace "tarfile.TarInfo.replace") method to create a modified copy in one step.

Several attributes can be set to `None` to indicate that a piece of metadata
is unused or unknown.
Different [`TarInfo`](#tarfile.TarInfo "tarfile.TarInfo") methods handle `None` differently:

* The [`extract()`](#tarfile.TarFile.extract "tarfile.TarFile.extract") or [`extractall()`](#tarfile.TarFile.extractall "tarfile.TarFile.extractall") methods will
  ignore the corresponding metadata, leaving it set to a default.
* [`addfile()`](#tarfile.TarFile.addfile "tarfile.TarFile.addfile") will fail.
* [`list()`](#tarfile.TarFile.list "tarfile.TarFile.list") will print a placeholder string.

*class* tarfile.TarInfo(*name=''*)[Â¶](#tarfile.TarInfo "Link to this definition")

Create a [`TarInfo`](#tarfile.TarInfo "tarfile.TarInfo") object.

*classmethod* TarInfo.frombuf(*buf*, *encoding*, *errors*)[Â¶](#tarfile.TarInfo.frombuf "Link to this definition")

Create and return a [`TarInfo`](#tarfile.TarInfo "tarfile.TarInfo") object from string buffer *buf*.

Raises [`HeaderError`](#tarfile.HeaderError "tarfile.HeaderError") if the buffer is invalid.

*classmethod* TarInfo.fromtarfile(*tarfile*)[Â¶](#tarfile.TarInfo.fromtarfile "Link to this definition")

Read the next member from the [`TarFile`](#tarfile.TarFile "tarfile.TarFile") object *tarfile* and return it as
a [`TarInfo`](#tarfile.TarInfo "tarfile.TarInfo") object.

TarInfo.tobuf(*format=DEFAULT\_FORMAT*, *encoding=ENCODING*, *errors='surrogateescape'*)[Â¶](#tarfile.TarInfo.tobuf "Link to this definition")

Create a string buffer from a [`TarInfo`](#tarfile.TarInfo "tarfile.TarInfo") object. For information on the
arguments see the constructor of the [`TarFile`](#tarfile.TarFile "tarfile.TarFile") class.

Changed in version 3.2: Use `'surrogateescape'` as the default for the *errors* argument.

A `TarInfo` object has the following public data attributes:

TarInfo.name*: [str](stdtypes.html#str "str")*[Â¶](#tarfile.TarInfo.name "Link to this definition")

Name of the archive member.

TarInfo.size*: [int](functions.html#int "int")*[Â¶](#tarfile.TarInfo.size "Link to this definition")

Size in bytes.

TarInfo.mtime*: [int](functions.html#int "int") | [float](functions.html#float "float")*[Â¶](#tarfile.TarInfo.mtime "Link to this definition")

Time of last modification in seconds since the [epoch](time.html#epoch),
as in [`os.stat_result.st_mtime`](os.html#os.stat_result.st_mtime "os.stat_result.st_mtime").

Changed in version 3.12: Can be set to `None` for [`extract()`](#tarfile.TarFile.extract "tarfile.TarFile.extract") and
[`extractall()`](#tarfile.TarFile.extractall "tarfile.TarFile.extractall"), causing extraction to skip applying this
attribute.

TarInfo.mode*: [int](functions.html#int "int")*[Â¶](#tarfile.TarInfo.mode "Link to this definition")

Permission bits, as for [`os.chmod()`](os.html#os.chmod "os.chmod").

Changed in version 3.12: Can be set to `None` for [`extract()`](#tarfile.TarFile.extract "tarfile.TarFile.extract") and
[`extractall()`](#tarfile.TarFile.extractall "tarfile.TarFile.extractall"), causing extraction to skip applying this
attribute.

TarInfo.type[Â¶](#tarfile.TarInfo.type "Link to this definition")

File type. *type* is usually one of these constants: [`REGTYPE`](#tarfile.REGTYPE "tarfile.REGTYPE"),
[`AREGTYPE`](#tarfile.AREGTYPE "tarfile.AREGTYPE"), [`LNKTYPE`](#tarfile.LNKTYPE "tarfile.LNKTYPE"), [`SYMTYPE`](#tarfile.SYMTYPE "tarfile.SYMTYPE"), [`DIRTYPE`](#tarfile.DIRTYPE "tarfile.DIRTYPE"),
[`FIFOTYPE`](#tarfile.FIFOTYPE "tarfile.FIFOTYPE"), [`CONTTYPE`](#tarfile.CONTTYPE "tarfile.CONTTYPE"), [`CHRTYPE`](#tarfile.CHRTYPE "tarfile.CHRTYPE"), [`BLKTYPE`](#tarfile.BLKTYPE "tarfile.BLKTYPE"),
[`GNUTYPE_SPARSE`](#tarfile.GNUTYPE_SPARSE "tarfile.GNUTYPE_SPARSE"). To determine the type of a [`TarInfo`](#tarfile.TarInfo "tarfile.TarInfo") object
more conveniently, use the `is*()` methods below.

TarInfo.linkname*: [str](stdtypes.html#str "str")*[Â¶](#tarfile.TarInfo.linkname "Link to this definition")

Name of the target file name, which is only present in [`TarInfo`](#tarfile.TarInfo "tarfile.TarInfo") objects
of type [`LNKTYPE`](#tarfile.LNKTYPE "tarfile.LNKTYPE") and [`SYMTYPE`](#tarfile.SYMTYPE "tarfile.SYMTYPE").

For symbolic links (`SYMTYPE`), the *linkname* is relative to the directory
that contains the link.
For hard links (`LNKTYPE`), the *linkname* is relative to the root of
the archive.

TarInfo.uid*: [int](functions.html#int "int")*[Â¶](#tarfile.TarInfo.uid "Link to this definition")

User ID of the user who originally stored this member.

Changed in version 3.12: Can be set to `None` for [`extract()`](#tarfile.TarFile.extract "tarfile.TarFile.extract") and
[`extractall()`](#tarfile.TarFile.extractall "tarfile.TarFile.extractall"), causing extraction to skip applying this
attribute.

TarInfo.gid*: [int](functions.html#int "int")*[Â¶](#tarfile.TarInfo.gid "Link to this definition")

Group ID of the user who originally stored this member.

Changed in version 3.12: Can be set to `None` for [`extract()`](#tarfile.TarFile.extract "tarfile.TarFile.extract") and
[`extractall()`](#tarfile.TarFile.extractall "tarfile.TarFile.extractall"), causing extraction to skip applying this
attribute.

TarInfo.uname*: [str](stdtypes.html#str "str")*[Â¶](#tarfile.TarInfo.uname "Link to this definition")

User name.

Changed in version 3.12: Can be set to `None` for [`extract()`](#tarfile.TarFile.extract "tarfile.TarFile.extract") and
[`extractall()`](#tarfile.TarFile.extractall "tarfile.TarFile.extractall"), causing extraction to skip applying this
attribute.

TarInfo.gname*: [str](stdtypes.html#str "str")*[Â¶](#tarfile.TarInfo.gname "Link to this definition")

Group name.

Changed in version 3.12: Can be set to `None` for [`extract()`](#tarfile.TarFile.extract "tarfile.TarFile.extract") and
[`extractall()`](#tarfile.TarFile.extractall "tarfile.TarFile.extractall"), causing extraction to skip applying this
attribute.

TarInfo.chksum*: [int](functions.html#int "int")*[Â¶](#tarfile.TarInfo.chksum "Link to this definition")

Header checksum.

TarInfo.devmajor*: [int](functions.html#int "int")*[Â¶](#tarfile.TarInfo.devmajor "Link to this definition")

Device major number.

TarInfo.devminor*: [int](functions.html#int "int")*[Â¶](#tarfile.TarInfo.devminor "Link to this definition")

Device minor number.

TarInfo.offset*: [int](functions.html#int "int")*[Â¶](#tarfile.TarInfo.offset "Link to this definition")

The tar header starts here.

TarInfo.offset\_data*: [int](functions.html#int "int")*[Â¶](#tarfile.TarInfo.offset_data "Link to this definition")

The fileâs data starts here.

TarInfo.sparse[Â¶](#tarfile.TarInfo.sparse "Link to this definition")

Sparse member information.

TarInfo.pax\_headers*: [dict](stdtypes.html#dict "dict")*[Â¶](#tarfile.TarInfo.pax_headers "Link to this definition")

A dictionary containing key-value pairs of an associated pax extended header.

TarInfo.replace(*name=...*, *mtime=...*, *mode=...*, *linkname=...*, *uid=...*, *gid=...*, *uname=...*, *gname=...*, *deep=True*)[Â¶](#tarfile.TarInfo.replace "Link to this definition")

Added in version 3.12.

Return a *new* copy of the `TarInfo` object with the given attributes
changed. For example, to return a `TarInfo` with the group name set to
`'staff'`, use:

```
new_tarinfo = old_tarinfo.replace(gname='staff')

```

By default, a deep copy is made.
If *deep* is false, the copy is shallow, i.e. `pax_headers`
and any custom attributes are shared with the original `TarInfo` object.

A [`TarInfo`](#tarfile.TarInfo "tarfile.TarInfo") object also provides some convenient query methods:

TarInfo.isfile()[Â¶](#tarfile.TarInfo.isfile "Link to this definition")

Return [`True`](constants.html#True "True") if the [`TarInfo`](#tarfile.TarInfo "tarfile.TarInfo") object is a regular file.

TarInfo.isreg()[Â¶](#tarfile.TarInfo.isreg "Link to this definition")

Same as [`isfile()`](#tarfile.TarInfo.isfile "tarfile.TarInfo.isfile").

TarInfo.isdir()[Â¶](#tarfile.TarInfo.isdir "Link to this definition")

Return [`True`](constants.html#True "True") if it is a directory.

TarInfo.issym()[Â¶](#tarfile.TarInfo.issym "Link to this definition")

Return [`True`](constants.html#True "True") if it is a symbolic link.

TarInfo.islnk()[Â¶](#tarfile.TarInfo.islnk "Link to this definition")

Return [`True`](constants.html#True "True") if it is a hard link.

TarInfo.ischr()[Â¶](#tarfile.TarInfo.ischr "Link to this definition")

Return [`True`](constants.html#True "True") if it is a character device.

TarInfo.isblk()[Â¶](#tarfile.TarInfo.isblk "Link to this definition")

Return [`True`](constants.html#True "True") if it is a block device.

TarInfo.isfifo()[Â¶](#tarfile.TarInfo.isfifo "Link to this definition")

Return [`True`](constants.html#True "True") if it is a FIFO.

TarInfo.isdev()[Â¶](#tarfile.TarInfo.isdev "Link to this definition")

Return [`True`](constants.html#True "True") if it is one of character device, block device or FIFO.

## Extraction filters[Â¶](#extraction-filters "Link to this heading")

Added in version 3.12.

The *tar* format is designed to capture all details of a UNIX-like filesystem,
which makes it very powerful.
Unfortunately, the features make it easy to create tar files that have
unintended â and possibly malicious â effects when extracted.
For example, extracting a tar file can overwrite arbitrary files in various
ways (e.g. by using absolute paths, `..` path components, or symlinks that
affect later members).

In most cases, the full functionality is not needed.
Therefore, *tarfile* supports extraction filters: a mechanism to limit
functionality, and thus mitigate some of the security issues.

See also

[**PEP 706**](https://peps.python.org/pep-0706/)

Contains further motivation and rationale behind the design.

The *filter* argument to [`TarFile.extract()`](#tarfile.TarFile.extract "tarfile.TarFile.extract") or [`extractall()`](#tarfile.TarFile.extractall "tarfile.TarFile.extractall")
can be:

* the string `'fully_trusted'`: Honor all metadata as specified in the
  archive.
  Should be used if the user trusts the archive completely, or implements
  their own complex verification.
* the string `'tar'`: Honor most *tar*-specific features (i.e. features of
  UNIX-like filesystems), but block features that are very likely to be
  surprising or malicious. See [`tar_filter()`](#tarfile.tar_filter "tarfile.tar_filter") for details.
* the string `'data'`: Ignore or block most features specific to UNIX-like
  filesystems. Intended for extracting cross-platform data archives.
  See [`data_filter()`](#tarfile.data_filter "tarfile.data_filter") for details.
* `None` (default): Use [`TarFile.extraction_filter`](#tarfile.TarFile.extraction_filter "tarfile.TarFile.extraction_filter").

  If that is also `None` (the default), raise a `DeprecationWarning`,
  and fall back to the `'fully_trusted'` filter, whose dangerous behavior
  matches previous versions of Python.

  In Python 3.14, the `'data'` filter will become the default instead.
  Itâs possible to switch earlier; see [`TarFile.extraction_filter`](#tarfile.TarFile.extraction_filter "tarfile.TarFile.extraction_filter").
* A callable which will be called for each extracted member with a
  [TarInfo](#tarinfo-objects) describing the member and the destination
  path to where the archive is extracted (i.e. the same path is used for all
  members):

  ```
  filter(member: TarInfo, path: str, /) -> TarInfo | None

  ```

  The callable is called just before each member is extracted, so it can
  take the current state of the disk into account.
  It can:

  + return a [`TarInfo`](#tarfile.TarInfo "tarfile.TarInfo") object which will be used instead of the metadata
    in the archive, or
  + return `None`, in which case the member will be skipped, or
  + raise an exception to abort the operation or skip the member,
    depending on [`errorlevel`](#tarfile.TarFile.errorlevel "tarfile.TarFile.errorlevel").
    Note that when extraction is aborted, [`extractall()`](#tarfile.TarFile.extractall "tarfile.TarFile.extractall") may leave
    the archive partially extracted. It does not attempt to clean up.

### Default named filters[Â¶](#default-named-filters "Link to this heading")

The pre-defined, named filters are available as functions, so they can be
reused in custom filters:

tarfile.fully\_trusted\_filter(*member*, *path*)[Â¶](#tarfile.fully_trusted_filter "Link to this definition")

Return *member* unchanged.

This implements the `'fully_trusted'` filter.

tarfile.tar\_filter(*member*, *path*)[Â¶](#tarfile.tar_filter "Link to this definition")

Implements the `'tar'` filter.

* Strip leading slashes (`/` and [`os.sep`](os.html#os.sep "os.sep")) from filenames.
* [Refuse](#tarfile-extraction-refuse) to extract files with absolute
  paths (in case the name is absolute
  even after stripping slashes, e.g. `C:/foo` on Windows).
  This raises [`AbsolutePathError`](#tarfile.AbsolutePathError "tarfile.AbsolutePathError").
* [Refuse](#tarfile-extraction-refuse) to extract files whose absolute
  path (after following symlinks) would end up outside the destination.
  This raises [`OutsideDestinationError`](#tarfile.OutsideDestinationError "tarfile.OutsideDestinationError").
* Clear high mode bits (setuid, setgid, sticky) and group/other write bits
  ([`S_IWGRP`](stat.html#stat.S_IWGRP "stat.S_IWGRP") | [`S_IWOTH`](stat.html#stat.S_IWOTH "stat.S_IWOTH")).

Return the modified `TarInfo` member.

tarfile.data\_filter(*member*, *path*)[Â¶](#tarfile.data_filter "Link to this definition")

Implements the `'data'` filter.
In addition to what `tar_filter` does:

* [Refuse](#tarfile-extraction-refuse) to extract links (hard or soft)
  that link to absolute paths, or ones that link outside the destination.

  This raises [`AbsoluteLinkError`](#tarfile.AbsoluteLinkError "tarfile.AbsoluteLinkError") or
  [`LinkOutsideDestinationError`](#tarfile.LinkOutsideDestinationError "tarfile.LinkOutsideDestinationError").

  Note that such files are refused even on platforms that do not support
  symbolic links.
* [Refuse](#tarfile-extraction-refuse) to extract device files
  (including pipes).
  This raises [`SpecialFileError`](#tarfile.SpecialFileError "tarfile.SpecialFileError").
* For regular files, including hard links:

  + Set the owner read and write permissions
    ([`S_IRUSR`](stat.html#stat.S_IRUSR "stat.S_IRUSR") | [`S_IWUSR`](stat.html#stat.S_IWUSR "stat.S_IWUSR")).
  + Remove the group & other executable permission
    ([`S_IXGRP`](stat.html#stat.S_IXGRP "stat.S_IXGRP") | [`S_IXOTH`](stat.html#stat.S_IXOTH "stat.S_IXOTH"))
    if the owner doesnât have it ([`S_IXUSR`](stat.html#stat.S_IXUSR "stat.S_IXUSR")).
* For other files (directories), set `mode` to `None`, so
  that extraction methods skip applying permission bits.
* Set user and group info (`uid`, `gid`, `uname`, `gname`)
  to `None`, so that extraction methods skip setting it.

Return the modified `TarInfo` member.

### Filter errors[Â¶](#filter-errors "Link to this heading")

When a filter refuses to extract a file, it will raise an appropriate exception,
a subclass of [`FilterError`](#tarfile.FilterError "tarfile.FilterError").
This will abort the extraction if [`TarFile.errorlevel`](#tarfile.TarFile.errorlevel "tarfile.TarFile.errorlevel") is 1 or more.
With `errorlevel=0` the error will be logged and the member will be skipped,
but extraction will continue.

### Hints for further verification[Â¶](#hints-for-further-verification "Link to this heading")

Even with `filter='data'`, *tarfile* is not suited for extracting untrusted
files without prior inspection.
Among other issues, the pre-defined filters do not prevent denial-of-service
attacks. Users should do additional checks.

Here is an incomplete list of things to consider:

* Extract to a [`new temporary directory`](tempfile.html#tempfile.mkdtemp "tempfile.mkdtemp")
  to prevent e.g. exploiting pre-existing links, and to make it easier to
  clean up after a failed extraction.
* When working with untrusted data, use external (e.g. OS-level) limits on
  disk, memory and CPU usage.
* Check filenames against an allow-list of characters
  (to filter out control characters, confusables, foreign path separators,
  etc.).
* Check that filenames have expected extensions (discouraging files that
  execute when you âclick on themâ, or extension-less files like Windows special device names).
* Limit the number of extracted files, total size of extracted data,
  filename length (including symlink length), and size of individual files.
* Check for files that would be shadowed on case-insensitive filesystems.

Also note that:

* Tar files may contain multiple versions of the same file.
  Later ones are expected to overwrite any earlier ones.
  This feature is crucial to allow updating tape archives, but can be abused
  maliciously.
* *tarfile* does not protect against issues with âliveâ data,
  e.g. an attacker tinkering with the destination (or source) directory while
  extraction (or archiving) is in progress.

### Supporting older Python versions[Â¶](#supporting-older-python-versions "Link to this heading")

Extraction filters were added to Python 3.12, but may be backported to older
versions as security updates.
To check whether the feature is available, use e.g.
`hasattr(tarfile, 'data_filter')` rather than checking the Python version.

The following examples show how to support Python versions with and without
the feature.
Note that setting `extraction_filter` will affect any subsequent operations.

* Fully trusted archive:

  ```
  my_tarfile.extraction_filter = (lambda member, path: member)
  my_tarfile.extractall()

  ```
* Use the `'data'` filter if available, but revert to Python 3.11 behavior
  (`'fully_trusted'`) if this feature is not available:

  ```
  my_tarfile.extraction_filter = getattr(tarfile, 'data_filter',
                                         (lambda member, path: member))
  my_tarfile.extractall()

  ```
* Use the `'data'` filter; *fail* if it is not available:

  ```
  my_tarfile.extractall(filter=tarfile.data_filter)

  ```

  or:

  ```
  my_tarfile.extraction_filter = tarfile.data_filter
  my_tarfile.extractall()

  ```
* Use the `'data'` filter; *warn* if it is not available:

  ```
  if hasattr(tarfile, 'data_filter'):
      my_tarfile.extractall(filter='data')
  else:
      # remove this when no longer needed
      warn_the_user('Extracting may be unsafe; consider updating Python')
      my_tarfile.extractall()

  ```

### Stateful extraction filter example[Â¶](#stateful-extraction-filter-example "Link to this heading")

While *tarfile*âs extraction methods take a simple *filter* callable,
custom filters may be more complex objects with an internal state.
It may be useful to write these as context managers, to be used like this:

```
with StatefulFilter() as filter_func:
    tar.extractall(path, filter=filter_func)

```

Such a filter can be written as, for example:

```
class StatefulFilter:
    def __init__(self):
        self.file_count = 0

    def __enter__(self):
        return self

    def __call__(self, member, path):
        self.file_count += 1
        return member

    def __exit__(self, *exc_info):
        print(f'{self.file_count} files extracted')

```

## Command-Line Interface[Â¶](#command-line-interface "Link to this heading")

Added in version 3.4.

The [`tarfile`](#module-tarfile "tarfile: Read and write tar-format archive files.") module provides a simple command-line interface to interact
with tar archives.

If you want to create a new tar archive, specify its name after the [`-c`](#cmdoption-tarfile-c)
option and then list the filename(s) that should be included:

```
$ python -m tarfile -c monty.tar  spam.txt eggs.txt

```

Passing a directory is also acceptable:

```
$ python -m tarfile -c monty.tar life-of-brian_1979/

```

If you want to extract a tar archive into the current directory, use
the [`-e`](#cmdoption-tarfile-e) option:

```
$ python -m tarfile -e monty.tar

```

You can also extract a tar archive into a different directory by passing the
directoryâs name:

```
$ python -m tarfile -e monty.tar  other-dir/

```

For a list of the files in a tar archive, use the [`-l`](#cmdoption-tarfile-l) option:

```
$ python -m tarfile -l monty.tar

```

### Command-line options[Â¶](#command-line-options "Link to this heading")

-l <tarfile>[Â¶](#cmdoption-tarfile-l "Link to this definition")
--list <tarfile>[Â¶](#cmdoption-tarfile-list "Link to this definition")

List files in a tarfile.

-c <tarfile> <source1> ... <sourceN>[Â¶](#cmdoption-tarfile-c "Link to this definition")
--create <tarfile> <source1> ... <sourceN>[Â¶](#cmdoption-tarfile-create "Link to this definition")

Create tarfile from source files.

-e <tarfile> [<output\_dir>][Â¶](#cmdoption-tarfile-e "Link to this definition")
--extract <tarfile> [<output\_dir>][Â¶](#cmdoption-tarfile-extract "Link to this definition")

Extract tarfile into the current directory if *output\_dir* is not specified.

-t <tarfile>[Â¶](#cmdoption-tarfile-t "Link to this definition")
--test <tarfile>[Â¶](#cmdoption-tarfile-test "Link to this definition")

Test whether the tarfile is valid or not.

-v, --verbose[Â¶](#cmdoption-tarfile-v "Link to this definition")

Verbose output.

--filter <filtername>[Â¶](#cmdoption-tarfile-filter "Link to this definition")

Specifies the *filter* for `--extract`.
See [Extraction filters](#tarfile-extraction-filter) for details.
Only string names are accepted (that is, `fully_trusted`, `tar`,
and `data`).

## Examples[Â¶](#examples "Link to this heading")

How to extract an entire tar archive to the current working directory:

```
import tarfile
tar = tarfile.open("sample.tar.gz")
tar.extractall(filter='data')
tar.close()

```

How to extract a subset of a tar archive with [`TarFile.extractall()`](#tarfile.TarFile.extractall "tarfile.TarFile.extractall") using
a generator function instead of a list:

```
import os
import tarfile

def py_files(members):
    for tarinfo in members:
        if os.path.splitext(tarinfo.name)[1] == ".py":
            yield tarinfo

tar = tarfile.open("sample.tar.gz")
tar.extractall(members=py_files(tar))
tar.close()

```

How to create an uncompressed tar archive from a list of filenames:

```
import tarfile
tar = tarfile.open("sample.tar", "w")
for name in ["foo", "bar", "quux"]:
    tar.add(name)
tar.close()

```

The same example using the [`with`](../reference/compound_stmts.html#with) statement:

```
import tarfile
with tarfile.open("sample.tar", "w") as tar:
    for name in ["foo", "bar", "quux"]:
        tar.add(name)

```

How to read a gzip compressed tar archive and display some member information:

```
import tarfile
tar = tarfile.open("sample.tar.gz", "r:gz")
for tarinfo in tar:
    print(tarinfo.name, "is", tarinfo.size, "bytes in size and is ", end="")
    if tarinfo.isreg():
        print("a regular file.")
    elif tarinfo.isdir():
        print("a directory.")
    else:
        print("something else.")
tar.close()

```

How to create an archive and reset the user information using the *filter*
parameter in [`TarFile.add()`](#tarfile.TarFile.add "tarfile.TarFile.add"):

```
import tarfile
def reset(tarinfo):
    tarinfo.uid = tarinfo.gid = 0
    tarinfo.uname = tarinfo.gname = "root"
    return tarinfo
tar = tarfile.open("sample.tar.gz", "w:gz")
tar.add("foo", filter=reset)
tar.close()

```

## Supported tar formats[Â¶](#supported-tar-formats "Link to this heading")

There are three tar formats that can be created with the [`tarfile`](#module-tarfile "tarfile: Read and write tar-format archive files.") module:

* The POSIX.1-1988 ustar format ([`USTAR_FORMAT`](#tarfile.USTAR_FORMAT "tarfile.USTAR_FORMAT")). It supports filenames
  up to a length of at best 256 characters and linknames up to 100 characters.
  The maximum file size is 8 GiB. This is an old and limited but widely
  supported format.
* The GNU tar format ([`GNU_FORMAT`](#tarfile.GNU_FORMAT "tarfile.GNU_FORMAT")). It supports long filenames and
  linknames, files bigger than 8 GiB and sparse files. It is the de facto
  standard on GNU/Linux systems. [`tarfile`](#module-tarfile "tarfile: Read and write tar-format archive files.") fully supports the GNU tar
  extensions for long names, sparse file support is read-only.
* The POSIX.1-2001 pax format ([`PAX_FORMAT`](#tarfile.PAX_FORMAT "tarfile.PAX_FORMAT")). It is the most flexible
  format with virtually no limits. It supports long filenames and linknames, large
  files and stores pathnames in a portable way. Modern tar implementations,
  including GNU tar, bsdtar/libarchive and star, fully support extended *pax*
  features; some old or unmaintained libraries may not, but should treat
  *pax* archives as if they were in the universally supported *ustar* format.
  It is the current default format for new archives.

  It extends the existing *ustar* format with extra headers for information
  that cannot be stored otherwise. There are two flavours of pax headers:
  Extended headers only affect the subsequent file header, global
  headers are valid for the complete archive and affect all following files.
  All the data in a pax header is encoded in *UTF-8* for portability reasons.

There are some more variants of the tar format which can be read, but not
created:

* The ancient V7 format. This is the first tar format from Unix Seventh Edition,
  storing only regular files and directories. Names must not be longer than 100
  characters, there is no user/group name information. Some archives have
  miscalculated header checksums in case of fields with non-ASCII characters.
* The SunOS tar extended format. This format is a variant of the POSIX.1-2001
  pax format, but is not compatible.

## Unicode issues[Â¶](#unicode-issues "Link to this heading")

The tar format was originally conceived to make backups on tape drives with the
main focus on preserving file system information. Nowadays tar archives are
commonly used for file distribution and exchanging archives over networks. One
problem of the original format (which is the basis of all other formats) is
that there is no concept of supporting different character encodings. For
example, an ordinary tar archive created on a *UTF-8* system cannot be read
correctly on a *Latin-1* system if it contains non-*ASCII* characters. Textual
metadata (like filenames, linknames, user/group names) will appear damaged.
Unfortunately, there is no way to autodetect the encoding of an archive. The
pax format was designed to solve this problem. It stores non-ASCII metadata
using the universal character encoding *UTF-8*.

The details of character conversion in [`tarfile`](#module-tarfile "tarfile: Read and write tar-format archive files.") are controlled by the
*encoding* and *errors* keyword arguments of the [`TarFile`](#tarfile.TarFile "tarfile.TarFile") class.

*encoding* defines the character encoding to use for the metadata in the
archive. The default value is [`sys.getfilesystemencoding()`](sys.html#sys.getfilesystemencoding "sys.getfilesystemencoding") or `'ascii'`
as a fallback. Depending on whether the archive is read or written, the
metadata must be either decoded or encoded. If *encoding* is not set
appropriately, this conversion may fail.

The *errors* argument defines how characters are treated that cannot be
converted. Possible values are listed in section [Error Handlers](codecs.html#error-handlers).
The default scheme is `'surrogateescape'` which Python also uses for its
file system calls, see [File Names, Command Line Arguments, and Environment Variables](os.html#os-filenames).

For [`PAX_FORMAT`](#tarfile.PAX_FORMAT "tarfile.PAX_FORMAT") archives (the default), *encoding* is generally not needed
because all the metadata is stored using *UTF-8*. *encoding* is only used in
the rare cases when binary pax headers are decoded or when strings with
surrogate characters are stored.

### [Table of Contents](../contents.html)

* `tarfile` â Read and write tar archive files
  + [TarFile Objects](#tarfile-objects)
  + [TarInfo Objects](#tarinfo-objects)
  + [Extraction filters](#extraction-filters)
    - [Default named filters](#default-named-filters)
    - [Filter errors](#filter-errors)
    - [Hints for further verification](#hints-for-further-verification)
    - [Supporting older Python versions](#supporting-older-python-versions)
    - [Stateful extraction filter example](#stateful-extraction-filter-example)
  + [Command-Line Interface](#command-line-interface)
    - [Command-line options](#command-line-options)
  + [Examples](#examples)
  + [Supported tar formats](#supported-tar-formats)
  + [Unicode issues](#unicode-issues)

#### Previous topic

[`zipfile` â Work with ZIP archives](zipfile.html "previous chapter")

#### Next topic

[File Formats](fileformats.html "next chapter")

### This Page

* [Report a Bug](../bugs.html)
* [Show Source](https://github.com/python/cpython/blob/main/Doc/library/tarfile.rst)

Â«

### Navigation

* [index](../genindex.html "General Index")
* [modules](../py-modindex.html "Python Module Index") |
* [next](fileformats.html "File Formats") |
* [previous](zipfile.html "zipfile â Work with ZIP archives") |
* ![Python logo](../_static/py.svg)
* [Python](https://www.python.org/) »
* [3.13.1 Documentation](../index.html) »
* [The Python Standard Library](index.html) »
* [Data Compression and Archiving](archiving.html) »
* `tarfile` â Read and write tar archive files
* |
* Theme
  Auto
  Light
  Dark

   |

©
[Copyright](../copyright.html)
2001-2025, Python Software Foundation.

This page is licensed under the Python Software Foundation License Version 2.

Examples, recipes, and other code in the documentation are additionally licensed under the Zero Clause BSD License.

See [History and License](/license.html) for more information.

The Python Software Foundation is a non-profit corporation.
[Please donate.](https://www.python.org/psf/donations/)

Last updated on Jan 14, 2025 (15:48 UTC).
[Found a bug](/bugs.html)?

Created using [Sphinx](https://www.sphinx-doc.org/) 8.1.3.



=== Content from keras.io_98977f13_20250114_205545.html ===


[![](/img/logo-small.png)](/)
[About Keras](/about/)
[Getting started](/getting_started/)
[Developer guides](/guides/)
[Code examples](/examples/)
[Keras 3 API documentation](/api/)
[Models API](/api/models/)
[Layers API](/api/layers/)
[Callbacks API](/api/callbacks/)
[Ops API](/api/ops/)
[Optimizers](/api/optimizers/)
[Metrics](/api/metrics/)
[Losses](/api/losses/)
[Data loading](/api/data_loading/)
[Built-in small datasets](/api/datasets/)
[Keras Applications](/api/applications/)
[Mixed precision](/api/mixed_precision/)
[Multi-device distribution](/api/distribution/)
[RNG API](/api/random/)
[Utilities](/api/utils/)
[Keras 2 API documentation](/2.18/api/)
[KerasTuner: Hyperparam Tuning](/keras_tuner/)
[KerasHub: Pretrained Models](/keras_hub/)

search

âº Keras 3 API documentation

# Keras 3 API documentation

### [Models API](/api/models/)

* [The Model class](/api/models/model)
* [The Sequential class](/api/models/sequential)
* [Model training APIs](/api/models/model_training_apis)
* [Saving & serialization](/api/models/model_saving_apis/)

### [Layers API](/api/layers/)

* [The base Layer class](/api/layers/base_layer)
* [Layer activations](/api/layers/activations)
* [Layer weight initializers](/api/layers/initializers)
* [Layer weight regularizers](/api/layers/regularizers)
* [Layer weight constraints](/api/layers/constraints)
* [Core layers](/api/layers/core_layers/)
* [Convolution layers](/api/layers/convolution_layers/)
* [Pooling layers](/api/layers/pooling_layers/)
* [Recurrent layers](/api/layers/recurrent_layers/)
* [Preprocessing layers](/api/layers/preprocessing_layers/)
* [Normalization layers](/api/layers/normalization_layers/)
* [Regularization layers](/api/layers/regularization_layers/)
* [Attention layers](/api/layers/attention_layers/)
* [Reshaping layers](/api/layers/reshaping_layers/)
* [Merging layers](/api/layers/merging_layers/)
* [Activation layers](/api/layers/activation_layers/)
* [Backend-specific layers](/api/layers/backend_specific_layers/)

### [Callbacks API](/api/callbacks/)

* [Base Callback class](/api/callbacks/base_callback)
* [ModelCheckpoint](/api/callbacks/model_checkpoint)
* [BackupAndRestore](/api/callbacks/backup_and_restore)
* [TensorBoard](/api/callbacks/tensorboard)
* [EarlyStopping](/api/callbacks/early_stopping)
* [LearningRateScheduler](/api/callbacks/learning_rate_scheduler)
* [ReduceLROnPlateau](/api/callbacks/reduce_lr_on_plateau)
* [RemoteMonitor](/api/callbacks/remote_monitor)
* [LambdaCallback](/api/callbacks/lambda_callback)
* [TerminateOnNaN](/api/callbacks/terminate_on_nan)
* [CSVLogger](/api/callbacks/csv_logger)
* [ProgbarLogger](/api/callbacks/progbar_logger)
* [SwapEMAWeights](/api/callbacks/swap_ema_weights)

### [Ops API](/api/ops/)

* [NumPy ops](/api/ops/numpy/)
* [NN ops](/api/ops/nn/)
* [Linear algebra ops](/api/ops/linalg/)
* [Core ops](/api/ops/core/)
* [Image ops](/api/ops/image/)
* [FFT ops](/api/ops/fft/)

### [Optimizers](/api/optimizers/)

* [SGD](/api/optimizers/sgd)
* [RMSprop](/api/optimizers/rmsprop)
* [Adam](/api/optimizers/adam)
* [AdamW](/api/optimizers/adamw)
* [Adadelta](/api/optimizers/adadelta)
* [Adagrad](/api/optimizers/adagrad)
* [Adamax](/api/optimizers/adamax)
* [Adafactor](/api/optimizers/adafactor)
* [Nadam](/api/optimizers/Nadam)
* [Ftrl](/api/optimizers/ftrl)
* [Lion](/api/optimizers/lion)
* [Lamb](/api/optimizers/lamb)
* [Loss Scale Optimizer](/api/optimizers/loss_scale_optimizer)

### [Metrics](/api/metrics/)

* [Base Metric class](/api/metrics/base_metric)
* [Accuracy metrics](/api/metrics/accuracy_metrics)
* [Probabilistic metrics](/api/metrics/probabilistic_metrics)
* [Regression metrics](/api/metrics/regression_metrics)
* [Classification metrics based on True/False positives & negatives](/api/metrics/classification_metrics)
* [Image segmentation metrics](/api/metrics/segmentation_metrics)
* [Hinge metrics for "maximum-margin" classification](/api/metrics/hinge_metrics)
* [Metric wrappers and reduction metrics](/api/metrics/metrics_wrappers)

### [Losses](/api/losses/)

* [Probabilistic losses](/api/losses/probabilistic_losses)
* [Regression losses](/api/losses/regression_losses)
* [Hinge losses for "maximum-margin" classification](/api/losses/hinge_losses)

### [Data loading](/api/data_loading/)

* [Image data loading](/api/data_loading/image)
* [Timeseries data loading](/api/data_loading/timeseries)
* [Text data loading](/api/data_loading/text)
* [Audio data loading](/api/data_loading/audio)

### [Built-in small datasets](/api/datasets/)

* [MNIST digits classification dataset](/api/datasets/mnist)
* [CIFAR10 small images classification dataset](/api/datasets/cifar10)
* [CIFAR100 small images classification dataset](/api/datasets/cifar100)
* [IMDB movie review sentiment classification dataset](/api/datasets/imdb)
* [Reuters newswire classification dataset](/api/datasets/reuters)
* [Fashion MNIST dataset, an alternative to MNIST](/api/datasets/fashion_mnist)
* [California Housing price regression dataset](/api/datasets/california_housing)

### [Keras Applications](/api/applications/)

* [Xception](/api/applications/xception)
* [EfficientNet B0 to B7](/api/applications/efficientnet)
* [EfficientNetV2 B0 to B3 and S, M, L](/api/applications/efficientnet_v2)
* [ConvNeXt Tiny, Small, Base, Large, XLarge](/api/applications/convnext)
* [VGG16 and VGG19](/api/applications/vgg)
* [ResNet and ResNetV2](/api/applications/resnet)
* [MobileNet, MobileNetV2, and MobileNetV3](/api/applications/mobilenet)
* [DenseNet](/api/applications/densenet)
* [NasNetLarge and NasNetMobile](/api/applications/nasnet)
* [InceptionV3](/api/applications/inceptionv3)
* [InceptionResNetV2](/api/applications/inceptionresnetv2)

### [Mixed precision](/api/mixed_precision/)

* [Mixed precision policy API](/api/mixed_precision/policy)

### [Multi-device distribution](/api/distribution/)

* [LayoutMap API](/api/distribution/layout_map)
* [DataParallel API](/api/distribution/data_parallel)
* [ModelParallel API](/api/distribution/model_parallel)
* [ModelParallel API](/api/distribution/model_parallel)
* [Distribution utilities](/api/distribution/distribution_utils)

### [RNG API](/api/random/)

* [SeedGenerator class](/api/random/seed_generator)
* [Random operations](/api/random/random_ops)

### [Utilities](/api/utils/)

* [Experiment management utilities](/api/utils/experiment_management_utils)
* [Model plotting utilities](/api/utils/model_plotting_utils)
* [Structured data preprocessing utilities](/api/utils/feature_space)
* [Tensor utilities](/api/utils/tensor_utils)
* [Python & NumPy utilities](/api/utils/python_utils)
* [Scikit-Learn API wrappers](/api/utils/sklearn_wrappers)
* [Keras configuration utilities](/api/utils/config_utils)

[Keras 3 API documentation](#keras-3-api-documentation)

[Models API](#models-api)

[Layers API](#layers-api)

[Callbacks API](#callbacks-api)

[Ops API](#ops-api)

[Optimizers](#optimizers)

[Metrics](#metrics)

[Losses](#losses)

[Data loading](#data-loading)

[Built-in small datasets](#builtin-small-datasets)

[Keras Applications](#keras-applications)

[Mixed precision](#mixed-precision)

[Multi-device distribution](#multidevice-distribution)

[RNG API](#rng-api)

[Utilities](#utilities)



=== Content from github.com_368eec24_20250114_205543.html ===

[Skip to content](#start-of-content)

## Navigation Menu

Toggle navigation

[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fblob%2Fb8cad4c631096a34461ff8a07840d5f4d123ce32%2Ftensorflow%2Fpython%2Fkeras%2Futils%2Fdata_utils.py)

* Product

  + [GitHub Copilot
    Write better code with AI](https://github.com/features/copilot)
  + [Security
    Find and fix vulnerabilities](https://github.com/features/security)
  + [Actions
    Automate any workflow](https://github.com/features/actions)
  + [Codespaces
    Instant dev environments](https://github.com/features/codespaces)
  + [Issues
    Plan and track work](https://github.com/features/issues)
  + [Code Review
    Manage code changes](https://github.com/features/code-review)
  + [Discussions
    Collaborate outside of code](https://github.com/features/discussions)
  + [Code Search
    Find more, search less](https://github.com/features/code-search)

  Explore
  + [All features](https://github.com/features)
  + [Documentation](https://docs.github.com)
  + [GitHub Skills](https://skills.github.com)
  + [Blog](https://github.blog)
* Solutions

  By company size
  + [Enterprises](https://github.com/enterprise)
  + [Small and medium teams](https://github.com/team)
  + [Startups](https://github.com/enterprise/startups)
  By use case
  + [DevSecOps](/solutions/use-case/devsecops)
  + [DevOps](/solutions/use-case/devops)
  + [CI/CD](/solutions/use-case/ci-cd)
  + [View all use cases](/solutions/use-case)

  By industry
  + [Healthcare](/solutions/industry/healthcare)
  + [Financial services](/solutions/industry/financial-services)
  + [Manufacturing](/solutions/industry/manufacturing)
  + [Government](/solutions/industry/government)
  + [View all industries](/solutions/industry)

  [View all solutions](/solutions)
* Resources

  Topics
  + [AI](/resources/articles/ai)
  + [DevOps](/resources/articles/devops)
  + [Security](/resources/articles/security)
  + [Software Development](/resources/articles/software-development)
  + [View all](/resources/articles)

  Explore
  + [Learning Pathways](https://resources.github.com/learn/pathways)
  + [White papers, Ebooks, Webinars](https://resources.github.com)
  + [Customer Stories](https://github.com/customer-stories)
  + [Partners](https://partner.github.com)
  + [Executive Insights](https://github.com/solutions/executive-insights)
* Open Source

  + [GitHub Sponsors
    Fund open source developers](/sponsors)
  + [The ReadME Project
    GitHub community articles](https://github.com/readme)
  Repositories
  + [Topics](https://github.com/topics)
  + [Trending](https://github.com/trending)
  + [Collections](https://github.com/collections)
* Enterprise

  + [Enterprise platform
    AI-powered developer platform](/enterprise)
  Available add-ons
  + [Advanced Security
    Enterprise-grade security features](https://github.com/enterprise/advanced-security)
  + [GitHub Copilot
    Enterprise-grade AI features](/features/copilot#enterprise)
  + [Premium Support
    Enterprise-grade 24/7 support](/premium-support)
* [Pricing](https://github.com/pricing)

Search or jump to...

# Search code, repositories, users, issues, pull requests...

Search

Clear

[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)

# Provide feedback

We read every piece of feedback, and take your input very seriously.

Include my email address so I can be contacted

  Cancel

 Submit feedback

# Saved searches

## Use saved searches to filter your results more quickly

Name

Query

To see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).

  Cancel

 Create saved search

[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fblob%2Fb8cad4c631096a34461ff8a07840d5f4d123ce32%2Ftensorflow%2Fpython%2Fkeras%2Futils%2Fdata_utils.py)

[Sign up](/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E%2Fblob%2Fshow&source=header-repo&source_repo=tensorflow%2Ftensorflow)
Reseting focus

You signed in with another tab or window. Reload to refresh your session.
You signed out in another tab or window. Reload to refresh your session.
You switched accounts on another tab or window. Reload to refresh your session.

Dismiss alert

{{ message }}

[tensorflow](/tensorflow)
/
**[tensorflow](/tensorflow/tensorflow)**
Public

* [Notifications](/login?return_to=%2Ftensorflow%2Ftensorflow) You must be signed in to change notification settings
* [Fork
  74.4k](/login?return_to=%2Ftensorflow%2Ftensorflow)
* [Star
   187k](/login?return_to=%2Ftensorflow%2Ftensorflow)

* [Code](/tensorflow/tensorflow)
* [Issues
  827](/tensorflow/tensorflow/issues)
* [Pull requests
  5k+](/tensorflow/tensorflow/pulls)
* [Actions](/tensorflow/tensorflow/actions)
* [Projects
  2](/tensorflow/tensorflow/projects)
* [Security](/tensorflow/tensorflow/security)
* [Insights](/tensorflow/tensorflow/pulse)

Additional navigation options

* [Code](/tensorflow/tensorflow)
* [Issues](/tensorflow/tensorflow/issues)
* [Pull requests](/tensorflow/tensorflow/pulls)
* [Actions](/tensorflow/tensorflow/actions)
* [Projects](/tensorflow/tensorflow/projects)
* [Security](/tensorflow/tensorflow/security)
* [Insights](/tensorflow/tensorflow/pulse)

## Files

 b8cad4c
## Breadcrumbs

1. [tensorflow](/tensorflow/tensorflow/tree/b8cad4c631096a34461ff8a07840d5f4d123ce32)
2. /[tensorflow](/tensorflow/tensorflow/tree/b8cad4c631096a34461ff8a07840d5f4d123ce32/tensorflow)
3. /[python](/tensorflow/tensorflow/tree/b8cad4c631096a34461ff8a07840d5f4d123ce32/tensorflow/python)
4. /[keras](/tensorflow/tensorflow/tree/b8cad4c631096a34461ff8a07840d5f4d123ce32/tensorflow/python/keras)
5. /[utils](/tensorflow/tensorflow/tree/b8cad4c631096a34461ff8a07840d5f4d123ce32/tensorflow/python/keras/utils)
/
# data\_utils.py

Copy path Blame  Blame
## Latest commit

## History

[History](/tensorflow/tensorflow/commits/b8cad4c631096a34461ff8a07840d5f4d123ce32/tensorflow/python/keras/utils/data_utils.py)897 lines (722 loc) · 26.9 KB b8cad4c
## Breadcrumbs

1. [tensorflow](/tensorflow/tensorflow/tree/b8cad4c631096a34461ff8a07840d5f4d123ce32)
2. /[tensorflow](/tensorflow/tensorflow/tree/b8cad4c631096a34461ff8a07840d5f4d123ce32/tensorflow)
3. /[python](/tensorflow/tensorflow/tree/b8cad4c631096a34461ff8a07840d5f4d123ce32/tensorflow/python)
4. /[keras](/tensorflow/tensorflow/tree/b8cad4c631096a34461ff8a07840d5f4d123ce32/tensorflow/python/keras)
5. /[utils](/tensorflow/tensorflow/tree/b8cad4c631096a34461ff8a07840d5f4d123ce32/tensorflow/python/keras/utils)
/
# data\_utils.py

Top
## File metadata and controls

* Code
* Blame

897 lines (722 loc) · 26.9 KB[Raw](https://github.com/tensorflow/tensorflow/raw/b8cad4c631096a34461ff8a07840d5f4d123ce32/tensorflow/python/keras/utils/data_utils.py)123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580581582583584585586587588589590591592593594595596597598599600601602603604605606607608609610611612613614615616617618619620621622623624625626627628629630631632633634635636637638639640641642643644645646647648649650651652653654655656657658659660661662663664665666667668669670671672673674675676677678679680681682683684685686687688689690691692693694695696697698699700701702703704705706707708709710711712713714715716717718719720721722723724725726727728729730731732733734735736737738739740741742743744745746747748749750751752753754755756757758759760761762763764765766767768769770771772773774775776777778779780781782783784785786787788789790791792793794795796797798799800801802803804805806807808809810811812813814815816817818819820821822823824825826827828829830831832833834835836837838839840841842843844845846847848849850851852853854855856857858859860861862863864865866867868869870871872873874875876877878879880881882883884885886887888889890891892893894895896897# Lint as python3# Copyright 2018 The TensorFlow Authors. All Rights Reserved.## Licensed under the Apache License, Version 2.0 (the "License");# you may not use this file except in compliance with the License.# You may obtain a copy of the License at## http://www.apache.org/licenses/LICENSE-2.0## Unless required by applicable law or agreed to in writing, software# distributed under the License is distributed on an "AS IS" BASIS,# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.# See the License for the specific language governing permissions and# limitations under the License.# ==============================================================================# pylint: disable=g-import-not-at-top"""Utilities for file download and caching."""
from abc import abstractmethodfrom contextlib import closingimport functoolsimport hashlibimport multiprocessingimport multiprocessing.dummyimport osimport queueimport randomimport shutilimport sys # pylint: disable=unused-importimport tarfileimport threadingimport timeimport typingimport urllibimport weakrefimport zipfile
import numpy as np
from tensorflow.python.framework import opsfrom six.moves.urllib.request import urlopenfrom tensorflow.python.keras.utils import tf\_inspectfrom tensorflow.python.keras.utils.generic\_utils import Progbarfrom tensorflow.python.keras.utils.io\_utils import path\_to\_stringfrom tensorflow.python.util.tf\_export import keras\_export
# Required to support google internal urlretrieveif sys.version\_info[0] == 2:
 def urlretrieve(url, filename, reporthook=None, data=None): """Replacement for `urlretrieve` for Python 2.
 Under Python 2, `urlretrieve` relies on `FancyURLopener` from legacy `urllib` module, known to have issues with proxy management.
 Args: url: url to retrieve. filename: where to store the retrieved data locally. reporthook: a hook function that will be called once on establishment of the network connection and once after each block read thereafter. The hook will be passed three arguments; a count of blocks transferred so far, a block size in bytes, and the total size of the file. data: `data` argument passed to `urlopen`. """
 def chunk\_read(response, chunk\_size=8192, reporthook=None): content\_type = response.info().get('Content-Length') total\_size = -1 if content\_type is not None: total\_size = int(content\_type.strip()) count = 0 while True: chunk = response.read(chunk\_size) count += 1 if reporthook is not None: reporthook(count, chunk\_size, total\_size) if chunk: yield chunk else: break
 response = urlopen(url, data) with open(filename, 'wb') as fd: for chunk in chunk\_read(response, reporthook=reporthook): fd.write(chunk)else: from urllib.request import urlretrieve # pylint: disable=g-importing-member
def is\_generator\_or\_sequence(x): """Check if `x` is a Keras generator type.""" builtin\_iterators = (str, list, tuple, dict, set, frozenset) if isinstance(x, (ops.Tensor, np.ndarray) + builtin\_iterators): return False return (tf\_inspect.isgenerator(x) or isinstance(x, Sequence) or isinstance(x, typing.Iterator))
def \_extract\_archive(file\_path, path='.', archive\_format='auto'): """Extracts an archive if it matches tar, tar.gz, tar.bz, or zip formats.
 Args: file\_path: path to the archive file path: path to extract the archive file archive\_format: Archive format to try for extracting the file. Options are 'auto', 'tar', 'zip', and None. 'tar' includes tar, tar.gz, and tar.bz files. The default 'auto' is ['tar', 'zip']. None or an empty list will return no matches found.
 Returns: True if a match was found and an archive extraction was completed, False otherwise. """ if archive\_format is None: return False if archive\_format == 'auto': archive\_format = ['tar', 'zip'] if isinstance(archive\_format, str): archive\_format = [archive\_format]
 file\_path = path\_to\_string(file\_path) path = path\_to\_string(path)
 for archive\_type in archive\_format: if archive\_type == 'tar': open\_fn = tarfile.open is\_match\_fn = tarfile.is\_tarfile if archive\_type == 'zip': open\_fn = zipfile.ZipFile is\_match\_fn = zipfile.is\_zipfile
 if is\_match\_fn(file\_path): with open\_fn(file\_path) as archive: try: archive.extractall(path) except (tarfile.TarError, RuntimeError, KeyboardInterrupt): if os.path.exists(path): if os.path.isfile(path): os.remove(path) else: shutil.rmtree(path) raise return True return False
@keras\_export('keras.utils.get\_file')def get\_file(fname, origin, untar=False, md5\_hash=None, file\_hash=None, cache\_subdir='datasets', hash\_algorithm='auto', extract=False, archive\_format='auto', cache\_dir=None): """Downloads a file from a URL if it not already in the cache.
 By default the file at the url `origin` is downloaded to the cache\_dir `~/.keras`, placed in the cache\_subdir `datasets`, and given the filename `fname`. The final location of a file `example.txt` would therefore be `~/.keras/datasets/example.txt`.
 Files in tar, tar.gz, tar.bz, and zip formats can also be extracted. Passing a hash will verify the file after download. The command line programs `shasum` and `sha256sum` can compute the hash.
 Example:
 ```python path\_to\_downloaded\_file = tf.keras.utils.get\_file( "flower\_photos", "https://storage.googleapis.com/download.tensorflow.org/example\_images/flower\_photos.tgz", untar=True) ```
 Args: fname: Name of the file. If an absolute path `/path/to/file.txt` is specified the file will be saved at that location. origin: Original URL of the file. untar: Deprecated in favor of `extract` argument. boolean, whether the file should be decompressed md5\_hash: Deprecated in favor of `file\_hash` argument. md5 hash of the file for verification file\_hash: The expected hash string of the file after download. The sha256 and md5 hash algorithms are both supported. cache\_subdir: Subdirectory under the Keras cache dir where the file is saved. If an absolute path `/path/to/folder` is specified the file will be saved at that location. hash\_algorithm: Select the hash algorithm to verify the file. options are `'md5'`, `'sha256'`, and `'auto'`. The default 'auto' detects the hash algorithm in use. extract: True tries extracting the file as an Archive, like tar or zip. archive\_format: Archive format to try for extracting the file. Options are `'auto'`, `'tar'`, `'zip'`, and `None`. `'tar'` includes tar, tar.gz, and tar.bz files. The default `'auto'` corresponds to `['tar', 'zip']`. None or an empty list will return no matches found. cache\_dir: Location to store cached files, when None it defaults to the default directory `~/.keras/`.
 Returns: Path to the downloaded file """ if cache\_dir is None: cache\_dir = os.path.join(os.path.expanduser('~'), '.keras') if md5\_hash is not None and file\_hash is None: file\_hash = md5\_hash hash\_algorithm = 'md5' datadir\_base = os.path.expanduser(cache\_dir) if not os.access(datadir\_base, os.W\_OK): datadir\_base = os.path.join('/tmp', '.keras') datadir = os.path.join(datadir\_base, cache\_subdir) \_makedirs\_exist\_ok(datadir)
 fname = path\_to\_string(fname)
 if untar: untar\_fpath = os.path.join(datadir, fname) fpath = untar\_fpath + '.tar.gz' else: fpath = os.path.join(datadir, fname)
 download = False if os.path.exists(fpath): # File found; verify integrity if a hash was provided. if file\_hash is not None: if not validate\_file(fpath, file\_hash, algorithm=hash\_algorithm): print('A local file was found, but it seems to be ' 'incomplete or outdated because the ' + hash\_algorithm + ' file hash does not match the original value of ' + file\_hash + ' so we will re-download the data.') download = True else: download = True
 if download: print('Downloading data from', origin)
 class ProgressTracker(object): # Maintain progbar for the lifetime of download. # This design was chosen for Python 2.7 compatibility. progbar = None
 def dl\_progress(count, block\_size, total\_size): if ProgressTracker.progbar is None: if total\_size == -1: total\_size = None ProgressTracker.progbar = Progbar(total\_size) else: ProgressTracker.progbar.update(count \* block\_size)
 error\_msg = 'URL fetch failure on {}: {} -- {}' try: try: urlretrieve(origin, fpath, dl\_progress) except urllib.error.HTTPError as e: raise Exception(error\_msg.format(origin, e.code, e.msg)) except urllib.error.URLError as e: raise Exception(error\_msg.format(origin, e.errno, e.reason)) except (Exception, KeyboardInterrupt) as e: if os.path.exists(fpath): os.remove(fpath) raise ProgressTracker.progbar = None
 if untar: if not os.path.exists(untar\_fpath): \_extract\_archive(fpath, datadir, archive\_format='tar') return untar\_fpath
 if extract: \_extract\_archive(fpath, datadir, archive\_format)
 return fpath
def \_makedirs\_exist\_ok(datadir): os.makedirs(datadir, exist\_ok=True) # pylint: disable=unexpected-keyword-arg
def \_resolve\_hasher(algorithm, file\_hash=None): """Returns hash algorithm as hashlib function.""" if algorithm == 'sha256': return hashlib.sha256()
 if algorithm == 'auto' and file\_hash is not None and len(file\_hash) == 64: return hashlib.sha256()
 # This is used only for legacy purposes. return hashlib.md5()
def \_hash\_file(fpath, algorithm='sha256', chunk\_size=65535): """Calculates a file sha256 or md5 hash.
 Example:
 ```python \_hash\_file('/path/to/file.zip') 'e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855' ```
 Args: fpath: path to the file being validated algorithm: hash algorithm, one of `'auto'`, `'sha256'`, or `'md5'`. The default `'auto'` detects the hash algorithm in use. chunk\_size: Bytes to read at a time, important for large files.
 Returns: The file hash """ if isinstance(algorithm, str): hasher = \_resolve\_hasher(algorithm) else: hasher = algorithm
 with open(fpath, 'rb') as fpath\_file: for chunk in iter(lambda: fpath\_file.read(chunk\_size), b''): hasher.update(chunk)
 return hasher.hexdigest()
def validate\_file(fpath, file\_hash, algorithm='auto', chunk\_size=65535): """Validates a file against a sha256 or md5 hash.
 Args: fpath: path to the file being validated file\_hash: The expected hash string of the file. The sha256 and md5 hash algorithms are both supported. algorithm: Hash algorithm, one of 'auto', 'sha256', or 'md5'. The default 'auto' detects the hash algorithm in use. chunk\_size: Bytes to read at a time, important for large files.
 Returns: Whether the file is valid """ hasher = \_resolve\_hasher(algorithm, file\_hash)
 if str(\_hash\_file(fpath, hasher, chunk\_size)) == str(file\_hash): return True else: return False
class ThreadsafeIter(object): """Wrap an iterator with a lock and propagate exceptions to all threads."""
 def \_\_init\_\_(self, it): self.it = it self.lock = threading.Lock()
 # After a generator throws an exception all subsequent next() calls raise a # StopIteration Exception. This, however, presents an issue when mixing # generators and threading because it means the order of retrieval need not # match the order in which the generator was called. This can make it appear # that a generator exited normally when in fact the terminating exception is # just in a different thread. In order to provide thread safety, once # self.it has thrown an exception we continue to throw the same exception. self.\_exception = None
 def \_\_iter\_\_(self): return self
 def next(self): return self.\_\_next\_\_()
 def \_\_next\_\_(self): with self.lock: if self.\_exception: raise self.\_exception # pylint: disable=raising-bad-type
 try: return next(self.it) except Exception as e: self.\_exception = e raise
def threadsafe\_generator(f):
 @functools.wraps(f) def g(\*a, \*\*kw): return ThreadsafeIter(f(\*a, \*\*kw))
 return g
@keras\_export('keras.utils.Sequence')class Sequence(object): """Base object for fitting to a sequence of data, such as a dataset.
 Every `Sequence` must implement the `\_\_getitem\_\_` and the `\_\_len\_\_` methods. If you want to modify your dataset between epochs you may implement `on\_epoch\_end`. The method `\_\_getitem\_\_` should return a complete batch.
 Notes:
 `Sequence` are a safer way to do multiprocessing. This structure guarantees that the network will only train once on each sample per epoch which is not the case with generators.
 Examples:
 ```python from skimage.io import imread from skimage.transform import resize import numpy as np import math
 # Here, `x\_set` is list of path to the images # and `y\_set` are the associated classes.
 class CIFAR10Sequence(Sequence):
 def \_\_init\_\_(self, x\_set, y\_set, batch\_size): self.x, self.y = x\_set, y\_set self.batch\_size = batch\_size
 def \_\_len\_\_(self): return math.ceil(len(self.x) / self.batch\_size)
 def \_\_getitem\_\_(self, idx): batch\_x = self.x[idx \* self.batch\_size:(idx + 1) \* self.batch\_size] batch\_y = self.y[idx \* self.batch\_size:(idx + 1) \* self.batch\_size]
 return np.array([ resize(imread(file\_name), (200, 200)) for file\_name in batch\_x]), np.array(batch\_y) ``` """
 @abstractmethod def \_\_getitem\_\_(self, index): """Gets batch at position `index`.
 Args: index: position of the batch in the Sequence.
 Returns: A batch """ raise NotImplementedError
 @abstractmethod def \_\_len\_\_(self): """Number of batch in the Sequence.
 Returns: The number of batches in the Sequence. """ raise NotImplementedError
 def on\_epoch\_end(self): """Method called at the end of every epoch. """ pass
 def \_\_iter\_\_(self): """Create a generator that iterate over the Sequence.""" for item in (self[i] for i in range(len(self))): yield item
def iter\_sequence\_infinite(seq): """Iterates indefinitely over a Sequence.
 Args: seq: `Sequence` instance.
 Yields: Batches of data from the `Sequence`. """ while True: for item in seq: yield item
# Global variables to be shared across processes\_SHARED\_SEQUENCES = {}# We use a Value to provide unique id to different processes.\_SEQUENCE\_COUNTER = None
# Because multiprocessing pools are inherently unsafe, starting from a clean# state can be essential to avoiding deadlocks. In order to accomplish this, we# need to be able to check on the status of Pools that we create.\_DATA\_POOLS = weakref.WeakSet()\_WORKER\_ID\_QUEUE = None # Only created if needed.\_WORKER\_IDS = set()\_FORCE\_THREADPOOL = False\_FORCE\_THREADPOOL\_LOCK = threading.RLock()
def dont\_use\_multiprocessing\_pool(f): @functools.wraps(f) def wrapped(\*args, \*\*kwargs): with \_FORCE\_THREADPOOL\_LOCK: global \_FORCE\_THREADPOOL old\_force\_threadpool, \_FORCE\_THREADPOOL = \_FORCE\_THREADPOOL, True out = f(\*args, \*\*kwargs) \_FORCE\_THREADPOOL = old\_force\_threadpool return out return wrapped
def get\_pool\_class(use\_multiprocessing): global \_FORCE\_THREADPOOL if not use\_multiprocessing or \_FORCE\_THREADPOOL: return multiprocessing.dummy.Pool # ThreadPool return multiprocessing.Pool
def get\_worker\_id\_queue(): """Lazily create the queue to track worker ids.""" global \_WORKER\_ID\_QUEUE if \_WORKER\_ID\_QUEUE is None: \_WORKER\_ID\_QUEUE = multiprocessing.Queue() return \_WORKER\_ID\_QUEUE
def init\_pool(seqs): global \_SHARED\_SEQUENCES \_SHARED\_SEQUENCES = seqs
def get\_index(uid, i): """Get the value from the Sequence `uid` at index `i`.
 To allow multiple Sequences to be used at the same time, we use `uid` to get a specific one. A single Sequence would cause the validation to overwrite the training Sequence.
 Args: uid: int, Sequence identifier i: index
 Returns: The value at index `i`. """ return \_SHARED\_SEQUENCES[uid][i]
@keras\_export('keras.utils.SequenceEnqueuer')class SequenceEnqueuer(object): """Base class to enqueue inputs.
 The task of an Enqueuer is to use parallelism to speed up preprocessing. This is done with processes or threads.
 Example:
 ```python enqueuer = SequenceEnqueuer(...) enqueuer.start() datas = enqueuer.get() for data in datas: # Use the inputs; training, evaluating, predicting. # ... stop sometime. enqueuer.stop() ```
 The `enqueuer.get()` should be an infinite stream of datas. """
 def \_\_init\_\_(self, sequence, use\_multiprocessing=False): self.sequence = sequence self.use\_multiprocessing = use\_multiprocessing
 global \_SEQUENCE\_COUNTER if \_SEQUENCE\_COUNTER is None: try: \_SEQUENCE\_COUNTER = multiprocessing.Value('i', 0) except OSError: # In this case the OS does not allow us to use # multiprocessing. We resort to an int # for enqueuer indexing. \_SEQUENCE\_COUNTER = 0
 if isinstance(\_SEQUENCE\_COUNTER, int): self.uid = \_SEQUENCE\_COUNTER \_SEQUENCE\_COUNTER += 1 else: # Doing Multiprocessing.Value += x is not process-safe. with \_SEQUENCE\_COUNTER.get\_lock(): self.uid = \_SEQUENCE\_COUNTER.value \_SEQUENCE\_COUNTER.value += 1
 self.workers = 0 self.executor\_fn = None self.queue = None self.run\_thread = None self.stop\_signal = None
 def is\_running(self): return self.stop\_signal is not None and not self.stop\_signal.is\_set()
 def start(self, workers=1, max\_queue\_size=10): """Starts the handler's workers.
 Args: workers: Number of workers. max\_queue\_size: queue size (when full, workers could block on `put()`) """ if self.use\_multiprocessing: self.executor\_fn = self.\_get\_executor\_init(workers) else: # We do not need the init since it's threads. self.executor\_fn = lambda \_: get\_pool\_class(False)(workers) self.workers = workers self.queue = queue.Queue(max\_queue\_size) self.stop\_signal = threading.Event() self.run\_thread = threading.Thread(target=self.\_run) self.run\_thread.daemon = True self.run\_thread.start()
 def \_send\_sequence(self): """Sends current Iterable to all workers.""" # For new processes that may spawn \_SHARED\_SEQUENCES[self.uid] = self.sequence
 def stop(self, timeout=None): """Stops running threads and wait for them to exit, if necessary.
 Should be called by the same thread which called `start()`.
 Args: timeout: maximum time to wait on `thread.join()` """ self.stop\_signal.set() with self.queue.mutex: self.queue.queue.clear() self.queue.unfinished\_tasks = 0 self.queue.not\_full.notify() self.run\_thread.join(timeout) \_SHARED\_SEQUENCES[self.uid] = None
 def \_\_del\_\_(self): if self.is\_running(): self.stop()
 @abstractmethod def \_run(self): """Submits request to the executor and queue the `Future` objects.""" raise NotImplementedError
 @abstractmethod def \_get\_executor\_init(self, workers): """Gets the Pool initializer for multiprocessing.
 Args: workers: Number of workers.
 Returns: Function, a Function to initialize the pool """ raise NotImplementedError
 @abstractmethod def get(self): """Creates a generator to extract data from the queue.
 Skip the data if it is `None`. # Returns Generator yielding tuples `(inputs, targets)` or `(inputs, targets, sample\_weights)`. """ raise NotImplementedError
@keras\_export('keras.utils.OrderedEnqueuer')class OrderedEnqueuer(SequenceEnqueuer): """Builds a Enqueuer from a Sequence.
 Args: sequence: A `tf.keras.utils.data\_utils.Sequence` object. use\_multiprocessing: use multiprocessing if True, otherwise threading shuffle: whether to shuffle the data at the beginning of each epoch """
 def \_\_init\_\_(self, sequence, use\_multiprocessing=False, shuffle=False): super(OrderedEnqueuer, self).\_\_init\_\_(sequence, use\_multiprocessing) self.shuffle = shuffle
 def \_get\_executor\_init(self, workers): """Gets the Pool initializer for multiprocessing.
 Args: workers: Number of workers.
 Returns: Function, a Function to initialize the pool """ def pool\_fn(seqs): pool = get\_pool\_class(True)( workers, initializer=init\_pool\_generator, initargs=(seqs, None, get\_worker\_id\_queue())) \_DATA\_POOLS.add(pool) return pool
 return pool\_fn
 def \_wait\_queue(self): """Wait for the queue to be empty.""" while True: time.sleep(0.1) if self.queue.unfinished\_tasks == 0 or self.stop\_signal.is\_set(): return
 def \_run(self): """Submits request to the executor and queue the `Future` objects.""" sequence = list(range(len(self.sequence))) self.\_send\_sequence() # Share the initial sequence while True: if self.shuffle: random.shuffle(sequence)
 with closing(self.executor\_fn(\_SHARED\_SEQUENCES)) as executor: for i in sequence: if self.stop\_signal.is\_set(): return
 self.queue.put( executor.apply\_async(get\_index, (self.uid, i)), block=True)
 # Done with the current epoch, waiting for the final batches self.\_wait\_queue()
 if self.stop\_signal.is\_set(): # We're done return
 # Call the internal on epoch end. self.sequence.on\_epoch\_end() self.\_send\_sequence() # Update the pool
 def get(self): """Creates a generator to extract data from the queue.
 Skip the data if it is `None`.
 Yields: The next element in the queue, i.e. a tuple `(inputs, targets)` or `(inputs, targets, sample\_weights)`. """ while self.is\_running(): try: inputs = self.queue.get(block=True, timeout=5).get() if self.is\_running(): self.queue.task\_done() if inputs is not None: yield inputs except queue.Empty: pass except Exception as e: # pylint: disable=broad-except self.stop() raise e
def init\_pool\_generator(gens, random\_seed=None, id\_queue=None): """Initializer function for pool workers.
 Args: gens: State which should be made available to worker processes. random\_seed: An optional value with which to seed child processes. id\_queue: A multiprocessing Queue of worker ids. This is used to indicate that a worker process was created by Keras and can be terminated using the cleanup\_all\_keras\_forkpools utility. """ global \_SHARED\_SEQUENCES \_SHARED\_SEQUENCES = gens
 worker\_proc = multiprocessing.current\_process()
 # name isn't used for anything, but setting a more descriptive name is helpful # when diagnosing orphaned processes. worker\_proc.name = 'Keras\_worker\_{}'.format(worker\_proc.name)
 if random\_seed is not None: np.random.seed(random\_seed + worker\_proc.ident)
 if id\_queue is not None: # If a worker dies during init, the pool will just create a replacement. id\_queue.put(worker\_proc.ident, block=True, timeout=0.1)
def next\_sample(uid): """Gets the next value from the generator `uid`.
 To allow multiple generators to be used at the same time, we use `uid` to get a specific one. A single generator would cause the validation to overwrite the training generator.
 Args: uid: int, generator identifier
 Returns: The next value of generator `uid`. """ return next(\_SHARED\_SEQUENCES[uid])
@keras\_export('keras.utils.GeneratorEnqueuer')class GeneratorEnqueuer(SequenceEnqueuer): """Builds a queue out of a data generator.
 The provided generator can be finite in which case the class will throw a `StopIteration` exception.
 Args: generator: a generator function which yields data use\_multiprocessing: use multiprocessing if True, otherwise threading random\_seed: Initial seed for workers, will be incremented by one for each worker. """
 def \_\_init\_\_(self, generator, use\_multiprocessing=False, random\_seed=None): super(GeneratorEnqueuer, self).\_\_init\_\_(generator, use\_multiprocessing) self.random\_seed = random\_seed
 def \_get\_executor\_init(self, workers): """Gets the Pool initializer for multiprocessing.
 Args: workers: Number of works.
 Returns: A Function to initialize the pool """ def pool\_fn(seqs): pool = get\_pool\_class(True)( workers, initializer=init\_pool\_generator, initargs=(seqs, self.random\_seed, get\_worker\_id\_queue())) \_DATA\_POOLS.add(pool) return pool return pool\_fn
 def \_run(self): """Submits request to the executor and queue the `Future` objects.""" self.\_send\_sequence() # Share the initial generator with closing(self.executor\_fn(\_SHARED\_SEQUENCES)) as executor: while True: if self.stop\_signal.is\_set(): return
 self.queue.put( executor.apply\_async(next\_sample, (self.uid,)), block=True)
 def get(self): """Creates a generator to extract data from the queue.
 Skip the data if it is `None`.
 Yields: The next element in the queue, i.e. a tuple `(inputs, targets)` or `(inputs, targets, sample\_weights)`. """ try: while self.is\_running(): inputs = self.queue.get(block=True).get() self.queue.task\_done() if inputs is not None: yield inputs except StopIteration: # Special case for finite generators last\_ones = [] while self.queue.qsize() > 0: last\_ones.append(self.queue.get(block=True)) # Wait for them to complete for f in last\_ones: f.wait() # Keep the good ones last\_ones = [future.get() for future in last\_ones if future.successful()] for inputs in last\_ones: if inputs is not None: yield inputs except Exception as e: # pylint: disable=broad-except self.stop() if 'generator already executing' in str(e): raise RuntimeError( 'Your generator is NOT thread-safe. ' 'Keras requires a thread-safe generator when ' '`use\_multiprocessing=False, workers > 1`. ') raise e

## Footer

© 2025 GitHub, Inc.

### Footer navigation

* [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)
* [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)
* [Security](https://github.com/security)
* [Status](https://www.githubstatus.com/)
* [Docs](https://docs.github.com/)
* [Contact](https://support.github.com?tags=dotcom-footer)
* Manage cookies
* Do not share my personal information

You can’t perform that action at this time.



=== Content from vuln.ryotak.me_a6fff626_20250114_205546.html ===
![](/images/logo-small.webp)[RyotaK's Vuln DB](/)Menu

* [Twitter](https://twitter.com/ryotkak)

* [Login](/login)
Advisory #52

| Title | Keras arbitrary file overwrite via path traversal in "keras.utils.get\_file" |
| --- | --- |
| CVE ID | CVE-2021-35958 |
| Vendor | Keras Team |
| Affected product | Keras |
| Affected versions | <= latest |
| Vulnerability type | CWE-22 (Path Traversal) |
| Description | DISPUTED: Keras has a vulnerability that allows a malicious archive file to overwrite arbitrary files on the machine via "keras.utils.get\_file" function with "extract=True". NOTE: The vendor states that "Security conscious users would check the file, especially if they also pass the extract flag" (I've published this advisory to notify users.) |
| Status | No fix available |
| Recommendation | Do not use "keras.utils.get\_file" to extract contents of untrusted archive file. |


