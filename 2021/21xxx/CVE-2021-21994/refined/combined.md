=== Content from kb.vmware.com_435e7331_20250115_115513.html ===


search

cancel

Search

### How to disable or enable the SFCB service (CIM Server) on ESXi host

book
#### Article ID: 313877

calendar\_today
#### Updated On:

#### Products

VMware vSphere ESXi

Show More
Show Less

#### Issue/Introduction

Disable the SFCB service if it causes the vpxa watchdog service to restart management services by using all the available memory and swapping the space in the service console.

SFCB is disabled by default. The service starts when you install a third-party CIM VIB. For example, when running `esxcli software vib install -n VIBname` command.

**Note**: The CIM (Common Information Model) agent is the process which provides hardware health information. Disabling this service will disable some sensors reported in the hardware health status.

![](https://api-broadcomcms-software.wolkenservicedesk.com/attachment/get_attachment_content?uniqueFileId=1512726342894)

#### Environment

VMware vSphere ESXi

#### Resolution

**Note:** Support for CIM and SLP is deprecated in vSphere ESXi 8.0 due to security issues. Refer to [Common Information Model (CIM) and Service Location Protocol (SLP) removal VOBs in ESXi 8.0 Update 3](https://knowledge.broadcom.com/external/article?articleId=313159) and [ESXiArgs: Questions & Answers](https://core.vmware.com/esxiargs-questions-answers#introduction) for more information.

There is no requirement to reboot the ESXi host to disable/enable the service.

**To enable or disable the CIM agent on an ESXi 6.x and ESXi 7.x host using the vSphere Client:**

1. Login into the vCenter Server vSphere Client
2. Select the ESXi Host
3. Click on Configure, then Services.
4. Click on "CIM Server" as per the screenshot below.
   ![](https://api-broadcomcms-software.wolkenservicedesk.com/attachment/get_attachment_content?uniqueFileId=1632958840357)
5. Click on Stop.
6. Click on OK in the pop up window.
   ![](https://api-broadcomcms-software.wolkenservicedesk.com/attachment/get_attachment_content?uniqueFileId=1632944658495)
7. Click on "Edit Startup Policy"
8. Change the "Startup Policy" to "Start and stop manually"
   ![](https://api-broadcomcms-software.wolkenservicedesk.com/attachment/get_attachment_content?uniqueFileId=1632960484214)
9. The final state of CIM Server service should be "Stopped" and "Start and stop manually"
   ![](https://api-broadcomcms-software.wolkenservicedesk.com/attachment/get_attachment_content?uniqueFileId=1632945904474)
10. Reverse the steps above to re-enable the service. i.e. Change the settings to "Running" by clicking "Start" and change the Startup policy to "Start and stop with host"

**To disable the CIM agent on an ESXi 6.x and ESXi 7.0.x host and earlier versions using the command line**

1. SSH to the ESXi host via root
2. Run these commands:

   `chkconfig sfcbd-watchdog off`
   `/etc/init.d/sfcbd-watchdog stop`
3. Confirm that the settings by running:

   `chkconfig sfcbd-watchdog`
   `/etc/init.d/sfcbd-watchdog status`

   Final output should be similar to

   ![](https://api-broadcomcms-software.wolkenservicedesk.com/attachment/get_attachment_content?uniqueFileId=1512274653567)

   **Note**: Changing the chkconfig disables the sfcbd service and is persistent across reboots

**To enable the CIM agent on an ESXi 6.x and ESXi 7.0.x host and earlier versions using the command line**

**First we need to enable the "wbem" service using esxcli:**

[root@localhost:~] esxcli system wbem set -e true

Next:

1. Run the following commands to enable CIM

   `chkconfig sfcbd-watchdog on`
   `/etc/init.d/sfcbd-watchdog start`
2. To check the status of the agent on ESXi:

   `/etc/init.d/sfcbd-watchdog status`

   **Not**:The CIM service starts automatically when you install a third-party CIM VIB.

#### Additional Information

[How to disable or enable the SFCB service (CIM Server) on ESXi host](https://knowledge.broadcom.com/external/article/313877/how-to-disable-or-enable-the-sfcb-servic.html)

#### Feedback

thumb\_up
Yes

thumb\_down
No

Powered by
[![Wolken Software](https://cdn.wolkenservicedesk.com/wolken-footer-image.png)](https://www.wolkensoftware.com/)



=== Content from docs.vmware.com_a97866be_20250115_115509.html ===
VMware ESXi 7.0 Update 2 Release Notes

ESXi 7.0 Update 2 | 09 MAR 2021 | ISO Build 17630552

Check for additions and updates to these release notes.

What's in the Release Notes

The release notes cover the following topics:

What's New

Earlier Releases of ESXi 7.0

Patches Contained in this Release

Product Support Notices

Resolved Issues

Known Issues

What's New

vSphere Fault Tolerance supports vSphere Virtual Machine Encryption: Starting with vSphere 7.0 Update 2, vSphere FT supports VM encryption. In-guest and

array-based encryption do not depend on or interfere with VM encryption, but having multiple encryption layers uses additional compute resources, which

might impact virtual machine performance. The impact varies with the hardware as well as the amount and type of I/O, therefore VMware cannot quantify it, but

overall performance impact is negligible for most workloads. The effectiveness and compatibility of back-end storage features such as deduplication,

compression, and replication might also be affected by VM encryption and you should take into consideration storage tradeoffs. For more information, see

Virtual Machine Encryption and Virtual Machine Encryption Best Practices.

ESXi 7.0 Update 2 supports vSphere Quick Boot on the following servers:

Dell Inc.

PowerEdge M830

PowerEdge R830

HPE

ProLiant XL675d Gen10 Plus

Lenovo

ThinkSystem SR 635

ThinkSystem SR 655

Some ESXi configuration files become read-only: As of ESXi 7.0 Update 2, configuration formerly stored in the files /etc/keymap, /etc/vmware/welcome,

/etc/sfcb/sfcb.cfg, /etc/vmware/snmp.xml, /etc/vmware/logfilters, /etc/vmsyslog.conf,  and /etc/vmsyslog.conf.d/*.conf files, now resides in the ConfigStore

database. You can modify this configuration only by using ESXCLI commands, and not by editing files. For more information, see VMware knowledge base

articles 82637 and 82638.

VMware vSphere Virtual Volumes statistics for better debugging: With ESXi 7.0 Update 2, you can track performance statistics for vSphere Virtual Volumes to

quickly identify issues such as latency in third-party VASA provider responses. By using a set of commands, you can get statistics for all VASA providers in your

system, or for a specified namespace or entity in the given namespace, or enable statistics tracking for the complete namespace. For more information, see

Collecting Statistical Information for vVols.

NVIDIA Ampere аrchitecture support: vSphere 7.0 Update 2 adds support for the NVIDIA Ampere architecture that enables you to perform high end AI/ML

training, and ML inference workloads, by using the accelerated capacity of the A100 GPU. In addition, vSphere 7.0 Update 2 improves GPU sharing and utilization

by supporting the Multi-Instance GPU (MIG) technology. With vSphere 7.0 Update 2, you also see enhanced performance of device-to-device communication,

building on the existing NVIDIA GPUDirect functionality, by enabling Address Translation Services (ATS) and Access Control Services (ACS) at the PCIe bus layer

in the ESXi kernel.

Support for Mellanox ConnectX-6 200G NICs: ESXi 7.0 Update 2 supports Mellanox Technologies MT28908 Family (ConnectX-6) and Mellanox Technologies

MT2892 Family (ConnectX-6 Dx) 200G NICs.

Performance improvements for AMD Zen CPUs: With ESXi 7.0 Update 2, out-of-the-box optimizations can increase AMD Zen CPU performance by up to 30% in

various benchmarks. The updated ESXi scheduler takes full advantage of the AMD NUMA architecture to make the most appropriate placement decisions for

virtual machines and containers. AMD Zen CPU optimizations allow a higher number of VMs or container deployments with better performance.

Reduced compute and I/O latency, and jitter for latency sensitive workloads: Latency sensitive workloads, such as in financial and telecom applications, can see

significant performance benefit from I/O latency and jitter optimizations in ESXi 7.0 Update 2. The optimizations reduce interference and jitter sources to provide

a consistent runtime environment. With ESXi 7.0 Update 2, you can also see higher speed in interrupt delivery for passthrough devices.

Confidential vSphere Pods on a Supervisor Cluster in vSphere with Tanzu: Starting with vSphere 7.0 Update 2, you can run confidential vSphere Pods, keeping

guest OS memory encrypted and protected against access from the hypervisor, on a Supervisor Cluster in vSphere with Tanzu. You can configure confidential

vSphere Pods by adding Secure Encrypted Virtualization-Encrypted State (SEV-ES) as an extra security enhancement. For more information, see Deploy a

Confidential vSphere Pod.

vSphere Lifecycle Manager fast upgrades: Starting with vSphere 7.0 Update 2, you can significantly reduce upgrade time and system downtime, and minimize

system boot time, by suspending virtual machines to memory and using the Quick Boot functionality. You can configure vSphere Lifecycle Manager to suspend

virtual machines to memory instead of migrating them, powering them off, or suspending them to disk when you update an ESXi host. For more information, see

Configuring vSphere Lifecycle Manager for Fast Upgrades.

Encrypted Fault Tolerance log traffic: Starting with vSphere 7.0 Update 2, you can encrypt Fault Tolerance log traffic to get enhanced security. vSphere Fault

Tolerance performs frequent checks between the primary and secondary VMs to enable quick resumption from the last successful checkpoint. The checkpoint

contains the VM state that has been modified since the previous checkpoint. Encrypting the log traffic prevents malicious access or network attacks.

Earlier Releases of ESXi 7.0

New features, resolved, and known issues of ESXi are described in the release notes for each release. Release notes for earlier releases of ESXi 7.0 are:

VMware ESXi 7.0, Patch Release ESXi 7.0 Update 1d

VMware ESXi 7.0, Patch Release ESXi 7.0 Update 1c

VMware ESXi 7.0, Patch Release ESXi 7.0 Update 1b

VMware ESXi 7.0, Patch Release ESXi 7.0 Update 1a

VMware ESXi 7.0, Patch Release ESXi 7.0 Update 1

VMware ESXi 7.0, Patch Release ESXi 7.0b

For internationalization, compatibility, and open source components, see the VMware vSphere 7.0 Release Notes.

Patches Contained in This Release

This release of ESXi 7.0 Update 2 delivers the following patches:

Build Details

Download Filename:

VMware-ESXi-7.0U2-17630552-depot

Build:

Download Size:

md5sum:

sha1checksum:

Host Reboot Required:

Virtual Machine Migration or Shutdown Required:

IMPORTANT:

17630552

390.9 MB

4eae7823678cc7c57785e4539fe89d81

7c6b70a0190bd78bcf118f856cf9c60b4ad7d4b5

Yes

Yes

In the Lifecycle Manager plug-in of the vSphere Client, the release date for the ESXi 7.0.2 base image, profiles, and components is 2021-02-17. This is expected.

To ensure you can use correct filters by release date, only the release date of the rollup bulletin is 2021-03-09.

Starting with vSphere 7.0, VMware uses components for packaging VIBs along with bulletins. The ESXi and esx-update bulletins are dependent on each other.

Always include both in a single ESXi host patch baseline or include the rollup bulletin in the baseline to avoid failure during host patching.

When patching ESXi hosts by using VMware Update Manager from a version prior to ESXi 7.0 Update 2, it is strongly recommended to use the rollup bulletin in

the patch baseline. If you cannot use the rollup bulletin, be sure to include all of the following packages in the patching baseline. If the following packages are not

included in the baseline, the update operation fails:

VMware-vmkusb_0.1-1vmw.701.0.0.16850804 or higher

VMware-vmkata_0.1-1vmw.701.0.0.16850804 or higher

VMware-vmkfcoe_1.0.0.2-1vmw.701.0.0.16850804 or higher

VMware-NVMeoF-RDMA_1.0.1.2-1vmw.701.0.0.16850804 or higher

Rollup Bulletin

This rollup bulletin contains the latest VIBs with all the fixes after the initial release of ESXi 7.0.

Bulletin ID

ESXi70U2-17630552

Image Profiles

Category

Enhancement

Severity

Important

VMware patch and update releases contain general and critical image profiles. Application of the general release image profile applies to new bug fixes.

Image Profile Name

ESXi-7.0.2-17630552-standard

ESXi-7.0.2-17630552-no-tools

ESXi Image

Name and Version

ESXi_7.0.2-0.0.17630552

Release Date

03/09/2021

Category

General

Detail

Bugfix image

For information about the individual components and bulletins, see the Product Patches page and the Resolved Issues section.

Patch Download and Installation

In vSphere 7.x, the Update Manager plug-in, used for administering vSphere Update Manager, is replaced with the Lifecycle Manager plug-in. Administrative operations

for vSphere Update Manager are still available under the Lifecycle Manager plug-in, along with new capabilities for vSphere Lifecycle Manager.

The typical way to apply patches to ESXi 7.x hosts is by using the vSphere Lifecycle Manager. For details, see About vSphere Lifecycle Manager and vSphere Lifecycle

Manager Baselines and Images.

You can also update ESXi hosts without using the Lifecycle Manager plug-in, and use an image profile instead. To do this, you must manually download the patch

offline bundle ZIP file from the VMware download page or the Product Patches page and use the esxcli software profile update command.

For more information, see the Upgrading Hosts by Using ESXCLI Commands and the VMware ESXi Upgrade guide.

Product Support Notices

The inbox i40en network driver changes name: Starting with vSphere 7.0 Update 2, the inbox i40en network driver for ESXi changes name to i40enu.

Removal of SHA1 from Secure Shell (SSH): In vSphere 7.0 Update 2, the SHA-1 cryptographic hashing algorithm is removed from the SSHD default configuration.

Deprecation of Sphere 6.0 to 6.7 REST APIs: VMware deprecates REST APIs from vSphere 6.0 to 6.7 that were served under /rest and are referred to as old

REST APIs. With vSphere 7.0 Update 2, REST APIs are served under /api and referred to as new REST APIs. For more information, see the bog vSphere 7

Update 2 - REST API Modernization and vSphere knowledge base article 83022.

Intent to deprecate SHA-1: The SHA-1 cryptographic hashing algorithm will be deprecated in a future release of vSphere. SHA-1 and the already-deprecated MD5

have known weaknesses, and practical attacks against them have been demonstrated.

Standard formats of log files and syslog transmissions: In a future major ESXi release, VMware plans to standardize the formats of all ESXi log files and syslog

transmissions. This standardization affects the metadata associated with each log file line or syslog transmission. For example, the time stamp, programmatic

source identifier, message severity, and operation identifier data. For more information, visit https://core.vmware.com/esxi-log-message-formats.

Resolved Issues

The resolved issues are grouped as follows.

Intel Microcode Updates

Storage Issues

Auto Deploy Issues

Networking Issues

Miscellaneous Issues

Upgrade Issues

Intel Microcode Updates

ESXi 7.0 Update 2 includes the following Intel microcode updates:

Code Name

FMS

Plt ID

MCU Rev

MCU Date

Brand Names

Nehalem EP

0x106a5

0x03

0x0000001d

5/11/2018

Intel Xeon 35xx Series;
Intel Xeon 55xx Series

Lynnfield

Clarkdale

0x106e5

0x13

0x0000000a

5/8/2018

Intel Xeon 34xx Lynnfield Series

0x20652

0x12

0x00000011

5/8/2018

Intel i3/i5 Clarkdale Series;
Intel Xeon 34xx Clarkdale Series

Arrandale

0x20655

0x92

0x00000007

4/23/2018

Intel Core i7-620LE Processor

Sandy Bridge DT

0x206a7

0x12

0x0000002f

2/17/2019

Westmere EP

0x206c2

0x03

0x0000001f

5/8/2018

Sandy Bridge EP

0x206d6

0x6d

0x00000621

3/4/2020

Sandy Bridge EP

0x206d7

0x6d

0x0000071a

3/24/2020

Intel Xeon E3-1100 Series;
Intel Xeon E3-1200 Series;
Intel i7-2655-LE Series;
Intel i3-2100 Series

Intel Xeon 56xx Series;
Intel Xeon 36xx Series

Intel Pentium 1400 Series;
Intel Xeon E5-1400 Series;
Intel Xeon E5-1600 Series;
Intel Xeon E5-2400 Series;
Intel Xeon E5-2600 Series;
Intel Xeon E5-4600 Series

Intel Pentium 1400 Series;
Intel Xeon E5-1400 Series;
Intel Xeon E5-1600 Series;
Intel Xeon E5-2400 Series;
Intel Xeon E5-2600 Series;
Intel Xeon E5-4600 Series

Code Name

FMS

Plt ID

MCU Rev

MCU Date

Brand Names

Nehalem EX

0x206e6

0x04

0x0000000d

5/15/2018

Westmere EX

0x206f2

0x05

0x0000003b

5/16/2018

Ivy Bridge DT

0x306a9

0x12

0x00000021

2/13/2019

Haswell DT

0x306c3

0x32

0x00000028

11/12/2019

Ivy Bridge EP

0x306e4

0xed

0x0000042e

3/14/2019

Intel Xeon 65xx Series;
Intel Xeon 75xx Series

Intel Xeon E7-8800 Series;
Intel Xeon E7-4800 Series;
Intel Xeon E7-2800 Series

Intel i3-3200 Series;
Intel i7-3500-LE/UE;
Intel i7-3600-QE;
Intel Xeon E3-1200-v2 Series;
Intel Xeon E3-1100-C-v2 Series;
Intel Pentium B925C

Intel Xeon E3-1200-v3 Series;
Intel i7-4700-EQ Series;
Intel i5-4500-TE Series;
Intel i3-4300 Series

Intel Xeon E5-4600-v2 Series;
Intel Xeon E5-2600-v2 Series;
Intel Xeon E5-2400-v2 Series;
Intel Xeon E5-1600-v2 Series;
Intel Xeon E5-1400-v2 Series

Ivy Bridge EX

0x306e7

0xed

0x00000715

3/14/2019

Intel Xeon E7-8800/4800/2800-v2 Series

Haswell EP

0x306f2

0x6f

0x00000044

5/27/2020

Intel Xeon E5-4600-v3 Series;
Intel Xeon E5-2600-v3 Series;
Intel Xeon E5-2400-v3 Series;
Intel Xeon E5-1600-v3 Series;
Intel Xeon E5-1400-v3 Series

Haswell EX

0x306f4

0x80

0x00000016

6/17/2019

Intel Xeon E7-8800/4800-v3 Series

Broadwell H

0x40671

0x22

0x00000022

11/12/2019

Avoton

0x406d8

0x01

0x0000012d

9/16/2019

Broadwell EP/EX

0x406f1

0xef

0x0b000038

6/18/2019

Skylake SP

0x50654

0xb7

0x02006a08

6/16/2020

Cascade Lake B-0

0x50656

0xbf

0x04003003

6/18/2020

Cascade Lake

0x50657

0xbf

0x05003003

6/18/2020

Cooper Lake

0x5065b

0xbf

0x0700001f

9/17/2020

Intel Core i7-5700EQ;
Intel Xeon E3-1200-v4 Series

Intel Atom C2300 Series;
Intel Atom C2500 Series;
Intel Atom C2700 Series

Intel Xeon E7-8800/4800-v4 Series;
Intel Xeon E5-4600-v4 Series;
Intel Xeon E5-2600-v4 Series;
Intel Xeon E5-1600-v4 Series

Intel Xeon Platinum 8100 Series;
Intel Xeon Gold 6100/5100, Silver 4100,
Bronze 3100 Series;
Intel Xeon D-2100 Series;
Intel Xeon D-1600 Series;
Intel Xeon W-3100 Series;
Intel Xeon W-2100 Series

Intel Xeon Platinum 9200/8200 Series;
Intel Xeon Gold 6200/5200;
Intel Xeon Silver 4200/Bronze 3200;
Intel Xeon W-3200

Intel Xeon Platinum 9200/8200 Series;
Intel Xeon Gold 6200/5200;
Intel Xeon Silver 4200/Bronze 3200;
Intel Xeon W-3200

Intel Xeon Platinum 8300 Series;
Intel Xeon Gold 6300/5300

Broadwell DE

Broadwell DE

Broadwell DE

Broadwell NS

0x50662

0x50663

0x50664

0x50665

0x10

0x10

0x10

0x10

0x0000001c

6/17/2019

Intel Xeon D-1500 Series

0x07000019

6/17/2019

Intel Xeon D-1500 Series

0x0f000017

6/17/2019

Intel Xeon D-1500 Series

0x0e00000f

6/17/2019

Intel Xeon D-1600 Series

Skylake H/S

0x506e3

0x36

0x000000e2

7/14/2020

Intel Xeon E3-1500-v5 Series;
Intel Xeon E3-1200-v5 Series

Denverton

Snow Ridge

0x506f1

0x80665

0x01

0x01

0x00000032

3/7/2020

Intel Atom C3000 Series

0x0b000007

2/25/2020

Intel Atom P5000 Series

Kaby Lake H/S/X

0x906e9

0x2a

0x000000de

5/26/2020

Coffee Lake

0x906ea

0x22

0x000000de

5/25/2020

Intel Xeon E3-1200-v6 Series;
Intel Xeon E3-1500-v6 Series

Intel Xeon E-2100 Series;
Intel Xeon E-2200 Series (4 or 6 core)

Code Name

Coffee Lake

FMS

Plt ID

MCU Rev

MCU Date

Brand Names

0x906eb

0x02

0x000000de

5/25/2020

Intel Xeon E-2100 Series

Coffee Lake

0x906ec

0x22

0x000000de

6/3/2020

Intel Xeon E-2100 Series

Coffee Lake Refresh

0x906ed

0x22

0x000000de

5/24/2020

Intel Xeon E-2200 Series (8 core)

Storage Issues

After recovering from APD or PDL conditions, VMFS datastore with enabled support for clustered virtual disks might remain inaccessible

You can encounter this problem only on datastores where the clustered virtual disk support is enabled. When the datastore recovers from an All Paths Down

(APD) or Permanent Device Loss (PDL) condition, it remains inaccessible. The VMkernel log might show multiple SCSI3 reservation conflict messages similar to

the following:

2020-02-18T07:41:10.273Z cpu22:1001391219)ScsiDeviceIO: vm 1001391219: SCSIDeviceCmdCompleteCB:2972: Reservation conflict retries 544 for command 0x45ba814b8340

(op: 0x89) to device "naa.624a9370b97601e346f64ba900024d53"

The problem can occur because the ESXi host participating in the cluster loses SCSI reservations for the datastore and cannot always reacquire them

automatically after the datastore recovers.

This issue is resolved in this release.

Auto Deploy Issues

PR 2710383: If you deploy an ESXi host by using the vSphere Auto Deploy stateful install, ESXi configurations migrated to the ConfigStore database are lost

during upgrade

If you deploy an ESXi host by using the Auto Deploy stateful install feature, an option indicating the stateful install boot in the boot.cfg file is not removed and

conflicts with the persistent state of the ConfigStore database. As a result, ESXi configurations migrated to the ConfigStore database are lost during upgrade to

ESXi 7.x.

This issue is resolved in this release.

Networking Issues

PR 2696435: You cannot use virtual guest tagging (VGT) by default in an SR-IOV environment

With the i40enu driver in ESXi 7.0 Update 2, you cannot use VGT by default to avoid untrusted SR-IOV virtual functions (VF) to transmit or receive packets

tagged with a VLAN ID different from the port VLAN.

You must set all VFs to trusted mode by using the following module parameter:

esxcli system module parameters set -a -m i40enu -p "trust_all_vfs=1,1,1,...

This issue is resolved in this release.

Miscellaneous Issues

NEW: If the Xorg process fails to restart while an ESXi host exits maintenance mode, the hostd service might become unresponsive

If the Xorg process fails to restart while an ESXi host exits maintenance mode, the hostd service might become unresponsive as it cannot complete the exit

operation.

This issue is resolved in this release.

Upgrade Issues

NEW: If a vCenter Server system is of version 7.0, ESXi host upgrades to a later version by using the vSphere Lifecycle Manager and an ISO image fail

If you use an ISO image to upgrade ESXi hosts to a version later than 7.0 by using the vSphere Lifecycle Manager, and the vCenter Server system is still on

version 7.0, the upgrade fails. In the vSphere Client, you see an error Upgrade is not supported for host.

This issue is resolved in this release.

Known Issues

The known issues are grouped as follows.

Networking Issues

Security Issues

Miscellaneous Issues

Storage Issues

Virtual Machines Management Issues

vSAN Issues

Installation, Upgrade and Migration Issues

Known Issues from Earlier Releases

Networking Issues

NEW: After an upgrade to ESXi 7.0 Update 2, VIBs of the async i40en network drivers for ESXi are skipped or reverted to the VMware inbox driver i40enu

Starting with vSphere 7.0 Update 2, the inbox i40en driver was renamed to i40enu. As a result, if you attempt to install an i40en partner async driver, the i40en

VIB is either skipped or reverted to the VMware i40enu inbox driver.

Workaround: To complete the upgrade to ESXi 7.0 Update 2, you must create a custom image replacing the i40en component version 1.8.1.136-

1vmw.702.0.0.17630552 with the i40en component version Intel-i40en_1.10.9.0-1OEM.700.1.0.15525992 or greater. For more information, see Customizing Installations

with vSphere ESXi Image Builder.

NEW: When you set auto-negotiation on a network adapter, the device might fail

In some environments, if you set link speed to auto-negotiation for network adapters by using the command esxcli network nic set -a -n vmmicx, the devices

might fail and reboot does not recover connectivity. The issue is specific to a combination of some Intel X710/X722 network adapters, a SFP+ module and a

physical switch, where auto-negotiate speed/duplex scenario is not supported.

Workaround: Make sure you use an Intel-branded SFP+ module. Alternatively, use a Direct Attach Copper (DAC) cable.

Paravirtual RDMA (PVRDMA) network adapters do not support NSX networking policies

If you configure an NSX distributed virtual port for use in PVRDMA traffic, the RDMA protocol traffic over the PVRDMA network adapters does not comply with

the NSX network policies.

Workaround: Do not configure NSX distributed virtual ports for use in PVRDMA traffic.

Solarflare x2542 and x2541 network adapters configured in 1x100G port mode achieve throughput of up to 70Gbps in a vSphere environment

vSphere 7.0 Update 2 supports Solarflare x2542 and x2541 network adapters configured in 1x100G port mode. However, you might see a hardware limitation in

the devices that causes the actual throughput to be up to some 70Gbps in a vSphere environment.

Workaround: None

VLAN traffic might fail after a NIC reset

A NIC with PCI device ID 8086:1537 might stop to send and receive VLAN tagged packets after a reset, for example, with a command vsish -e set

/net/pNics/vmnic0/reset 1.

Workaround: Avoid resetting the NIC. If you already face the issue, use the following commands to restore the VLAN capability, for example at vmnic0:

# esxcli network nic software set --tagging=1 -n vmnic0

# esxcli network nic software set --tagging=0 -n vmnic0

Any change in the NetQueue balancer settings causes NetQueue to be disabled after an ESXi host reboot

Any change in the NetQueue balancer settings by using the command esxcli/localcli network nic queue loadbalancer set -n <nicname> --<lb_setting> causes

NetQueue, which is enabled by default, to be disabled after an ESXi host reboot.

Workaround: After a change in the NetQueue balancer settings and host reboot, use the command configstorecli config current get -c esx -g network -k nics

to retrieve ConfigStore data to verify whether the /esx/network/nics/net_queue/load_balancer/enable is working as expected.

After you run the command, you see output similar to:

{

"mac": "02:00:0e:6d:14:3e",

"name": "vmnic1",

"net_queue": {

  "load_balancer": {

    "dynamic_pool": true,

      "enable": true

  }

 },

 "virtual_mac": "00:50:56:5a:21:11"

}

If the output is not as expected, for example "load_balancer": "enable": false", run the following command:

esxcli/localcli network nic queue loadbalancer state set -n <nicname> -e true

Security Issues

Turn off the Service Location Protocol service in ESXi, slpd, to prevent potential security vulnerabilities

Some services in ESXi that run on top of the host operating system, including slpd, the CIM object broker, sfcbd, and the related openwsmand service, have

proven security vulnerabilities. VMware has addressed all known vulnerabilities in VMSA-2019-0022 and VMSA-2020-0023, and the fixes are part of the vSphere

7.0 Update 2 release. While sfcbd and openwsmand are disabled by default in ESXi, slpd is enabled by default and you must turn it off, if not necessary, to

prevent exposure to a future vulnerability after an upgrade.

Workaround: To turn off the slpd service, run the following PowerCLI commands:

$ Get-VMHost | Get-VmHostService | Where-Object {$_.key -eq “slpd”} | Set-VMHostService -policy “off”

$ Get-VMHost | Get-VmHostService | Where-Object {$_.key -eq “slpd”} | Stop-VMHostService -Confirm:$false

Alternatively, you can use the command chkconfig slpd off && /etc/init.d/slpd stop.

The openwsmand service is not on the ESXi services list and you can check the service state by using the following PowerCLI commands:

$esx=(Get-EsxCli -vmhost xx.xx.xx.xx -v2)

$esx.system.process.list.invoke() | where CommandLine -like '*openwsman*' | select commandline

In the ESXi services list, the sfcbd service appears as sfcbd-watchdog.

For more information, see VMware knowledge base articles 76372 and 1025757.

Miscellaneous Issues

You cannot create snapshots of virtual machines due to an error that a digest operation has failed

A rare race condition when an All-Paths-Down (APD) state occurs during the update of the Content Based Read Cache (CBRC) digest file might cause

inconsistencies in the digest file. As a result, you cannot create virtual machine snapshots. You see an error such as An error occurred while saving the snapshot:

A digest operation has failed in the backtrace.

Workaround: Power cycle the virtual machines to trigger a recompute of the CBRC hashes and clear the inconsistencies in the digest file.

An ESXi host might fail with a purple diagnostic screen due to a rare race condition in the qedentv driver

A rare race condition in the qedentv driver might cause an ESXi host to fail with a purple diagnostic screen. The issue occurs when an Rx complete interrupt

arrives just after a General Services Interface (GSI) queue pair (QP) is destroyed, for example during a qedentv driver unload or a system shut down. In such a

case, the qedentv driver might access an already freed QP address that leads to a PF exception. The issue might occur in ESXi hosts that are connected to a

busy physical switch with heavy unsolicited GSI traffic. In the backtrace, you see messages such as:

cpu4:2107287)0x45389609bcb0:[0x42001d3e6f72]qedrntv_ll2_rx_cb@(qedrntv)#<None>+0x1be stack: 0x45b8f00a7740, 0x1e146d040, 0x432d65738d40, 0x0, 0x

2021-02-11T03:31:53.882Z cpu4:2107287)0x45389609bd50:[0x42001d421d2a]ecore_ll2_rxq_completion@(qedrntv)#<None>+0x2ab stack: 0x432bc20020ed, 0x4c1e74ef0,

0x432bc2002000,

2021-02-11T03:31:53.967Z cpu4:2107287)0x45389609bdf0:[0x42001d1296d0]ecore_int_sp_dpc@(qedentv)#<None>+0x331 stack: 0x0, 0x42001c3bfb6b, 0x76f1e5c0, 0x2000097,

0x14c2002

2021-02-11T03:31:54.039Z cpu4:2107287)0x45389609be60:[0x42001c0db867]IntrCookieBH@vmkernel#nover+0x17c stack: 0x45389609be80, 0x40992f09ba, 0x43007a436690,

0x43007a43669

2021-02-11T03:31:54.116Z cpu4:2107287)0x45389609bef0:[0x42001c0be6b0]BH_Check@vmkernel#nover+0x121 stack: 0x98ba, 0x33e72f6f6e20, 0x0, 0x8000000000000000,

0x430000000001

2021-02-11T03:31:54.187Z cpu4:2107287)0x45389609bf70:[0x42001c28370c]NetPollWorldCallback@vmkernel#nover+0x129 stack: 0x61, 0x42001d0e0000, 0x42001c283770, 0x0,

0x0

2021-02-11T03:31:54.256Z cpu4:2107287)0x45389609bfe0:[0x42001c380bad]CpuSched_StartWorld@vmkernel#nover+0x86 stack: 0x0, 0x42001c0c2b44, 0x0, 0x0, 0x0

2021-02-11T03:31:54.319Z cpu4:2107287)0x45389609c000:[0x42001c0c2b43]Debug_IsInitialized@vmkernel#nover+0xc stack: 0x0, 0x0, 0x0, 0x0, 0x0

2021-02-11T03:31:54.424Z cpu4:2107287)^[[45m^[[33;1mVMware ESXi 7.0.2 [Releasebuild-17435195 x86_64]^[[0m

#PF Exception 14 in world 2107287:vmnic7-pollW IP 0x42001d3e6f72 addr 0x1c

Workaround: None

Storage Issues

NEW: If you use a USB as a boot device, ESXi hosts might become unresponsive and you see host not-responding and boot bank is not found alerts

USB devices have a small queue depth and due to a race condition in the ESXi storage stack, some I/O operations might not get to the device. Such I/Os queue

in the ESXi storage stack and ultimately time out. As a result, ESXi hosts become unresponsive. In the vSphere Client, you see alerts such as Alert: /bootbank not

to be found at path '/bootbank' and Host not-responding.

In vmkernel logs, you see errors such as:

2021-04-12T04:47:44.940Z cpu0:2097441)ScsiPath: 8058: Cancelled Cmd(0x45b92ea3fd40) 0xa0, cmdId.initiator=0x4538c859b8f8 CmdSN 0x0 from world 0 to path

"vmhba32:C0:T0:L0". Cmd count Active:0 Queued:1.

2021-04-12T04:48:50.527Z cpu2:2097440)ScsiDeviceIO: 4315: Cmd(0x45b92ea76d40) 0x28, cmdId.initiator=0x4305f74cc780 CmdSN 0x1279 from world 2099370 to dev

"mpx.vmhba32:C0:T0:L0" failed H:0x5 D:0x0 P:0x0 Cancelled from path layer. Cmd count Active:1

2021-04-12T04:48:50.527Z cpu2:2097440)Queued:4

Workaround: None.

If an NVMe device is hot added and hot removed in a short interval, the ESXi host might fail with a purple diagnostic screen

If an NVMe device is hot added and hot removed in a short interval, the NVMe driver might fail to initialize the NVMe controller due to a command timeout. As a

result, the driver might access memory that is already freed in a cleanup process. In the backtrace, you see a message such as WARNING: NVMEDEV:

NVMEInitializeController:4045: Failed to get controller identify data, status: Timeout.

Eventually, the ESXi host might fail with a purple diagnostic screen with an error similar to #PF Exception ... in world ...:vmkdevmgr.

Workaround: Perform hot-plug operations on a slot only after the previous hot-plug operation on the slot is complete. For example, if you want to run a hot-

remove after a hot-add operation, wait until the HBAs are created and LUNs are discovered. For the alternative scenario, hot-add after a hot-remove operation,

wait until all the LUNs and HBAs are removed.

Virtual Machines Management Issues

UEFI HTTP booting of virtual machines on ESXi hosts of version earlier than 7.0 Update 2 fails

UEFI HTTP booting of virtual machines is supported only on hosts of version ESXi 7.0 Update 2 and later and VMs with HW version 19 or later.

Workaround: Use UEFI HTTP booting only in virtual machines with HW version 19 or later. Using HW version 19 ensures the virtual machines are placed only on

hosts with ESXi version 7.0 Update 2 or later.

vSAN Issues

If you change the preferred site in a VMware vSAN Stretched Cluster, some objects might incorrectly appear as compliant

If you change the preferred site in a stretched cluster, some objects might incorrectly appear as compliant, because their policy settings might not automatically

change. For example, if you configure a virtual machine to keep data at the preferred site, when you change the preferred site, data might remain on the

nonpreferred site.

Workaround: Before you change a preferred site, in Advanced Settings, lower the ClomMaxCrawlCycleMinutes setting to 15 min to make sure objects policies are

updated. After the change, revert the ClomMaxCrawlCycleMinutes option to the earlier value.

Installation, Upgrade and Migration Issues

UEFI booting of ESXi hosts might stop with an error during an update to ESXi 7.0 Update 2 from an earlier version of ESXi 7.0

If you attempt to update your environment to 7.0 Update 2 from an earlier version of ESXi 7.0 by using vSphere Lifecycle Manager patch baselines, UEFI booting

of ESXi hosts might stop with an error such as:

Loading /boot.cfg

Failed to load crypto64.efi

Fatal error: 15 (Not found)

Workaround: For more information, see VMware knowledge base articles 83063 and 83107 .

If legacy VIBs are in use on an ESXi host, vSphere Lifecycle Manager cannot extract a desired software specification to seed to a new cluster

With vCenter Server 7.0 Update 2, you can create a new cluster by importing the desired software specification from a single reference host.  However, if legacy

VIBs are in use on an ESXi host, vSphere Lifecycle Manager cannot extract in the vCenter Server instance where you create the cluster a reference software

specification from such a host. In the /var/log/lifecycle.log, you see messages such as:

020-11-11T06:54:03Z lifecycle: 1000082644: HostSeeding:499 ERROR Extract depot failed: Checksum doesn't match. Calculated

5b404e28e83b1387841bb417da93c8c796ef2497c8af0f79583fd54e789d8826, expected: 0947542e30b794c721e21fb595f1851b247711d0619c55489a6a8cae6675e796 2020-11-

11T06:54:04Z lifecycle: 1000082644: imagemanagerctl:366 ERROR Extract depot failed. 2020-11-11T06:54:04Z lifecycle: 1000082644: imagemanagerctl:145 ERROR

[VibChecksumError]

Workaround: Follow the steps described in VMware knowledge base article 83042.

You see a short burst of log messages in the syslog.log after every ESXi boot

After updating to ESXi 7.0 Update 2, you might see a short burst of log messages after every ESXi boot.

Such logs do not indicate any issue with ESXi and you can ignore these messages. For example:

 2021-01-19T22:44:22Z watchdog-vaai-nasd: '/usr/lib/vmware/nfs/bin/vaai-nasd -f' exited after 0 seconds (quick failure 127) 1

2021-01-19T22:44:22Z watchdog-vaai-nasd: Executing '/usr/lib/vmware/nfs/bin/vaai-nasd -f'

2021-01-19T22:44:22.990Z aainasd[1000051135]: Log for VAAI-NAS Daemon for NFS version=1.0 build=build-00000 option=DEBUG

2021-01-19T22:44:22.990Z vaainasd[1000051135]: DictionaryLoadFile: No entries loaded by dictionary.

2021-01-19T22:44:22.990Z vaainasd[1000051135]: DictionaryLoad: Cannot open file "/usr/lib/vmware/config": No such file or directory.

2021-01-19T22:44:22.990Z vaainasd[1000051135]: DictionaryLoad: Cannot open file "//.vmware/config": No such file or directory.

2021-01-19T22:44:22.990Z vaainasd[1000051135]: DictionaryLoad: Cannot open file "//.vmware/preferences": No such file or directory.

2021-01-19T22:44:22.990Z vaainasd[1000051135]: Switching to VMware syslog extensions

2021-01-19T22:44:22.992Z vaainasd[1000051135]: Loading VAAI-NAS plugin(s).

2021-01-19T22:44:22.992Z vaainasd[1000051135]: DISKLIB-PLUGIN : Not loading plugin /usr/lib/vmware/nas_plugins/lib64: Not a shared library.

Workaround: None

You see warning messages for missing VIBs in vSphere Quick Boot compatibility check reports

After you upgrade to ESXi 7.0 Update 2, if you check vSphere Quick Boot compatibility of your environment by using the

/usr/lib/vmware/loadesx/bin/loadESXCheckCompat.py command, you might see some warning messages for missing VIBs in the shell. For example:

Cannot find VIB(s) ... in the given VIB collection.

Ignoring missing reserved VIB(s) ..., they are removed from reserved VIB IDs.

Such warnings do not indicate a compatibility issue.

Workaround: The missing VIB messages can be safely ignored and do not affect the reporting of vSphere Quick Boot compatibility. The final output line of the

loadESXCheckCompat command unambiguously indicates if the host is compatible.

Auto bootstrapping a cluster that you manage with a vSphere Lifecycle Manager image fails with an error

If you attempt auto bootstrapping a cluster that you manage with a vSphere Lifecycle Manager image to perform a stateful install and overwrite the VMFS

partitions, the operation fails with an error. In the support bundle, you see messages such as:

2021-02-11T19:37:43Z Host Profiles[265671 opID=MainThread]: ERROR: EngineModule::ApplyHostConfig. Exception: [Errno 30] Read-only file system

Workaround: Follow vendor guidance to clean the VMFS partition in the target host and retry the operation. Alternatively, use an empty disk. For more

information on the disk-partitioning utility on ESXi, see VMware knowledge base article 1036609.

Upgrades to ESXi 7.x from 6.5.x and 6.7.0 by using ESXCLI might fail due to a space limitation

Upgrades to ESXi 7.x from 6.5.x and 6.7.0 by using the esxcli software profile update or esxcli software profile install ESXCLI commands might fail, because

the ESXi bootbank might be less than the size of the image profile. In the ESXi Shell or the PowerCLI shell, you see an error such as:

[InstallationError]

 The pending transaction requires 244 MB free space, however the maximum supported size is 239 MB.

 Please refer to the log file for more details.

The issue also occurs when you attempt an ESXi host upgrade by using the ESXCLI commands esxcli software vib update or esxcli software vib install.

Workaround: You can perform the upgrade in two steps, by using the esxcli software profile update command to update ESXi hosts to ESXi 6.7 Update 1 or later,

and then update to 7.0 Update 1c. Alternatively, you can run an upgrade by using an ISO image and the vSphere Lifecycle Manager.

Known Issues from Earlier Releases

To view a list of previous known issues, click here.

Copyright © Broadcom



=== Content from www.vmware.com_14028878_20250114_195955.html ===


Menu

* [Products](https://www.broadcom.com/products/)
* [Solutions](https://www.broadcom.com/solutions)
* [Support and Services](https://www.broadcom.com/support)
* [Company](https://www.broadcom.com/company/about-us/)
* [How To Buy](https://www.broadcom.com/how-to-buy/#sales)

* Log in

  [Log In](/c/portal/login)
  [Register](https://profile.broadcom.com/web/registration)

[Register](https://profile.broadcom.com/web/registration)
[Login](/c/portal/login)

VMSA-2021-0014:VMware ESXi updates address authentication and denial of service vulnerabilities

Product/Component

VMware Cloud Foundation

1 more products

List of Products

2 Products

* VMware Cloud Foundation
* VMware vSphere ESXi

Notification Id

23606

Last Updated

22 August 2021

Initial Publication Date

11 July 2021

Status

CLOSED

Severity

HIGH

CVSS Base Score

5.3-7.0

WorkAround

Affected CVE

CVE-2021-21994,CVE-2021-21995

             Advisory ID: VMSA-2021-0014.1   CVSSv3 Range: 5.3-7.0   Issue Date:2021-07-13   Updated On: 2021-08-24    CVE(s): CVE-2021-21994, CVE-2021-21995   Synopsis: VMware ESXi updates address authentication and denial of service vulnerabilities (CVE-2021-21994, CVE-2021-21995)

 [RSS Feed](https://www.vmware.com/security/advisories/VMSA-2021-0014.xml)

 Download PDF

 Download Text File

Share this page on social media:

##### **1. Impacted Products**

* VMware ESXi
* VMware Cloud Foundation (Cloud Foundation)

##### **2. Introduction**

Multiple vulnerabilities in VMware ESXi were privately reported to VMware. Updates and workarounds are available to remediate these vulnerabilities in affected VMware products.

##### **3a. ESXi SFCB improper authentication vulnerability (CVE-2021-21994)**

**Description**

SFCB (Small Footprint CIM Broker) as used in ESXi has an authentication bypass vulnerability.VMware has evaluated the severity of this issue to be in the [Important severity range](https://www.vmware.com/support/policies/security_response.html) with a maximum CVSSv3 base score of [7.0](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:N/AC:H/PR:N/UI:N/S:U/C:H/I:L/A:L).

**Known Attack Vectors**

A malicious actor with network access to port 5989 on ESXi may exploit this issue to bypass SFCB authentication by sending a specially crafted request.

**Resolution**

To remediate CVE-2021-21994 apply the patches listed in the 'Fixed Version' column of the 'Response Matrix' found below.

**Workarounds**

Workarounds for CVE-2021-21994 have been listed in the 'Workarounds' column of the 'Response Matrix' below.

**Additional Documentation**

None.

**Notes**

SFCB service is not enabled by default on ESXi. For successful exploitation, SFCB service should be running. The status of the service can be checked by following the steps mentioned in [KB1025757](https://kb.vmware.com/s/article/1025757).

**Acknowledgements**

VMware would like to thank Douglas Everson of Voya Financial for reporting this issue to us.

**Response Matrix**

| Product | Version | Running On | CVE Identifier | CVSSv3 | Severity | Fixed Version | Workarounds | Additional Documentation |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| ESXi | 7.0 | Any | CVE-2021-21994 | [7.0](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:N/AC:H/PR:N/UI:N/S:U/C:H/I:L/A:L) | important | [ESXi70U2-17630552](https://docs.vmware.com/en/VMware-vSphere/7.0/rn/vsphere-esxi-702-release-notes.html) | [KB1025757](https://kb.vmware.com/s/article/1025757) | None |
| ESXi | 6.7 | Any | CVE-2021-21994 | [7.0](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:N/AC:H/PR:N/UI:N/S:U/C:H/I:L/A:L) | important | [ESXi670-202103101-SG](https://docs.vmware.com/en/VMware-vSphere/6.7/rn/esxi670-202103001.html) | [KB1025757](https://kb.vmware.com/s/article/1025757) | None |
| ESXi | 6.5 | Any | CVE-2021-21994 | [7.0](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:N/AC:H/PR:N/UI:N/S:U/C:H/I:L/A:L) | important | [ESXi650-202107401-SG](https://docs.vmware.com/en/VMware-vSphere/6.5/rn/esxi650-202107001.html) | [KB1025757](https://kb.vmware.com/s/article/1025757) | None |

**Impacted Product Suites that Deploy Response Matrix 3a Components:**

| Product | Version | Running On | CVE Identifier | CVSSv3 | Severity | Fixed Version | Workarounds | Additional Documentation |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| Cloud Foundation (ESXi) | 4.x | Any | CVE-2021-21994 | [7.0](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:N/AC:H/PR:N/UI:N/S:U/C:H/I:L/A:L) | important | [4.3](https://docs.vmware.com/en/VMware-Cloud-Foundation/4.3/rn/VMware-Cloud-Foundation-43-Release-Notes.html) | [KB1025757](https://kb.vmware.com/s/article/1025757) | None |
| Cloud Foundation (ESXi) | 3.x | Any | CVE-2021-21994 | [7.0](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:N/AC:H/PR:N/UI:N/S:U/C:H/I:L/A:L) | important | [3.10.2](https://docs.vmware.com/en/VMware-Cloud-Foundation/3.10.2/rn/VMware-Cloud-Foundation-3102-Release-Notes.html) | [KB1025757](https://kb.vmware.com/s/article/76372) | None |

##### **3b. ESXi OpenSLP denial-of-service vulnerability (CVE-2021-21995)**

**Description**

OpenSLP as used in ESXi has a denial-of-service vulnerability due a heap out-of-bounds read issue. VMware has evaluated the severity of this issue to be in the [Moderate severity range](https://www.vmware.com/support/policies/security_response.html) with a maximum CVSSv3 base score of [5.3](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:L).

**Known Attack Vectors**

A malicious actor with network access to port 427 on ESXi may be able to trigger a heap out-of-bounds read in OpenSLP service resulting in a denial-of-service condition.

**Resolution**

To remediate CVE-2021-21995 apply the patches listed in the 'Fixed Version' column of the 'Response Matrix' found below.

**Workarounds**

Workarounds for CVE-2021-21995 have been listed in the 'Workarounds' column of the 'Response Matrix' below.

**Additional Documentation**

None.

**Notes**

Per the Security Configuration Guides for VMware vSphere, VMware now recommends disabling the OpenSLP service in ESXi if it is not used. For more information, see our blog posting: <https://blogs.vmware.com/vsphere/2021/02/evolving-the-vmware-vsphere-security-configuration-guides.html>

**Acknowledgements**

VMware would like to thank VictorV(Tangtianwen) of Kunlun Lab for reporting this issue to us.

**Response Matrix**

| Product | Version | Running On | CVE Identifier | CVSSv3 | Severity | Fixed Version | Workarounds | Additional Documentation |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| ESXi | 7.0 | Any | CVE-2021-21995 | [5.3](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:L) | moderate | [ESXi70U2-17630552](https://docs.vmware.com/en/VMware-vSphere/7.0/rn/vsphere-esxi-702-release-notes.html) | [KB76372](https://kb.vmware.com/s/article/76372) | None |
| ESXi | 6.7 | Any | CVE-2021-21995 | [5.3](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:L) | moderate | [ESXi670-202103101-SG](https://docs.vmware.com/en/VMware-vSphere/6.7/rn/esxi670-202103001.html) | [KB76372](https://kb.vmware.com/s/article/76372) | None |
| ESXi | 6.5 | Any | CVE-2021-21995 | [5.3](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:L) | moderate | [ESXi650-202107401-SG](https://docs.vmware.com/en/VMware-vSphere/6.5/rn/esxi650-202107001.html) | [KB76372](https://kb.vmware.com/s/article/76372) | None |

**Impacted Product Suites that Deploy Response Matrix 3b Components:**

| Product | Version | Running On | CVE Identifier | CVSSv3 | Severity | Fixed Version | Workarounds | Additional Documentation |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| Cloud Foundation (ESXi) | 4.x | Any | CVE-2021-21995 | [5.3](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:L) | moderate | [4.3](https://docs.vmware.com/en/VMware-Cloud-Foundation/4.3/rn/VMware-Cloud-Foundation-43-Release-Notes.html) | [KB76372](https://kb.vmware.com/s/article/76372) | None |
| Cloud Foundation (ESXi) | 3.x | Any | CVE-2021-21995 | [5.3](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:L) | moderate | [3.10.2](https://docs.vmware.com/en/VMware-Cloud-Foundation/3.10.2/rn/VMware-Cloud-Foundation-3102-Release-Notes.html) | [KB76372](https://kb.vmware.com/s/article/76372) | None |

##### **4. References**

**VMware ESXi 7.0 ESXi70U2-17630552**
Downloads and Documentation:
<https://my.vmware.com/group/vmware/patch>
<https://docs.vmware.com/en/VMware-vSphere/7.0/rn/vsphere-esxi-702-release-notes.html>

**VMware ESXi 6.7 ESXi670-202103101-SG**
Downloads and Documentation:
<https://my.vmware.com/group/vmware/patch>
<https://docs.vmware.com/en/VMware-vSphere/6.7/rn/esxi670-202103001.html>

**VMware ESXi 6.5 ESXi650-202107401-SG**
Downloads and Documentation:
<https://my.vmware.com/group/vmware/patch>
<https://docs.vmware.com/en/VMware-vSphere/6.5/rn/esxi650-202107001.html>

**VMware Cloud Foundation 4.3**
Downloads and Documentation:

<https://docs.vmware.com/en/VMware-Cloud-Foundation/4.3/rn/VMware-Cloud-Foundation-43-Release-Notes.html>

**VMware Cloud Foundation 3.10.2**
Downloads and Documentation:
<https://docs.vmware.com/en/VMware-Cloud-Foundation/3.10.2/rn/VMware-Cloud-Foundation-3102-Release-Notes.html>

**Mitre CVE Dictionary Links:**
<https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-21994>
<https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-21995>

**FIRST CVSSv3 Calculator:**
CVE-2021-21994: <https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:N/AC:H/PR:N/UI:N/S:U/C:H/I:L/A:L>
CVE-2021-21995: <https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:L>

##### **5. Change Log**

**2021-07-13 VMSA-2021-0014**
Initial security advisory.

**2021-08-24 VMSA-2021-0014.1**

Added Cloud Foundation 4.x fixed version in the Response Matrix section of 3a and 3b.

##### **6. Contact**

E-mail list for product security notifications and announcements:

[https://lists.vmware.com/mailman/listinfo/security-announce](https://lists.vmware.com/cgi-bin/mailman/listinfo/security-announce)

This Security Advisory is posted to the following lists:

[[email protected]](/cdn-cgi/l/email-protection#0774626472756e737e2a6669696872696462476b6e74737429716a706675622964686a)

[[email protected]](/cdn-cgi/l/email-protection#4123342635332030013224223433283538272e2234326f222e2c)

[[email protected]](/cdn-cgi/l/email-protection#4325362f2f272a30202f2c30363126033026202f2a3037306d2c3124)

E-mail: [[email protected]](/cdn-cgi/l/email-protection#c6b5a3a5b3b4afb2bf86b0abb1a7b4a3e8a5a9ab)

PGP key at:

<https://kb.vmware.com/kb/1055>

VMware Security Advisories

<https://www.vmware.com/security/advisories>

VMware Security Response Policy

<https://www.vmware.com/support/policies/security_response.html>

VMware Lifecycle Support Phases

<https://www.vmware.com/support/policies/lifecycle.html>

VMware Security & Compliance Blog

<https://blogs.vmware.com/security>

Twitter

<https://twitter.com/VMwareSRC>

Copyright 2021 VMware Inc. All rights reserved.

Hidden

#####

×

It appears your Broadcom Products and Services are

supported by one of our certified Support partners

Click below to be redirected to the appropriate Support

Partner Portal to request support

For non-product related issues (Support Portal / Licensing) Click HERE

Continue

#####

×

For **Technical Support** (issues with products or services)

1. Select **Technical** to be redirected to the My Entitlements page
2. Expand the product you require support on
3. Select the case icon from the case column
4. You will be redirected to the appropriate vendor portal where you can raise your technical request

For **Non-Technical Support** (issues with portal access, license keys, software downloads)

1. Select **Non-Technical** to be redirected to Broadcom's case management portal

Technical
Non-Technical

#####

×

# Access Denied

This feature has been disabled by your administrator.

#####

×

To prevent this message from showing again, please enable pop-up blockers for [support.broadcom.com](https://support.broadcom.com/)
or click Continue to proceed.

Continue

Top

* [Products](https://www.broadcom.com/products)
* [Solutions](https://www.broadcom.com/solutions)
* [Support and Services](https://www.broadcom.com/support)
* [Company](https://www.broadcom.com/)
* [How to Buy](https://www.broadcom.com/how-to-buy)

 Copyright © 2005-2024 Broadcom. All Rights Reserved. The term “Broadcom” refers to Broadcom Inc. and/or its subsidiaries.

* [Accessibility](https://www.broadcom.com/company/legal/accessibility)
* [Privacy](https://www.broadcom.com/company/legal/privacy)
* [Supplier Responsibility](https://www.broadcom.com/company/citizenship/supplier-responsibility)
* [Terms of Use](https://www.broadcom.com/company/legal/terms-of-use)
* [Site Map](https://www.broadcom.com/sitemap)



=== Content from docs.vmware.com_1901038f_20250115_115514.html ===
VMware ESXi650-202107001 Release Notes

Release Date: 13 JUL, 2021

What's in the Release Notes

The release notes cover the following topics:

Patch Build Details

Resolved Issues

Known Issues from Previous Releases

Build Details

Download Filename:

ESXi650-202107001.zip

Build:

Download Size:

md5sum:

sha1checksum:

Host Reboot Required:

Virtual Machine Migration or Shutdown Required:

Bulletins

Bulletin ID

ESXi650-202107401-SG

Rollup Bulletin

18071574

345.8 MB

5456e1f635294028946f1b9151ff91d1

a4363a816f58fea974c77e83865c8e8d641ee69f

Yes

Yes

Category

Security

Severity

Important

This rollup bulletin contains the latest VIBs with all the fixes since the initial release of ESXi 6.5.

Bulletin ID

ESXi650-202107001

Category

Security

Severity

Important

IMPORTANT: For clusters using VMware vSAN, you must first upgrade the vCenter Server system. Upgrading only ESXi is not supported.

Before an upgrade, always verify in the VMware Product Interoperability Matrix compatible upgrade paths from earlier versions of ESXi, vCenter Server and vSAN to

the current version.

Image Profiles

VMware patch and update releases contain general and critical image profiles. Application of the general release image profile applies to new bug fixes.

Image Profile Name

ESXi-6.5.0-20210704001-standard

ESXi-6.5.0-20210704001-no-tools

For more information about the individual bulletins, see the Download Patches page and the Resolved Issues section.

Patch Download and Installation

The typical way to apply patches to ESXi hosts is by using the VMware vSphere Update Manager. For details, see About Installing and Administering VMware vSphere

Update Manager.

ESXi hosts can be updated by manually downloading the patch ZIP file from the VMware download page and installing the VIB by using the esxcli software vib update

command. Additionally, the system can be updated by using the image profile and the esxcli software profile update command.

For more information, see vSphere Command-Line Interface Concepts and Examples and vSphere Upgrade Guide.

Resolved Issues

The resolved issues are grouped as follows.

ESXi650-202107401-SG

ESXi-6.5.0-20210704001-standard

ESXi-6.5.0-20210704001-no-tools

ESXi650-202107401-SG

Patch Category

Patch Severity

Security

Important

Host Reboot Required

Virtual Machine Migration or
Shutdown Required

Affected Hardware

Affected Software

Yes

Yes

N/A

N/A

VIBs Included

VMware_bootbank_esx-tboot_6.5.0-3.161.18071574
VMware_bootbank_vsanhealth_6.5.0-3.161.18071576
VMware_bootbank_vsan_6.5.0-3.161.18071575
VMware_bootbank_esx-base_6.5.0-3.161.18071574

PRs Fixed

2711589, 2700917

Related CVE numbers

CVE-2021-21994, CVE-2021-21995

Updates esx-base, esx-tboot, vsan and vsanhealth VIBs:

SFCB (Small Footprint CIM Broker) as used in ESXi has an authentication bypass vulnerability. The Common Vulnerabilities and Exposures project

(cve.mitre.org) has assigned the identifier CVE-2021-21994 to this issue. For more information, see VMware Security Advisory VMSA-2021-0014.

OpenSLP as used in ESXi has a denial-of-service vulnerability. The Common Vulnerabilities and Exposures project (cve.mitre.org) has assigned the identifier

CVE-2021-21995 to this issue. For more information, see VMware Security Advisory VMSA-2021-0014.

ESXi-6.5.0-20210704001-standard

Profile Name

Build

Vendor

Release Date

Acceptance Level

Affected Hardware

Affected Software

Affected VIBs

ESXi-6.5.0-20210704001-standard

For build information, see the top of the page.

VMware, Inc.

July 13, 2021

PartnerSupported

N/A

N/A

VMware_bootbank_esx-tboot_6.5.0-3.161.18071574
VMware_bootbank_vsanhealth_6.5.0-3.161.18071576
VMware_bootbank_vsan_6.5.0-3.161.18071575
VMware_bootbank_esx-base_6.5.0-3.161.18071574

PRs Fixed

2711589, 2700917

Related CVE numbers

CVE-2021-21994, CVE-2021-21995

This patch updates the following issues:

SFCB (Small Footprint CIM Broker) as used in ESXi has an authentication bypass vulnerability. The Common Vulnerabilities and Exposures project

(cve.mitre.org) has assigned the identifier CVE-2021-21994 to this issue. For more information, see VMware Security Advisory VMSA-2021-0014.

OpenSLP as used in ESXi has a denial-of-service vulnerability. The Common Vulnerabilities and Exposures project (cve.mitre.org) has assigned the identifier

CVE-2021-21995 to this issue. For more information, see VMware Security Advisory VMSA-2021-0014.

ESXi-6.5.0-20210704001-no-tools

Profile Name

Build

Vendor

Release Date

Acceptance Level

Affected Hardware

Affected Software

Affected VIBs

ESXi-6.5.0-20210704001-no-tools

For build information, see the top of the page.

VMware, Inc.

July 13, 2021

PartnerSupported

N/A

N/A

VMware_bootbank_esx-tboot_6.5.0-3.161.18071574
VMware_bootbank_vsanhealth_6.5.0-3.161.18071576
VMware_bootbank_vsan_6.5.0-3.161.18071575
VMware_bootbank_esx-base_6.5.0-3.161.18071574

PRs Fixed

2711589, 2700917

Related CVE numbers

CVE-2021-21994, CVE-2021-21995

This patch updates the following issues:

SFCB (Small Footprint CIM Broker) as used in ESXi has an authentication bypass vulnerability. The Common Vulnerabilities and Exposures project

(cve.mitre.org) has assigned the identifier CVE-2021-21994 to this issue. For more information, see VMware Security Advisory VMSA-2021-0014.

OpenSLP as used in ESXi has a denial-of-service vulnerability. The Common Vulnerabilities and Exposures project (cve.mitre.org) has assigned the identifier

CVE-2021-21995 to this issue. For more information, see VMware Security Advisory VMSA-2021-0014.

Known Issues from Previous Releases

To view a list of previous known issues, click here.

Copyright © Broadcom



=== Content from kb.vmware.com_da692398_20250115_115511.html ===


search

cancel

Search

### How to Disable/Enable the SLP Service on VMware ESXi

book
#### Article ID: 318790

calendar\_today
#### Updated On:

#### Products

VMware vSphere ESXi

Show More
Show Less

#### Issue/Introduction

OpenSLP vulnerabilities have been disclosed that affect ESXi. These vulnerabilities and their impact on VMware products are documented in the following VMware Security Advisories (VMSAs), please review these before continuing as there may be considerations outside the scope of this document:

[VMSA-2022-0030](https://www.vmware.com/security/advisories/VMSA-2022-0030.html) (CVE-2022-31699)
[VMSA-2021-0014](https://www.vmware.com/security/advisories/VMSA-2021-0014.html) (CVE-2021-21995)
[VMSA-2021-0002](https://www.vmware.com/security/advisories/VMSA-2021-0002.html) (CVE-2021-21974)
[VMSA-2020-0023](https://www.vmware.com/security/advisories/VMSA-2020-0023.html) (CVE-2020-3992)
[VMSA-2019-0022](https://www.vmware.com/security/advisories/VMSA-2019-0022.html) (CVE-2019-5544)

The ESXi team has investigated these vulnerabilities and determined that the possibility of exploitation can be removed by performing the steps detailed in the resolution section of this article. This workaround is meant to be a temporary solution only and customers are advised to deploy the patches documented in the aforementioned VMSAs.

**Warning**:

This workaround is applicable ONLY to ESXi. Do not apply this workaround to other VMware products.

**Functionality Impacts:**

With the workaround, CIM clients which uses SLP to find CIM servers over port #427 will not be able to locate the service.

There is no requirement to reboot the ESXi host to disable/enable the service

#### Resolution

To implement the workaround perform the following steps:

       1 Login to the ESXi hosts using an SSH session (such as putty)

2 Stop the SLP service on the ESXi host with this command:

/etc/init.d/slpd stop

**Note**: The SLP service can only be stopped when the service is not in use. Use the following command to view the operational state of Service Location Protocol Daemon:

esxcli system slp stats get

3 Run the following command to disable the SLP service:

esxcli network firewall ruleset set -r CIMSLP -e 0

To make this change persist across reboots:

chkconfig slpd off

To check if the change is applied across reboots:

chkconfig --list | grep slpd

*output: slpd off*

![](https://api-broadcomcms-software.wolkenservicedesk.com/attachment/get_attachment_content?uniqueFileId=1512273899654)

To remove the workaround perform the following steps:

1. Run the following command to enable the ruleset of SLP service:

esxcli network firewall ruleset set -r CIMSLP -e 1

2. Run the following command to change the current startup information of slpd service:

chkconfig slpd on

Run the following command to check if the change is applied after running the above step (Step 2#):

chkconfig --list | grep slpd

*output:*slpd on

3. Run the following command to start the SLP service:

/etc/init.d/slpd start

4. Disable and enable the CIM agent, see [How to disable or enable the CIM agent on the ESX/ESXi host](https://knowledge.broadcom.com/external/article/313877)

Later versions of ESXi report the SLPD service in the vCenter GUI

        1. To check if you can update the SLP service via the vSphere client, login to the vCenter

        2 Select the ESXi host and click on "Configure"  -- "Services". Look for SLP in the list
            If SLP is not listed, then use the process detailed above

![](https://api-broadcomcms-software.wolkenservicedesk.com/attachment/get_attachment_content?uniqueFileId=1512276587924)

        3 Select SLPD and click on "Stop" and then click "Ok"

![](https://api-broadcomcms-software.wolkenservicedesk.com/attachment/get_attachment_content?uniqueFileId=1512273460105)
           4 Select " Edit Startup Policy" and select "Start and stop manually". Click Ok

![](https://api-broadcomcms-software.wolkenservicedesk.com/attachment/get_attachment_content?uniqueFileId=1512276788978)

             5 Reverse the steps above to re-enable the service

#### Additional Information

[VMware Skyline Health Diagnostics for vSphere - FAQ](https://knowledge.broadcom.com/external/article/345059)

#### Feedback

thumb\_up
Yes

thumb\_down
No

Powered by
[![Wolken Software](https://cdn.wolkenservicedesk.com/wolken-footer-image.png)](https://www.wolkensoftware.com/)



=== Content from docs.vmware.com_5969315b_20250115_115521.html ===
VMware ESXi670-202103001 Release Notes

Release Date: MAR 18, 2021

Build Details

Download Filename:

ESXi670-202103001.zip

17700523

476.7 MB

26f177706bce4a0432d9e8af016a5bad

0263a25351d003d34321a062b391d9fe5c312bd2

Yes

Yes

Build:

Download Size:

md5sum:

sha1checksum:

Host Reboot Required:

Virtual Machine Migration or Shutdown Required:

Bulletins

Bulletin ID

Category Severity

ESXi670-202103401-BG Bugfix

Critical

ESXi670-202103402-BG Bugfix

Important

ESXi670-202103403-BG Bugfix

Important

ESXi670-202103101-SG Security

Important

ESXi670-202103102-SG Security Moderate

ESXi670-202103103-SG Security Moderate

Rollup Bulletin

This rollup bulletin contains the latest VIBs with all the fixes since the initial release of ESXi 6.7.

Bulletin ID

ESXi670-202103001

Category

Bugfix

Severity

Critical

IMPORTANT: For clusters using VMware vSAN, you must first upgrade the vCenter Server system. Upgrading only the ESXi hosts is not supported.

Before an upgrade, always verify in the VMware Product Interoperability Matrix compatible upgrade paths from earlier versions of ESXi, vCenter Server and vSAN to

the current version.

Image Profiles

VMware patch and update releases contain general and critical image profiles. Application of the general release image profile applies to new bug fixes.

Image Profile Name

ESXi-6.7.0-20210304001-standard

ESXi-6.7.0-20210304001-no-tools

ESXi-6.7.0-20210301001s-standard

ESXi-6.7.0-20210301001s-no-tools

For more information about the individual bulletins, see the Download Patches page and the Resolved Issues section.

Patch Download and Installation

The typical way to apply patches to ESXi hosts is by using the VMware vSphere Update Manager. For details, see the About Installing and Administering VMware

vSphere Update Manager.

You can update ESXi hosts by manually downloading the patch ZIP file from the VMware download page and installing VIBs by using the esxcli software vib

update command. Additionally, you can update the system by using the image profile and the esxcli software profile update command.

For more information, see the vSphere Command-Line Interface Concepts and Examples and the vSphere Upgrade Guide.

Resolved Issues

The resolved issues are grouped as follows.

ESXi670-202103401-BG

ESXi670-202103402-BG

ESXi670-202103403-BG

ESXi670-202103101-SG

ESXi670-202103102-SG

ESXi670-202103103-SG

ESXi-6.7.0-20210304001-standard

ESXi-6.7.0-20210304001-no-tools

ESXi-6.7.0-20210301001s-standard

ESXi-6.7.0-20210301001s-no-tools

ESXi670-202103401-BG

Patch Category

Patch Severity

Host Reboot Required

Virtual Machine Migration or Shutdown
Required

Affected Hardware

Affected Software

VIBs Included

PRs Fixed

Bugfix

Critical

Yes

Yes

N/A

N/A

VMware_bootbank_vsan_6.7.0-3.143.17661912
VMware_bootbank_esx-update_6.7.0-3.143.17700523
VMware_bootbank_esx-base_6.7.0-3.143.17700523
VMware_bootbank_vsanhealth_6.7.0-3.143.17665851

2677275, 2664834, 2678794, 2681680, 2649392, 2680976,
2681253, 2701199, 2687411, 2664504, 2684836, 2687729, 2685806,
2683749, 2713397, 2706021, 2710158, 2669702, 2660017, 2685772,
2704741, 2645044, 2683393, 2678705, 2693303, 2695382,
2678639, 2679986, 2713569, 2682261, 2652285, 2697898, 2681921,
2691872, 2719154, 2711959, 2681786, 2659768, 2663643

CVE numbers

N/A

Updates esx-base, esx-update, vsan, and vsanhealth VIBs to resolve the following issues:

PR 2677275: If the Xorg process fails to restart while an ESXi host exits maintenance mode, the hostd service might become unresponsive

If the Xorg process fails to restart while an ESXi host exits maintenance mode, the hostd service might become unresponsive as it cannot complete the exit

operation.

This issue is resolved in this release.

PR 2664834: A storage outage might cause errors in your environment due to a disk layout issue in virtual machines

After a brief storage outage, it is possible that upon recovery of the virtual machines, the disk layout is not refreshed and remains incomplete. As a result, you

might see errors in your environment. For example, in the View Composer logs in a VMware Horizon environment, you might see a repeating error such as

InvalidSnapshotDiskConfiguration.

This issue is resolved in this release.

PR 2678794: An ESX host becomes unresponsive due to failure of the hostd service

A missing NULL check in a vim.VirtualDiskManager.revertToChildDisk operation triggered by VMware vSphere Replication on virtual disks that do not support this

operation might cause the hostd service to fail. As a result, the ESXi host loses connectivity to the vCenter Server system.

This issue is resolved in this release.

PR 2681680: If you disable RC4, Active Directory user authentication on ESXi hosts might fail

If you disable RC4 from your Active Directory configuration, user authentication to ESXi hosts might start to fail with Failed to authenticate user errors.

This issue is resolved in this release.

PR 2649392: Insufficient heap of a DvFilter agent might cause virtual machine migration by using vSphere vMotion to fail

The DVFilter agent uses a common heap to allocate space for both its internal structures and buffers, as well as for the temporary allocations used for moving

the state of client agents during vSphere vMotion operations.

In some deployments and scenarios, the filter states can be very large in size and exhaust the heap during vSphere vMotion operations.

In the vmkernel logs, you see an error such as Failed waiting for data. Error bad0014. Out of memory.

This issue is resolved in this release.

PR 2680976: The hostd service might fail due to missing data while creating a ContainerView managed object

If not all data required for the creation of a ContainerView managed object is available during the VM Object Management Infrastructure session, the hostd

service might fail.

This issue is resolved in this release. If you already face the issue, remove the third-party solution that you use to create ContainerView objects.

PR 2681253: An ESXi host might fail with a purple diagnostic screen due to a race condition in container ports

Due to a rare race condition, when a container port tries to re-acquire a lock it already holds, an ESXi host might fail with a purple diagnostic screen while virtual

machines with container ports power off. You see a backtrace such as:

gdb) bt #0

LockCheckSelfDeadlockInt (lck=0x4301ad7d6094) at bora/vmkernel/core/lock.c:1820

#1 0x000041801a911dff in Lock_CheckSelfDeadlock (lck=0x4301ad7d6094) at bora/vmkernel/private/lock.h:598

#2 MCSLockRWContended (rwl=rwl@entry=0x4301ad7d6080, writer=writer@entry=1 '\001', downgrade=downgrade@entry=0 '\000', flags=flags@entry=MCS_ALWAYS_LOCK) at

bora/vmkernel/main/mcslock.c:2078

#3 0x000041801a9124fd in MCS_DoAcqWriteLockWithRA (rwl=rwl@entry=0x4301ad7d6080, try=try@entry=0 '\000', flags=flags@entry=MCS_ALWAYS_LOCK, ra= ) at

bora/vmkernel/main/mcslock.c:2541

#4 0x000041801a90ee41 in MCS_AcqWriteLockWithFlags (ra= , flags=MCS_ALWAYS_LOCK, rwl=0x4301ad7d6080) at bora/vmkernel/private/mcslock.h:1246

#5 RefCountBlockSpin (ra=0x41801aa753ca, refCounter=0x4301ad7d6070) at bora/vmkernel/main/refCount.c:565

#6 RefCountBlock (refCounter=0x4301ad7d6070, ra=ra@entry=0x41801aa753ca) at bora/vmkernel/main/refCount.c:646

#7 0x000041801aa3c0a8 in RefCount_BlockWithRA (ra=0x41801aa753ca, refCounter= ) at bora/vmkernel/private/refCount.h:793

#8 Portset_LockExclWithRA (ra=0x41801aa753ca, ps=0x4305d081c010) at bora/vmkernel/net/portset.h:763

#9 Portset_GetPortExcl (portID=1329) at bora/vmkernel/net/portset.c:3254

#10 0x000041801aa753ca in Net_WorldCleanup (initFnArgs= ) at bora/vmkernel/net/vmkernel_exports.c:1605

#11 0x000041801a8ee8bb in InitTable_Cleanup (steps=steps@entry=0x41801acaa640, numSteps=numSteps@entry=35, clientData=clientData@entry=0x451a86f9bf28) at

bora/vmkernel/main/initTable.c:381

#12 0x000041801a940912 in WorldCleanup (world= ) at bora/vmkernel/main/world.c:1522 #13 World_TryReap (worldID= ) at bora/vmkernel/main/world.c:6113

#14 0x000041801a90e133 in ReaperWorkerWorld (data= ) at bora/vmkernel/main/reaper.c:425

#15 0x000041801ab107db in CpuSched_StartWorld (destWorld= , previous= )

at bora/vmkernel/sched/cpusched.c:11957 #16 0x0000000000000000 in ?? ()

This issue is resolved in this release.

PR 2701199: Virtual machines might power off during an NFS server failover

During an NFS server failover, the client reclaims all open files. In rare cases, the reclaim operation fails and virtual machines power off, because the NFS server

rejects failed requests.

This issue is resolved in this release. The fix makes sure that reclaim requests succeed.

PR 2687411: You might not see NFS datastores using a fault tolerance solution by Nutanix mounted in a vCenter Server system after an ESXi host reboots

You might not see NFS datastores using a fault tolerance solution by Nutanix mounted in a vCenter Server system after an ESXi host reboots. However, you can

see the volumes in the ESXi host.

This issue is resolved in this release.

PR 2664504: vSAN host fails while resizing or disabling large cache

vSAN hosts running in a non-preemptible context might tax the CPU when freeing up 256Gb or more of memory in the cache buffer. The host might fail with a

purple diagnostic screen.

This issue is resolved in this release.

PR 2684836: If you use large block sizes, I/O bandwidth might drop

In some configurations, if you use block sizes higher than the supported max transfer length of the storage device, you might see a drop in the I/O bandwidth.

The issue occurs due to buffer allocations in the I/O split layer in the storage stack that cause a lock contention.

This issue is resolved in this release. The fix optimizes the I/O split layer to avoid new buffer allocations. However, the optimization depends on the buffers that

the guest OS creates while issuing I/O and might not work in all cases.

PR 2687729: ESXi hosts might fail with a purple diagnostic screen due to stuck I/O traffic

If a high priority task holds a physical CPU for a long time, I/O traffic might stop and eventually cause the ESXi host to fail with a purple diagnostic screen. In the

backtrace, you see a message such as WARNING: PLOG: PLOGStuckIOCB:7729: Stuck IOs detected on vSAN device:xxx.

This issue is resolved in this release.

PR 2685806: Booting ESXi on hosts with a large number of VMFS datastores might take long

If you have set up a virtual flash resource by using the Virtual Flash File System (VFFS), booting an ESXi hosts with a large number of VMFS datastores might

take long. The delay is due to the process of comparing file types, VFFS and VMFS.

This issue is resolved in this release. The fix removes the file types comparison.

PR 2683749: Virtual machines with a virtual network device might randomly become unresponsive without a backtrace

In rare cases, virtual machines with a virtual network device might randomly become unresponsive and you need to shut them down. Attempts to generate a

coredump for diagnostic reasons fail.

This issue is resolved in this release.

PR 2713397: After the hostd service restarts, the bootTime property of an ESXi host might change

Boot time is recalculated on every start of the hostd service. The VMkernel only provides the uptime of the ESXi host. Timers do not move synchronously and as

a result, on hostd restart, the boot time might change, back or forth.

This issue is resolved in this release.

PR 2706021: vSAN reports component creation failure when available capacity exceeds required capacity

When vSAN reports that the required capacity is less than the available capacity, the corresponding log message might be misleading. The values reported for

Required space and Available space might be incorrect.

This issue is resolved in this release.

PR 2710158: You must manually add the claim rules to an ESXi host for FUJITSU ETERNUS storage

You must manually add the claim rules to an ESXi host for FUJITSU ETERNUS AB/HB Series storage arrays.

This issue is resolved in this release. This fix sets Storage Array Type Plugin (SATP) to VMW_SATP_ALUA, Path Selection Policy (PSP) to VMW_PSP_RR, and Claim Options

to tpgs_on as default for FUJITSU ETERNUS AB/HB Series storage arrays.

PR 2669702: A vSAN host might fail with a purple diagnostic screen due to slab metadata corruption

A vSAN host might fail with a purple diagnostic screen due to a Use-After-Free (UAF) error in the LSOM/VSAN slab. You can observe the issue in the following

stack:

cpu11:2100603)0x451c5f09bd68:[0x418027d285d2]vmk_SlabAlloc@vmkernel#nover+0x2e stack: 0x418028f981ba, 0x0, 0x431d231dbae0, 0x451c5f09bde8, 0x459c836cb960

cpu11:2100603)0x451c5f09bd70:[0x418028f5e28c]VSANSlab_Alloc@com.vmware.vsanutil#0.0.0.1+0xd stack: 0x0, 0x431d231dbae0, 0x451c5f09bde8, 0x459c836cb960,

0x431d231f7c98

cpu11:2100603)0x451c5f09bd80:[0x418028f981b9]LSOM_Alloc@LSOMCommon#1+0xaa stack: 0x451c5f09bde8, 0x459c836cb960, 0x431d231f7c98, 0x418028f57eb3, 0x431d231dbae0

cpu11:2100603)0x451c5f09bdb0:[0x418028f57eb2]VSANServer_InstantiateOperation@com.vmware.vsanutil#0.0.0.1+0x5f stack: 0x1, 0x459c836cb960, 0x418029164388,

0x41802903c607, 0xffffffffffffffff

cpu11:2100603)0x451c5f09bde0:[0x41802903c606]PLOG_GetDedupInfo@com.vmware.plog#0.0.0.1+0x87 stack: 0x459c836cb740, 0x0, 0x43218d3b7c00, 0x43036e815070,

0x451c5f0a3000

cpu11:2100603)0x451c5f09be20:[0x418029165075]LSOMDedupComponentScannerHelper@com.vmware.lsom#0.0.0.1+0xfa stack: 0x0, 0x418027d15483, 0x451bc0580000,

0x418027d1c57a, 0x26f5abad8d0ac0

cpu11:2100603)0x451c5f09bf30:[0x418027ceaf52]HelperQueueFunc@vmkernel#nover+0x30f stack: 0x431d231ba618, 0x431d231ba608, 0x431d231ba640, 0x451c5f0a3000,

0x431d231ba618

cpu11:2100603)0x451c5f09bfe0:[0x418027f0eaa2]CpuSched_StartWorld@vmkernel#nover+0x77 stack: 0x0, 0x0, 0x0, 0x0, 0x0

This issue is resolved in this release.

PR 2660017: In the vSphere Client, you see hardware health warning with status Unknown

In the vSphere Client, you see hardware health warning with status Unknown for some sensors on ESXi hosts.

This issue is resolved in this release. Sensors that are not supported for decoding are ignored and are not included in system health reports.

PR 2685772: The wsman service might become unresponsive and stop accepting incoming requests until restarted

Due to internal memory leaks, the memory usage of the wsman service on some servers might increase beyond the max limit of 23 MB. As a result, the service

stops accepting incoming requests until restarted. The issue affects Dell and Lenovo servers.

This issue is resolved in this release. However, the fix applies specifically to Dell servers.

PR 2704741: The hostd service intermittently becomes unresponsive and virtual machine snapshots time out

In some environments, the vSphere Storage Appliance (VSA) plug-in, also known as Security Virtual Appliance (SVA), might cause the hostd service to become

intermittently unresponsive during the creation of a TCP connection. In addition, virtual machine snapshots might time out. VSA is not supported since 2014.

This issue is resolved in this release. For earlier versions, remove the plug-in is removed from /usr/lib/vmware/nas_plugins/lib64 and

/usr/lib/vmware/nas_plugins/lib32 after every ESXi reboot.

PR 2645044: An unlocked spinlock might cause an ESXi host to fail with a purple diagnostic screen when restarting a virtual machine on a MSCS cluster

In rare cases, an unlocked spinlock might cause an ESXi host to fail with a purple diagnostic screen when restarting a virtual machine that is part of a Microsoft

Cluster Service (MSCS) cluster. If you have VMware vSphere High Availability enabled in your environment, the issue might affect many ESXi hosts, because

vSphere HA tries to power on the virtual machine in other hosts.

This issue is resolved in this release.

PR 2683393: A vSAN host might fail with a purple diagnostic screen due to slab allocation issue

When a slab allocation failure is not handled properly in the RCRead path, a vSAN host might fail with a purple diagnostic screen. You see a warning such as:

bora/modules/vmkernel/lsom/rc_io.c:2115 -- NOT REACHED.

This issue is resolved in this release.

PR 2678705: The VMXNET3 driver does not copy trailing bytes of TCP packets, such as Ethernet trailers, to guest virtual machines

The VMXNET3 driver copies and calculates the checksums from the payload indicated in the IP total length field. However, VMXNET3 does not copy any

additional bytes after the payload that are not included in the length field and such bytes do not pass to guest virtual machines. For example, VMXNET3 does

not copy and pass Ethernet trailers.

This issue is resolved in this release.

PR 2693303: Transient errors due to faulty vSAN device might cause latency and congestion

When a storage device in a vSAN disk group has a transient error due to read failures, Full Rebuild Avoidance (FRA) might not be able to repair the device. In

such cases, FRA does not perform relog, which might lead to a log build up at PLOG, causing congestion and latency issues.

This issue is resolved in this release.

PR 2695382: Booting virtual machines with BIOS firmware from a network interface takes long

When you boot virtual machines with BIOS firmware from a network interface, interactions with the different servers in the environment might take long due to a

networking issue. This issue does not impact network performance during guest OS runtime.

This issue is resolved in this release.

PR 2678639: If a physical port in a link aggregation group goes down or restarts, link aggregation might be temporarily unstable

If a physical port in a link aggregation group goes down or restarts, but at the same time an ESXi host sends out of sync LACP packet to the physical switch, the

vmnic might also go down. This might cause link aggregation to be temporarily unstable.

This issue is resolved in this release.

PR 2679986: During vSphere vMotion operations to NFS datastores, virtual machines might become unresponsive for around 40 seconds

During vSphere vMotion operations to NFS datastores, the NFS disk-based lock files might not be deleted. As a result, virtual machines on the destination host

become unresponsive for around 40 seconds.

This issue is resolved in this release. The fix ensures lock files are removed from disks during vSphere vMotion operations.

PR 2713569: You do not see vCenter Server alerts in the vSphere Client and the vSphere Web Client

If you use the /bin/services.sh restart command to restart vCenter Server management services, the vobd daemon, which is responsible for sending ESXi host

events to vCenter Server, might not restart. As a result, you do not see alerts in the vSphere Client and the vSphere Web Client.

This issue is resolved in this release. The fix makes sure than the vobd daemon does not shut down when using the /bin/services.sh restart.

PR 2682261: If vSphere HA restarts or fails over, a virtual machine vNIC might disconnect from the network

In rare occasions, when vSphere HA restarts or fails over, a virtual machine on the vSphere HA cluster might lose connectivity with an error such as VMXNET3 user:

failed to connect 'Ethernet0' to DV Port 'xx'.

This issue is resolved in this release.

PR 2652285: If a middlebox or a TCP proxy removes the timestamp option in the SYN-ACK phase, the TCP connection to an ESXi host fails

TCP uses a 3-step synchronize (SYN) and acknowledge (ACK) process to establish connections, SYN, SYN-ACK and ACK of the SYN-ACK. Options such as TCP

timestamps are negotiated as part of this process. For traffic optimization reasons, middleboxes or TCP proxies might remove the timestamp option in the SYN-

ACK phase. As a result, such TCP connections to ESXi hosts reset and close. A packet capture shows an RST packet from the server, immediately after the TCP

handshake.

This issue is fixed in this release.

PR 2697898: An ESXi host might fail with a purple diagnostic screen due to invalid value in the heap memory

A rare condition when an uninitialized field in the heap memory returns an invalid value for a VMFS resource cluster number might cause subsequent searches for

this cluster on the VMFS volume to fail. As a result, the ESXi host fails with a purple diagnostic screen.

This issue is resolved in this release.

PR 2681921: An ESXi host might fail with a purple diagnostic screen while disconnecting from a vSphere Distributed Switch

A rare race condition of LAG port read locks might cause an ESXi host might fail with a purple diagnostic screen while disconnecting from a VDS.

This issue is resolved in this release.

PR 2691872: IORETRY queue exhaustion might cause I/O failures

Issuance of read I/Os by the vSAN read cache in order to fill 1 MB read cache lines of a hybrid vSAN array from a highly fragmented write buffer, might result in

IORETRY splitting of all 64 KB read I/Os issued by the read cache into 16 4 KB I/Os. This might cause IORETRY queue exhaustion.

This issue is resolved in this release. You can change the default value of LSOM maxQueudIos from 25,000 to 100,000 by using the following command for each non-

witness vSAN host:

esxcfg-advcfg --set 100000 /LSOM/maxQueudIos

The new value takes effect when the in-memory instance of each vSAN disk group is re-established, by re-mounting the disk groups or by rebooting the host.

PR 2719154: If vSAN datastore contains many idle VMs, you might see performance degradation on active VMs

If a vSAN datastore contains many powered off or suspended VMs that are not receiving any I/Os, active VMs on the vSAN datastore might experience

performance degradation due to increasing LSOM memory congestion in vSAN.

This issue is resolved in this release.

PR 2711959: A vSAN host might fail with a purple diagnostic screen due to an object in a bad state

In rare cases, if you commit an object on a disk group, but the object appears as not committed after the vSAN host with that disk group reboots, vSAN might

incorrectly consider the object to be in bad state. As a result, the vSAN host might fail with a purple diagnostic screen with a message such as:

Object <uuid> corrupted. lastCWLSN = <number>, pendingLSN = <number>

PanicvPanicInt@vmkernel#nover+0x439

Panic_vPanic@vmkernel#nover+0x23

vmk_PanicWithModuleID@vmkernel#nover+0x41

DOMOwnerReconcileProcessLogsStartTask@com.vmware.vsan#0.0.0.1+0x6b3

DOMOperationStartTask@com.vmware.vsan#0.0.0.1+0xa5

DOMOperationDispatch@com.vmware.vsan#0.0.0.1+0xce

VSANServerMainLoop@com.vmware.vsanutil#0.0.0.1+0x5e1

vmkWorldFunc@vmkernel#nover+0x4f

CpuSched_StartWorld@vmkernel#nover+0x77

This issue is resolved in this release.

PR 2681786: After a power cycle, a powered on virtual machine might show as powered off

If you set a power cycle flag by using the vmx.reboot.powerCycle=TRUE advanced setting during a hardware upgrade of virtual machines, the hostd service might

lose track of the VMs power state. After the VMs reboot, hostd might report powered on VMs as powered off.

This issue is resolved in this release. The fix makes sure hostd keeps track of the power state of virtual machines across reboots during upgrades with

the vmx.reboot.powerCycle=TRUE setting. If you already face the issue, reload all powered off VMs.

To reload all powered off VMs, use this command:

Get-View -ViewType VirtualMachine -Property Runtime.PowerState -Filter @{ "Runtime.PowerState" = "poweredOff" }).reload()

If you want to filter the powered off VMs by cluster, use this command:

$ClusterName = "MyClusterName" ; (Get-View -ViewType VirtualMachine -Property Runtime.PowerState -Filter @{ "Runtime.PowerState" = "poweredOff" } -SearchRoot

$(Get-View -ViewType "ClusterComputeResource" -Property Name -Filter @{"Name"="$ClusterName"}).MoRef).reload()

PR 2659768: An ESXi host might become unresponsive due to a lock during virtual machine disk consolidation operations

During virtual machine disk consolidation operations, such as snapshot disk consolidation, the hostd service might hold a lock for a long time. As a result, hostd

delays responds to other tasks and state requests. Consequently, if the disk consolidation task takes too long, the ESXi host becomes unresponsive.

This issue is resolved in this release.

PR 2663643: Virtual machines lose network connectivity after migration operations by using vSphere vMotion

If the UUID of a virtual machine changes, such as after a migration by using vSphere vMotion, and the virtual machine has a vNIC on an NSX-T managed switch,

the VM loses network connectivity. The vNIC cannot reconnect.

This issue is resolved in this release.

ESXi670-202103402-BG

Patch Category

Patch Severity

Host Reboot Required

Virtual Machine Migration or Shutdown
Required

Affected Hardware

Affected Software

VIBs Included

PRs Fixed

CVE numbers

Bugfix

Important

Yes

Yes

N/A

N/A

VMW_bootbank_vmw-ahci_2.0.7-2vmw.670.3.143.17700523

2666166

N/A

Updates the vmw-ahci VIB to resolve the following issue:

PR 2666166: ESXi hosts intermittently become unresponsive

One or more ESXi hosts might intermittently become unresponsive due to a race condition in the VMware AHCI SATA Storage Controller Driver. In the

vmkernel.log file, you see a lot of messages such as:

…ahciAbortIO:(curr) HWQD: 0 BusyL: 0

This issue is resolved in this release.

ESXi670-202103403-BG

Patch Category

Patch Severity

Host Reboot Required

Virtual Machine Migration or Shutdown
Required

Affected Hardware

Affected Software

Bugfix

Important

Yes

Yes

N/A

N/A

VIBs Included

PRs Fixed

CVE numbers

Updates the vmkusb VIB to resolve the following issue:

VMW_bootbank_vmkusb_0.1-1vmw.670.3.143.17700523

2688190

N/A

PR 2688190: If the process control I/O of an USB device exceeds 1K, I/O traffic to virtual machines on an ESXi host might fail

The ESXi USB driver, vmkusb, might fail to process control I/O that exceeds 1K. As a result, some USB devices cause failures of I/O traffic to virtual machines on an

ESXi host.

This issue is resolved in this release.

ESXi670-202103101-SG

Patch Category

Patch Severity

Host Reboot Required

Virtual Machine Migration or Shutdown
Required

Affected Hardware

Affected Software

Security

Important

Yes

Yes

N/A

N/A

VIBs Included

PRs Fixed

CVE numbers

VMware_bootbank_esx-update_6.7.0-3.139.17700514
VMware_bootbank_vsan_6.7.0-3.139.17598146
VMware_bootbank_vsanhealth_6.7.0-3.139.17598147
VMware_bootbank_esx-base_6.7.0-3.139.17700514

2704743

N/A

Updates esx-base, esx-update, vsan, and vsanhealth VIB to resolve the following issues:

Update to the OpenSSH

The OpenSSH version is updated to 8.4p1.

Update to the Python library

The Python third-party library is updated to version 3.5.10.

Update to OpenSSL library

The ESXi userworld OpenSSL library is updated to version openssl-1.0.2x.

ESXi670-202103102-SG

Patch Category

Patch Severity

Host Reboot Required

Virtual Machine Migration or Shutdown
Required

Affected Hardware

Affected Software

VIBs Included

PRs Fixed

CVE numbers

Updates the tools-light VIB.

Security

Moderate

No

No

N/A

N/A

VMware_locker_tools-light_11.2.5.17337674-17700514

2713501

N/A

The following VMware Tools ISO images are bundled with ESXi 670-202103001:

windows.iso: VMware Tools 11.2.5 supports Windows 7 SP1 or Windows Server 2008 R2 SP1 and later.

linux.iso: VMware Tools 10.3.23 ISO image for Linux OS with glibc 2.11 or later.

The following VMware Tools ISO images are available for download:

VMware Tools 10.0.12:

winPreVista.iso: for Windows 2000, Windows XP, and Windows 2003.

linuxPreGLibc25.iso: for Linux OS with a glibc version less than 2.5.

VMware Tools 11.0.6:

windows.iso: for Windows Vista (SP2) and Windows Server 2008 Service Pack 2 (SP2).

solaris.iso: VMware Tools image 10.3.10 for Solaris.

darwin.iso: Supports Mac OS X versions 10.11 and later.

Follow the procedures listed in the following documents to download VMware Tools for platforms not bundled with ESXi:

VMware Tools 11.2.5 Release Notes

Earlier versions of VMware Tools

What Every vSphere Admin Must Know About VMware Tools

VMware Tools for hosts provisioned with Auto Deploy

Updating VMware Tools

ESXi670-202103103-SG

Patch Category

Patch Severity

Host Reboot Required

Virtual Machine Migration or
Shutdown Required

Affected Hardware

Affected Software

VIBs Included

PRs Fixed

Related CVE numbers

This patch updates the cpu-microcode  VIB.

Security

Moderate

Yes

Yes

N/A

N/A

VMware_bootbank_cpu-microcode_6.7.0-3.139.17700514

2673509

N/A

The cpu-microcode VIB includes the following Intel microcode:
Code Name

Plt ID MCU Rev

FMS

MCU Date

Brand Names

Nehalem EP

0x106a5

0x03

0x0000001d

5/11/2018

Intel Xeon 35xx Series;
Intel Xeon 55xx Series

Lynnfield

0x106e5

0x13

0x0000000a

5/8/2018

Intel Xeon 34xx Lynnfield Series

Clarkdale

0x20652

0x12

0x00000011

5/8/2018

Intel i3/i5 Clarkdale Series;
Intel Xeon 34xx Clarkdale Series

Arrandale

0x20655

0x92

0x00000007

4/23/2018

Intel Core i7-620LE Processor

Sandy Bridge DT

0x206a7

0x12

0x0000002f

2/17/2019

Westmere EP

0x206c2

0x03

0x0000001f

5/8/2018

Sandy Bridge EP

0x206d6

0x6d

0x00000621

3/4/2020

Sandy Bridge EP

0x206d7

0x6d

0x0000071a

3/24/2020

Nehalem EX

0x206e6

0x04

0x0000000d

5/15/2018

Westmere EX

0x206f2

0x05

0x0000003b

5/16/2018

Ivy Bridge DT

0x306a9

0x12

0x00000021

2/13/2019

Haswell DT

0x306c3

0x32

0x00000028

11/12/2019

Intel Xeon E3-1100 Series;
Intel Xeon E3-1200 Series;
Intel i7-2655-LE Series;
Intel i3-2100 Series

Intel Xeon 56xx Series;
Intel Xeon 36xx Series

Intel Pentium 1400 Series;
Intel Xeon E5-1400 Series;
Intel Xeon E5-1600 Series;
Intel Xeon E5-2400 Series;
Intel Xeon E5-2600 Series;
Intel Xeon E5-4600 Series

Intel Pentium 1400 Series;
Intel Xeon E5-1400 Series;
Intel Xeon E5-1600 Series;
Intel Xeon E5-2400 Series;
Intel Xeon E5-2600 Series;
Intel Xeon E5-4600 Series

Intel Xeon 65xx Series;
Intel Xeon 75xx Series

Intel Xeon E7-8800 Series;
Intel Xeon E7-4800 Series;
Intel Xeon E7-2800 Series

Intel i3-3200 Series;
Intel i7-3500-LE/UE;
Intel i7-3600-QE;
Intel Xeon E3-1200-v2 Series;
Intel Xeon E3-1100-C-v2 Series;
Intel Pentium B925C

Intel Xeon E3-1200-v3 Series;
Intel i7-4700-EQ Series;
Intel i5-4500-TE Series;
Intel i3-4300 Series

Code Name

FMS

Plt ID MCU Rev

MCU Date

Brand Names

Ivy Bridge EP

0x306e4

0xed

0x0000042e

3/14/2019

Ivy Bridge EX

0x306e7

0xed

0x00000715

3/14/2019

Haswell EP

0x306f2

0x6f

0x00000044

5/27/2020

Intel Xeon E5-4600-v2 Series;
Intel Xeon E5-2600-v2 Series;
Intel Xeon E5-2400-v2 Series;
Intel Xeon E5-1600-v2 Series;
Intel Xeon E5-1400-v2 Series

Intel Xeon E7-8800/4800/2800-v2
Series

Intel Xeon E5-4600-v3 Series;
Intel Xeon E5-2600-v3 Series;
Intel Xeon E5-2400-v3 Series;
Intel Xeon E5-1600-v3 Series;
Intel Xeon E5-1400-v3 Series

Haswell EX

0x306f4

0x80

0x00000016

6/17/2019

Intel Xeon E7-8800/4800-v3 Series

Broadwell H

0x40671

0x22

0x00000022

11/12/2019

Avoton

0x406d8

0x01

0x0000012d

9/16/2019

Broadwell EP/EX

0x406f1

0xef

0x0b000038

6/18/2019

Skylake SP

0x50654

0xb7

0x02006a08

6/16/2020

Cascade Lake B-0 0x50656

0xbf

0x04003003

6/18/2020

Cascade Lake

0x50657

0xbf

0x05003003

6/18/2020

Cooper Lake

0x5065b

0xbf

0x0700001f

9/17/2020

Intel Core i7-5700EQ;
Intel Xeon E3-1200-v4 Series

Intel Atom C2300 Series;
Intel Atom C2500 Series;
Intel Atom C2700 Series

Intel Xeon E7-8800/4800-v4 Series;
Intel Xeon E5-4600-v4 Series;
Intel Xeon E5-2600-v4 Series;
Intel Xeon E5-1600-v4 Series

Intel Xeon Platinum 8100 Series;
Intel Xeon Gold 6100/5100, Silver
4100, Bronze 3100 Series;
Intel Xeon D-2100 Series;
Intel Xeon D-1600 Series;
Intel Xeon W-3100 Series;
Intel Xeon W-2100 Series

Intel Xeon Platinum 9200/8200
Series;
Intel Xeon Gold 6200/5200;
Intel Xeon Silver 4200/Bronze 3200;
Intel Xeon W-3200

Intel Xeon Platinum 9200/8200
Series;
Intel Xeon Gold 6200/5200;
Intel Xeon Silver 4200/Bronze 3200;
Intel Xeon W-3200

Intel Xeon Platinum 8300 Series;
Intel Xeon Gold 6300/5300

Broadwell DE

0x50662

0x10

0x0000001c

6/17/2019

Intel Xeon D-1500 Series

Broadwell DE

0x50663

0x10

0x07000019

6/17/2019

Intel Xeon D-1500 Series

Broadwell DE

0x50664

0x10

0x0f000017

6/17/2019

Intel Xeon D-1500 Series

Broadwell NS

0x50665

0x10

0x0e00000f

6/17/2019

Intel Xeon D-1600 Series

Skylake H/S

0x506e3

0x36

0x000000e2

7/14/2020

Intel Xeon E3-1500-v5 Series;
Intel Xeon E3-1200-v5 Series

Denverton

0x506f1

0x01

0x00000032

3/7/2020

Intel Atom C3000 Series

Snow Ridge

0x80665

0x01

0x0b000007

2/25/2020

Intel Atom P5000 Series

Kaby Lake H/S/X

0x906e9

0x2a

0x000000de

5/26/2020

Coffee Lake

0x906ea

0x22

0x000000de

5/25/2020

Intel Xeon E3-1200-v6 Series;
Intel Xeon E3-1500-v6 Series

Intel Xeon E-2100 Series;
Intel Xeon E-2200 Series (4 or 6
core)

Coffee Lake

0x906eb

0x02

0x000000de

5/25/2020

Intel Xeon E-2100 Series

Coffee Lake

0x906ec

0x22

0x000000de

6/3/2020

Intel Xeon E-2100 Series

Coffee Lake
Refresh

0x906ed

0x22

0x000000de

5/24/2020

Intel Xeon E-2200 Series (8 core)

ESXi-6.7.0-20210304001-standard

Profile Name

ESXi-6.7.0-20210304001-standard

Build

Vendor

For build information, see Patches Contained in this Release.

VMware, Inc.

Release Date

Acceptance Level

Affected Hardware

Affected Software

Affected VIBs

PRs Fixed

March 18, 2021

PartnerSupported

N/A

N/A

VMware_bootbank_vsan_6.7.0-3.143.17661912
VMware_bootbank_esx-update_6.7.0-3.143.17700523
VMware_bootbank_esx-base_6.7.0-3.143.17700523
VMware_bootbank_vsanhealth_6.7.0-3.143.17665851
VMW_bootbank_vmw-ahci_2.0.7-2vmw.670.3.143.17700523
VMW_bootbank_vmkusb_0.1-1vmw.670.3.143.17700523
VMware_locker_tools-light_11.2.5.17337674-17700514
VMware_bootbank_cpu-microcode_6.7.0-3.139.17700514

2677275, 2664834, 2678794, 2681680, 2649392, 2680976, 2681253,
2701199, 2687411, 2664504, 2684836, 2687729, 2685806, 2683749,
2713397, 2706021, 2710158, 2669702, 2660017, 2685772, 2704741,
2645044, 2683393, 2678705, 2693303, 2695382, 2678639, 2679986,
2713569, 2682261, 2652285, 2697898, 2681921, 2691872, 2719154, 2711959,
2681786, 2659768, 2663643, 2666166, 2688190

Related CVE numbers

N/A

This patch updates the following issues:

If the Xorg process fails to restart while an ESXi host exits maintenance mode, the hostd service might become unresponsive as it cannot complete the exit

operation.

After a brief storage outage, it is possible that upon recovery of the virtual machines, the disk layout is not refreshed and remains incomplete. As a result, you

might see errors in your environment. For example, in the View Composer logs in a VMware Horizon environment, you might see a repeating error such

as InvalidSnapshotDiskConfiguration.

A missing NULL check in a vim.VirtualDiskManager.revertToChildDisk operation triggered by VMware vSphere Replication on virtual disks that do not support this

operation might cause the hostd service to fail. As a result, the ESXi host loses connectivity to the vCenter Server system.

If you disable RC4 from your Active Directory configuration, user authentication to ESXi hosts might start to fail with Failed to authenticate user errors.

The DVFilter agent uses a common heap to allocate space for both its internal structures and buffers, as well as for the temporary allocations used for moving

the state of client agents during vSphere vMotion operations.

In some deployments and scenarios, the filter states can be very large in size and exhaust the heap during vSphere vMotion operations.

In the vmkernel logs, you see an error such as Failed waiting for data. Error bad0014. Out of memory.

If not all data required for the creation of a ContainerView managed object is available during the VM Object Management Infrastructure session, the hostd

service might fail.

Due to a rare race condition, when a container port tries to re-acquire a lock it already holds, an ESXi host might fail with a purple diagnostic screen while

virtual machines with container ports power off. You see a backtrace such as:

gdb) bt #0

LockCheckSelfDeadlockInt (lck=0x4301ad7d6094) at bora/vmkernel/core/lock.c:1820

#1 0x000041801a911dff in Lock_CheckSelfDeadlock (lck=0x4301ad7d6094) at bora/vmkernel/private/lock.h:598

#2 MCSLockRWContended (rwl=rwl@entry=0x4301ad7d6080, writer=writer@entry=1 '\001', downgrade=downgrade@entry=0 '\000', flags=flags@entry=MCS_ALWAYS_LOCK) at

bora/vmkernel/main/mcslock.c:2078

#3 0x000041801a9124fd in MCS_DoAcqWriteLockWithRA (rwl=rwl@entry=0x4301ad7d6080, try=try@entry=0 '\000', flags=flags@entry=MCS_ALWAYS_LOCK, ra=) at

bora/vmkernel/main/mcslock.c:2541

#4 0x000041801a90ee41 in MCS_AcqWriteLockWithFlags (ra=, flags=MCS_ALWAYS_LOCK, rwl=0x4301ad7d6080) at bora/vmkernel/private/mcslock.h:1246

#5 RefCountBlockSpin (ra=0x41801aa753ca, refCounter=0x4301ad7d6070) at bora/vmkernel/main/refCount.c:565

#6 RefCountBlock (refCounter=0x4301ad7d6070, ra=ra@entry=0x41801aa753ca) at bora/vmkernel/main/refCount.c:646

#7 0x000041801aa3c0a8 in RefCount_BlockWithRA (ra=0x41801aa753ca, refCounter=) at bora/vmkernel/private/refCount.h:793

#8 Portset_LockExclWithRA (ra=0x41801aa753ca, ps=0x4305d081c010) at bora/vmkernel/net/portset.h:763

#9 Portset_GetPortExcl (portID=1329) at bora/vmkernel/net/portset.c:3254

#10 0x000041801aa753ca in Net_WorldCleanup (initFnArgs=) at bora/vmkernel/net/vmkernel_exports.c:1605

#11 0x000041801a8ee8bb in InitTable_Cleanup (steps=steps@entry=0x41801acaa640, numSteps=numSteps@entry=35, clientData=clientData@entry=0x451a86f9bf28) at

bora/vmkernel/main/initTable.c:381

#12 0x000041801a940912 in WorldCleanup (world=) at bora/vmkernel/main/world.c:1522 #13 World_TryReap (worldID=) at bora/vmkernel/main/world.c:6113

#14 0x000041801a90e133 in ReaperWorkerWorld (data=) at bora/vmkernel/main/reaper.c:425

#15 0x000041801ab107db in CpuSched_StartWorld (destWorld=, previous=)

at bora/vmkernel/sched/cpusched.c:11957 #16 0x0000000000000000 in ?? ()

During an NFS server failover, the client reclaims all open files. In rare cases, the reclaim operation fails and virtual machines power off, because the NFS server

rejects failed requests.

You might not see NFS datastores using a fault tolerance solution by Nutanix mounted in a vCenter Server system after an ESXi host reboots. However, you

can see the volumes in the ESXi host.

vSAN hosts running in a non-preemptible context might tax the CPU when freeing up 256Gb or more of memory in the cache buffer. The host might fail with a

purple diagnostic screen.

In some configurations, if you use block sizes higher than the supported max transfer length of the storage device, you might see a drop in the I/O bandwidth.

The issue occurs due to buffer allocations in the I/O split layer in the storage stack that cause a lock contention.

If a high priority task holds a physical CPU for a long time, I/O traffic might stop and eventually cause the ESXi host to fail with a purple diagnostic screen. In

the backtrace, you see a message such as WARNING: PLOG: PLOGStuckIOCB:7729: Stuck IOs detected on vSAN device:xxx.

If you have set up a virtual flash resource by using the Virtual Flash File System (VFFS), booting an ESXi hosts with a large number of VMFS datastores might

take long. The delay is due to the process of comparing file types, VFFS and VMFS.

In rare cases, virtual machines with a virtual network device might randomly become unresponsive and you need to shut them down. Attempts to generate a

coredump for diagnostic reasons fail.

Boot time is recalculated on every start of the hostd service. The VMkernel only provides the uptime of the ESXi host. Timers do not move synchronously and

as a result, on hostd restart, the boot time might change, back or forth.

When vSAN reports that the required capacity is less than the available capacity, the corresponding log message might be misleading. The values reported for

Required space and Available space might be incorrect.

You must manually add the claim rules to an ESXi host for FUJITSU ETERNUS AB/HB Series storage arrays.

A vSAN host might fail with a purple diagnostic screen due to a Use-After-Free (UAF) error in the LSOM/VSAN slab. You can observe the issue in the following

stack:

cpu11:2100603)0x451c5f09bd68:[0x418027d285d2]vmk_SlabAlloc@vmkernel#nover+0x2e stack: 0x418028f981ba, 0x0, 0x431d231dbae0, 0x451c5f09bde8, 0x459c836cb960

cpu11:2100603)0x451c5f09bd70:[0x418028f5e28c]VSANSlab_Alloc@com.vmware.vsanutil#0.0.0.1+0xd stack: 0x0, 0x431d231dbae0, 0x451c5f09bde8, 0x459c836cb960,

0x431d231f7c98

cpu11:2100603)0x451c5f09bd80:[0x418028f981b9]LSOM_Alloc@LSOMCommon#1+0xaa stack: 0x451c5f09bde8, 0x459c836cb960, 0x431d231f7c98, 0x418028f57eb3,

0x431d231dbae0

cpu11:2100603)0x451c5f09bdb0:[0x418028f57eb2]VSANServer_InstantiateOperation@com.vmware.vsanutil#0.0.0.1+0x5f stack: 0x1, 0x459c836cb960, 0x418029164388,

0x41802903c607, 0xffffffffffffffff

cpu11:2100603)0x451c5f09bde0:[0x41802903c606]PLOG_GetDedupInfo@com.vmware.plog#0.0.0.1+0x87 stack: 0x459c836cb740, 0x0, 0x43218d3b7c00, 0x43036e815070,

0x451c5f0a3000

cpu11:2100603)0x451c5f09be20:[0x418029165075]LSOMDedupComponentScannerHelper@com.vmware.lsom#0.0.0.1+0xfa stack: 0x0, 0x418027d15483, 0x451bc0580000,

0x418027d1c57a, 0x26f5abad8d0ac0

cpu11:2100603)0x451c5f09bf30:[0x418027ceaf52]HelperQueueFunc@vmkernel#nover+0x30f stack: 0x431d231ba618, 0x431d231ba608, 0x431d231ba640, 0x451c5f0a3000,

0x431d231ba618

cpu11:2100603)0x451c5f09bfe0:[0x418027f0eaa2]CpuSched_StartWorld@vmkernel#nover+0x77 stack: 0x0, 0x0, 0x0, 0x0, 0x0

In the vSphere Client, you see hardware health warning with status Unknown for some sensors on ESXi hosts.

Due to internal memory leaks, the memory usage of the wsman service on some servers might increase beyond the max limit of 23 MB. As a result, the service

stops accepting incoming requests until restarted. The issue affects Dell and Lenovo servers.

In some environments, the vSphere Storage Appliance (VSA) plug-in, also known as Security Virtual Appliance (SVA), might cause the hostd service to

become intermittently unresponsive during the creation of a TCP connection. In addition, virtual machine snapshots might time out. VSA is not supported since

2014.

In rare cases, an unlocked spinlock might cause an ESXi host to fail with a purple diagnostic screen when restarting a virtual machine that is part of a Microsoft

Cluster Service (MSCS) cluster. If you have VMware vSphere High Availability enabled in your environment, the issue might affect many ESXi hosts, because

vSphere HA tries to power on the virtual machine in other hosts.

When a slab allocation failure is not handled properly in the RCRead path, a vSAN host might fail with a purple diagnostic screen. You see a warning such

as: bora/modules/vmkernel/lsom/rc_io.c:2115 -- NOT REACHED.

The VMXNET3 driver copies and calculates the checksums from the payload indicated in the IP total length field. However, VMXNET3 does not copy any

additional bytes after the payload that are not included in the length field and such bytes do not pass to guest virtual machines. For example, VMXNET3 does

not copy and pass Ethernet trailers.

When a storage device in a vSAN disk group has a transient error due to read failures, Full Rebuild Avoidance (FRA) might not be able to repair the device. In

such cases, FRA does not perform relog, which might lead to a log build up at PLOG, causing congestion and latency issues.

When you boot virtual machines with BIOS firmware from a network interface, interactions with the different servers in the environment might take long due to

a networking issue. This issue does not impact network performance during guest OS runtime.

If a physical port in a link aggregation group goes down or restarts, but at the same time an ESXi host sends out of sync LACP packet to the physical switch,

the vmnic might also go down. This might cause link aggregation to be temporarily unstable.

During vSphere vMotion operations to NFS datastores, the NFS disk-based lock files might not be deleted. As a result, virtual machines on the destination host

become unresponsive for around 40 seconds.

If you use the /bin/services.sh restart command to restart vCenter Server management services, the vobd daemon, which is responsible for sending ESXi

host events to vCenter Server, might not restart. As a result, you do not see alerts in the vSphere Client and the vSphere Web Client.

In rare occasions, when vSphere HA restarts or fails over, a virtual machine on the vSphere HA cluster might lose connectivity with an error such as VMXNET3

user: failed to connect 'Ethernet0' to DV Port 'xx'.

TCP uses a 3-step synchronize (SYN) and acknowledge (ACK) process to establish connections, SYN, SYN-ACK and ACK of the SYN-ACK. Options such as TCP

timestamps are negotiated as part of this process. For traffic optimization reasons, middleboxes or TCP proxies might remove the timestamp option in the

SYN-ACK phase. As a result, such TCP connections to ESXi hosts reset and close. A packet capture shows an RST packet from the server, immediately after

the TCP handshake.

A rare condition when an uninitialized field in the heap memory returns an invalid value for a VMFS resource cluster number might cause subsequent searches

for this cluster on the VMFS volume to fail. As a result, the ESXi host fails with a purple diagnostic screen.

A rare race condition of LAG port read locks might cause an ESXi host might fail with a purple diagnostic screen while disconnecting from a VDS.

Issuance of read I/Os by the vSAN read cache in order to fill 1 MB read cache lines of a hybrid vSAN array from a highly fragmented write buffer, might result

in IORETRY splitting of all 64 KB read I/Os issued by the read cache into 16 4 KB I/Os. This might cause IORETRY queue exhaustion.

If a vSAN datastore contains many powered off or suspended VMs that are not receiving any I/Os, active VMs on the vSAN datastore might experience

performance degradation due to increasing LSOM memory congestion in vSAN.

In rare cases, if you commit an object on a disk group, but the object appears as not committed after the vSAN host with that disk group reboots, vSAN might

incorrectly consider the object to be in bad state. As a result, the vSAN host might fail with a purple diagnostic screen with a message such as:

Object <uuid> corrupted. lastCWLSN = <number>, pendingLSN = <number>

PanicvPanicInt@vmkernel#nover+0x439

Panic_vPanic@vmkernel#nover+0x23

vmk_PanicWithModuleID@vmkernel#nover+0x41

DOMOwnerReconcileProcessLogsStartTask@com.vmware.vsan#0.0.0.1+0x6b3

DOMOperationStartTask@com.vmware.vsan#0.0.0.1+0xa5

DOMOperationDispatch@com.vmware.vsan#0.0.0.1+0xce

VSANServerMainLoop@com.vmware.vsanutil#0.0.0.1+0x5e1

vmkWorldFunc@vmkernel#nover+0x4f

CpuSched_StartWorld@vmkernel#nover+0x77

If you set a power cycle flag by using the vmx.reboot.powerCycle=TRUE advanced setting during a hardware upgrade of virtual machines, the hostd service might

lose track of the VMs power state. After the VMs reboot, hostd might report powered on VMs as powered off.

During virtual machine disk consolidation operations, such as snapshot disk consolidation, the hostd service might hold a lock for a long time. As a result, hostd

delays responds to other tasks and state requests. Consequently, if the disk consolidation task takes too long, the ESXi host becomes unresponsive.

If the UUID of a virtual machine changes, such as after a migration by using vSphere vMotion, and the virtual machine has a vNIC on an NSX-T managed

switch, the VM loses network connectivity. The vNIC cannot reconnect.

One or more ESXi hosts might intermittently become unresponsive due to a race condition in the VMware AHCI SATA Storage Controller Driver. In

the vmkernel.log file, you see a lot of messages such as:

…ahciAbortIO:(curr) HWQD: 0 BusyL: 0

The ESXi USB driver, vmkusb, might fail to process control I/O that exceeds 1K. As a result, some USB devices cause failures of I/O traffic to virtual machines on

an ESXi host.

ESXi-6.7.0-20210304001-no-tools

Profile Name

ESXi-6.7.0-20210304001-no-tools

Build

Vendor

Release Date

Acceptance Level

Affected Hardware

Affected Software

For build information, see Patches Contained in this Release.

VMware, Inc.

March 18, 2021

PartnerSupported

N/A

N/A

Affected VIBs

PRs Fixed

VMware_bootbank_vsan_6.7.0-3.143.17661912
VMware_bootbank_esx-update_6.7.0-3.143.17700523
VMware_bootbank_esx-base_6.7.0-3.143.17700523
VMware_bootbank_vsanhealth_6.7.0-3.143.17665851
VMW_bootbank_vmw-ahci_2.0.7-2vmw.670.3.143.17700523
VMW_bootbank_vmkusb_0.1-1vmw.670.3.143.17700523
VMware_bootbank_cpu-microcode_6.7.0-3.139.17700514

2677275, 2664834, 2678794, 2681680, 2649392, 2680976, 2681253,
2701199, 2687411, 2664504, 2684836, 2687729, 2685806, 2683749,
2713397, 2706021, 2710158, 2669702, 2660017, 2685772, 2704741,
2645044, 2683393, 2678705, 2693303, 2695382, 2678639, 2679986,
2713569, 2682261, 2652285, 2697898, 2681921, 2691872, 2719154, 2711959,
2681786, 2659768, 2663643, 2666166, 2688190

Related CVE numbers

N/A

This patch updates the following issues:

If the Xorg process fails to restart while an ESXi host exits maintenance mode, the hostd service might become unresponsive as it cannot complete the exit

operation.

After a brief storage outage, it is possible that upon recovery of the virtual machines, the disk layout is not refreshed and remains incomplete. As a result, you

might see errors in your environment. For example, in the View Composer logs in a VMware Horizon environment, you might see a repeating error such

as InvalidSnapshotDiskConfiguration.

A missing NULL check in a vim.VirtualDiskManager.revertToChildDisk operation triggered by VMware vSphere Replication on virtual disks that do not support this

operation might cause the hostd service to fail. As a result, the ESXi host loses connectivity to the vCenter Server system.

If you disable RC4 from your Active Directory configuration, user authentication to ESXi hosts might start to fail with Failed to authenticate user errors.

The DVFilter agent uses a common heap to allocate space for both its internal structures and buffers, as well as for the temporary allocations used for moving

the state of client agents during vSphere vMotion operations.

In some deployments and scenarios, the filter states can be very large in size and exhaust the heap during vSphere vMotion operations.

In the vmkernel logs, you see an error such as Failed waiting for data. Error bad0014. Out of memory.

If not all data required for the creation of a ContainerView managed object is available during the VM Object Management Infrastructure session, the hostd

service might fail.

Due to a rare race condition, when a container port tries to re-acquire a lock it already holds, an ESXi host might fail with a purple diagnostic screen while

virtual machines with container ports power off. You see a backtrace such as:

gdb) bt #0

LockCheckSelfDeadlockInt (lck=0x4301ad7d6094) at bora/vmkernel/core/lock.c:1820

#1 0x000041801a911dff in Lock_CheckSelfDeadlock (lck=0x4301ad7d6094) at bora/vmkernel/private/lock.h:598

#2 MCSLockRWContended (rwl=rwl@entry=0x4301ad7d6080, writer=writer@entry=1 '\001', downgrade=downgrade@entry=0 '\000', flags=flags@entry=MCS_ALWAYS_LOCK) at

bora/vmkernel/main/mcslock.c:2078

#3 0x000041801a9124fd in MCS_DoAcqWriteLockWithRA (rwl=rwl@entry=0x4301ad7d6080, try=try@entry=0 '\000', flags=flags@entry=MCS_ALWAYS_LOCK, ra=) at

bora/vmkernel/main/mcslock.c:2541

#4 0x000041801a90ee41 in MCS_AcqWriteLockWithFlags (ra=, flags=MCS_ALWAYS_LOCK, rwl=0x4301ad7d6080) at bora/vmkernel/private/mcslock.h:1246

#5 RefCountBlockSpin (ra=0x41801aa753ca, refCounter=0x4301ad7d6070) at bora/vmkernel/main/refCount.c:565

#6 RefCountBlock (refCounter=0x4301ad7d6070, ra=ra@entry=0x41801aa753ca) at bora/vmkernel/main/refCount.c:646

#7 0x000041801aa3c0a8 in RefCount_BlockWithRA (ra=0x41801aa753ca, refCounter=) at bora/vmkernel/private/refCount.h:793

#8 Portset_LockExclWithRA (ra=0x41801aa753ca, ps=0x4305d081c010) at bora/vmkernel/net/portset.h:763

#9 Portset_GetPortExcl (portID=1329) at bora/vmkernel/net/portset.c:3254

#10 0x000041801aa753ca in Net_WorldCleanup (initFnArgs=) at bora/vmkernel/net/vmkernel_exports.c:1605

#11 0x000041801a8ee8bb in InitTable_Cleanup (steps=steps@entry=0x41801acaa640, numSteps=numSteps@entry=35, clientData=clientData@entry=0x451a86f9bf28) at

bora/vmkernel/main/initTable.c:381

#12 0x000041801a940912 in WorldCleanup (world=) at bora/vmkernel/main/world.c:1522 #13 World_TryReap (worldID=) at bora/vmkernel/main/world.c:6113

#14 0x000041801a90e133 in ReaperWorkerWorld (data=) at bora/vmkernel/main/reaper.c:425

#15 0x000041801ab107db in CpuSched_StartWorld (destWorld=, previous=)

at bora/vmkernel/sched/cpusched.c:11957 #16 0x0000000000000000 in ?? ()

During an NFS server failover, the client reclaims all open files. In rare cases, the reclaim operation fails and virtual machines power off, because the NFS server

rejects failed requests.

You might not see NFS datastores using a fault tolerance solution by Nutanix mounted in a vCenter Server system after an ESXi host reboots. However, you

can see the volumes in the ESXi host.

vSAN hosts running in a non-preemptible context might tax the CPU when freeing up 256Gb or more of memory in the cache buffer. The host might fail with a

purple diagnostic screen.

In some configurations, if you use block sizes higher than the supported max transfer length of the storage device, you might see a drop in the I/O bandwidth.

The issue occurs due to buffer allocations in the I/O split layer in the storage stack that cause a lock contention.

If a high priority task holds a physical CPU for a long time, I/O traffic might stop and eventually cause the ESXi host to fail with a purple diagnostic screen. In

the backtrace, you see a message such as WARNING: PLOG: PLOGStuckIOCB:7729: Stuck IOs detected on vSAN device:xxx.

If you have set up a virtual flash resource by using the Virtual Flash File System (VFFS), booting an ESXi hosts with a large number of VMFS datastores might

take long. The delay is due to the process of comparing file types, VFFS and VMFS.

In rare cases, virtual machines with a virtual network device might randomly become unresponsive and you need to shut them down. Attempts to generate a

coredump for diagnostic reasons fail.

Boot time is recalculated on every start of the hostd service. The VMkernel only provides the uptime of the ESXi host. Timers do not move synchronously and

as a result, on hostd restart, the boot time might change, back or forth.

When vSAN reports that the required capacity is less than the available capacity, the corresponding log message might be misleading. The values reported for

Required space and Available space might be incorrect.

You must manually add the claim rules to an ESXi host for FUJITSU ETERNUS AB/HB Series storage arrays.

A vSAN host might fail with a purple diagnostic screen due to a Use-After-Free (UAF) error in the LSOM/VSAN slab. You can observe the issue in the following

stack:

cpu11:2100603)0x451c5f09bd68:[0x418027d285d2]vmk_SlabAlloc@vmkernel#nover+0x2e stack: 0x418028f981ba, 0x0, 0x431d231dbae0, 0x451c5f09bde8, 0x459c836cb960

cpu11:2100603)0x451c5f09bd70:[0x418028f5e28c]VSANSlab_Alloc@com.vmware.vsanutil#0.0.0.1+0xd stack: 0x0, 0x431d231dbae0, 0x451c5f09bde8, 0x459c836cb960,

0x431d231f7c98

cpu11:2100603)0x451c5f09bd80:[0x418028f981b9]LSOM_Alloc@LSOMCommon#1+0xaa stack: 0x451c5f09bde8, 0x459c836cb960, 0x431d231f7c98, 0x418028f57eb3,

0x431d231dbae0

cpu11:2100603)0x451c5f09bdb0:[0x418028f57eb2]VSANServer_InstantiateOperation@com.vmware.vsanutil#0.0.0.1+0x5f stack: 0x1, 0x459c836cb960, 0x418029164388,

0x41802903c607, 0xffffffffffffffff

cpu11:2100603)0x451c5f09bde0:[0x41802903c606]PLOG_GetDedupInfo@com.vmware.plog#0.0.0.1+0x87 stack: 0x459c836cb740, 0x0, 0x43218d3b7c00, 0x43036e815070,

0x451c5f0a3000

cpu11:2100603)0x451c5f09be20:[0x418029165075]LSOMDedupComponentScannerHelper@com.vmware.lsom#0.0.0.1+0xfa stack: 0x0, 0x418027d15483, 0x451bc0580000,

0x418027d1c57a, 0x26f5abad8d0ac0

cpu11:2100603)0x451c5f09bf30:[0x418027ceaf52]HelperQueueFunc@vmkernel#nover+0x30f stack: 0x431d231ba618, 0x431d231ba608, 0x431d231ba640, 0x451c5f0a3000,

0x431d231ba618

cpu11:2100603)0x451c5f09bfe0:[0x418027f0eaa2]CpuSched_StartWorld@vmkernel#nover+0x77 stack: 0x0, 0x0, 0x0, 0x0, 0x0

In the vSphere Client, you see hardware health warning with status Unknown for some sensors on ESXi hosts.

Due to internal memory leaks, the memory usage of the wsman service on some servers might increase beyond the max limit of 23 MB. As a result, the service

stops accepting incoming requests until restarted. The issue affects Dell and Lenovo servers.

In some environments, the vSphere Storage Appliance (VSA) plug-in, also known as Security Virtual Appliance (SVA), might cause the hostd service to

become intermittently unresponsive during the creation of a TCP connection. In addition, virtual machine snapshots might time out. VSA is not supported since

2014.

In rare cases, an unlocked spinlock might cause an ESXi host to fail with a purple diagnostic screen when restarting a virtual machine that is part of a Microsoft

Cluster Service (MSCS) cluster. If you have VMware vSphere High Availability enabled in your environment, the issue might affect many ESXi hosts, because

vSphere HA tries to power on the virtual machine in other hosts.

When a slab allocation failure is not handled properly in the RCRead path, a vSAN host might fail with a purple diagnostic screen. You see a warning such

as: bora/modules/vmkernel/lsom/rc_io.c:2115 -- NOT REACHED.

The VMXNET3 driver copies and calculates the checksums from the payload indicated in the IP total length field. However, VMXNET3 does not copy any

additional bytes after the payload that are not included in the length field and such bytes do not pass to guest virtual machines. For example, VMXNET3 does

not copy and pass Ethernet trailers.

When a storage device in a vSAN disk group has a transient error due to read failures, Full Rebuild Avoidance (FRA) might not be able to repair the device. In

such cases, FRA does not perform relog, which might lead to a log build up at PLOG, causing congestion and latency issues.

When you boot virtual machines with BIOS firmware from a network interface, interactions with the different servers in the environment might take long due to

a networking issue. This issue does not impact network performance during guest OS runtime.

If a physical port in a link aggregation group goes down or restarts, but at the same time an ESXi host sends out of sync LACP packet to the physical switch,

the vmnic might also go down. This might cause link aggregation to be temporarily unstable.

During vSphere vMotion operations to NFS datastores, the NFS disk-based lock files might not be deleted. As a result, virtual machines on the destination host

become unresponsive for around 40 seconds.

If you use the /bin/services.sh restart command to restart vCenter Server management services, the vobd daemon, which is responsible for sending ESXi

host events to vCenter Server, might not restart. As a result, you do not see alerts in the vSphere Client and the vSphere Web Client.

In rare occasions, when vSphere HA restarts or fails over, a virtual machine on the vSphere HA cluster might lose connectivity with an error such as VMXNET3

user: failed to connect 'Ethernet0' to DV Port 'xx'.

TCP uses a 3-step synchronize (SYN) and acknowledge (ACK) process to establish connections, SYN, SYN-ACK and ACK of the SYN-ACK. Options such as TCP

timestamps are negotiated as part of this process. For traffic optimization reasons, middleboxes or TCP proxies might remove the timestamp option in the

SYN-ACK phase. As a result, such TCP connections to ESXi hosts reset and close. A packet capture shows an RST packet from the server, immediately after

the TCP handshake.

A rare condition when an uninitialized field in the heap memory returns an invalid value for a VMFS resource cluster number might cause subsequent searches

for this cluster on the VMFS volume to fail. As a result, the ESXi host fails with a purple diagnostic screen.

A rare race condition of LAG port read locks might cause an ESXi host might fail with a purple diagnostic screen while disconnecting from a VDS.

Issuance of read I/Os by the vSAN read cache in order to fill 1 MB read cache lines of a hybrid vSAN array from a highly fragmented write buffer, might result

in IORETRY splitting of all 64 KB read I/Os issued by the read cache into 16 4 KB I/Os. This might cause IORETRY queue exhaustion.

If a vSAN datastore contains many powered off or suspended VMs that are not receiving any I/Os, active VMs on the vSAN datastore might experience

performance degradation due to increasing LSOM memory congestion in vSAN.

In rare cases, if you commit an object on a disk group, but the object appears as not committed after the vSAN host with that disk group reboots, vSAN might

incorrectly consider the object to be in bad state. As a result, the vSAN host might fail with a purple diagnostic screen with a message such as:

Object <uuid> corrupted. lastCWLSN = <number>, pendingLSN = <number>

PanicvPanicInt@vmkernel#nover+0x439

Panic_vPanic@vmkernel#nover+0x23

vmk_PanicWithModuleID@vmkernel#nover+0x41

DOMOwnerReconcileProcessLogsStartTask@com.vmware.vsan#0.0.0.1+0x6b3

DOMOperationStartTask@com.vmware.vsan#0.0.0.1+0xa5

DOMOperationDispatch@com.vmware.vsan#0.0.0.1+0xce

VSANServerMainLoop@com.vmware.vsanutil#0.0.0.1+0x5e1

vmkWorldFunc@vmkernel#nover+0x4f

CpuSched_StartWorld@vmkernel#nover+0x77

If you set a power cycle flag by using the vmx.reboot.powerCycle=TRUE advanced setting during a hardware upgrade of virtual machines, the hostd service might

lose track of the VMs power state. After the VMs reboot, hostd might report powered on VMs as powered off.

During virtual machine disk consolidation operations, such as snapshot disk consolidation, the hostd service might hold a lock for a long time. As a result, hostd

delays responds to other tasks and state requests. Consequently, if the disk consolidation task takes too long, the ESXi host becomes unresponsive.

If the UUID of a virtual machine changes, such as after a migration by using vSphere vMotion, and the virtual machine has a vNIC on an NSX-T managed

switch, the VM loses network connectivity. The vNIC cannot reconnect.

One or more ESXi hosts might intermittently become unresponsive due to a race condition in the VMware AHCI SATA Storage Controller Driver. In

the vmkernel.log file, you see a lot of messages such as:

…ahciAbortIO:(curr) HWQD: 0 BusyL: 0

The ESXi USB driver, vmkusb, might fail to process control I/O that exceeds 1K. As a result, some USB devices cause failures of I/O traffic to virtual machines on

an ESXi host.

ESXi-6.7.0-20210301001s-standard

Profile Name

ESXi-6.7.0-20210301001s-standard

Build

Vendor

Release Date

Acceptance Level

Affected Hardware

Affected Software

Affected VIBs

For build information, see Patches Contained in this Release.

VMware, Inc.

March 18, 2021

PartnerSupported

N/A

N/A

VMware_bootbank_esx-update_6.7.0-3.139.17700514
VMware_bootbank_vsan_6.7.0-3.139.17598146
VMware_bootbank_vsanhealth_6.7.0-3.139.17598147
VMware_bootbank_esx-base_6.7.0-3.139.17700514
VMware_locker_tools-light_11.2.5.17337674-17700514
VMware_bootbank_cpu-microcode_6.7.0-3.139.17700514

PRs Fixed

2704743, 2713501, 2673509

Related CVE numbers

N/A

This patch updates the following issues:

The OpenSSH version is updated to 8.4p1.

The Python third-party library is updated to version 3.5.10.

The ESXi userworld OpenSSL library is updated to version openssl-1.0.2x.

 The following VMware Tools ISO images are bundled with ESXi 670-202103001:

windows.iso: VMware Tools 11.2.5 supports Windows 7 SP1 or Windows Server 2008 R2 SP1 and later.

linux.iso: VMware Tools 10.3.23 ISO image for Linux OS with glibc 2.11 or later.

The following VMware Tools ISO images are available for download:

VMware Tools 10.0.12:

winPreVista.iso: for Windows 2000, Windows XP, and Windows 2003.

linuxPreGLibc25.iso: for Linux OS with a glibc version less than 2.5.

VMware Tools 11.0.6:

windows.iso: for Windows Vista (SP2) and Windows Server 2008 Service Pack 2 (SP2).

solaris.iso: VMware Tools image 10.3.10 for Solaris.

darwin.iso: Supports Mac OS X versions 10.11 and later.

Follow the procedures listed in the following documents to download VMware Tools for platforms not bundled with ESXi:

VMware Tools 11.2.5 Release Notes

Earlier versions of VMware Tools

What Every vSphere Admin Must Know About VMware Tools

VMware Tools for hosts provisioned with Auto Deploy

Updating VMware Tools

Code Name

FMS

Plt ID MCU Rev

MCU Date Brand Names

Nehalem EP 0x106a5 0x03 0x0000001d 5/11/2018

Lynnfield

0x106e5 0x13 0x0000000a 5/8/2018

Clarkdale

0x20652 0x12 0x00000011

5/8/2018

Arrandale

0x20655 0x92 0x00000007 4/23/2018

Sandy
Bridge DT

Westmere
EP

0x206a7 0x12 0x0000002f 2/17/2019

0x206c2 0x03 0x0000001f

5/8/2018

Sandy
Bridge EP

0x206d6 0x6d 0x00000621 3/4/2020

Sandy
Bridge EP

0x206d7 0x6d 0x0000071a

3/24/2020

Intel Xeon 35xx Series;
Intel Xeon 55xx Series

Intel Xeon 34xx
Lynnfield Series

Intel i3/i5 Clarkdale
Series;
Intel Xeon 34xx
Clarkdale Series

Intel Core i7-620LE
Processor

Intel Xeon E3-1100
Series;
Intel Xeon E3-1200
Series;
Intel i7-2655-LE Series;
Intel i3-2100 Series

Intel Xeon 56xx Series;
Intel Xeon 36xx Series

Intel Pentium 1400
Series;
Intel Xeon E5-1400
Series;
Intel Xeon E5-1600
Series;
Intel Xeon E5-2400
Series;
Intel Xeon E5-2600
Series;
Intel Xeon E5-4600
Series

Intel Pentium 1400
Series;
Intel Xeon E5-1400
Series;
Intel Xeon E5-1600
Series;
Intel Xeon E5-2400
Series;
Intel Xeon E5-2600
Series;
Intel Xeon E5-4600
Series

Nehalem EX 0x206e6 0x04 0x0000000d 5/15/2018

Intel Xeon 65xx Series;
Intel Xeon 75xx Series

Code Name

FMS

Plt ID MCU Rev

MCU Date Brand Names

Westmere
EX

0x206f2 0x05 0x0000003b 5/16/2018

Ivy Bridge
DT

0x306a9 0x12 0x00000021 2/13/2019

Haswell DT

0x306c3 0x32 0x00000028 11/12/2019

Ivy Bridge
EP

0x306e4 0xed 0x0000042e 3/14/2019

Ivy Bridge
EX

0x306e7 0xed 0x00000715 3/14/2019

Haswell EP

0x306f2 0x6f 0x00000044 5/27/2020

Haswell EX

0x306f4 0x80 0x00000016 6/17/2019

Broadwell H 0x40671 0x22 0x00000022 11/12/2019

Avoton

0x406d8 0x01 0x0000012d 9/16/2019

Broadwell
EP/EX

0x406f1 0xef 0x0b000038 6/18/2019

Skylake SP

0x50654 0xb7 0x02006a08 6/16/2020

Intel Xeon E7-8800
Series;
Intel Xeon E7-4800
Series;
Intel Xeon E7-2800
Series

Intel i3-3200 Series;
Intel i7-3500-LE/UE;
Intel i7-3600-QE;
Intel Xeon E3-1200-v2
Series;
Intel Xeon E3-1100-C-v2
Series;
Intel Pentium B925C

Intel Xeon E3-1200-v3
Series;
Intel i7-4700-EQ Series;
Intel i5-4500-TE Series;
Intel i3-4300 Series

Intel Xeon E5-4600-v2
Series;
Intel Xeon E5-2600-v2
Series;
Intel Xeon E5-2400-v2
Series;
Intel Xeon E5-1600-v2
Series;
Intel Xeon E5-1400-v2
Series

Intel Xeon E7-
8800/4800/2800-v2
Series

Intel Xeon E5-4600-v3
Series;
Intel Xeon E5-2600-v3
Series;
Intel Xeon E5-2400-v3
Series;
Intel Xeon E5-1600-v3
Series;
Intel Xeon E5-1400-v3
Series

Intel Xeon E7-
8800/4800-v3 Series

Intel Core i7-5700EQ;
Intel Xeon E3-1200-v4
Series

Intel Atom C2300 Series;
Intel Atom C2500 Series;
Intel Atom C2700 Series

Intel Xeon E7-
8800/4800-v4 Series;
Intel Xeon E5-4600-v4
Series;
Intel Xeon E5-2600-v4
Series;
Intel Xeon E5-1600-v4
Series

Intel Xeon Platinum 8100
Series;
Intel Xeon Gold
6100/5100, Silver 4100,
Bronze 3100 Series;
Intel Xeon D-2100 Series;
Intel Xeon D-1600 Series;
Intel Xeon W-3100
Series;
Intel Xeon W-2100
Series

Code Name

FMS

Plt ID MCU Rev

MCU Date Brand Names

Cascade
Lake B-0

Cascade
Lake

0x50656 0xbf 0x04003003 6/18/2020

0x50657 0xbf 0x05003003 6/18/2020

Cooper Lake 0x5065b 0xbf 0x0700001f

9/17/2020

Intel Xeon Platinum
9200/8200 Series;
Intel Xeon Gold
6200/5200;
Intel Xeon Silver
4200/Bronze 3200;
Intel Xeon W-3200

Intel Xeon Platinum
9200/8200 Series;
Intel Xeon Gold
6200/5200;
Intel Xeon Silver
4200/Bronze 3200;
Intel Xeon W-3200

Intel Xeon Platinum
8300 Series;
Intel Xeon Gold
6300/5300

Broadwell
DE

Broadwell
DE

Broadwell
DE

Broadwell
NS

0x50662 0x10 0x0000001c 6/17/2019 Intel Xeon D-1500 Series

0x50663 0x10 0x07000019 6/17/2019 Intel Xeon D-1500 Series

0x50664 0x10 0x0f000017

6/17/2019 Intel Xeon D-1500 Series

0x50665 0x10 0x0e00000f 6/17/2019 Intel Xeon D-1600 Series

Skylake H/S 0x506e3 0x36 0x000000e2 7/14/2020

Intel Xeon E3-1500-v5
Series;
Intel Xeon E3-1200-v5
Series

Denverton

0x506f1 0x01 0x00000032 3/7/2020 Intel Atom C3000 Series

Snow Ridge 0x80665 0x01 0x0b000007 2/25/2020 Intel Atom P5000 Series

Kaby Lake
H/S/X

0x906e9 0x2a 0x000000de 5/26/2020

Coffee Lake 0x906ea 0x22 0x000000de 5/25/2020

Intel Xeon E3-1200-v6
Series;
Intel Xeon E3-1500-v6
Series

Intel Xeon E-2100 Series;
Intel Xeon E-2200 Series
(4 or 6 core)

Coffee Lake 0x906eb 0x02 0x000000de 5/25/2020 Intel Xeon E-2100 Series

Coffee Lake 0x906ec 0x22 0x000000de 6/3/2020 Intel Xeon E-2100 Series

Coffee Lake
Refresh

0x906ed 0x22 0x000000de 5/24/2020

Intel Xeon E-2200 Series
(8 core)

ESXi-6.7.0-20210301001s-no-tools

Profile Name

ESXi-6.7.0-20210301001s-no-tools

Build

Vendor

Release Date

Acceptance Level

Affected Hardware

Affected Software

Affected VIBs

For build information, see Patches Contained in this Release.

VMware, Inc.

March 18, 2021

PartnerSupported

N/A

N/A

VMware_bootbank_esx-update_6.7.0-3.139.17700514
VMware_bootbank_vsan_6.7.0-3.139.17598146
VMware_bootbank_vsanhealth_6.7.0-3.139.17598147
VMware_bootbank_esx-base_6.7.0-3.139.17700514
VMware_bootbank_cpu-microcode_6.7.0-3.139.17700514

PRs Fixed

2704743, 2713501, 2673509

Related CVE numbers

N/A

This patch updates the following issues:

The OpenSSH version is updated to 8.4p1.

The Python third-party library is updated to version 3.5.10.

The ESXi userworld OpenSSL library is updated to version openssl-1.0.2x.

 The following VMware Tools ISO images are bundled with ESXi 670-202103001:

windows.iso: VMware Tools 11.2.5 supports Windows 7 SP1 or Windows Server 2008 R2 SP1 and later.

linux.iso: VMware Tools 10.3.23 ISO image for Linux OS with glibc 2.11 or later.

The following VMware Tools ISO images are available for download:

VMware Tools 10.0.12:

winPreVista.iso: for Windows 2000, Windows XP, and Windows 2003.

linuxPreGLibc25.iso: for Linux OS with a glibc version less than 2.5.

VMware Tools 11.0.6:

windows.iso: for Windows Vista (SP2) and Windows Server 2008 Service Pack 2 (SP2).

solaris.iso: VMware Tools image 10.3.10 for Solaris.

darwin.iso: Supports Mac OS X versions 10.11 and later.

Follow the procedures listed in the following documents to download VMware Tools for platforms not bundled with ESXi:

VMware Tools 11.2.5 Release Notes

Earlier versions of VMware Tools

What Every vSphere Admin Must Know About VMware Tools

VMware Tools for hosts provisioned with Auto Deploy

Updating VMware Tools

Code Name

FMS

Plt ID MCU Rev

MCU Date Brand Names

Nehalem EP 0x106a5 0x03 0x0000001d 5/11/2018

Lynnfield

0x106e5 0x13 0x0000000a 5/8/2018

Clarkdale

0x20652 0x12 0x00000011

5/8/2018

Arrandale

0x20655 0x92 0x00000007 4/23/2018

Sandy
Bridge DT

Westmere
EP

0x206a7 0x12 0x0000002f 2/17/2019

0x206c2 0x03 0x0000001f

5/8/2018

Sandy
Bridge EP

0x206d6 0x6d 0x00000621 3/4/2020

Sandy
Bridge EP

0x206d7 0x6d 0x0000071a

3/24/2020

Intel Xeon 35xx Series;
Intel Xeon 55xx Series

Intel Xeon 34xx
Lynnfield Series

Intel i3/i5 Clarkdale
Series;
Intel Xeon 34xx
Clarkdale Series

Intel Core i7-620LE
Processor

Intel Xeon E3-1100
Series;
Intel Xeon E3-1200
Series;
Intel i7-2655-LE Series;
Intel i3-2100 Series

Intel Xeon 56xx Series;
Intel Xeon 36xx Series

Intel Pentium 1400
Series;
Intel Xeon E5-1400
Series;
Intel Xeon E5-1600
Series;
Intel Xeon E5-2400
Series;
Intel Xeon E5-2600
Series;
Intel Xeon E5-4600
Series

Intel Pentium 1400
Series;
Intel Xeon E5-1400
Series;
Intel Xeon E5-1600
Series;
Intel Xeon E5-2400
Series;
Intel Xeon E5-2600
Series;
Intel Xeon E5-4600
Series

Nehalem EX 0x206e6 0x04 0x0000000d 5/15/2018

Intel Xeon 65xx Series;
Intel Xeon 75xx Series

Code Name

FMS

Plt ID MCU Rev

MCU Date Brand Names

Westmere
EX

0x206f2 0x05 0x0000003b 5/16/2018

Ivy Bridge
DT

0x306a9 0x12 0x00000021 2/13/2019

Haswell DT

0x306c3 0x32 0x00000028 11/12/2019

Ivy Bridge
EP

0x306e4 0xed 0x0000042e 3/14/2019

Ivy Bridge
EX

0x306e7 0xed 0x00000715 3/14/2019

Haswell EP

0x306f2 0x6f 0x00000044 5/27/2020

Haswell EX

0x306f4 0x80 0x00000016 6/17/2019

Broadwell H 0x40671 0x22 0x00000022 11/12/2019

Avoton

0x406d8 0x01 0x0000012d 9/16/2019

Broadwell
EP/EX

0x406f1 0xef 0x0b000038 6/18/2019

Skylake SP

0x50654 0xb7 0x02006a08 6/16/2020

Intel Xeon E7-8800
Series;
Intel Xeon E7-4800
Series;
Intel Xeon E7-2800
Series

Intel i3-3200 Series;
Intel i7-3500-LE/UE;
Intel i7-3600-QE;
Intel Xeon E3-1200-v2
Series;
Intel Xeon E3-1100-C-v2
Series;
Intel Pentium B925C

Intel Xeon E3-1200-v3
Series;
Intel i7-4700-EQ Series;
Intel i5-4500-TE Series;
Intel i3-4300 Series

Intel Xeon E5-4600-v2
Series;
Intel Xeon E5-2600-v2
Series;
Intel Xeon E5-2400-v2
Series;
Intel Xeon E5-1600-v2
Series;
Intel Xeon E5-1400-v2
Series

Intel Xeon E7-
8800/4800/2800-v2
Series

Intel Xeon E5-4600-v3
Series;
Intel Xeon E5-2600-v3
Series;
Intel Xeon E5-2400-v3
Series;
Intel Xeon E5-1600-v3
Series;
Intel Xeon E5-1400-v3
Series

Intel Xeon E7-
8800/4800-v3 Series

Intel Core i7-5700EQ;
Intel Xeon E3-1200-v4
Series

Intel Atom C2300 Series;
Intel Atom C2500 Series;
Intel Atom C2700 Series

Intel Xeon E7-
8800/4800-v4 Series;
Intel Xeon E5-4600-v4
Series;
Intel Xeon E5-2600-v4
Series;
Intel Xeon E5-1600-v4
Series

Intel Xeon Platinum 8100
Series;
Intel Xeon Gold
6100/5100, Silver 4100,
Bronze 3100 Series;
Intel Xeon D-2100 Series;
Intel Xeon D-1600 Series;
Intel Xeon W-3100
Series;
Intel Xeon W-2100
Series

Code Name

FMS

Plt ID MCU Rev

MCU Date Brand Names

Cascade
Lake B-0

Cascade
Lake

0x50656 0xbf 0x04003003 6/18/2020

0x50657 0xbf 0x05003003 6/18/2020

Cooper Lake 0x5065b 0xbf 0x0700001f

9/17/2020

Intel Xeon Platinum
9200/8200 Series;
Intel Xeon Gold
6200/5200;
Intel Xeon Silver
4200/Bronze 3200;
Intel Xeon W-3200

Intel Xeon Platinum
9200/8200 Series;
Intel Xeon Gold
6200/5200;
Intel Xeon Silver
4200/Bronze 3200;
Intel Xeon W-3200

Intel Xeon Platinum
8300 Series;
Intel Xeon Gold
6300/5300

Broadwell
DE

Broadwell
DE

Broadwell
DE

Broadwell
NS

0x50662 0x10 0x0000001c 6/17/2019 Intel Xeon D-1500 Series

0x50663 0x10 0x07000019 6/17/2019 Intel Xeon D-1500 Series

0x50664 0x10 0x0f000017

6/17/2019 Intel Xeon D-1500 Series

0x50665 0x10 0x0e00000f 6/17/2019 Intel Xeon D-1600 Series

Skylake H/S 0x506e3 0x36 0x000000e2 7/14/2020

Intel Xeon E3-1500-v5
Series;
Intel Xeon E3-1200-v5
Series

Denverton

0x506f1 0x01 0x00000032 3/7/2020 Intel Atom C3000 Series

Snow Ridge 0x80665 0x01 0x0b000007 2/25/2020 Intel Atom P5000 Series

Kaby Lake
H/S/X

0x906e9 0x2a 0x000000de 5/26/2020

Coffee Lake 0x906ea 0x22 0x000000de 5/25/2020

Intel Xeon E3-1200-v6
Series;
Intel Xeon E3-1500-v6
Series

Intel Xeon E-2100 Series;
Intel Xeon E-2200 Series
(4 or 6 core)

Coffee Lake 0x906eb 0x02 0x000000de 5/25/2020 Intel Xeon E-2100 Series

Coffee Lake 0x906ec 0x22 0x000000de 6/3/2020 Intel Xeon E-2100 Series

Coffee Lake
Refresh

0x906ed 0x22 0x000000de 5/24/2020

Intel Xeon E-2200 Series
(8 core)

Known Issues from Earlier Releases

To view a list of previous known issues, click here.

Copyright © Broadcom



=== Content from docs.vmware.com_4abe7a2b_20250115_115516.html ===


[![](/uicontent/images/VMware_by_Broadcom.png)Docs](/)

* ![](/uicontent/images/VMware_by_Broadcom_Gray-Black.png) Docs
  times
* [(current)](/)

* ![](/uicontent/images/nwMyLibrary.png)
  #####

  ![](/uicontent/images/icon-handshake.png)

  ![](/uicontent/images/icon-support.png)

  ![](/uicontent/images/icon-networking-bg.png)
  [VMware Communities](https://communities.vmware.com/)

  ![](/uicontent/images/icon-download-bg.png)

 This site will be decommissioned on January 30th 2025. After that date content will be available at [techdocs.broadcom.com](https://techdocs.broadcom.com/).

![](/uicontent/images/share-mylibrary.svg)

![](/uicontent/images/add-to-library-colorless.svg)

#

![](/uicontent/images/twitter.svg)
![](/uicontent/images/facebook.svg)
![](/uicontent/images/linkedin.svg)
![](/uicontent/images/weibo.svg)

![](/uicontent/images/pdf.svg)

![](/uicontent/images/feedback.svg)

![](/uicontent/images/edit.svg)

![](/uicontent/images/review.svg)

Twitter
Facebook
LinkedIn
微博

| VMware Cloud Foundation 4.3 | 24 AUG 2021 | Build 18433963  Check for additions and updates to these release notes. |

## What's New

The VMware Cloud Foundation (VCF) 4.3 release includes the following:

* **Flexibility in Application Virtual Networks (AVN):** Application Virtual Networks (AVN)s, which include the NSX Edge Cluster and NSX network segments, are no longer deployed and configured during bring-up. Instead they are implemented as a Day-N operations in SDDC Manager, providing greater flexibility.
* **FIPS Support**: You can enable FIPS mode during bring-up, which will enable it on all the VMware Cloud Foundation components that support FIPS.
* **Scheduled Automatic Password Rotations**: In addition to the on-demand password rotation capability, it is now possible to schedule automatic password rotations for accounts managed through SDDC Manager (excluding ESXi accounts). Automatic password rotation is enabled by default for service accounts.
* **SAN in Certificate Signing Requests (CSR)** : You can now add a Subject Alternative Name (SAN) when you generate a Certificate Signing Request (CSR) in SDDC Manager.
* **Improvements for vSphere Lifecycle Manager images**: For workload domains that use vSphere Lifecycle Manager images, this release includes several improvements. These include: prechecks to proactively identify issues that may affect upgrade operations; enabling concurrent upgrades for NSX-T Data Center components; and enabling provisioning and upgrade of Workload Management.
* **Add vSphere Clusters in Parallel:** You can add up to 7 vSphere clusters to a workload domain in parallel, improving the performance and speed of the workflow.
* **Add and Remove NSX Edge Nodes in NSX Edge Clusters:** For NSX Edge clusters deployed through SDDC Manager or the VMware Cloud Foundation API, you can expand and shrink NSX Edge clusters by adding or removing NSX Edge nodes from the cluster.
* **Guidance for Day-N operations in NSX Federated VCF environments:** You can federate NSX-T Data Center environments across VMware Cloud Foundation instances. You can manage federated NSX-T Data Center environments with a single pane of glass, create gateways and segments that span VMware Cloud Foundation instances, and configure and enforce firewall rules consistently across instances. Guidance is also provided for password rotation, certificate management, backup and restore, and lifecycle management for federated environments.
* **Backup Enhancements:** You can now configure an SDDC Manager backup schedule and retention policy from the SDDC Manager UI.
* **VMware Validated Solutions**: VMware Validated Solutions are a series of technical reference validated implementations designed to help customers build secure, high-performing, resilient, and efficient infrastructure for their applications and workloads deployed on VMware Cloud Foundation. Each VMware Validated Solution will come with detailed design with design decisions, implementation guidance consisting of manual UI-based step-by-step procedures and, where applicable, automated steps using infrastructure as code. These solutions based on VMware Cloud Foundation will be available on [core.vmware.com](http://core.vmware.com). The first set of validated solutions, that can be applied on vSAN ReadyNodes, include the following:

  + [Identity and Access Management for VMware Cloud Foundation](https://core.vmware.com/identity-and-access-management-vmware-cloud-foundation)
  + [Developer Ready Infrastructure for VMware Cloud Foundation](https://core.vmware.com/developer-ready-infrastructure-vmware-cloud-foundation)
  + [Advanced Load Balancing for VMware Cloud Foundation](https://core.vmware.com/advanced-load-balancing-vmware-cloud-foundation)
  + [Private Cloud Automation for VMware Cloud Foundation](https://core.vmware.com/private-cloud-automation-vmware-cloud-foundation)
  + [Intelligent Operations Management for VMware Cloud Foundation](https://core.vmware.com/intelligent-operations-management-vmware-cloud-foundation)
  + [Intelligent Logging and Analytics for VMware Cloud Foundation](https://core.vmware.com/intelligent-logging-and-analytics-vmware-cloud-foundation)

* **Documentation Enhancements:** The content from VMware Validated Design documentation has now been unified with core VMware Cloud Foundation documentation or has been integrated into a [VMware Validated Solution](https://core.vmware.com/vmware-validated-solutions). Additional documentation enhancements include:

  + Design Documents for VMware Cloud Foundation foundational components with design decisions
  + Design for the Management Domain
  + Design for the Virtual Infrastructure Workload Domain
  + Design for vRealize Suite Lifecyle and Access Management
  + Getting Started with VMware Cloud Foundation publication
  + Procedure enhancements through unification of content between VMware Validated Design and VMware Cloud Foundation publications

* **Capacity Planner tool**: Administrators can use the [VCF Capacity Planner](https://vcfcapacityplanner.esp.vmware.com/) online tool to model and generate a Software Defined Data Center build of materials. This interactive tool generates detailed guidance of hyper-converged server, storage, network, and cloud software SKUs required to successfully deploy an on-premises cloud.
* **Private APIs**: Access to private APIs that use basic authentication is deprecated in this release. You must switch to using public APIs.
* **BOM updates**: Updated Bill of Materials with new product versions.

## VMware Cloud Foundation Bill of Materials (BOM)

The Cloud Foundation software product is comprised of the following software Bill-of-Materials (BOM). The components in the BOM are interoperable and compatible.

**VMware Response to Apache Log4j Remote Code Execution Vulnerability**: VMware Cloud Foundation is impacted by CVE-2021-44228, and CVE-2021-45046 as described in [VMSA-2021-0028](https://www.vmware.com/security/advisories/VMSA-2021-0028.html). To remediate these issues, see [Workaround instructions to address CVE-2021-44228 & CVE-2021-45046 in VMware Cloud Foundation (KB 87095)](https://kb.vmware.com/s/article/87095).

| Software Component | Version | Date | Build Number |
| --- | --- | --- | --- |
| Cloud Builder VM | 4.3 | 24 AUG 2021 | 18433963 |
| SDDC Manager | 4.3 | 24 AUG 2021 | 18433963 |
| VMware vCenter Server Appliance | 7.0 Update 2c | 24 AUG 2021 | 18356314 |
| VMware ESXi | 7.0 Update 2a | 29 APR 2021 | 17867351 |
| [VMware Virtual SAN Witness Appliance](https://customerconnect.vmware.com/downloads/details?downloadGroup=WITNESS-OVA-70U2&productId=974) | 7.0 Update 2 | 08 JUL 2021 | 18188211 |
| VMware NSX-T Data Center | 3.1.3 | 22 JUL 2021 | 18328989 |
| VMware vRealize Suite Lifecycle Manager | 8.4.1 | 27 MAY 2021 | 18067607 |
| Workspace ONE Access | 3.3.5 | 20 MAY 2021 | 18049997 |
| vRealize Automation | 8.4.1 | 27 MAY 2021 | 18054500 |
| vRealize Log Insight | 8.4 | 15 APR 2021 | 17828109 |
| vRealize Log Insight Content Pack for NSX-T | 4.0.2 | n/a | n/a |
| vRealize Log Insight Content Pack for vRealize Automation 8.3+ | 1.0 | n/a | n/a |
| vRealize Log Insight Content Pack for Linux | 2.1.0 | n/a | n/a |
| vRealize Log Insight Content Pack for Linux - Systemd | 1.0.0 | n/a | n/a |
| vRealize Log Insight Content Pack for vRealize Suite Lifecycle Manager 8.0.1+ | 1.0.2 | n/a | n/a |
| vRealize Log Insight Content Pack for VMware Identity Manager | 2.0 | n/a | n/a |
| vRealize Operations Manager | 8.4 | 15 APR 2021 | 17863947 |
| vRealize Operations Management Pack for VMware Identity Manager | 1.3 | n/a | n/a |

* VMware vSAN is included in the VMware ESXi bundle.
* You can use vRealize Suite Lifecycle Manager to deploy vRealize Automation, vRealize Operations Manager, vRealize Log Insight, and Workspace ONE Access.
* vRealize Log Insight content packs are installed when you deploy vRealize Log Insight.
* The vRealize Operations Manager management pack is installed when you deploy vRealize Operations Manager.
* VMware Solution Exchange and the vRealize Log Insight in-product marketplace store only the latest versions of the content packs for vRealize Log Insight. The Bill of Materials table contains the latest versions of the packs that were available at the time VMware Cloud Foundation is released. When you deploy the Cloud Foundation components, it is possible that the version of a content pack within the in-product marketplace for vRealize Log Insight is newer than the one used for this release.

## VMware Software Edition License Information

The SDDC Manager software is licensed under the Cloud Foundation license. As part of this product, the SDDC Manager software deploys specific VMware software products.

The following VMware software components deployed by SDDC Manager are licensed under the Cloud Foundation license:

* VMware ESXi
* VMware vSAN
* VMware NSX-T Data Center

The following VMware software components deployed by SDDC Manager are licensed separately:

* vCenter Server

**NOTE**: Only one vCenter Server license is required for all vCenter Servers deployed in a Cloud Foundation system.

For details about the specific VMware software editions that are licensed under the licenses you have purchased, see the Cloud Foundation Bill of Materials (BOM) section above.

For general information about the product, see [VMware Cloud Foundation](http://www.vmware.com/products/cloud-foundation.html).

## Supported Hardware

For details on supported configurations, see the [VMware Compatibility Guide (VCG)](https://www.vmware.com/resources/compatibility/search.php) and the Hardware Requirements section on the Prerequisite Checklist tab in the [Planning and Preparation Workbook](https://docs.vmware.com/en/VMware-Cloud-Foundation/4.3/vcf-planning-and-preparation-workbook.zip).

## Documentation

To access the Cloud Foundation documentation, go to the [VMware Cloud Foundation product documentation](https://docs.vmware.com/en/VMware-Cloud-Foundation/index.html).

To access the documentation for VMware software products that SDDC Manager can deploy, see the product documentation and use the drop-down menus on the page to choose the appropriate version:

* [VMware vSphere product documentation](https://docs.vmware.com/en/VMware-vSphere/index.html), also has documentation about ESXi and vCenter Server
* [VMware vSAN product documentation](https://docs.vmware.com/en/VMware-vSAN/index.html)
* [VMware NSX-T Data Center product documentation](https://docs.vmware.com/en/VMware-NSX-T/index.html)

## Browser Compatibility and Screen Resolutions

The Cloud Foundation web-based interface supports the latest two versions of the following web browsers except Internet Explorer:

* Google Chrome
* Mozilla Firefox
* Microsoft Edge
* Internet Explorer: Version 11

For the Web-based user interfaces, the supported standard resolution is 1024 by 768 pixels. For best results, use a screen resolution within these tested resolutions:

* 1024 by 768 pixels (standard)
* 1366 by 768 pixels
* 1280 by 1024 pixels
* 1680 by 1050 pixels

Resolutions below 1024 by 768, such as 640 by 960 or 480 by 800, are not supported.

## Installation and Upgrade Information

You can install VMware Cloud Foundation 4.3 as a new release or perform a sequential or skip-level upgrade to VMware Cloud Foundation 4.3.

* Installing as a New Release

The new installation process has three phases:

**Phase One: Prepare the Environment**

The [Planning and Preparation Workbook](https://docs.vmware.com/en/VMware-Cloud-Foundation/4.3/vcf-planning-and-preparation-workbook.zip) provides detailed information about the software, tools, and external services that are required to implement a Software-Defined Data Center (SDDC) with VMware Cloud Foundation, using a standard architecture model.

**Phase Two: Image all servers with ESXi**

Image all servers with the ESXi version mentioned in the Cloud Foundation Bill of Materials (BOM) section. See the [VMware Cloud Foundation Deployment Guide](https://docs.vmware.com/en/VMware-Cloud-Foundation/4.3/vcf-deploy/GUID-F2DCF1B2-4EF6-444E-80BA-8F529A6D0725.html) for information on installing ESXi.

**Phase Three: Install Cloud Foundation 4.3**

Refer to the [VMware Cloud Foundation Deployment Guide](https://docs.vmware.com/en/VMware-Cloud-Foundation/4.3/vcf-deploy/GUID-F2DCF1B2-4EF6-444E-80BA-8F529A6D0725.html) for information on deploying Cloud Foundation.

* Upgrading to Cloud Foundation 4.3

You can perform a sequential or skip-level upgrade to VMware Cloud Foundation 4.3 from VMware Cloud Foundation 4.2.1, 4.2, 4.1.0.1, or 4.1. If your environment is at a version earlier than 4.1, you must upgrade the management domain and all VI workload domains to VMware Cloud Foundation 4.1 and then upgrade to VMware Cloud Foundation 4.3. For more information see [VMware Cloud Foundation Lifecycle Management](https://docs.vmware.com/en/VMware-Cloud-Foundation/4.3/vcf-lifecycle/GUID-B384B08D-3652-45E2-8AA9-AF53066F5F70.html).

**IMPORTANT**: Before you upgrade a vCenter Server, take a file-based backup. See [Manually Back Up vCenter Server](https://docs.vmware.com/en/VMware-Cloud-Foundation/4.3/vcf-admin/GUID-E7D707F7-96C1-4228-8DDA-283345184AFC.html).

## Resolved Issues

**The following issues are resolved in this release.**

* Special characters not allowed in the Username, Password, and Template Name fields on the Microsoft CA Configuration page
* Applying the configuration drift upgrade bundle fails
* Bundle transfer utility command retrieves incorrect bundle types
* vRealize Automation upgrade fails at vRealize Upgrade stage
* Host information on the Inventory > Hosts page takes too long to load or does not load
* Host commissioning may fail if API /v1/hosts/validations/commissions is used immediately after the host validation API /v1/hosts/validationsDescription
* Add host may fail for cluster using vLCM images
* Connecting vRealize Log Insight to a workload domain fails at the "Enable Log Collection for vSphere" step
* vRealize Suite product deployment fails with error "Failed to get Environment ID by given Host Name"
* VMware vRealize Log Insight 8.4 resolves a Cross Site Scripting (XSS) vulnerability due to improper user input validation. An attacker with user privileges may be able to inject a malicious payload via the Log Insight UI which would be executed when the victim accesses the shared dashboard link. The Common Vulnerabilities and Exposures project (cve.mitre.org) has assigned identifier CVE-2021-22021 to this issue. For more information, see VMware Security Advisory [VMSA-2021-0019](https://www.vmware.com/security/advisories/VMSA-2021-0019.html).

## Known Issues

* [VMware Cloud Foundation Known Issues](#Known Issues- VMware Cloud Foundation Known Issues)
* [Upgrade Known Issues](#Known Issues- Upgrade Known Issues)
* [Bring-up Known Issues](#Known Issues- Bring-up Known Issues)
* [SDDC Manager Known Issues](#Known Issues- SDDC Manager Known Issues)
* [Workload Domain Known Issues](#Known Issues- Workload Domain Known Issues)
* [Multi-Instance Management Known Issues](#Known Issues-Multi-Instance Management Known Issues)
* [API Known Issues](#Known Issues-API Known Issues)
* [vRealize Suite Known Issues](#Known Issues- vRealize Suite Known Issues )

### VMware Cloud Foundation Known Issues

* **Workload Management does not support NSX-T Data Center Federation**

  You cannot deploy Workload Management (vSphere with Tanzu) to a workload domain when that workload domain's NSX-T Data Center instance is participating in an NSX-T Data Center Federation.

  None.
* **NSX-T Guest Introspection (GI) and NSX-T Service Insertion (SI) are not supported on stretched clusters**

  There is no support for stretching clusters where NSX-T Guest Introspection (GI) or NSX-T Service Insertion (SI) are enabled. VMware Cloud Foundation detaches Transport Node Profiles from AZ2 hosts to allow AZ-specific network configurations. NSX-T GI and NSX-T SI require that the same Transport Node Profile be attached to all hosts in the cluster.

  None.
* **Stretched clusters and Workload Management**

  You cannot stretch a cluster on which Workload Management is deployed.

  None.

### Upgrade Known Issues

* **Network outage on NSX-T Data Center after upgrading VDS to 7.0.2**

  After upgrading NSX-T Data Center, if you upgrade the vSphere Distributed Switch (VDS) to version 7.0.2, you may experience network traffic disruption.

  Workaround: Do not upgrade vSphere Distributed Switches to version 7.0.2.
* **NSX-T upgrade causing host PSOD**

  ESXi host can PSOD during NSX-T upgrade when there is a mass migration of DFW filters, where flows are being revalidated while configuration cycle is occurring.

  See KB [87803](http://kb.vmware.com/s/article/87803) for more information. This issue is fixed in NSX-T 3.1.3.7.

  If a VCF upgrade is tried post application of this workaround, the LCM pre-check on DRS configuration will fail. This is expected behavior.
* **Cluster-level ESXi upgrade fails**

  Cluster-level selection during upgrade does not consider the health status of the clusters and may show a cluster's status as **Available**, even for a faulty cluster. If you select a faulty cluster, the upgrade fails.

  Always perform an update precheck to validate the health status of the clusters. Resolve any issues before upgrading.
* **You are unable to update NSX-T Data Center in the management domain or in a workload domain with vSAN principal storage because of an error during the NSX-T transport node precheck stage**

  In SDDC Manager, when you run the upgrade precheck before updating NSX-T Data Center, the NSX-T transport node validation results with the following error.

  **No coredump target has been configured. Host core dumps cannot be saved.:System logs on host sfo01-m01-esx04.sfo.rainpole.io are stored on non-persistent storage. Consult product documentation to configure a syslog server or a scratch partition.**

  Because the upgrade precheck results with an error, you cannot proceed with updating the NSX-T Data Center instance in the domain. VMware Validated Design supports vSAN as the principal storage in the management domain. However, vSAN datastores do no support scratch partitions. See VMware Knowledge Base article [2074026](https://kb.vmware.com/s/article/2074026).

  Deactivate the update precheck validation for the subsequent NSX-T Data Center update.

  1. Log in to SDDC Manager as **vcf** using a Secure Shell (SSH) client.
  2. Open the `application-prod.properties` file for editing: `vi /opt/vmware/vcf/lcm/lcm-app/conf/application-prod.properties`
  3. Add the following property and save the file: `lcm.nsxt.suppress.prechecks=true`
  4. Restart the life cycle management service: `systemctl restart lcm`
  5. Log in to the SDDC Manager user interface and proceed with the update of NSX-T Data Center.
* **NSX-T upgrade may fail at the step NSX T TRANSPORT NODE POSTCHECK STAGE**

  NSX-T upgrade may not proceed beyond the `NSX T TRANSPORT NODE POSTCHECK STAGE`.

  Contact VMware support.
* **ESXi upgrade fails with the error "Incompatible patch or upgrade files. Please verify that the patch file is compatible with the host. Refer LCM and VUM log file."**

  This error occurs if any of the ESXi hosts that you are upgrading have detached storage devices.

  Workaround: Attach all storage devices to the ESXi hosts being upgraded, reboot the hosts, and retry the upgrade.
* **Update precheck fails with the error "Password has expired"**

  If the vCenter Single Sign-On password policy specifies a maximum lifetime of zero (never expires), the precheck fails.

  Workaround: Set the maximum lifetime password policy to something other than zero and retry the precheck.
* **Skip level upgrades are not enabled for some product components after VMware Cloud Foundation is upgraded to 4.3**

  After performing skip level upgrade to VMware Cloud Foundation 4.3 from 4.1.x or 4.2.x, one or more of the following symptoms is observed:

  + vRealize bundles do not show up as available for upgrade
  + Bundles for previous versions of some product components (NSX-T Data Center, vCenter Server, ESXi) show up as available for upgrade

  See: [KB 85505](https://kb.vmware.com/s/article/85505)
* **vRealize Operations Manager upgrade fails on the step VREALIZE\_UPGRADE\_PREPARE\_BACKUP with the error: Waiting for vRealize Operations cluster to change state timed out**

  When upgrading vRealize Operations Manager, SDDC Manager takes the vRealize Operations Manager cluster offline and takes snapshots of the vRealize Operations Manager virtual machines. In some circumstances, taking the cluster offline takes a long time and the operation times out.

  Workaround: Take the vRealize Operations Manager cluster back online and retry the upgrade.

  1. Log in to the vRealize Operations Manager Administration UI (https://*<vrops\_ip>*/admin) using the admin credentials.
  2. If the cluster status is offline, in the Cluster Status section click **Take Cluster Online**. Wait for the cluster to initialize and be marked as green.
  3. In the SDDC Manager UI, the option to retry vRealize Operations Manager upgrade should be available. Retry the upgrade.

  If the upgrade continues to fail, take the snapshots manually and retry the upgrade. Since the snapshots already exist, SDDC Manager will skip that step and proceed with the upgrade.

  1. Log in to the vRealize Operations Manager Administration UI (https://*<vrops\_ip>*/admin) using the admin credentials.
  2. Ensure that that the vRealize Operations Manager Cluster Status is offline. If it is online, click **Take Cluster Offline** in the Cluster Status section. Wait for the cluster to be marked as offline.
  3. Log in to the management domain vCenter Server using the vSphere Client.
  4. Navigate to the vRealize Operations Manager virtual machines and create a snapshot for each virtual machine in the vRealize Operations Manager cluster. Use the following prefix "vROPS\_LCM\_UPGRADE\_MANUAL\_BACKUP" for the snapshots. Please note that the prefix should match the letter casing.
  5. After the snapshots are done, log in to the vRealize Operations Manager UI and take cluster online. Wait for the cluster initialization.
  6. In the SDDC Manager UI, the option to retry vRealize Operations Manager upgrade should be available. Retry the upgrade.

### Bring-up Known Issues

* **The Cloud Foundation Builder VM remains locked after more than 15 minutes.**

  The VMware Imaging Appliance (VIA) locks out the admin user after three unsuccessful login attempts. Normally, the lockout is reset after fifteen minutes but the underlying Cloud Foundation Builder VM does not automatically reset.

  Log in to the VM console of the Cloud Foundation Builder VM as the `root` user. Unlock the account by resetting the password of the admin user with the following command:

  `pam_tally2 --user=<user> --reset`
* **Bringup fails when creating NSX-T Data Center transport nodes**

  The bringup task "Create NSX-T Data Center Transport Nodes from Discovered Nodes" might fail if there's an ESXi host in the management cluster which is pending a reboot.

  Workaround: Reboot all ESXi hosts that are pending reboot and retry bringup.

### SDDC Manager Known Issues

* **Rotating or updating vSphere Single-Sign On (PSC) password can cause issues**

  If you have multiple VMware Cloud Foundation instances that share a single SSO domain, rotating or updating the vSphere SSO password for the first VCF instance causes the second VCF instance to become inaccessible.

  Workaround: See [KB 85485](https://kb.vmware.com/s/article/85485).
* **Deactivating CEIP on SDDC Manager does not deactivating CEIP on vRealize Automation and vRealize Suite Lifecycle Manager**

  When you deactivate CEIP on the SDDC Manager Dashboard, data collection is not deactivated on vRealize Automation and vRealize Suite Lifecycle Manager. This is because of API deprecation in vRealize Suite 8.x.

  Workaround: Manually deactivate CEIP in vRealize Automation and vRealize Suite Lifecycle Manager. For more information, see VMware vRealize Automation Documentation and VMware vRealize Suite Lifecycle Manager Documentation.
* **Generate CSR task for a component hangs**

  When you generate a CSR, the task may fail to complete due to issues with the component's resources. For example, when you generate a CSR for NSX Manager, the task may fail to complete due to issues with an NSX Manager node. You cannot retry the task once the resource is up and running again.

  1. Log in to the UI for the component to troubleshoot and resolve any issues.
  2. Using SSH, log in to the SDDC Manager VM with the user name `vcf`.
  3. Type **su** to switch to the root account.
  4. Run the following command: `systemctl restart operationsmanager`
  5. Retry generating the CSR.
* **SoS utility options for health check are missing information**

  Due to limitations of the ESXi service account, some information is unavailable in the following health check options:

  + `--hardware-compatibility-report:` No `Devices and Driver` information for ESXi hosts.
  + `--storage-health:` No `vSAN Health Status` or `Total no. of disks` information for ESXi hosts.

  None.
* **Supportability and Serviceability (SoS) Utility health checks fail with the error "Failed to get details"**

  SoS is not able to handle ESXi host names that include uppercase letters.

  Workaround: Use the precheck functionality in the SDDC Manager UI to check the health of the ESXi hosts.
* **SDDC Manager UI does not load correctly**

  If you log in to the SDDC Manager UI using an Active Directory user name that includes a space, the UI does not load correctly.

  Workaround: None

### Workload Domain Known Issues

* **Cannot reuse a static IP pool that includes special characters in its name**

  If you chose Static IP Pool as the IP allocation method when creating a VI workload domain and you used special characters or spaces in the IP pool name, you are not able to reuse the IP pool when creating a new VI workload domain or adding a vSphere cluster to the workload domain.

  Workaround: Use only supported characters when naming a static IP pool. Supported characters:

  + a-z
  + A-Z
  + 0-9
  + - and \_
  + No spaces

  If you have an existing static IP pool that includes unsupported characters in its name, you can use the NSX Manager UI to rename it.
* **Adding host fails when host is on a different VLAN**

  A host add operation can sometimes fail if the host is on a different VLAN.

  1. Before adding the host, add a new portgroup to the VDS for that cluster.
  2. Tag the new portgroup with the VLAN ID of the host to be added.
  3. Add the Host. This workflow fails at the "Migrate host vmknics to dvs" operation.
  4. Locate the failed host in vCenter, and migrate the vmk0 of the host to the new portgroup you created in step 1. For more information, see [Migrate VMkernel Adapters to a vSphere Distributed Switch](https://docs.vmware.com/en/VMware-vSphere/6.7/com.vmware.vsphere.networking.doc/GUID-0DE237B8-1938-4DE9-90EC-718E345B56A0.html) in the vSphere product documentation.
  5. Retry the Add Host operation.

  **NOTE**: If you later remove this host in the future, you must manually remove the portgroup as well if it is not being used by any other host.
* **Deploying partner services on an NSX-T workload domain displays an error**

  Deploying partner services, such as McAfee or Trend, on a workload domain enabled for vSphere Update Manager (VUM), displays the “Configure NSX at cluster level to deploy Service VM” error.

  Attach the Transport node profile to the cluster and try deploying the partner service. After the service is deployed, detach the transport node profile from the cluster.
* **If the witness ESXi version does not match with the host ESXi version in the cluster, vSAN cluster partition may occur**

  vSAN stretch cluster workflow does not check the ESXi version of the witness host. If the witness ESXi version does not match the host version in the cluster, then vSAN cluster partition may happen.

  1. Upgrade the witness host manually with the matching ESXi version using the vCenter VUM functionality.
  2. Replace or deploy the witness appliance matching with the ESXi version.
* **vSAN partition and critical alerts are generated when the witness MTU is not set to 9000**

  If the MTU of the witness switch in the witness appliance is not set to 9000, the vSAN stretch cluster partition may occur.

  Set the MTU of the witness switch in the witness appliance to 9000 MTU.
* **Adding a host to a vLCM-enabled workload domain configured with the Dell Hardware Support Manager (OMIVV) fails**

  When you try to add a host to a vSphere cluster for a workload domain enabled with vSphere Lifecycle Manager (vLCM), the task fails and the domain manager log reports "The host (host-name) is currently not managed by OMIVV." The domain manager logs are located at /var/log/vmware/vcf/domainmanager on the SDDC Manager VM.

  Update the hosts inventory in OMIVV and retry the add host task in the SDDC Manager UI. See the Dell documentation for information about updating the hosts inventory in OMIVV.
* **Adding a vSphere cluster or adding a host to a workload domain fails**

  Under certain circumstances, adding a host or vSphere cluster to a workload domain fails at the **Configure NSX-T Transport Node** or **Create Transport Node Collection** subtask.

  1. Enable SSH for the NSX Manager VMs.
  2. SSH into the NSX Manager VMs as **admin** and then log in as **root**.
  3. Run the following command on each NSX Manager VM: **sysctl -w net.ipv4.tcp\_en=0**
  4. Login to NSX Manager UI for the workload domain.
  5. Navigate to **System > Fabric > Nodes > Host Transport Nodes**.
  6. Select the vCenter server for the workload domain from the **Managed by** drop-down menu.
  7. Expand the vSphere cluster and navigate to the transport nodes that are in a **partial success** state.
  8. Select the check box next to a **partial success** node, click **Configure NSX.**
  9. Click **Next** and then click **Apply**.
  10. Repeat steps 7-9 for each **partial success** node.

  When all host issues are resolved, transport node creation starts for the failed nodes. When all hosts are successfully created as transport nodes, retry the failed add vSphere cluster or add host task from the SDDC Manager UI.
* **The vSAN Performance Service is not enabled for vSAN clusters when CEIP is not enabled**

  If you do not enable the VMware Customer Experience Improvement Program (CEIP) in SDDC Manager, when you create a workload domain or add a vSphere cluster to a workload domain, the vSAN Performance Service is not enabled for vSAN clusters. When CEIP is enabled, data from the vSAN Performance Service is provided to VMware and this data is used to aid VMware Support with troubleshooting and for products such as VMware Skyline, a proactive cloud monitoring service. See [Customer Experience Improvement Program](https://www.vmware.com/solutions/trustvmware/ceip.html) for more information on the data collected by CEIP.

  Enable CEIP in SDDC Manager. See the [VMware Cloud Foundation Documentation](https://docs.vmware.com/en/VMware-Cloud-Foundation/index.html). After CEIP is enabled, a scheduled task that enables the vSAN Performance Service on existing clusters in workload domains runs every three hours. The service is also enabled for new workload domains and clusters. To enable the vSAN Performance Service immediately, see the [VMware vSphere Documentation](https://docs.vmware.com/en/VMware-vSphere/index.html).
* **Creation or expansion of a vSAN cluster with more than 32 hosts fails**

  By default, a vSAN cluster can grow up to 32 hosts. With large cluster support enabled, a vSAN cluster can grow up to a maximum of 64 hosts. However, even with large cluster support enabled, a creation or expansion task can fail on the sub-task **Enable vSAN on vSphere Cluster**.

  1. Enable Large Cluster Support for the vSAN cluster in the vSphere Client. If it is already enabled skip to step 2.

     1. Select the vSAN cluster in the vSphere Client.
     2. Select **Configure > vSAN > Advanced Options**.
     3. Enable Large Cluster Support.
     4. Click **Apply**.
     5. Click **Yes**.
  2. Run a vSAN health check to see which hosts require rebooting.
  3. Put the hosts into Maintenance Mode and reboot the hosts.

  For more information about large cluster support, see <https://kb.vmware.com/kb/2110081>.
* **Removing a host from a cluster, deleting a cluster from a workload domain, or deleting a workload domain fails if Service VMs (SVMs) are present**

  If you deployed an endpoint protection service (such as guest introspection) to a cluster through NSX-T Data Center, then removing a host from the cluster, deleting the cluster, or deleting the workload domain containing the cluster will fail on the subtask **Enter Maintenance Mode on ESXi Hosts**.

  + For host removal: Delete the Service VM from the host and retry the operation.
  + For cluster deletion: Delete the service deployment for the cluster and retry the operation.
  + For workload domain deletion: Delete the service deployment for all clusters in the workload domain and retry the operation.
* **vCenter Server overwrites the NFS datastore name when adding a cluster to a VI workload domain**

  If you add an NFS datastore with the same NFS server IP address, but a different NFS datastore name, as an NFS datastore that already exists in the workload domain, then vCenter Server applies the existing datastore name to the new datastore.

  If you want to add an NFS datastore with a different datastore name, then it must use a different NFS server IP address.

### Multi-Instance Management Known Issues

* **Federation creation information not displayed if you leave the Multi-Instance Management Dashboard**

  Federation creation progress is displayed on the Multi-Instance Management Dashboard. If you navigate to another screen and then return to the Multi-Instance Management Dashboard, progress messages are not displayed. Instead, an empty map with no Cloud Foundation instances are displayed until the federation is created.

  Stay on the Multi-Instance Dashboard till the task is complete. If you have navigated away, wait for around 20 minutes and then return to the dashboard by which time the operation should have completed.
* **Multi-Instance Management Dashboard operation fails**

  After a controller joins or leaves a federation, Kafka is restarted on all controllers in the federation. It can take up to 20 minutes for the federation to stabilize. Any operations performed on the dashboard during this time may fail.

  Re-try the operation.
* **Join operation fails**

  A join operation may fail if a controller SDDC Manager has a public certificate with a depth greater than one (that is, it has intermediate certificates).

  Trust the intermediate certificate of the controller SDDC Manager. See [KB 80986](https://kb.vmware.com/s/article/80986).

### API Known Issues

* **Updating DNS/NTP server does not apply the update to all NSX Managers**

  If you update the NTP or DNS server information for a VMware Cloud Foundation instance that includes more than one NSX Manager, only one of the NSX Managers gets updated with the new information.

  Workaround: Use the NSX Manager API or CLI to manually update the DNS/NTP server information for the remaining NSX Manager(s).
* **The VMware Cloud Foundation API ignores NSX VDS uplink information for in-cluster expansion of an NSX Edge cluster**

  When you use the VMware Cloud Foundation API to expand an NSX Edge cluster and the new NSX Edge node is going to be hosted on the same vSphere cluster as the existing NSX Edge nodes (in-cluster), the `edgeClusterExpansionSpec` ignores any information you provide for `firstNsxVdsUplink` and `secondNsxVdsUplink`.

  Workaround: None. This is by design. For in-cluster expansions, new NSX Edge nodes use the same NSX VDS uplinks as the existing NSX Edge nodes in the NSX Edge cluster.
* **Stretch cluster operation fails**

  If the cluster that you are stretching does not include a powered-on VM with an operating system installed, the operation fails at the "Validate Cluster for Zero VMs" task.

  Make sure the cluster has a powered-on VM with an operating system installed before stretching the cluster.

### vRealize Suite Known Issues

* **vRealize Operations Manager: VMware Security Advisory VMSA-2021-0018**

  [VMSA-2021-0018](https://www.vmware.com/security/advisories/VMSA-2021-0018.html) describes security vulnerabilities that affect VMware Cloud Foundation.

  + The vRealize Operations Manager API contains an arbitrary file read vulnerability. A malicious actor with administrative access to vRealize Operations Manager API can read any arbitrary file on server leading to information disclosure.The Common Vulnerabilities and Exposures project (cve.mitre.org) has assigned identifier CVE-2021-22022 to this issue.
  + The vRealize Operations Manager API has insecure object reference vulnerability. A malicious actor with administrative access to vRealize Operations Manager API may be able to modify other users information leading to an account takeover.The Common Vulnerabilities and Exposures project (cve.mitre.org) has assigned identifier CVE-2021-22023 to this issue.
  + The vRealize Operations Manager API contains an arbitrary log-file read vulnerability. An unauthenticated malicious actor with network access to the vRealize Operations Manager API can read any log file resulting in sensitive information disclosure. The Common Vulnerabilities and Exposures project (cve.mitre.org) has assigned identifier CVE-2021-22024 to this issue.
  + The vRealize Operations Manager API contains a broken access control vulnerability leading to unauthenticated API access. An unauthenticated malicious actor with network access to the vRealize Operations Manager API can add new nodes to existing vROps cluster. The Common Vulnerabilities and Exposures project (cve.mitre.org) has assigned identifier CVE-2021-22025 to this issue.
  + The vRealize Operations Manager API contains a Server Side Request Forgery in multiple end points. An unauthenticated malicious actor with network access to the vRealize Operations Manager API can perform a Server Side Request Forgery attack leading to information disclosure. The Common Vulnerabilities and Exposures project (cve.mitre.org) has assigned identifiers CVE-2021-22026 and CVE-2021-22027 to this issue.

  Workaround: See [KB 85452](https://kb.vmware.com/s/article/85452) for information about applying vRealize Operations Security Patches that resolve the issues.
* **Updating the DNS or NTP server configuration does not apply the update to vRealize Automation**

  Using the Cloud Foundation API to update the DNS or NTP servers does not apply the update to vRealize Automation due to a bug in vRealize Suite Lifecycle Manager.

  Workaround: Manually update the DNS or NTP server(s) for vRealize Automation.

  + **Update the DNS server(s) for vRealize Automation**
  1. SSH to the first vRealize Automation node using root credentials.
  2. Delete the current DNS server using the following command: `sed '/nameserver.*/d' -i /etc/resolv.conf`
  3. Add the new DNS server IP with following command:

     `echo nameserver [DNS server IP] >> /etc/resolv.conf`
  4. Repeat this command if there are multiple DNS servers.
  5. Validate the update with the following command:

     `cat /etc/resolv.conf`

  Repeat these steps for each vRealize Automation node.

  **Update the NTP server(s) for vRealize Automation**

  1. SSH to the first vRealize Automation node using root credentials.
  2. Run the following command to specify the new NTP server: `vracli ntp systemd --set [NTP server IP]` To add multiple NTP servers:

     + `vracli ntp systemd --set [NTP server 1 IP,NTP server 2 IP]`
  3. Validate the update with the following command:

     `vracli ntp show-config`
  4. Apply the update to all vRealize Automation nodes with the following command:

     + `vracli ntp apply`
  5. Validate the update by running the following command on each vRealize Automation node: `vracli ntp show-config`
* **Connecting vRealize Operations Manager to a workload domain fails at the "Create vCenter Server Adapter in vRealize Operations Manager for the Workload Domain" step**

  When you connect vRealize Operations Manager to a workload domain, it fails at the `Create vCenter Server Adapter in vRealize Operations Manager for the Workload Domain` step with a message similar to `Failed to configure vCenter <vcenter-hostname> in vROps <vrops-hostname>, because Failed to manage vROps adapter`. This issue can occur when the vRealize Operations cluster is offline.

  Workaround: Make sure that the vRealize Operations cluster is online.

  1. Log in to the vRealize Operations Manager administration interface.
  2. Click **Administration > Cluster Management** and check the cluster status.
  3. If the vRealize Operations cluster is offline, bring the cluster online.
  4. When the cluster status displays as online, retry connecting vRealize Operations Manager to a workload domain
* **vRealize Operations Management Pack for VMware Identity Manager is not installed**

  If you install vRealize Operations Manager before you install Workspace ONE Access, then the vRealize Operations Management Pack for VMware Identity Manager is not installed.

  Workaround:

  1. Log in to the vRealize Suite Lifecycle Manager appliance.
  2. Click **VMware Marketplace**.
  3. Enter "Identity Manager" in the **Search** text box.
  4. Download and install the vRealize Operations Management Pack for VMware Identity Manager.
  5. Log in to vRealize Operations Manager.
  6. On the main navigation bar, click **Administration**.
  7. In the left pane, select **Solutions > Other accounts**.
  8. Click **Add account**.
  9. On the **Account types** page, click **VMware Identity Manager adapter**.
  10. Configure the settings, choosing the default collector group.
  11. In the **Connection information** section, click the **Add** icon.
  12. In the **Manage credential** dialog box, configure the Workspace ONE Access credentials and click **OK**.
  13. On the **New account** page, click **Validate** connection.
  14. In the **Info** dialog box, click **OK**.
  15. Click **Add**.
  16. On the **Other accounts** page, verify that the collection status of the adapter is **OK**.
* **Deploying a second vRealize Suite Lifecycle Manager fails**

  If you have multiple instances of VMware Cloud Foundation in the same SSO domain and you try to deploy vRealize Lifecycle Manager on both, the second deployment will fail with the message `Add vCenter Server and Data Center to vRealize Suite Lifecycle Manager Failed`.

  Workaround: Use a single vRealize Suite Lifecycle Manager to manage instances of VMware Cloud Foundation in the same SSO domain.
* **vRealize Suite Lifecycle Manager reports a "FAILED" inventory sync**

  After rotating a vCenter Server service account password in SDDC Manager, the inventory sync may fail for vRealize Suite environments managed by VMware Cloud Foundation.

  Workaround: Log in to vRealize Suite Lifecycle Manager to identify and troubleshoot the failed environment(s).

check-circle-line

exclamation-circle-line

close-line

![Scroll to top icon](/uicontent/images/scroll_top.svg)

![](/uicontent/images/feedback.svg)

             [![VMware](https://www.vmware.com/content/dam/vmwaredesigns/scrapercontent/resources/logos/vmware-logo-grey.svg "VMware")](https://www.vmware.com/)

Resources

* [Blogs](https://blogs.vmware.com/)
* [Careers](https://www.broadcom.com/jobs)
* [Communities](https://community.broadcom.com/)
* [Customer Stories](https://www.vmware.com/resources/customers)

* [News and Stories](https://news.broadcom.com/)
* [Topics](https://www.vmware.com/topics/)
* [Trust Center](https://www.vmware.com/info/trust-center/)

Support

* [Broadcom Support](https://support.broadcom.com/)
* [Documentation](https://docs.vmware.com)
* [Hands-On Labs](https://www.vmware.com/resources/hands-on-labs)
* [Licensing](https://www.broadcom.com/licensing)

* [Twitter](https://twitter.com/VMware)
* [YouTube](https://www.youtube.com/user/vmwaretv)
* [Facebook](https://www.facebook.com/vmware)
* [LinkedIn](https://www.linkedin.com/company/vmware/mycompany/)
* [Contact Sales](https://go-vmware.broadcom.com/contact-us)

---

  Copyright © 2005-2024 Broadcom. All Rights Reserved. The term "Broadcom" refers to Broadcom Inc. and/or its subsidiaries. [Accessibility](https://www.broadcom.com/company/legal/accessibility "Accessibility") [Privacy](https://www.broadcom.com/privacy "Privacy") [Supplier Responsibility](https://www.broadcom.com/company/citizenship/supplier-responsibility "Supplier Responsibility") [Terms Of Use](https://www.broadcom.com/company/legal/terms-of-use "Terms Of Use")

#####

×

Share on Social Media?

×

#####

×

exclamation-circle-line

check-circle-line

 :

#####

×

exclamation-circle-line

|

check-circle-line

exclamation-circle-line

x

#####

×

"
"?

×


