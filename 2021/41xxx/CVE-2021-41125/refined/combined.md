=== Content from lists.debian.org_390d74f8_20250115_080057.html ===


---

[[Date Prev](msg00020.html)][[Date Next](msg00022.html)]
[[Thread Prev](msg00020.html)][[Thread Next](msg00022.html)]
[[Date Index](maillist.html#00021)]
[[Thread Index](threads.html#00021)]

# [SECURITY] [DLA 2950-1] python-scrapy security update

---

* *To*: <debian-lts-announce@lists.debian.org>
* *Subject*: [SECURITY] [DLA 2950-1] python-scrapy security update
* *From*: Emilio Pozuelo Monfort <pochu@debian.org>
* *Date*: Wed, 16 Mar 2022 12:57:06 +0100 (CET)
* *Message-id*: <[[🔎]](/msgid-search/20220316115706.B3FBF2A6D3B%40andromeda) [20220316115706.B3FBF2A6D3B@andromeda](msg00021.html)>
* *Mail-followup-to*: debian-lts@lists.debian.org
* *Reply-to*: debian-lts@lists.debian.org

---

```
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256

- -------------------------------------------------------------------------
Debian LTS Advisory DLA-2950-1                debian-lts@lists.debian.org
<https://www.debian.org/lts/security/>               Emilio Pozuelo Monfort
March 16, 2022                                <https://wiki.debian.org/LTS>
- -------------------------------------------------------------------------

Package        : python-scrapy
Version        : 1.0.3-2+deb9u1
CVE ID         : CVE-2021-41125 CVE-2022-0577

It was found that Scrapy, a framework for extracting data from websites,
could send HTTP Authorization as well as cookies to other domains in case
of redirections, possibly leaking user credentials.

For Debian 9 stretch, these problems have been fixed in version
1.0.3-2+deb9u1.

We recommend that you upgrade your python-scrapy packages.

For the detailed security status of python-scrapy please refer to
its security tracker page at:
<https://security-tracker.debian.org/tracker/python-scrapy>

Further information about Debian LTS security advisories, how to apply
these updates to your system and frequently asked questions can be
found at: <https://wiki.debian.org/LTS>
-----BEGIN PGP SIGNATURE-----

iQIzBAEBCAAdFiEEcJymx+vmJZxd92Q+nUbEiOQ2gwIFAmIx0I8ACgkQnUbEiOQ2
gwKNvw/+MniywnqwPiX+P/pT2FQ9wzPEd/ECTGqOVw237jR+Iz9kWX3qecnTyQrx
VwoK5AYvjX0u5kDuw8mLD1iVP7JGWtF+HGnsSJEVlQhvPSMUsOp2kZoB4+lI5UyV
j7xZgl1uuRddjuc9o370nJIoWWep4DVrd64Ssa5eAeTspTeNC4Vmo0BIHIcE1Now
2jGJ+g/mk+KzqckY8GJyYujVLaTeYpcLC7QzaF1uw2N1qtT+i67m6BRJ+fJprO9s
pRerH5ltqXiQ5lTHOUNPTY4E/zxpUnwwvEaF9PLIWwpMGH0wGPDkQKkbrrC3njtq
E6v9QxptB/Wt/fbiITNbCTJ6xIhcef9FVfyqvlM7M8RtWrX/F/94hLVFXHGJDPnB
LTxmYKia9x6o7yZ3mg1DJbrpZgrYszhVIwZ/1h4kJYHrQQEp6PhH7ANFuvXoRdeX
29JOPoVc+T7NV5VD0XsGT7ShYwzG6NDFiLlui5aA8DdDY/62tEbjaxrZbW728wkw
a/TwFBOJVHz4Gm0xg14iiNrHAX0n0nQrzB9BbjLnX2EYEfPWokXg2tx3l6xih5F6
UMr8Coukzwt67ZUcnVAFvZTMqyT91tpwbYRJF981vY9pOtJR9VEOHiUPpRm2p52P
/8STCNQaTe1BbXQczQjibDTv86xgwXfit9Nf1cywaorij0tDQ1o=
=u+85
-----END PGP SIGNATURE-----

```

---



=== Content from github.com_b4138042_20250115_080056.html ===

[Skip to content](#start-of-content)

## Navigation Menu

Toggle navigation

[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fscrapy%2Fscrapy%2Fcommit%2Fb01d69a1bf48060daec8f751368622352d8b85a6)

* Product

  + [GitHub Copilot
    Write better code with AI](https://github.com/features/copilot)
  + [Security
    Find and fix vulnerabilities](https://github.com/features/security)
  + [Actions
    Automate any workflow](https://github.com/features/actions)
  + [Codespaces
    Instant dev environments](https://github.com/features/codespaces)
  + [Issues
    Plan and track work](https://github.com/features/issues)
  + [Code Review
    Manage code changes](https://github.com/features/code-review)
  + [Discussions
    Collaborate outside of code](https://github.com/features/discussions)
  + [Code Search
    Find more, search less](https://github.com/features/code-search)

  Explore
  + [All features](https://github.com/features)
  + [Documentation](https://docs.github.com)
  + [GitHub Skills](https://skills.github.com)
  + [Blog](https://github.blog)
* Solutions

  By company size
  + [Enterprises](https://github.com/enterprise)
  + [Small and medium teams](https://github.com/team)
  + [Startups](https://github.com/enterprise/startups)
  By use case
  + [DevSecOps](/solutions/use-case/devsecops)
  + [DevOps](/solutions/use-case/devops)
  + [CI/CD](/solutions/use-case/ci-cd)
  + [View all use cases](/solutions/use-case)

  By industry
  + [Healthcare](/solutions/industry/healthcare)
  + [Financial services](/solutions/industry/financial-services)
  + [Manufacturing](/solutions/industry/manufacturing)
  + [Government](/solutions/industry/government)
  + [View all industries](/solutions/industry)

  [View all solutions](/solutions)
* Resources

  Topics
  + [AI](/resources/articles/ai)
  + [DevOps](/resources/articles/devops)
  + [Security](/resources/articles/security)
  + [Software Development](/resources/articles/software-development)
  + [View all](/resources/articles)

  Explore
  + [Learning Pathways](https://resources.github.com/learn/pathways)
  + [White papers, Ebooks, Webinars](https://resources.github.com)
  + [Customer Stories](https://github.com/customer-stories)
  + [Partners](https://partner.github.com)
  + [Executive Insights](https://github.com/solutions/executive-insights)
* Open Source

  + [GitHub Sponsors
    Fund open source developers](/sponsors)
  + [The ReadME Project
    GitHub community articles](https://github.com/readme)
  Repositories
  + [Topics](https://github.com/topics)
  + [Trending](https://github.com/trending)
  + [Collections](https://github.com/collections)
* Enterprise

  + [Enterprise platform
    AI-powered developer platform](/enterprise)
  Available add-ons
  + [Advanced Security
    Enterprise-grade security features](https://github.com/enterprise/advanced-security)
  + [GitHub Copilot
    Enterprise-grade AI features](/features/copilot#enterprise)
  + [Premium Support
    Enterprise-grade 24/7 support](/premium-support)
* [Pricing](https://github.com/pricing)

Search or jump to...

# Search code, repositories, users, issues, pull requests...

Search

Clear

[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)

# Provide feedback

We read every piece of feedback, and take your input very seriously.

Include my email address so I can be contacted

  Cancel

 Submit feedback

# Saved searches

## Use saved searches to filter your results more quickly

Name

Query

To see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).

  Cancel

 Create saved search

[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fscrapy%2Fscrapy%2Fcommit%2Fb01d69a1bf48060daec8f751368622352d8b85a6)

[Sign up](/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E%2Fvoltron%2Fcommit_fragments%2Frepo_layout&source=header-repo&source_repo=scrapy%2Fscrapy)
Reseting focus

You signed in with another tab or window. Reload to refresh your session.
You signed out in another tab or window. Reload to refresh your session.
You switched accounts on another tab or window. Reload to refresh your session.

Dismiss alert

{{ message }}

[scrapy](/scrapy)
/
**[scrapy](/scrapy/scrapy)**
Public

* [Notifications](/login?return_to=%2Fscrapy%2Fscrapy) You must be signed in to change notification settings
* [Fork
  10.6k](/login?return_to=%2Fscrapy%2Fscrapy)
* [Star
   53.8k](/login?return_to=%2Fscrapy%2Fscrapy)

* [Code](/scrapy/scrapy)
* [Issues
  427](/scrapy/scrapy/issues)
* [Pull requests
  182](/scrapy/scrapy/pulls)
* [Discussions](/scrapy/scrapy/discussions)
* [Actions](/scrapy/scrapy/actions)
* [Projects
  0](/scrapy/scrapy/projects)
* [Wiki](/scrapy/scrapy/wiki)
* [Security](/scrapy/scrapy/security)
* [Insights](/scrapy/scrapy/pulse)

Additional navigation options

* [Code](/scrapy/scrapy)
* [Issues](/scrapy/scrapy/issues)
* [Pull requests](/scrapy/scrapy/pulls)
* [Discussions](/scrapy/scrapy/discussions)
* [Actions](/scrapy/scrapy/actions)
* [Projects](/scrapy/scrapy/projects)
* [Wiki](/scrapy/scrapy/wiki)
* [Security](/scrapy/scrapy/security)
* [Insights](/scrapy/scrapy/pulse)

## Commit

[Permalink](/scrapy/scrapy/commit/b01d69a1bf48060daec8f751368622352d8b85a6)

This commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.

Add http\_auth\_domain to HttpAuthMiddleware.

[Browse files](/scrapy/scrapy/tree/b01d69a1bf48060daec8f751368622352d8b85a6)
Browse the repository at this point in the history

* Loading branch information

[![@wRAR](https://avatars.githubusercontent.com/u/241039?s=40&v=4)](/wRAR) [![@Gallaecio](https://avatars.githubusercontent.com/u/705211?s=40&v=4)](/Gallaecio)

[wRAR](/scrapy/scrapy/commits?author=wRAR "View all commits by wRAR")
authored and
[Gallaecio](/scrapy/scrapy/commits?author=Gallaecio "View all commits by Gallaecio")
committed
Oct 5, 2021

1 parent
[4183925](/scrapy/scrapy/commit/4183925b4b7114f772883c04e7cef44d766f0f80)

commit b01d69a

 Show file tree

 Hide file tree

Showing
**3 changed files**
with
**118 additions**
and
**6 deletions**.

* Whitespace
* Ignore whitespace

* Split
* Unified

* docs/topics

  + docs/topics/downloader-middleware.rst
    [downloader-middleware.rst](#diff-0b93523e8e3fd80c4ef4c935b5915356e7721de87a48818c675440f8f6169dfe)
* scrapy/downloadermiddlewares

  + scrapy/downloadermiddlewares/httpauth.py
    [httpauth.py](#diff-67882d4bd0f27f4148738bcb0a5eddd25b20aa3a9bad8bbd9829e702231a939c)
* tests

  + tests/test\_downloadermiddleware\_httpauth.py
    [test\_downloadermiddleware\_httpauth.py](#diff-81ad57e9663ea1c6b81ecf40a256aae4bb736890be826bf240b3a1ce1d09fe22)

## There are no files selected for viewing

18 changes: 16 additions & 2 deletions

18
[docs/topics/downloader-middleware.rst](#diff-0b93523e8e3fd80c4ef4c935b5915356e7721de87a48818c675440f8f6169dfe "docs/topics/downloader-middleware.rst")

Show comments

[View file](/scrapy/scrapy/blob/b01d69a1bf48060daec8f751368622352d8b85a6/docs/topics/downloader-middleware.rst)
Edit file

Delete file

This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters.
[Learn more about bidirectional Unicode characters](https://github.co/hiddenchars)

  [Show hidden characters](%7B%7B%20revealButtonHref%20%7D%7D)

| Original file line number | Diff line number | Diff line change |
| --- | --- | --- |

| Expand Up | | @@ -315,8 +315,21 @@ HttpAuthMiddleware |
|  |  | This middleware authenticates all requests generated from certain spiders |
|  |  | using `Basic access authentication`\_ (aka. HTTP auth). |
|  |  |  |
|  |  | To enable HTTP authentication from certain spiders, set the ``http\_user`` |
|  |  | and ``http\_pass`` attributes of those spiders. |
|  |  | To enable HTTP authentication for a spider, set the ``http\_user`` and |
|  |  | ``http\_pass`` spider attributes to the authentication data and the |
|  |  | ``http\_auth\_domain`` spider attribute to the domain which requires this |
|  |  | authentication (its subdomains will be also handled in the same way). |
|  |  | You can set ``http\_auth\_domain`` to ``None`` to enable the |
|  |  | authentication for all requests but usually this is not needed. |
|  |  |  |
|  |  | .. warning:: |
|  |  | In the previous Scrapy versions HttpAuthMiddleware sent the |
|  |  | authentication data with all requests, which is a security problem if |
|  |  | the spider makes requests to several different domains. Currently if |
|  |  | the ``http\_auth\_domain`` attribute is not set, the middleware will use |
|  |  | the domain of the first request, which will work for some spider but |
|  |  | not for others. In the future the middleware will produce an error |
|  |  | instead. |
|  |  |  |
|  |  | Example:: |
|  |  |  |
| Expand All | | @@ -326,6 +339,7 @@ HttpAuthMiddleware |
|  |  |  |
|  |  | http\_user = 'someuser' |
|  |  | http\_pass = 'somepass' |
|  |  | http\_auth\_domain = 'intranet.example.com' |
|  |  | name = 'intranet.example.com' |
|  |  |  |
|  |  | # .. rest of the spider code omitted ... |
| Expand Down | |  |

21 changes: 20 additions & 1 deletion

21
[scrapy/downloadermiddlewares/httpauth.py](#diff-67882d4bd0f27f4148738bcb0a5eddd25b20aa3a9bad8bbd9829e702231a939c "scrapy/downloadermiddlewares/httpauth.py")

Show comments

[View file](/scrapy/scrapy/blob/b01d69a1bf48060daec8f751368622352d8b85a6/scrapy/downloadermiddlewares/httpauth.py)
Edit file

Delete file

This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters.
[Learn more about bidirectional Unicode characters](https://github.co/hiddenchars)

  [Show hidden characters](%7B%7B%20revealButtonHref%20%7D%7D)

| Original file line number | Diff line number | Diff line change |
| --- | --- | --- |
| Expand Up | | @@ -3,10 +3,14 @@ |
|  |  |  |
|  |  | See documentation in docs/topics/downloader-middleware.rst |
|  |  | """ |
|  |  | import warnings |
|  |  |  |
|  |  | from w3lib.http import basic\_auth\_header |
|  |  |  |
|  |  | from scrapy import signals |
|  |  | from scrapy.exceptions import ScrapyDeprecationWarning |
|  |  | from scrapy.utils.httpobj import urlparse\_cached |
|  |  | from scrapy.utils.url import url\_is\_from\_any\_domain |
|  |  |  |
|  |  |  |
|  |  | class HttpAuthMiddleware(object): |
| Expand All | | @@ -24,8 +28,23 @@ def spider\_opened(self, spider): |
|  |  | pwd = getattr(spider, 'http\_pass', '') |
|  |  | if usr or pwd: |
|  |  | self.auth = basic\_auth\_header(usr, pwd) |
|  |  | if not hasattr(spider, 'http\_auth\_domain'): |
|  |  | warnings.warn('Using HttpAuthMiddleware without http\_auth\_domain is deprecated and can cause security ' |
|  |  | 'problems if the spider makes requests to several different domains. http\_auth\_domain ' |
|  |  | 'will be set to the domain of the first request, please set it to the correct value ' |
|  |  | 'explicitly.', |
|  |  | category=ScrapyDeprecationWarning) |
|  |  | self.domain\_unset = True |
|  |  | else: |
|  |  | self.domain = spider.http\_auth\_domain |
|  |  | self.domain\_unset = False |
|  |  |  |
|  |  | def process\_request(self, request, spider): |
|  |  | auth = getattr(self, 'auth', None) |
|  |  | if auth and b'Authorization' not in request.headers: |
|  |  | request.headers[b'Authorization'] = auth |
|  |  | domain = urlparse\_cached(request).hostname |
|  |  | if self.domain\_unset: |
|  |  | self.domain = domain |
|  |  | self.domain\_unset = False |
|  |  | if not self.domain or url\_is\_from\_any\_domain(request.url, [self.domain]): |
|  |  | request.headers[b'Authorization'] = auth |

85 changes: 82 additions & 3 deletions

85
[tests/test\_downloadermiddleware\_httpauth.py](#diff-81ad57e9663ea1c6b81ecf40a256aae4bb736890be826bf240b3a1ce1d09fe22 "tests/test_downloadermiddleware_httpauth.py")

Show comments

[View file](/scrapy/scrapy/blob/b01d69a1bf48060daec8f751368622352d8b85a6/tests/test_downloadermiddleware_httpauth.py)
Edit file

Delete file

This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters.
[Learn more about bidirectional Unicode characters](https://github.co/hiddenchars)

  [Show hidden characters](%7B%7B%20revealButtonHref%20%7D%7D)

| Original file line number | Diff line number | Diff line change |
| --- | --- | --- |
|  |  | @@ -1,13 +1,60 @@ |
|  |  | import unittest |
|  |  |  |
|  |  | from w3lib.http import basic\_auth\_header |
|  |  |  |
|  |  | from scrapy.http import Request |
|  |  | from scrapy.downloadermiddlewares.httpauth import HttpAuthMiddleware |
|  |  | from scrapy.spiders import Spider |
|  |  |  |
|  |  |  |
|  |  | class TestSpiderLegacy(Spider): |
|  |  | http\_user = 'foo' |
|  |  | http\_pass = 'bar' |
|  |  |  |
|  |  |  |
|  |  | class TestSpider(Spider): |
|  |  | http\_user = 'foo' |
|  |  | http\_pass = 'bar' |
|  |  | http\_auth\_domain = 'example.com' |
|  |  |  |
|  |  |  |
|  |  | class TestSpiderAny(Spider): |
|  |  | http\_user = 'foo' |
|  |  | http\_pass = 'bar' |
|  |  | http\_auth\_domain = None |
|  |  |  |
|  |  |  |
|  |  | class HttpAuthMiddlewareLegacyTest(unittest.TestCase): |
|  |  |  |
|  |  | def setUp(self): |
|  |  | self.spider = TestSpiderLegacy('foo') |
|  |  |  |
|  |  | def test\_auth(self): |
|  |  | mw = HttpAuthMiddleware() |
|  |  | mw.spider\_opened(self.spider) |
|  |  |  |
|  |  | # initial request, sets the domain and sends the header |
|  |  | req = Request('http://example.com/') |
|  |  | assert mw.process\_request(req, self.spider) is None |
|  |  | self.assertEqual(req.headers['Authorization'], basic\_auth\_header('foo', 'bar')) |
|  |  |  |
|  |  | # subsequent request to the same domain, should send the header |
|  |  | req = Request('http://example.com/') |
|  |  | assert mw.process\_request(req, self.spider) is None |
|  |  | self.assertEqual(req.headers['Authorization'], basic\_auth\_header('foo', 'bar')) |
|  |  |  |
|  |  | # subsequent request to a different domain, shouldn't send the header |
|  |  | req = Request('http://example-noauth.com/') |
|  |  | assert mw.process\_request(req, self.spider) is None |
|  |  | self.assertNotIn('Authorization', req.headers) |
|  |  |  |
|  |  | def test\_auth\_already\_set(self): |
|  |  | mw = HttpAuthMiddleware() |
|  |  | mw.spider\_opened(self.spider) |
|  |  | req = Request('http://example.com/', |
|  |  | headers=dict(Authorization='Digest 123')) |
|  |  | assert mw.process\_request(req, self.spider) is None |
|  |  | self.assertEqual(req.headers['Authorization'], b'Digest 123') |
|  |  |  |
|  |  |  |
|  |  | class HttpAuthMiddlewareTest(unittest.TestCase): |
| Expand All | | @@ -20,13 +67,45 @@ def setUp(self): |
|  |  | def tearDown(self): |
|  |  | del self.mw |
|  |  |  |
|  |  | def test\_no\_auth(self): |
|  |  | req = Request('http://example-noauth.com/') |
|  |  | assert self.mw.process\_request(req, self.spider) is None |
|  |  | self.assertNotIn('Authorization', req.headers) |
|  |  |  |
|  |  | def test\_auth\_domain(self): |
|  |  | req = Request('http://example.com/') |
|  |  | assert self.mw.process\_request(req, self.spider) is None |
|  |  | self.assertEqual(req.headers['Authorization'], basic\_auth\_header('foo', 'bar')) |
|  |  |  |
|  |  | def test\_auth\_subdomain(self): |
|  |  | req = Request('http://foo.example.com/') |
|  |  | assert self.mw.process\_request(req, self.spider) is None |
|  |  | self.assertEqual(req.headers['Authorization'], basic\_auth\_header('foo', 'bar')) |
|  |  |  |
|  |  | def test\_auth\_already\_set(self): |
|  |  | req = Request('http://example.com/', |
|  |  | headers=dict(Authorization='Digest 123')) |
|  |  | assert self.mw.process\_request(req, self.spider) is None |
|  |  | self.assertEqual(req.headers['Authorization'], b'Digest 123') |
|  |  |  |
|  |  |  |
|  |  | class HttpAuthAnyMiddlewareTest(unittest.TestCase): |
|  |  |  |
|  |  | def setUp(self): |
|  |  | self.mw = HttpAuthMiddleware() |
|  |  | self.spider = TestSpiderAny('foo') |
|  |  | self.mw.spider\_opened(self.spider) |
|  |  |  |
|  |  | def tearDown(self): |
|  |  | del self.mw |
|  |  |  |
|  |  | def test\_auth(self): |
|  |  | req = Request('http://scrapytest.org/') |
|  |  | req = Request('http://example.com/') |
|  |  | assert self.mw.process\_request(req, self.spider) is None |
|  |  | self.assertEqual(req.headers['Authorization'], b'Basic Zm9vOmJhcg==') |
|  |  | self.assertEqual(req.headers['Authorization'], basic\_auth\_header('foo', 'bar')) |
|  |  |  |
|  |  | def test\_auth\_already\_set(self): |
|  |  | req = Request('http://scrapytest.org/', |
|  |  | req = Request('http://example.com/', |
|  |  | headers=dict(Authorization='Digest 123')) |
|  |  | assert self.mw.process\_request(req, self.spider) is None |
|  |  | self.assertEqual(req.headers['Authorization'], b'Digest 123') |

Toggle all file notes
Toggle all file annotations

### 0 comments on commit `b01d69a`

Please
[sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fscrapy%2Fscrapy%2Fcommit%2Fb01d69a1bf48060daec8f751368622352d8b85a6) to comment.

## Footer

© 2025 GitHub, Inc.

### Footer navigation

* [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)
* [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)
* [Security](https://github.com/security)
* [Status](https://www.githubstatus.com/)
* [Docs](https://docs.github.com/)
* [Contact](https://support.github.com?tags=dotcom-footer)
* Manage cookies
* Do not share my personal information

You can’t perform that action at this time.



=== Content from w3lib.readthedocs.io_490324b1_20250115_080057.html ===

[w3lib](index.html)

* w3lib Package
  + [`encoding` Module](#module-w3lib.encoding)
    - [`html_body_declared_encoding()`](#w3lib.encoding.html_body_declared_encoding)
    - [`html_to_unicode()`](#w3lib.encoding.html_to_unicode)
    - [`http_content_type_encoding()`](#w3lib.encoding.http_content_type_encoding)
    - [`read_bom()`](#w3lib.encoding.read_bom)
    - [`resolve_encoding()`](#w3lib.encoding.resolve_encoding)
    - [`to_unicode()`](#w3lib.encoding.to_unicode)
  + [`html` Module](#module-w3lib.html)
    - [`get_base_url()`](#w3lib.html.get_base_url)
    - [`get_meta_refresh()`](#w3lib.html.get_meta_refresh)
    - [`remove_comments()`](#w3lib.html.remove_comments)
    - [`remove_tags()`](#w3lib.html.remove_tags)
    - [`remove_tags_with_content()`](#w3lib.html.remove_tags_with_content)
    - [`replace_entities()`](#w3lib.html.replace_entities)
    - [`replace_escape_chars()`](#w3lib.html.replace_escape_chars)
    - [`replace_tags()`](#w3lib.html.replace_tags)
    - [`strip_html5_whitespace()`](#w3lib.html.strip_html5_whitespace)
    - [`unquote_markup()`](#w3lib.html.unquote_markup)
  + [`http` Module](#module-w3lib.http)
    - [`basic_auth_header()`](#w3lib.http.basic_auth_header)
    - [`headers_dict_to_raw()`](#w3lib.http.headers_dict_to_raw)
    - [`headers_raw_to_dict()`](#w3lib.http.headers_raw_to_dict)
  + [`url` Module](#module-w3lib.url)
    - [`ParseDataURIResult`](#w3lib.url.ParseDataURIResult)
      * [`ParseDataURIResult.data`](#w3lib.url.ParseDataURIResult.data)
      * [`ParseDataURIResult.media_type`](#w3lib.url.ParseDataURIResult.media_type)
      * [`ParseDataURIResult.media_type_parameters`](#w3lib.url.ParseDataURIResult.media_type_parameters)
    - [`add_or_replace_parameter()`](#w3lib.url.add_or_replace_parameter)
    - [`add_or_replace_parameters()`](#w3lib.url.add_or_replace_parameters)
    - [`any_to_uri()`](#w3lib.url.any_to_uri)
    - [`canonicalize_url()`](#w3lib.url.canonicalize_url)
    - [`file_uri_to_path()`](#w3lib.url.file_uri_to_path)
    - [`parse_data_uri()`](#w3lib.url.parse_data_uri)
    - [`path_to_file_uri()`](#w3lib.url.path_to_file_uri)
    - [`safe_download_url()`](#w3lib.url.safe_download_url)
    - [`safe_url_string()`](#w3lib.url.safe_url_string)
    - [`url_query_cleaner()`](#w3lib.url.url_query_cleaner)
    - [`url_query_parameter()`](#w3lib.url.url_query_parameter)

[w3lib](index.html)

* w3lib Package
* [View page source](_sources/w3lib.rst.txt)

---

# w3lib Package[](#w3lib-package "Link to this heading")

## [`encoding`](#module-w3lib.encoding "w3lib.encoding") Module[](#module-w3lib.encoding "Link to this heading")

Functions for handling encoding of web pages

w3lib.encoding.html\_body\_declared\_encoding(*html\_body\_str: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [bytes](https://docs.python.org/3/library/stdtypes.html#bytes "(in Python v3.13)")*) → [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[[source]](_modules/w3lib/encoding.html#html_body_declared_encoding)[](#w3lib.encoding.html_body_declared_encoding "Link to this definition")

Return the encoding specified in meta tags in the html body,
or `None` if no suitable encoding was found

```
>>> import w3lib.encoding
>>> w3lib.encoding.html_body_declared_encoding(
... """<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
...      "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
... <html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
... <head>
...     <title>Some title</title>
...     <meta http-equiv="content-type" content="text/html;charset=utf-8" />
... </head>
... <body>
... ...
... </body>
... </html>""")
'utf-8'
>>>

```

w3lib.encoding.html\_to\_unicode(*content\_type\_header: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")*, *html\_body\_str: [bytes](https://docs.python.org/3/library/stdtypes.html#bytes "(in Python v3.13)")*, *default\_encoding: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") = 'utf8'*, *auto\_detect\_fun: [Callable](https://docs.python.org/3/library/typing.html#typing.Callable "(in Python v3.13)")[[[bytes](https://docs.python.org/3/library/stdtypes.html#bytes "(in Python v3.13)")], [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")] | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*) → [Tuple](https://docs.python.org/3/library/typing.html#typing.Tuple "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")][[source]](_modules/w3lib/encoding.html#html_to_unicode)[](#w3lib.encoding.html_to_unicode "Link to this definition")

Convert raw html bytes to unicode

This attempts to make a reasonable guess at the content encoding of the
html body, following a similar process to a web browser.

It will try in order:

* BOM (byte-order mark)
* http content type header
* meta or xml tag declarations
* auto-detection, if the auto\_detect\_fun keyword argument is not `None`
* default encoding in keyword arg (which defaults to utf8)

If an encoding other than the auto-detected or default encoding is used,
overrides will be applied, converting some character encodings to more
suitable alternatives.

If a BOM is found matching the encoding, it will be stripped.

The auto\_detect\_fun argument can be used to pass a function that will
sniff the encoding of the text. This function must take the raw text as an
argument and return the name of an encoding that python can process, or
None. To use chardet, for example, you can define the function as:

```
auto_detect_fun=lambda x: chardet.detect(x).get('encoding')

```

or to use UnicodeDammit (shipped with the BeautifulSoup library):

```
auto_detect_fun=lambda x: UnicodeDammit(x).originalEncoding

```

If the locale of the website or user language preference is known, then a
better default encoding can be supplied.

If content\_type\_header is not present, `None` can be passed signifying
that the header was not present.

This method will not fail, if characters cannot be converted to unicode,
`\\ufffd` (the unicode replacement character) will be inserted instead.

Returns a tuple of `(<encoding used>, <unicode_string>)`

Examples:

```
>>> import w3lib.encoding
>>> w3lib.encoding.html_to_unicode(None,
... b"""<!DOCTYPE html>
... <head>
... <meta charset="UTF-8" />
... <meta name="viewport" content="width=device-width" />
... <title>Creative Commons France</title>
... <link rel='canonical' href='http://creativecommons.fr/' />
... <body>
... <p>Creative Commons est une organisation \xc3\xa0 but non lucratif
... qui a pour dessein de faciliter la diffusion et le partage des oeuvres
... tout en accompagnant les nouvelles pratiques de cr\xc3\xa9ation \xc3\xa0 l\xe2\x80\x99\xc3\xa8re numerique.</p>
... </body>
... </html>""")
('utf-8', '<!DOCTYPE html>\n<head>\n<meta charset="UTF-8" />\n<meta name="viewport" content="width=device-width" />\n<title>Creative Commons France</title>\n<link rel=\'canonical\' href=\'http://creativecommons.fr/\' />\n<body>\n<p>Creative Commons est une organisation \xe0 but non lucratif\nqui a pour dessein de faciliter la diffusion et le partage des oeuvres\ntout en accompagnant les nouvelles pratiques de cr\xe9ation \xe0 l\u2019\xe8re numerique.</p>\n</body>\n</html>')
>>>

```

w3lib.encoding.http\_content\_type\_encoding(*content\_type: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")*) → [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[[source]](_modules/w3lib/encoding.html#http_content_type_encoding)[](#w3lib.encoding.http_content_type_encoding "Link to this definition")

Extract the encoding in the content-type header

```
>>> import w3lib.encoding
>>> w3lib.encoding.http_content_type_encoding("Content-Type: text/html; charset=ISO-8859-4")
'iso8859-4'

```

w3lib.encoding.read\_bom(*data: [bytes](https://docs.python.org/3/library/stdtypes.html#bytes "(in Python v3.13)")*) → [Tuple](https://docs.python.org/3/library/typing.html#typing.Tuple "(in Python v3.13)")[[None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)"), [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")] | [Tuple](https://docs.python.org/3/library/typing.html#typing.Tuple "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [bytes](https://docs.python.org/3/library/stdtypes.html#bytes "(in Python v3.13)")][[source]](_modules/w3lib/encoding.html#read_bom)[](#w3lib.encoding.read_bom "Link to this definition")

Read the byte order mark in the text, if present, and
return the encoding represented by the BOM and the BOM.

If no BOM can be detected, `(None, None)` is returned.

```
>>> import w3lib.encoding
>>> w3lib.encoding.read_bom(b'\xfe\xff\x6c\x34')
('utf-16-be', '\xfe\xff')
>>> w3lib.encoding.read_bom(b'\xff\xfe\x34\x6c')
('utf-16-le', '\xff\xfe')
>>> w3lib.encoding.read_bom(b'\x00\x00\xfe\xff\x00\x00\x6c\x34')
('utf-32-be', '\x00\x00\xfe\xff')
>>> w3lib.encoding.read_bom(b'\xff\xfe\x00\x00\x34\x6c\x00\x00')
('utf-32-le', '\xff\xfe\x00\x00')
>>> w3lib.encoding.read_bom(b'\x01\x02\x03\x04')
(None, None)
>>>

```

w3lib.encoding.resolve\_encoding(*encoding\_alias: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*) → [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[[source]](_modules/w3lib/encoding.html#resolve_encoding)[](#w3lib.encoding.resolve_encoding "Link to this definition")

Return the encoding that encoding\_alias maps to, or `None`
if the encoding cannot be interpreted

```
>>> import w3lib.encoding
>>> w3lib.encoding.resolve_encoding('latin1')
'cp1252'
>>> w3lib.encoding.resolve_encoding('gb_2312-80')
'gb18030'
>>>

```

w3lib.encoding.to\_unicode(*data\_str: [bytes](https://docs.python.org/3/library/stdtypes.html#bytes "(in Python v3.13)")*, *encoding: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*) → [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")[[source]](_modules/w3lib/encoding.html#to_unicode)[](#w3lib.encoding.to_unicode "Link to this definition")

Convert a str object to unicode using the encoding given

Characters that cannot be converted will be converted to `\ufffd` (the
unicode replacement character).

## [`html`](#module-w3lib.html "w3lib.html") Module[](#module-w3lib.html "Link to this heading")

Functions for dealing with markup text

w3lib.html.get\_base\_url(*text: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [bytes](https://docs.python.org/3/library/stdtypes.html#bytes "(in Python v3.13)")*, *baseurl: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [bytes](https://docs.python.org/3/library/stdtypes.html#bytes "(in Python v3.13)") = ''*, *encoding: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") = 'utf-8'*) → [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")[[source]](_modules/w3lib/html.html#get_base_url)[](#w3lib.html.get_base_url "Link to this definition")

Return the base url if declared in the given HTML text,
relative to the given base url.

If no base url is found, the given baseurl is returned.

w3lib.html.get\_meta\_refresh(*text: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [bytes](https://docs.python.org/3/library/stdtypes.html#bytes "(in Python v3.13)")*, *baseurl: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") = ''*, *encoding: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") = 'utf-8'*, *ignore\_tags: [Iterable](https://docs.python.org/3/library/typing.html#typing.Iterable "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")] = ('script', 'noscript')*) → [Tuple](https://docs.python.org/3/library/typing.html#typing.Tuple "(in Python v3.13)")[[None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)"), [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")] | [Tuple](https://docs.python.org/3/library/typing.html#typing.Tuple "(in Python v3.13)")[[float](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)"), [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")][[source]](_modules/w3lib/html.html#get_meta_refresh)[](#w3lib.html.get_meta_refresh "Link to this definition")

Return the http-equiv parameter of the HTML meta element from the given
HTML text and return a tuple `(interval, url)` where interval is an integer
containing the delay in seconds (or zero if not present) and url is a
string with the absolute url to redirect.

If no meta redirect is found, `(None, None)` is returned.

w3lib.html.remove\_comments(*text: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [bytes](https://docs.python.org/3/library/stdtypes.html#bytes "(in Python v3.13)")*, *encoding: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*) → [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")[[source]](_modules/w3lib/html.html#remove_comments)[](#w3lib.html.remove_comments "Link to this definition")

Remove HTML Comments.

```
>>> import w3lib.html
>>> w3lib.html.remove_comments(b"test <!--textcoment--> whatever")
'test  whatever'
>>>

```

w3lib.html.remove\_tags(*text: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [bytes](https://docs.python.org/3/library/stdtypes.html#bytes "(in Python v3.13)")*, *which\_ones: [Iterable](https://docs.python.org/3/library/typing.html#typing.Iterable "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")] = ()*, *keep: [Iterable](https://docs.python.org/3/library/typing.html#typing.Iterable "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")] = ()*, *encoding: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*) → [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")[[source]](_modules/w3lib/html.html#remove_tags)[](#w3lib.html.remove_tags "Link to this definition")

Remove HTML Tags only.

which\_ones and keep are both tuples, there are four cases:

| `which_ones` | `keep` | what it does |
| --- | --- | --- |
| **not empty** | empty | remove all tags in `which_ones` |
| empty | **not empty** | remove all tags except the ones in `keep` |
| empty | empty | remove all tags |
| **not empty** | **not empty** | not allowed |

Remove all tags:

```
>>> import w3lib.html
>>> doc = '<div><p><b>This is a link:</b> <a href="http://www.example.com">example</a></p></div>'
>>> w3lib.html.remove_tags(doc)
'This is a link: example'
>>>

```

Keep only some tags:

```
>>> w3lib.html.remove_tags(doc, keep=('div',))
'<div>This is a link: example</div>'
>>>

```

Remove only specific tags:

```
>>> w3lib.html.remove_tags(doc, which_ones=('a','b'))
'<div><p>This is a link: example</p></div>'
>>>

```

You can’t remove some and keep some:

```
>>> w3lib.html.remove_tags(doc, which_ones=('a',), keep=('p',))
Traceback (most recent call last):
    ...
ValueError: Cannot use both which_ones and keep
>>>

```

w3lib.html.remove\_tags\_with\_content(*text: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [bytes](https://docs.python.org/3/library/stdtypes.html#bytes "(in Python v3.13)")*, *which\_ones: [Iterable](https://docs.python.org/3/library/typing.html#typing.Iterable "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")] = ()*, *encoding: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*) → [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")[[source]](_modules/w3lib/html.html#remove_tags_with_content)[](#w3lib.html.remove_tags_with_content "Link to this definition")

Remove tags and their content.

which\_ones is a tuple of which tags to remove including their content.
If is empty, returns the string unmodified.

```
>>> import w3lib.html
>>> doc = '<div><p><b>This is a link:</b> <a href="http://www.example.com">example</a></p></div>'
>>> w3lib.html.remove_tags_with_content(doc, which_ones=('b',))
'<div><p> <a href="http://www.example.com">example</a></p></div>'
>>>

```

w3lib.html.replace\_entities(*text: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [bytes](https://docs.python.org/3/library/stdtypes.html#bytes "(in Python v3.13)")*, *keep: [Iterable](https://docs.python.org/3/library/typing.html#typing.Iterable "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")] = ()*, *remove\_illegal: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)") = True*, *encoding: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") = 'utf-8'*) → [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")[[source]](_modules/w3lib/html.html#replace_entities)[](#w3lib.html.replace_entities "Link to this definition")

Remove entities from the given text by converting them to their
corresponding unicode character.

text can be a unicode string or a byte string encoded in the given
encoding (which defaults to ‘utf-8’).

If keep is passed (with a list of entity names) those entities will
be kept (they won’t be removed).

It supports both numeric entities (`&#nnnn;` and `&#hhhh;`)
and named entities (such as `&nbsp;` or `&gt;`).

If remove\_illegal is `True`, entities that can’t be converted are removed.
If remove\_illegal is `False`, entities that can’t be converted are kept “as
is”. For more information see the tests.

Always returns a unicode string (with the entities removed).

```
>>> import w3lib.html
>>> w3lib.html.replace_entities(b'Price: &pound;100')
'Price: \xa3100'
>>> print(w3lib.html.replace_entities(b'Price: &pound;100'))
Price: £100
>>>

```

w3lib.html.replace\_escape\_chars(*text: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [bytes](https://docs.python.org/3/library/stdtypes.html#bytes "(in Python v3.13)")*, *which\_ones: [Iterable](https://docs.python.org/3/library/typing.html#typing.Iterable "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")] = ('\n', '\t', '\r')*, *replace\_by: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [bytes](https://docs.python.org/3/library/stdtypes.html#bytes "(in Python v3.13)") = ''*, *encoding: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*) → [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")[[source]](_modules/w3lib/html.html#replace_escape_chars)[](#w3lib.html.replace_escape_chars "Link to this definition")

Remove escape characters.

which\_ones is a tuple of which escape characters we want to remove.
By default removes `\n`, `\t`, `\r`.

replace\_by is the string to replace the escape characters by.
It defaults to `''`, meaning the escape characters are removed.

w3lib.html.replace\_tags(*text: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [bytes](https://docs.python.org/3/library/stdtypes.html#bytes "(in Python v3.13)")*, *token: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") = ''*, *encoding: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*) → [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")[[source]](_modules/w3lib/html.html#replace_tags)[](#w3lib.html.replace_tags "Link to this definition")

Replace all markup tags found in the given text by the given token.
By default token is an empty string so it just removes all tags.

text can be a unicode string or a regular string encoded as encoding
(or `'utf-8'` if encoding is not given.)

Always returns a unicode string.

Examples:

```
>>> import w3lib.html
>>> w3lib.html.replace_tags('This text contains <a>some tag</a>')
'This text contains some tag'
>>> w3lib.html.replace_tags('<p>Je ne parle pas <b>fran\xe7ais</b></p>', ' -- ', 'latin-1')
' -- Je ne parle pas  -- fran\xe7ais --  -- '
>>>

```

w3lib.html.strip\_html5\_whitespace(*text: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*) → [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")[[source]](_modules/w3lib/html.html#strip_html5_whitespace)[](#w3lib.html.strip_html5_whitespace "Link to this definition")

Strip all leading and trailing space characters (as defined in
<https://www.w3.org/TR/html5/infrastructure.html#space-character>).

Such stripping is useful e.g. for processing HTML element attributes which
contain URLs, like `href`, `src` or form `action` - HTML5 standard
defines them as “valid URL potentially surrounded by spaces”
or “valid non-empty URL potentially surrounded by spaces”.

```
>>> strip_html5_whitespace(' hello\n')
'hello'

```

w3lib.html.unquote\_markup(*text: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [bytes](https://docs.python.org/3/library/stdtypes.html#bytes "(in Python v3.13)")*, *keep: [Iterable](https://docs.python.org/3/library/typing.html#typing.Iterable "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")] = ()*, *remove\_illegal: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)") = True*, *encoding: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*) → [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")[[source]](_modules/w3lib/html.html#unquote_markup)[](#w3lib.html.unquote_markup "Link to this definition")

This function receives markup as a text (always a unicode string or
a UTF-8 encoded string) and does the following:

1. removes entities (except the ones in keep) from any part of it

   that is not inside a CDATA
2. searches for CDATAs and extracts their text (if any) without modifying it.
3. removes the found CDATAs

## [`http`](#module-w3lib.http "w3lib.http") Module[](#module-w3lib.http "Link to this heading")

w3lib.http.basic\_auth\_header(*username: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [bytes](https://docs.python.org/3/library/stdtypes.html#bytes "(in Python v3.13)")*, *password: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [bytes](https://docs.python.org/3/library/stdtypes.html#bytes "(in Python v3.13)")*, *encoding: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") = 'ISO-8859-1'*) → [bytes](https://docs.python.org/3/library/stdtypes.html#bytes "(in Python v3.13)")[[source]](_modules/w3lib/http.html#basic_auth_header)[](#w3lib.http.basic_auth_header "Link to this definition")

Return an Authorization header field value for [HTTP Basic Access Authentication (RFC 2617)](http://www.ietf.org/rfc/rfc2617.txt)

```
>>> import w3lib.http
>>> w3lib.http.basic_auth_header('someuser', 'somepass')
'Basic c29tZXVzZXI6c29tZXBhc3M='

```

w3lib.http.headers\_dict\_to\_raw(*headers\_dict: [Mapping](https://docs.python.org/3/library/typing.html#typing.Mapping "(in Python v3.13)")[[bytes](https://docs.python.org/3/library/stdtypes.html#bytes "(in Python v3.13)"), [Any](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)") | [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[bytes](https://docs.python.org/3/library/stdtypes.html#bytes "(in Python v3.13)")]]*) → [bytes](https://docs.python.org/3/library/stdtypes.html#bytes "(in Python v3.13)")[[source]](_modules/w3lib/http.html#headers_dict_to_raw)[](#w3lib.http.headers_dict_to_raw "Link to this definition")
w3lib.http.headers\_dict\_to\_raw(*headers\_dict: [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")

Returns a raw HTTP headers representation of headers

For example:

```
>>> import w3lib.http
>>> w3lib.http.headers_dict_to_raw({b'Content-type': b'text/html', b'Accept': b'gzip'})
'Content-type: text/html\\r\\nAccept: gzip'
>>>

```

Note that keys and values must be bytes.

Argument is `None` (returns `None`):

```
>>> w3lib.http.headers_dict_to_raw(None)
>>>

```

w3lib.http.headers\_raw\_to\_dict(*headers\_raw: [bytes](https://docs.python.org/3/library/stdtypes.html#bytes "(in Python v3.13)")*) → [MutableMapping](https://docs.python.org/3/library/typing.html#typing.MutableMapping "(in Python v3.13)")[[bytes](https://docs.python.org/3/library/stdtypes.html#bytes "(in Python v3.13)"), [List](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.13)")[[bytes](https://docs.python.org/3/library/stdtypes.html#bytes "(in Python v3.13)")]][[source]](_modules/w3lib/http.html#headers_raw_to_dict)[](#w3lib.http.headers_raw_to_dict "Link to this definition")
w3lib.http.headers\_raw\_to\_dict(*headers\_raw: [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")*) → [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")

Convert raw headers (single multi-line bytestring)
to a dictionary.

For example:

```
>>> import w3lib.http
>>> w3lib.http.headers_raw_to_dict(b"Content-type: text/html\n\rAccept: gzip\n\n")
{'Content-type': ['text/html'], 'Accept': ['gzip']}

```

Incorrect input:

```
>>> w3lib.http.headers_raw_to_dict(b"Content-typt gzip\n\n")
{}
>>>

```

Argument is `None` (return `None`):

```
>>> w3lib.http.headers_raw_to_dict(None)
>>>

```

## [`url`](#module-w3lib.url "w3lib.url") Module[](#module-w3lib.url "Link to this heading")

This module contains general purpose URL functions not found in the standard
library.

*class* w3lib.url.ParseDataURIResult(*media\_type: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, *media\_type\_parameters: [Dict](https://docs.python.org/3/library/typing.html#typing.Dict "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")]*, *data: [bytes](https://docs.python.org/3/library/stdtypes.html#bytes "(in Python v3.13)")*)[[source]](_modules/w3lib/url.html#ParseDataURIResult)[](#w3lib.url.ParseDataURIResult "Link to this definition")

Named tuple returned by [`parse_data_uri()`](#w3lib.url.parse_data_uri "w3lib.url.parse_data_uri").

data*: [bytes](https://docs.python.org/3/library/stdtypes.html#bytes "(in Python v3.13)")*[](#w3lib.url.ParseDataURIResult.data "Link to this definition")

Data, decoded if it was encoded in base64 format.

media\_type*: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*[](#w3lib.url.ParseDataURIResult.media_type "Link to this definition")

MIME type type and subtype, separated by / (e.g. `"text/plain"`).

media\_type\_parameters*: [Dict](https://docs.python.org/3/library/typing.html#typing.Dict "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")]*[](#w3lib.url.ParseDataURIResult.media_type_parameters "Link to this definition")

MIME type parameters (e.g. `{"charset": "US-ASCII"}`).

w3lib.url.add\_or\_replace\_parameter(*url: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, *name: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, *new\_value: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*) → [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")[[source]](_modules/w3lib/url.html#add_or_replace_parameter)[](#w3lib.url.add_or_replace_parameter "Link to this definition")

Add or remove a parameter to a given url

```
>>> import w3lib.url
>>> w3lib.url.add_or_replace_parameter('http://www.example.com/index.php', 'arg', 'v')
'http://www.example.com/index.php?arg=v'
>>> w3lib.url.add_or_replace_parameter('http://www.example.com/index.php?arg1=v1&arg2=v2&arg3=v3', 'arg4', 'v4')
'http://www.example.com/index.php?arg1=v1&arg2=v2&arg3=v3&arg4=v4'
>>> w3lib.url.add_or_replace_parameter('http://www.example.com/index.php?arg1=v1&arg2=v2&arg3=v3', 'arg3', 'v3new')
'http://www.example.com/index.php?arg1=v1&arg2=v2&arg3=v3new'
>>>

```

w3lib.url.add\_or\_replace\_parameters(*url: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, *new\_parameters: [Dict](https://docs.python.org/3/library/typing.html#typing.Dict "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)"), [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")]*) → [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")[[source]](_modules/w3lib/url.html#add_or_replace_parameters)[](#w3lib.url.add_or_replace_parameters "Link to this definition")

Add or remove a parameters to a given url

```
>>> import w3lib.url
>>> w3lib.url.add_or_replace_parameters('http://www.example.com/index.php', {'arg': 'v'})
'http://www.example.com/index.php?arg=v'
>>> args = {'arg4': 'v4', 'arg3': 'v3new'}
>>> w3lib.url.add_or_replace_parameters('http://www.example.com/index.php?arg1=v1&arg2=v2&arg3=v3', args)
'http://www.example.com/index.php?arg1=v1&arg2=v2&arg3=v3new&arg4=v4'
>>>

```

w3lib.url.any\_to\_uri(*uri\_or\_path: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*) → [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")[[source]](_modules/w3lib/url.html#any_to_uri)[](#w3lib.url.any_to_uri "Link to this definition")

If given a path name, return its File URI, otherwise return it
unmodified

w3lib.url.canonicalize\_url(*url: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [bytes](https://docs.python.org/3/library/stdtypes.html#bytes "(in Python v3.13)") | [ParseResult](https://docs.python.org/3/library/urllib.parse.html#urllib.parse.ParseResult "(in Python v3.13)")*, *keep\_blank\_values: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)") = True*, *keep\_fragments: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)") = False*, *encoding: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*) → [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")[[source]](_modules/w3lib/url.html#canonicalize_url)[](#w3lib.url.canonicalize_url "Link to this definition")

Canonicalize the given url by applying the following procedures:

* make the URL safe
* sort query arguments, first by key, then by value
* normalize all spaces (in query arguments) ‘+’ (plus symbol)
* normalize percent encodings case (%2f -> %2F)
* remove query arguments with blank values (unless keep\_blank\_values is True)
* remove fragments (unless keep\_fragments is True)

The url passed can be bytes or unicode, while the url returned is
always a native str (bytes in Python 2, unicode in Python 3).

```
>>> import w3lib.url
>>>
>>> # sorting query arguments
>>> w3lib.url.canonicalize_url('http://www.example.com/do?c=3&b=5&b=2&a=50')
'http://www.example.com/do?a=50&b=2&b=5&c=3'
>>>
>>> # UTF-8 conversion + percent-encoding of non-ASCII characters
>>> w3lib.url.canonicalize_url('http://www.example.com/r\u00e9sum\u00e9')
'http://www.example.com/r%C3%A9sum%C3%A9'
>>>

```

For more examples, see the tests in tests/test\_url.py.

w3lib.url.file\_uri\_to\_path(*uri: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*) → [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")[[source]](_modules/w3lib/url.html#file_uri_to_path)[](#w3lib.url.file_uri_to_path "Link to this definition")

Convert File URI to local filesystem path according to:
<http://en.wikipedia.org/wiki/File_URI_scheme>

w3lib.url.parse\_data\_uri(*uri: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [bytes](https://docs.python.org/3/library/stdtypes.html#bytes "(in Python v3.13)")*) → [ParseDataURIResult](#w3lib.url.ParseDataURIResult "w3lib.url.ParseDataURIResult")[[source]](_modules/w3lib/url.html#parse_data_uri)[](#w3lib.url.parse_data_uri "Link to this definition")

Parse a data: URI into [`ParseDataURIResult`](#w3lib.url.ParseDataURIResult "w3lib.url.ParseDataURIResult").

w3lib.url.path\_to\_file\_uri(*path: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*) → [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")[[source]](_modules/w3lib/url.html#path_to_file_uri)[](#w3lib.url.path_to_file_uri "Link to this definition")

Convert local filesystem path to legal File URIs as described in:
<http://en.wikipedia.org/wiki/File_URI_scheme>

w3lib.url.safe\_download\_url(*url: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [bytes](https://docs.python.org/3/library/stdtypes.html#bytes "(in Python v3.13)")*, *encoding: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") = 'utf8'*, *path\_encoding: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") = 'utf8'*) → [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")[[source]](_modules/w3lib/url.html#safe_download_url)[](#w3lib.url.safe_download_url "Link to this definition")

Make a url for download. This will call safe\_url\_string
and then strip the fragment, if one exists. The path will
be normalised.

If the path is outside the document root, it will be changed
to be within the document root.

w3lib.url.safe\_url\_string(*url: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [bytes](https://docs.python.org/3/library/stdtypes.html#bytes "(in Python v3.13)")*, *encoding: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") = 'utf8'*, *path\_encoding: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") = 'utf8'*, *quote\_path: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)") = True*) → [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")[[source]](_modules/w3lib/url.html#safe_url_string)[](#w3lib.url.safe_url_string "Link to this definition")

Return a URL equivalent to *url* that a wide range of web browsers and
web servers consider valid.

*url* is parsed according to the rules of the [URL living standard](https://url.spec.whatwg.org/),
and during serialization additional characters are percent-encoded to make
the URL valid by additional URL standards.

The returned URL should be valid by *all* of the following URL standards
known to be enforced by modern-day web browsers and web servers:

* [URL living standard](https://url.spec.whatwg.org/)
* [RFC 3986](https://www.ietf.org/rfc/rfc3986.txt)
* [RFC 2396](https://www.ietf.org/rfc/rfc2396.txt) and [RFC 2732](https://www.ietf.org/rfc/rfc2732.txt), as interpreted by [Java 8’s java.net.URI
  class](https://docs.oracle.com/javase/8/docs/api/java/net/URI.html).

If a bytes URL is given, it is first converted to str using the given
encoding (which defaults to ‘utf-8’). If quote\_path is True (default),
path\_encoding (‘utf-8’ by default) is used to encode URL path component
which is then quoted. Otherwise, if quote\_path is False, path component
is not encoded or quoted. Given encoding is used for query string
or form data.

When passing an encoding, you should use the encoding of the
original page (the page from which the URL was extracted from).

Calling this function on an already “safe” URL will return the URL
unmodified.

w3lib.url.url\_query\_cleaner(*url: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [bytes](https://docs.python.org/3/library/stdtypes.html#bytes "(in Python v3.13)")*, *parameterlist: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [bytes](https://docs.python.org/3/library/stdtypes.html#bytes "(in Python v3.13)") | [Sequence](https://docs.python.org/3/library/typing.html#typing.Sequence "(in Python v3.13)")[[str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [bytes](https://docs.python.org/3/library/stdtypes.html#bytes "(in Python v3.13)")] = ()*, *sep: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") = '&'*, *kvsep: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") = '='*, *remove: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)") = False*, *unique: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)") = True*, *keep\_fragments: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)") = False*) → [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")[[source]](_modules/w3lib/url.html#url_query_cleaner)[](#w3lib.url.url_query_cleaner "Link to this definition")

Clean URL arguments leaving only those passed in the parameterlist keeping order

```
>>> import w3lib.url
>>> w3lib.url.url_query_cleaner("product.html?id=200&foo=bar&name=wired", ('id',))
'product.html?id=200'
>>> w3lib.url.url_query_cleaner("product.html?id=200&foo=bar&name=wired", ['id', 'name'])
'product.html?id=200&name=wired'
>>>

```

If unique is `False`, do not remove duplicated keys

```
>>> w3lib.url.url_query_cleaner("product.html?d=1&e=b&d=2&d=3&other=other", ['d'], unique=False)
'product.html?d=1&d=2&d=3'
>>>

```

If remove is `True`, leave only those **not in parameterlist**.

```
>>> w3lib.url.url_query_cleaner("product.html?id=200&foo=bar&name=wired", ['id'], remove=True)
'product.html?foo=bar&name=wired'
>>> w3lib.url.url_query_cleaner("product.html?id=2&foo=bar&name=wired", ['id', 'foo'], remove=True)
'product.html?name=wired'
>>>

```

By default, URL fragments are removed. If you need to preserve fragments,
pass the `keep_fragments` argument as `True`.

```
>>> w3lib.url.url_query_cleaner('http://domain.tld/?bla=123#123123', ['bla'], remove=True, keep_fragments=True)
'http://domain.tld/#123123'

```

w3lib.url.url\_query\_parameter(*url: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [bytes](https://docs.python.org/3/library/stdtypes.html#bytes "(in Python v3.13)")*, *parameter: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, *default: [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") = None*, *keep\_blank\_values: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)") | [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)") = 0*) → [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[[source]](_modules/w3lib/url.html#url_query_parameter)[](#w3lib.url.url_query_parameter "Link to this definition")
w3lib.url.url\_query\_parameter(*url: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [bytes](https://docs.python.org/3/library/stdtypes.html#bytes "(in Python v3.13)")*, *parameter: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, *default: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")*, *keep\_blank\_values: [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)") | [int](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)") = 0*) → [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")

Return the value of a url parameter, given the url and parameter name

General case:

```
>>> import w3lib.url
>>> w3lib.url.url_query_parameter("product.html?id=200&foo=bar", "id")
'200'
>>>

```

Return a default value if the parameter is not found:

```
>>> w3lib.url.url_query_parameter("product.html?id=200&foo=bar", "notthere", "mydefault")
'mydefault'
>>>

```

Returns None if keep\_blank\_values not set or 0 (default):

```
>>> w3lib.url.url_query_parameter("product.html?id=", "id")
>>>

```

Returns an empty string if keep\_blank\_values set to 1:

```
>>> w3lib.url.url_query_parameter("product.html?id=", "id", keep_blank_values=1)
''
>>>

```

 [Previous](index.html "Welcome to w3lib’s documentation!")

---

© Copyright 2014, w3lib developers.

Built with [Sphinx](https://www.sphinx-doc.org/) using a
[theme](https://github.com/readthedocs/sphinx_rtd_theme)
provided by [Read the Docs](https://readthedocs.org).



=== Content from doc.scrapy.org_bf0da525_20250115_080053.html ===

[Scrapy](../index.html)
2.12

First steps

* [Scrapy at a glance](../intro/overview.html)
* [Installation guide](../intro/install.html)
* [Scrapy Tutorial](../intro/tutorial.html)
* [Examples](../intro/examples.html)

Basic concepts

* [Command line tool](commands.html)
* [Spiders](spiders.html)
* [Selectors](selectors.html)
* [Items](items.html)
* [Item Loaders](loaders.html)
* [Scrapy shell](shell.html)
* [Item Pipeline](item-pipeline.html)
* [Feed exports](feed-exports.html)
* [Requests and Responses](request-response.html)
* [Link Extractors](link-extractors.html)
* [Settings](settings.html)
* [Exceptions](exceptions.html)

Built-in services

* [Logging](logging.html)
* [Stats Collection](stats.html)
* [Sending e-mail](email.html)
* [Telnet Console](telnetconsole.html)

Solving specific problems

* [Frequently Asked Questions](../faq.html)
* [Debugging Spiders](debug.html)
* [Spiders Contracts](contracts.html)
* [Common Practices](practices.html)
* [Broad Crawls](broad-crawls.html)
* [Using your browser’s Developer Tools for scraping](developer-tools.html)
* [Selecting dynamically-loaded content](dynamic-content.html)
* [Debugging memory leaks](leaks.html)
* [Downloading and processing files and images](media-pipeline.html)
* [Deploying Spiders](deploy.html)
* [AutoThrottle extension](autothrottle.html)
* [Benchmarking](benchmarking.html)
* [Jobs: pausing and resuming crawls](jobs.html)
* [Coroutines](coroutines.html)
* [asyncio](asyncio.html)

Extending Scrapy

* [Architecture overview](architecture.html)
* [Add-ons](addons.html)
* Downloader Middleware
  + [Activating a downloader middleware](#activating-a-downloader-middleware)
  + [Writing your own downloader middleware](#writing-your-own-downloader-middleware)
    - [`DownloaderMiddleware`](#scrapy.downloadermiddlewares.DownloaderMiddleware)
      * [`DownloaderMiddleware.process_request()`](#scrapy.downloadermiddlewares.DownloaderMiddleware.process_request)
      * [`DownloaderMiddleware.process_response()`](#scrapy.downloadermiddlewares.DownloaderMiddleware.process_response)
      * [`DownloaderMiddleware.process_exception()`](#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception)
      * [`DownloaderMiddleware.from_crawler()`](#scrapy.downloadermiddlewares.DownloaderMiddleware.from_crawler)
  + [Built-in downloader middleware reference](#built-in-downloader-middleware-reference)
    - [CookiesMiddleware](#module-scrapy.downloadermiddlewares.cookies)
      * [`CookiesMiddleware`](#scrapy.downloadermiddlewares.cookies.CookiesMiddleware)
      * [Multiple cookie sessions per spider](#multiple-cookie-sessions-per-spider)
      * [COOKIES\_ENABLED](#cookies-enabled)
      * [COOKIES\_DEBUG](#cookies-debug)
    - [DefaultHeadersMiddleware](#module-scrapy.downloadermiddlewares.defaultheaders)
      * [`DefaultHeadersMiddleware`](#scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware)
    - [DownloadTimeoutMiddleware](#module-scrapy.downloadermiddlewares.downloadtimeout)
      * [`DownloadTimeoutMiddleware`](#scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware)
    - [HttpAuthMiddleware](#module-scrapy.downloadermiddlewares.httpauth)
      * [`HttpAuthMiddleware`](#scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware)
    - [HttpCacheMiddleware](#module-scrapy.downloadermiddlewares.httpcache)
      * [`HttpCacheMiddleware`](#scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware)
      * [Dummy policy (default)](#dummy-policy-default)
      * [RFC2616 policy](#rfc2616-policy)
      * [Filesystem storage backend (default)](#filesystem-storage-backend-default)
      * [DBM storage backend](#dbm-storage-backend)
      * [Writing your own storage backend](#writing-your-own-storage-backend)
      * [HTTPCache middleware settings](#httpcache-middleware-settings)
    - [HttpCompressionMiddleware](#module-scrapy.downloadermiddlewares.httpcompression)
      * [`HttpCompressionMiddleware`](#scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware)
      * [HttpCompressionMiddleware Settings](#httpcompressionmiddleware-settings)
    - [HttpProxyMiddleware](#module-scrapy.downloadermiddlewares.httpproxy)
      * [`HttpProxyMiddleware`](#scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware)
    - [OffsiteMiddleware](#module-scrapy.downloadermiddlewares.offsite)
      * [`OffsiteMiddleware`](#scrapy.downloadermiddlewares.offsite.OffsiteMiddleware)
    - [RedirectMiddleware](#module-scrapy.downloadermiddlewares.redirect)
      * [`RedirectMiddleware`](#scrapy.downloadermiddlewares.redirect.RedirectMiddleware)
      * [RedirectMiddleware settings](#redirectmiddleware-settings)
    - [MetaRefreshMiddleware](#metarefreshmiddleware)
      * [`MetaRefreshMiddleware`](#scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware)
      * [MetaRefreshMiddleware settings](#metarefreshmiddleware-settings)
    - [RetryMiddleware](#module-scrapy.downloadermiddlewares.retry)
      * [`RetryMiddleware`](#scrapy.downloadermiddlewares.retry.RetryMiddleware)
      * [`get_retry_request()`](#scrapy.downloadermiddlewares.retry.get_retry_request)
      * [RetryMiddleware Settings](#retrymiddleware-settings)
    - [RobotsTxtMiddleware](#module-scrapy.downloadermiddlewares.robotstxt)
      * [`RobotsTxtMiddleware`](#scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware)
      * [Protego parser](#protego-parser)
      * [RobotFileParser](#robotfileparser)
      * [Robotexclusionrulesparser](#robotexclusionrulesparser)
      * [Implementing support for a new parser](#implementing-support-for-a-new-parser)
    - [DownloaderStats](#module-scrapy.downloadermiddlewares.stats)
      * [`DownloaderStats`](#scrapy.downloadermiddlewares.stats.DownloaderStats)
    - [UserAgentMiddleware](#module-scrapy.downloadermiddlewares.useragent)
      * [`UserAgentMiddleware`](#scrapy.downloadermiddlewares.useragent.UserAgentMiddleware)
    - [AjaxCrawlMiddleware](#module-scrapy.downloadermiddlewares.ajaxcrawl)
      * [`AjaxCrawlMiddleware`](#scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware)
      * [AjaxCrawlMiddleware Settings](#ajaxcrawlmiddleware-settings)
      * [HttpProxyMiddleware settings](#httpproxymiddleware-settings)
* [Spider Middleware](spider-middleware.html)
* [Extensions](extensions.html)
* [Signals](signals.html)
* [Scheduler](scheduler.html)
* [Item Exporters](exporters.html)
* [Components](components.html)
* [Core API](api.html)

All the rest

* [Release notes](../news.html)
* [Contributing to Scrapy](../contributing.html)
* [Versioning and API stability](../versioning.html)

[Scrapy](../index.html)

* Downloader Middleware
* [View page source](../_sources/topics/downloader-middleware.rst.txt)

---

# Downloader Middleware[¶](#downloader-middleware "Permalink to this heading")

The downloader middleware is a framework of hooks into Scrapy’s
request/response processing. It’s a light, low-level system for globally
altering Scrapy’s requests and responses.

## Activating a downloader middleware[¶](#activating-a-downloader-middleware "Permalink to this heading")

To activate a downloader middleware component, add it to the
[`DOWNLOADER_MIDDLEWARES`](settings.html#std-setting-DOWNLOADER_MIDDLEWARES) setting, which is a dict whose keys are the
middleware class paths and their values are the middleware orders.

Here’s an example:

```
DOWNLOADER_MIDDLEWARES = {
    "myproject.middlewares.CustomDownloaderMiddleware": 543,
}

```

The [`DOWNLOADER_MIDDLEWARES`](settings.html#std-setting-DOWNLOADER_MIDDLEWARES) setting is merged with the
[`DOWNLOADER_MIDDLEWARES_BASE`](settings.html#std-setting-DOWNLOADER_MIDDLEWARES_BASE) setting defined in Scrapy (and not meant
to be overridden) and then sorted by order to get the final sorted list of
enabled middlewares: the first middleware is the one closer to the engine and
the last is the one closer to the downloader. In other words,
the [`process_request()`](#scrapy.downloadermiddlewares.DownloaderMiddleware.process_request "scrapy.downloadermiddlewares.DownloaderMiddleware.process_request")
method of each middleware will be invoked in increasing
middleware order (100, 200, 300, …) and the [`process_response()`](#scrapy.downloadermiddlewares.DownloaderMiddleware.process_response "scrapy.downloadermiddlewares.DownloaderMiddleware.process_response") method
of each middleware will be invoked in decreasing order.

To decide which order to assign to your middleware see the
[`DOWNLOADER_MIDDLEWARES_BASE`](settings.html#std-setting-DOWNLOADER_MIDDLEWARES_BASE) setting and pick a value according to
where you want to insert the middleware. The order does matter because each
middleware performs a different action and your middleware could depend on some
previous (or subsequent) middleware being applied.

If you want to disable a built-in middleware (the ones defined in
[`DOWNLOADER_MIDDLEWARES_BASE`](settings.html#std-setting-DOWNLOADER_MIDDLEWARES_BASE) and enabled by default) you must define it
in your project’s [`DOWNLOADER_MIDDLEWARES`](settings.html#std-setting-DOWNLOADER_MIDDLEWARES) setting and assign `None`
as its value. For example, if you want to disable the user-agent middleware:

```
DOWNLOADER_MIDDLEWARES = {
    "myproject.middlewares.CustomDownloaderMiddleware": 543,
    "scrapy.downloadermiddlewares.useragent.UserAgentMiddleware": None,
}

```

Finally, keep in mind that some middlewares may need to be enabled through a
particular setting. See each middleware documentation for more info.

## Writing your own downloader middleware[¶](#writing-your-own-downloader-middleware "Permalink to this heading")

Each downloader middleware is a Python class that defines one or more of the
methods defined below.

The main entry point is the `from_crawler` class method, which receives a
[`Crawler`](api.html#scrapy.crawler.Crawler "scrapy.crawler.Crawler") instance. The [`Crawler`](api.html#scrapy.crawler.Crawler "scrapy.crawler.Crawler")
object gives you access, for example, to the [settings](settings.html#topics-settings).

*class* scrapy.downloadermiddlewares.DownloaderMiddleware[¶](#scrapy.downloadermiddlewares.DownloaderMiddleware "Permalink to this definition")

Note

Any of the downloader middleware methods may also return a deferred.

process\_request(*request*, *spider*)[¶](#scrapy.downloadermiddlewares.DownloaderMiddleware.process_request "Permalink to this definition")

This method is called for each request that goes through the download
middleware.

[`process_request()`](#scrapy.downloadermiddlewares.DownloaderMiddleware.process_request "scrapy.downloadermiddlewares.DownloaderMiddleware.process_request") should either: return `None`, return a
`Response` object, return a [`Request`](request-response.html#scrapy.http.Request "scrapy.http.Request")
object, or raise [`IgnoreRequest`](exceptions.html#scrapy.exceptions.IgnoreRequest "scrapy.exceptions.IgnoreRequest").

If it returns `None`, Scrapy will continue processing this request, executing all
other middlewares until, finally, the appropriate downloader handler is called
the request performed (and its response downloaded).

If it returns a [`Response`](request-response.html#scrapy.http.Response "scrapy.http.Response") object, Scrapy won’t bother
calling *any* other [`process_request()`](#scrapy.downloadermiddlewares.DownloaderMiddleware.process_request "scrapy.downloadermiddlewares.DownloaderMiddleware.process_request") or [`process_exception()`](#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception "scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception") methods,
or the appropriate download function; it’ll return that response. The [`process_response()`](#scrapy.downloadermiddlewares.DownloaderMiddleware.process_response "scrapy.downloadermiddlewares.DownloaderMiddleware.process_response")
methods of installed middleware is always called on every response.

If it returns a `Request` object, Scrapy will stop calling
[`process_request()`](#scrapy.downloadermiddlewares.DownloaderMiddleware.process_request "scrapy.downloadermiddlewares.DownloaderMiddleware.process_request") methods and reschedule the returned request. Once the newly returned
request is performed, the appropriate middleware chain will be called on
the downloaded response.

If it raises an [`IgnoreRequest`](exceptions.html#scrapy.exceptions.IgnoreRequest "scrapy.exceptions.IgnoreRequest") exception, the
[`process_exception()`](#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception "scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception") methods of installed downloader middleware will be called.
If none of them handle the exception, the errback function of the request
(`Request.errback`) is called. If no code handles the raised exception, it is
ignored and not logged (unlike other exceptions).

Parameters:

* **request** (`Request` object) – the request being processed
* **spider** ([`Spider`](spiders.html#scrapy.Spider "scrapy.Spider") object) – the spider for which this request is intended

process\_response(*request*, *response*, *spider*)[¶](#scrapy.downloadermiddlewares.DownloaderMiddleware.process_response "Permalink to this definition")

[`process_response()`](#scrapy.downloadermiddlewares.DownloaderMiddleware.process_response "scrapy.downloadermiddlewares.DownloaderMiddleware.process_response") should either: return a [`Response`](request-response.html#scrapy.http.Response "scrapy.http.Response")
object, return a `Request` object or
raise a [`IgnoreRequest`](exceptions.html#scrapy.exceptions.IgnoreRequest "scrapy.exceptions.IgnoreRequest") exception.

If it returns a [`Response`](request-response.html#scrapy.http.Response "scrapy.http.Response") (it could be the same given
response, or a brand-new one), that response will continue to be processed
with the [`process_response()`](#scrapy.downloadermiddlewares.DownloaderMiddleware.process_response "scrapy.downloadermiddlewares.DownloaderMiddleware.process_response") of the next middleware in the chain.

If it returns a `Request` object, the middleware chain is
halted and the returned request is rescheduled to be downloaded in the future.
This is the same behavior as if a request is returned from [`process_request()`](#scrapy.downloadermiddlewares.DownloaderMiddleware.process_request "scrapy.downloadermiddlewares.DownloaderMiddleware.process_request").

If it raises an [`IgnoreRequest`](exceptions.html#scrapy.exceptions.IgnoreRequest "scrapy.exceptions.IgnoreRequest") exception, the errback
function of the request (`Request.errback`) is called. If no code handles the raised
exception, it is ignored and not logged (unlike other exceptions).

Parameters:

* **request** (is a `Request` object) – the request that originated the response
* **response** ([`Response`](request-response.html#scrapy.http.Response "scrapy.http.Response") object) – the response being processed
* **spider** ([`Spider`](spiders.html#scrapy.Spider "scrapy.Spider") object) – the spider for which this response is intended

process\_exception(*request*, *exception*, *spider*)[¶](#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception "Permalink to this definition")

Scrapy calls [`process_exception()`](#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception "scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception") when a download handler
or a [`process_request()`](#scrapy.downloadermiddlewares.DownloaderMiddleware.process_request "scrapy.downloadermiddlewares.DownloaderMiddleware.process_request") (from a downloader middleware) raises an
exception (including an [`IgnoreRequest`](exceptions.html#scrapy.exceptions.IgnoreRequest "scrapy.exceptions.IgnoreRequest") exception)

[`process_exception()`](#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception "scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception") should return: either `None`,
a [`Response`](request-response.html#scrapy.http.Response "scrapy.http.Response") object, or a `Request` object.

If it returns `None`, Scrapy will continue processing this exception,
executing any other [`process_exception()`](#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception "scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception") methods of installed middleware,
until no middleware is left and the default exception handling kicks in.

If it returns a [`Response`](request-response.html#scrapy.http.Response "scrapy.http.Response") object, the [`process_response()`](#scrapy.downloadermiddlewares.DownloaderMiddleware.process_response "scrapy.downloadermiddlewares.DownloaderMiddleware.process_response")
method chain of installed middleware is started, and Scrapy won’t bother calling
any other [`process_exception()`](#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception "scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception") methods of middleware.

If it returns a `Request` object, the returned request is
rescheduled to be downloaded in the future. This stops the execution of
[`process_exception()`](#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception "scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception") methods of the middleware the same as returning a
response would.

Parameters:

* **request** (is a `Request` object) – the request that generated the exception
* **exception** (an `Exception` object) – the raised exception
* **spider** ([`Spider`](spiders.html#scrapy.Spider "scrapy.Spider") object) – the spider for which this request is intended

from\_crawler(*cls*, *crawler*)[¶](#scrapy.downloadermiddlewares.DownloaderMiddleware.from_crawler "Permalink to this definition")

If present, this classmethod is called to create a middleware instance
from a [`Crawler`](api.html#scrapy.crawler.Crawler "scrapy.crawler.Crawler"). It must return a new instance
of the middleware. Crawler object provides access to all Scrapy core
components like settings and signals; it is a way for middleware to
access them and hook its functionality into Scrapy.

Parameters:

**crawler** ([`Crawler`](api.html#scrapy.crawler.Crawler "scrapy.crawler.Crawler") object) – crawler that uses this middleware

## Built-in downloader middleware reference[¶](#built-in-downloader-middleware-reference "Permalink to this heading")

This page describes all downloader middleware components that come with
Scrapy. For information on how to use them and how to write your own downloader
middleware, see the [downloader middleware usage guide](#topics-downloader-middleware).

For a list of the components enabled by default (and their orders) see the
[`DOWNLOADER_MIDDLEWARES_BASE`](settings.html#std-setting-DOWNLOADER_MIDDLEWARES_BASE) setting.

### CookiesMiddleware[¶](#module-scrapy.downloadermiddlewares.cookies "Permalink to this heading")

*class* scrapy.downloadermiddlewares.cookies.CookiesMiddleware[[source]](../_modules/scrapy/downloadermiddlewares/cookies.html#CookiesMiddleware)[¶](#scrapy.downloadermiddlewares.cookies.CookiesMiddleware "Permalink to this definition")

This middleware enables working with sites that require cookies, such as
those that use sessions. It keeps track of cookies sent by web servers, and
sends them back on subsequent requests (from that spider), just like web
browsers do.

Caution

When non-UTF8 encoded byte sequences are passed to a
`Request`, the `CookiesMiddleware` will log
a warning. Refer to [Advanced customization](logging.html#topics-logging-advanced-customization)
to customize the logging behaviour.

Caution

Cookies set via the `Cookie` header are not considered by the
[CookiesMiddleware](#cookies-mw). If you need to set cookies for a request, use the
`Request.cookies` parameter. This is a known
current limitation that is being worked on.

The following settings can be used to configure the cookie middleware:

* [`COOKIES_ENABLED`](#std-setting-COOKIES_ENABLED)
* [`COOKIES_DEBUG`](#std-setting-COOKIES_DEBUG)

#### Multiple cookie sessions per spider[¶](#multiple-cookie-sessions-per-spider "Permalink to this heading")

There is support for keeping multiple cookie sessions per spider by using the
[`cookiejar`](#std-reqmeta-cookiejar) Request meta key. By default it uses a single cookie jar
(session), but you can pass an identifier to use different ones.

For example:

```
for i, url in enumerate(urls):
    yield scrapy.Request(url, meta={"cookiejar": i}, callback=self.parse_page)

```

Keep in mind that the [`cookiejar`](#std-reqmeta-cookiejar) meta key is not “sticky”. You need to keep
passing it along on subsequent requests. For example:

```
def parse_page(self, response):
    # do some processing
    return scrapy.Request(
        "http://www.example.com/otherpage",
        meta={"cookiejar": response.meta["cookiejar"]},
        callback=self.parse_other_page,
    )

```

#### COOKIES\_ENABLED[¶](#cookies-enabled "Permalink to this heading")

Default: `True`

Whether to enable the cookies middleware. If disabled, no cookies will be sent
to web servers.

Notice that despite the value of [`COOKIES_ENABLED`](#std-setting-COOKIES_ENABLED) setting if
`Request.`[`meta['dont_merge_cookies']`](request-response.html#std-reqmeta-dont_merge_cookies)
evaluates to `True` the request cookies will **not** be sent to the
web server and received cookies in [`Response`](request-response.html#scrapy.http.Response "scrapy.http.Response") will
**not** be merged with the existing cookies.

For more detailed information see the `cookies` parameter in
`Request`.

#### COOKIES\_DEBUG[¶](#cookies-debug "Permalink to this heading")

Default: `False`

If enabled, Scrapy will log all cookies sent in requests (i.e. `Cookie`
header) and all cookies received in responses (i.e. `Set-Cookie` header).

Here’s an example of a log with [`COOKIES_DEBUG`](#std-setting-COOKIES_DEBUG) enabled:

```
2011-04-06 14:35:10-0300 [scrapy.core.engine] INFO: Spider opened
2011-04-06 14:35:10-0300 [scrapy.downloadermiddlewares.cookies] DEBUG: Sending cookies to: <GET http://www.diningcity.com/netherlands/index.html>
        Cookie: clientlanguage_nl=en_EN
2011-04-06 14:35:14-0300 [scrapy.downloadermiddlewares.cookies] DEBUG: Received cookies from: <200 http://www.diningcity.com/netherlands/index.html>
        Set-Cookie: JSESSIONID=B~FA4DC0C496C8762AE4F1A620EAB34F38; Path=/
        Set-Cookie: ip_isocode=US
        Set-Cookie: clientlanguage_nl=en_EN; Expires=Thu, 07-Apr-2011 21:21:34 GMT; Path=/
2011-04-06 14:49:50-0300 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.diningcity.com/netherlands/index.html> (referer: None)
[...]

```

### DefaultHeadersMiddleware[¶](#module-scrapy.downloadermiddlewares.defaultheaders "Permalink to this heading")

*class* scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware[[source]](../_modules/scrapy/downloadermiddlewares/defaultheaders.html#DefaultHeadersMiddleware)[¶](#scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware "Permalink to this definition")

This middleware sets all default requests headers specified in the
[`DEFAULT_REQUEST_HEADERS`](settings.html#std-setting-DEFAULT_REQUEST_HEADERS) setting.

### DownloadTimeoutMiddleware[¶](#module-scrapy.downloadermiddlewares.downloadtimeout "Permalink to this heading")

*class* scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware[[source]](../_modules/scrapy/downloadermiddlewares/downloadtimeout.html#DownloadTimeoutMiddleware)[¶](#scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware "Permalink to this definition")

This middleware sets the download timeout for requests specified in the
[`DOWNLOAD_TIMEOUT`](settings.html#std-setting-DOWNLOAD_TIMEOUT) setting or `download_timeout`
spider attribute.

Note

You can also set download timeout per-request using
[`download_timeout`](request-response.html#std-reqmeta-download_timeout) Request.meta key; this is supported
even when DownloadTimeoutMiddleware is disabled.

### HttpAuthMiddleware[¶](#module-scrapy.downloadermiddlewares.httpauth "Permalink to this heading")

*class* scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware[[source]](../_modules/scrapy/downloadermiddlewares/httpauth.html#HttpAuthMiddleware)[¶](#scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware "Permalink to this definition")

This middleware authenticates all requests generated from certain spiders
using [Basic access authentication](https://en.wikipedia.org/wiki/Basic_access_authentication) (aka. HTTP auth).

To enable HTTP authentication for a spider, set the `http_user` and
`http_pass` spider attributes to the authentication data and the
`http_auth_domain` spider attribute to the domain which requires this
authentication (its subdomains will be also handled in the same way).
You can set `http_auth_domain` to `None` to enable the
authentication for all requests but you risk leaking your authentication
credentials to unrelated domains.

Warning

In previous Scrapy versions HttpAuthMiddleware sent the authentication
data with all requests, which is a security problem if the spider
makes requests to several different domains. Currently if the
`http_auth_domain` attribute is not set, the middleware will use the
domain of the first request, which will work for some spiders but not
for others. In the future the middleware will produce an error instead.

Example:

```
from scrapy.spiders import CrawlSpider

class SomeIntranetSiteSpider(CrawlSpider):
    http_user = "someuser"
    http_pass = "somepass"
    http_auth_domain = "intranet.example.com"
    name = "intranet.example.com"

    # .. rest of the spider code omitted ...

```

### HttpCacheMiddleware[¶](#module-scrapy.downloadermiddlewares.httpcache "Permalink to this heading")

*class* scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware[[source]](../_modules/scrapy/downloadermiddlewares/httpcache.html#HttpCacheMiddleware)[¶](#scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware "Permalink to this definition")

This middleware provides low-level cache to all HTTP requests and responses.
It has to be combined with a cache storage backend as well as a cache policy.

Scrapy ships with the following HTTP cache storage backends:

> * [Filesystem storage backend (default)](#httpcache-storage-fs)
> * [DBM storage backend](#httpcache-storage-dbm)

You can change the HTTP cache storage backend with the [`HTTPCACHE_STORAGE`](#std-setting-HTTPCACHE_STORAGE)
setting. Or you can also [implement your own storage backend.](#httpcache-storage-custom)

Scrapy ships with two HTTP cache policies:

> * [RFC2616 policy](#httpcache-policy-rfc2616)
> * [Dummy policy (default)](#httpcache-policy-dummy)

You can change the HTTP cache policy with the [`HTTPCACHE_POLICY`](#std-setting-HTTPCACHE_POLICY)
setting. Or you can also implement your own policy.

You can also avoid caching a response on every policy using [`dont_cache`](#std-reqmeta-dont_cache) meta key equals `True`.

#### Dummy policy (default)[¶](#dummy-policy-default "Permalink to this heading")

*class* scrapy.extensions.httpcache.DummyPolicy[[source]](../_modules/scrapy/extensions/httpcache.html#DummyPolicy)[¶](#scrapy.extensions.httpcache.DummyPolicy "Permalink to this definition")

This policy has no awareness of any HTTP Cache-Control directives.
Every request and its corresponding response are cached. When the same
request is seen again, the response is returned without transferring
anything from the Internet.

The Dummy policy is useful for testing spiders faster (without having
to wait for downloads every time) and for trying your spider offline,
when an Internet connection is not available. The goal is to be able to
“replay” a spider run *exactly as it ran before*.

#### RFC2616 policy[¶](#rfc2616-policy "Permalink to this heading")

*class* scrapy.extensions.httpcache.RFC2616Policy[[source]](../_modules/scrapy/extensions/httpcache.html#RFC2616Policy)[¶](#scrapy.extensions.httpcache.RFC2616Policy "Permalink to this definition")

This policy provides a RFC2616 compliant HTTP cache, i.e. with HTTP
Cache-Control awareness, aimed at production and used in continuous
runs to avoid downloading unmodified data (to save bandwidth and speed up
crawls).

What is implemented:

* Do not attempt to store responses/requests with `no-store` cache-control directive set
* Do not serve responses from cache if `no-cache` cache-control directive is set even for fresh responses
* Compute freshness lifetime from `max-age` cache-control directive
* Compute freshness lifetime from `Expires` response header
* Compute freshness lifetime from `Last-Modified` response header (heuristic used by Firefox)
* Compute current age from `Age` response header
* Compute current age from `Date` header
* Revalidate stale responses based on `Last-Modified` response header
* Revalidate stale responses based on `ETag` response header
* Set `Date` header for any received response missing it
* Support `max-stale` cache-control directive in requests

This allows spiders to be configured with the full RFC2616 cache policy,
but avoid revalidation on a request-by-request basis, while remaining
conformant with the HTTP spec.

Example:

Add `Cache-Control: max-stale=600` to Request headers to accept responses that
have exceeded their expiration time by no more than 600 seconds.

See also: RFC2616, 14.9.3

What is missing:

* `Pragma: no-cache` support <https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.9.1>
* `Vary` header support <https://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.6>
* Invalidation after updates or deletes <https://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.10>
* … probably others ..

#### Filesystem storage backend (default)[¶](#filesystem-storage-backend-default "Permalink to this heading")

*class* scrapy.extensions.httpcache.FilesystemCacheStorage[[source]](../_modules/scrapy/extensions/httpcache.html#FilesystemCacheStorage)[¶](#scrapy.extensions.httpcache.FilesystemCacheStorage "Permalink to this definition")

File system storage backend is available for the HTTP cache middleware.

Each request/response pair is stored in a different directory containing
the following files:

* `request_body` - the plain request body
* `request_headers` - the request headers (in raw HTTP format)
* `response_body` - the plain response body
* `response_headers` - the request headers (in raw HTTP format)
* `meta` - some metadata of this cache resource in Python `repr()`
  format (grep-friendly format)
* `pickled_meta` - the same metadata in `meta` but pickled for more
  efficient deserialization

The directory name is made from the request fingerprint (see
`scrapy.utils.request.fingerprint`), and one level of subdirectories is
used to avoid creating too many files into the same directory (which is
inefficient in many file systems). An example directory could be:

```
/path/to/cache/dir/example.com/72/72811f648e718090f041317756c03adb0ada46c7

```

#### DBM storage backend[¶](#dbm-storage-backend "Permalink to this heading")

*class* scrapy.extensions.httpcache.DbmCacheStorage[[source]](../_modules/scrapy/extensions/httpcache.html#DbmCacheStorage)[¶](#scrapy.extensions.httpcache.DbmCacheStorage "Permalink to this definition")

A [DBM](https://en.wikipedia.org/wiki/Dbm) storage backend is also available for the HTTP cache middleware.

By default, it uses the [`dbm`](https://docs.python.org/3/library/dbm.html#module-dbm "(in Python v3.13)"), but you can change it with the
[`HTTPCACHE_DBM_MODULE`](#std-setting-HTTPCACHE_DBM_MODULE) setting.

#### Writing your own storage backend[¶](#writing-your-own-storage-backend "Permalink to this heading")

You can implement a cache storage backend by creating a Python class that
defines the methods described below.

*class* scrapy.extensions.httpcache.CacheStorage[¶](#scrapy.extensions.httpcache.CacheStorage "Permalink to this definition")
open\_spider(*spider*)[¶](#scrapy.extensions.httpcache.CacheStorage.open_spider "Permalink to this definition")

This method gets called after a spider has been opened for crawling. It handles
the [`open_spider`](signals.html#std-signal-spider_opened) signal.

Parameters:

**spider** ([`Spider`](spiders.html#scrapy.Spider "scrapy.Spider") object) – the spider which has been opened

close\_spider(*spider*)[¶](#scrapy.extensions.httpcache.CacheStorage.close_spider "Permalink to this definition")

This method gets called after a spider has been closed. It handles
the [`close_spider`](signals.html#std-signal-spider_closed) signal.

Parameters:

**spider** ([`Spider`](spiders.html#scrapy.Spider "scrapy.Spider") object) – the spider which has been closed

retrieve\_response(*spider*, *request*)[¶](#scrapy.extensions.httpcache.CacheStorage.retrieve_response "Permalink to this definition")

Return response if present in cache, or `None` otherwise.

Parameters:

* **spider** ([`Spider`](spiders.html#scrapy.Spider "scrapy.Spider") object) – the spider which generated the request
* **request** (`Request` object) – the request to find cached response for

store\_response(*spider*, *request*, *response*)[¶](#scrapy.extensions.httpcache.CacheStorage.store_response "Permalink to this definition")

Store the given response in the cache.

Parameters:

* **spider** ([`Spider`](spiders.html#scrapy.Spider "scrapy.Spider") object) – the spider for which the response is intended
* **request** (`Request` object) – the corresponding request the spider generated
* **response** ([`Response`](request-response.html#scrapy.http.Response "scrapy.http.Response") object) – the response to store in the cache

In order to use your storage backend, set:

* [`HTTPCACHE_STORAGE`](#std-setting-HTTPCACHE_STORAGE) to the Python import path of your custom storage class.

#### HTTPCache middleware settings[¶](#httpcache-middleware-settings "Permalink to this heading")

The `HttpCacheMiddleware` can be configured through the following
settings:

##### HTTPCACHE\_ENABLED[¶](#httpcache-enabled "Permalink to this heading")

Default: `False`

Whether the HTTP cache will be enabled.

##### HTTPCACHE\_EXPIRATION\_SECS[¶](#httpcache-expiration-secs "Permalink to this heading")

Default: `0`

Expiration time for cached requests, in seconds.

Cached requests older than this time will be re-downloaded. If zero, cached
requests will never expire.

##### HTTPCACHE\_DIR[¶](#httpcache-dir "Permalink to this heading")

Default: `'httpcache'`

The directory to use for storing the (low-level) HTTP cache. If empty, the HTTP
cache will be disabled. If a relative path is given, is taken relative to the
project data dir. For more info see: [Default structure of Scrapy projects](commands.html#topics-project-structure).

##### HTTPCACHE\_IGNORE\_HTTP\_CODES[¶](#httpcache-ignore-http-codes "Permalink to this heading")

Default: `[]`

Don’t cache response with these HTTP codes.

##### HTTPCACHE\_IGNORE\_MISSING[¶](#httpcache-ignore-missing "Permalink to this heading")

Default: `False`

If enabled, requests not found in the cache will be ignored instead of downloaded.

##### HTTPCACHE\_IGNORE\_SCHEMES[¶](#httpcache-ignore-schemes "Permalink to this heading")

Default: `['file']`

Don’t cache responses with these URI schemes.

##### HTTPCACHE\_STORAGE[¶](#httpcache-storage "Permalink to this heading")

Default: `'scrapy.extensions.httpcache.FilesystemCacheStorage'`

The class which implements the cache storage backend.

##### HTTPCACHE\_DBM\_MODULE[¶](#httpcache-dbm-module "Permalink to this heading")

Default: `'dbm'`

The database module to use in the [DBM storage backend](#httpcache-storage-dbm). This setting is specific to the DBM backend.

##### HTTPCACHE\_POLICY[¶](#httpcache-policy "Permalink to this heading")

Default: `'scrapy.extensions.httpcache.DummyPolicy'`

The class which implements the cache policy.

##### HTTPCACHE\_GZIP[¶](#httpcache-gzip "Permalink to this heading")

Default: `False`

If enabled, will compress all cached data with gzip.
This setting is specific to the Filesystem backend.

##### HTTPCACHE\_ALWAYS\_STORE[¶](#httpcache-always-store "Permalink to this heading")

Default: `False`

If enabled, will cache pages unconditionally.

A spider may wish to have all responses available in the cache, for
future use with `Cache-Control: max-stale`, for instance. The
DummyPolicy caches all responses but never revalidates them, and
sometimes a more nuanced policy is desirable.

This setting still respects `Cache-Control: no-store` directives in responses.
If you don’t want that, filter `no-store` out of the Cache-Control headers in
responses you feed to the cache middleware.

##### HTTPCACHE\_IGNORE\_RESPONSE\_CACHE\_CONTROLS[¶](#httpcache-ignore-response-cache-controls "Permalink to this heading")

Default: `[]`

List of Cache-Control directives in responses to be ignored.

Sites often set “no-store”, “no-cache”, “must-revalidate”, etc., but get
upset at the traffic a spider can generate if it actually respects those
directives. This allows to selectively ignore Cache-Control directives
that are known to be unimportant for the sites being crawled.

We assume that the spider will not issue Cache-Control directives
in requests unless it actually needs them, so directives in requests are
not filtered.

### HttpCompressionMiddleware[¶](#module-scrapy.downloadermiddlewares.httpcompression "Permalink to this heading")

*class* scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware[[source]](../_modules/scrapy/downloadermiddlewares/httpcompression.html#HttpCompressionMiddleware)[¶](#scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware "Permalink to this definition")

This middleware allows compressed (gzip, deflate) traffic to be
sent/received from web sites.

This middleware also supports decoding [brotli-compressed](https://www.ietf.org/rfc/rfc7932.txt) as well as
[zstd-compressed](https://www.ietf.org/rfc/rfc8478.txt) responses, provided that [brotli](https://pypi.org/project/Brotli/) or [zstandard](https://pypi.org/project/zstandard/) is
installed, respectively.

#### HttpCompressionMiddleware Settings[¶](#httpcompressionmiddleware-settings "Permalink to this heading")

##### COMPRESSION\_ENABLED[¶](#compression-enabled "Permalink to this heading")

Default: `True`

Whether the Compression middleware will be enabled.

### HttpProxyMiddleware[¶](#module-scrapy.downloadermiddlewares.httpproxy "Permalink to this heading")

*class* scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware[[source]](../_modules/scrapy/downloadermiddlewares/httpproxy.html#HttpProxyMiddleware)[¶](#scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware "Permalink to this definition")

This middleware sets the HTTP proxy to use for requests, by setting the
`proxy` meta value for `Request` objects.

Like the Python standard library module [`urllib.request`](https://docs.python.org/3/library/urllib.request.html#module-urllib.request "(in Python v3.13)"), it obeys
the following environment variables:

* `http_proxy`
* `https_proxy`
* `no_proxy`

You can also set the meta key `proxy` per-request, to a value like
`http://some_proxy_server:port` or `http://username:password@some_proxy_server:port`.
Keep in mind this value will take precedence over `http_proxy`/`https_proxy`
environment variables, and it will also ignore `no_proxy` environment variable.

### OffsiteMiddleware[¶](#module-scrapy.downloadermiddlewares.offsite "Permalink to this heading")

*class* scrapy.downloadermiddlewares.offsite.OffsiteMiddleware[[source]](../_modules/scrapy/downloadermiddlewares/offsite.html#OffsiteMiddleware)[¶](#scrapy.downloadermiddlewares.offsite.OffsiteMiddleware "Permalink to this definition")

New in version 2.11.2.

Filters out Requests for URLs outside the domains covered by the spider.

This middleware filters out every request whose host names aren’t in the
spider’s [`allowed_domains`](spiders.html#scrapy.Spider.allowed_domains "scrapy.Spider.allowed_domains") attribute.
All subdomains of any domain in the list are also allowed.
E.g. the rule `www.example.org` will also allow `bob.www.example.org`
but not `www2.example.com` nor `example.com`.

When your spider returns a request for a domain not belonging to those
covered by the spider, this middleware will log a debug message similar to
this one:

```
DEBUG: Filtered offsite request to 'offsite.example': <GET http://offsite.example/some/page.html>

```

To avoid filling the log with too much noise, it will only print one of
these messages for each new domain filtered. So, for example, if another
request for `offsite.example` is filtered, no log message will be
printed. But if a request for `other.example` is filtered, a message
will be printed (but only for the first request filtered).

If the spider doesn’t define an
[`allowed_domains`](spiders.html#scrapy.Spider.allowed_domains "scrapy.Spider.allowed_domains") attribute, or the
attribute is empty, the offsite middleware will allow all requests.

If the request has the `dont_filter` attribute
set, the offsite middleware will allow the request even if its domain is not
listed in allowed domains.

### RedirectMiddleware[¶](#module-scrapy.downloadermiddlewares.redirect "Permalink to this heading")

*class* scrapy.downloadermiddlewares.redirect.RedirectMiddleware[[source]](../_modules/scrapy/downloadermiddlewares/redirect.html#RedirectMiddleware)[¶](#scrapy.downloadermiddlewares.redirect.RedirectMiddleware "Permalink to this definition")

This middleware handles redirection of requests based on response status.

The urls which the request goes through (while being redirected) can be found
in the `redirect_urls` `Request.meta` key.

The reason behind each redirect in [`redirect_urls`](#std-reqmeta-redirect_urls) can be found in the
`redirect_reasons` `Request.meta` key. For
example: `[301, 302, 307, 'meta refresh']`.

The format of a reason depends on the middleware that handled the corresponding
redirect. For example, [`RedirectMiddleware`](#scrapy.downloadermiddlewares.redirect.RedirectMiddleware "scrapy.downloadermiddlewares.redirect.RedirectMiddleware") indicates the triggering
response status code as an integer, while [`MetaRefreshMiddleware`](#scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware "scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware")
always uses the `'meta refresh'` string as reason.

The [`RedirectMiddleware`](#scrapy.downloadermiddlewares.redirect.RedirectMiddleware "scrapy.downloadermiddlewares.redirect.RedirectMiddleware") can be configured through the following
settings (see the settings documentation for more info):

* [`REDIRECT_ENABLED`](#std-setting-REDIRECT_ENABLED)
* [`REDIRECT_MAX_TIMES`](#std-setting-REDIRECT_MAX_TIMES)

If `Request.meta` has `dont_redirect`
key set to True, the request will be ignored by this middleware.

If you want to handle some redirect status codes in your spider, you can
specify these in the `handle_httpstatus_list` spider attribute.

For example, if you want the redirect middleware to ignore 301 and 302
responses (and pass them through to your spider) you can do this:

```
class MySpider(CrawlSpider):
    handle_httpstatus_list = [301, 302]

```

The `handle_httpstatus_list` key of `Request.meta` can also be used to specify which response codes to
allow on a per-request basis. You can also set the meta key
`handle_httpstatus_all` to `True` if you want to allow any response code
for a request.

#### RedirectMiddleware settings[¶](#redirectmiddleware-settings "Permalink to this heading")

##### REDIRECT\_ENABLED[¶](#redirect-enabled "Permalink to this heading")

Default: `True`

Whether the Redirect middleware will be enabled.

##### REDIRECT\_MAX\_TIMES[¶](#redirect-max-times "Permalink to this heading")

Default: `20`

The maximum number of redirections that will be followed for a single request.
If maximum redirections are exceeded, the request is aborted and ignored.

### MetaRefreshMiddleware[¶](#metarefreshmiddleware "Permalink to this heading")

*class* scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware[[source]](../_modules/scrapy/downloadermiddlewares/redirect.html#MetaRefreshMiddleware)[¶](#scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware "Permalink to this definition")

This middleware handles redirection of requests based on meta-refresh html tag.

The [`MetaRefreshMiddleware`](#scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware "scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware") can be configured through the following
settings (see the settings documentation for more info):

* [`METAREFRESH_ENABLED`](#std-setting-METAREFRESH_ENABLED)
* [`METAREFRESH_IGNORE_TAGS`](#std-setting-METAREFRESH_IGNORE_TAGS)
* [`METAREFRESH_MAXDELAY`](#std-setting-METAREFRESH_MAXDELAY)

This middleware obey [`REDIRECT_MAX_TIMES`](#std-setting-REDIRECT_MAX_TIMES) setting, [`dont_redirect`](#std-reqmeta-dont_redirect),
[`redirect_urls`](#std-reqmeta-redirect_urls) and [`redirect_reasons`](#std-reqmeta-redirect_reasons) request meta keys as described
for [`RedirectMiddleware`](#scrapy.downloadermiddlewares.redirect.RedirectMiddleware "scrapy.downloadermiddlewares.redirect.RedirectMiddleware")

#### MetaRefreshMiddleware settings[¶](#metarefreshmiddleware-settings "Permalink to this heading")

##### METAREFRESH\_ENABLED[¶](#metarefresh-enabled "Permalink to this heading")

Default: `True`

Whether the Meta Refresh middleware will be enabled.

##### METAREFRESH\_IGNORE\_TAGS[¶](#metarefresh-ignore-tags "Permalink to this heading")

Default: `[]`

Meta tags within these tags are ignored.

Changed in version 2.0: The default value of [`METAREFRESH_IGNORE_TAGS`](#std-setting-METAREFRESH_IGNORE_TAGS) changed from
`["script", "noscript"]` to `[]`.

Changed in version 2.11.2: The default value of [`METAREFRESH_IGNORE_TAGS`](#std-setting-METAREFRESH_IGNORE_TAGS) changed from
`[]` to `["noscript"]`.

##### METAREFRESH\_MAXDELAY[¶](#metarefresh-maxdelay "Permalink to this heading")

Default: `100`

The maximum meta-refresh delay (in seconds) to follow the redirection.
Some sites use meta-refresh for redirecting to a session expired page, so we
restrict automatic redirection to the maximum delay.

### RetryMiddleware[¶](#module-scrapy.downloadermiddlewares.retry "Permalink to this heading")

*class* scrapy.downloadermiddlewares.retry.RetryMiddleware[[source]](../_modules/scrapy/downloadermiddlewares/retry.html#RetryMiddleware)[¶](#scrapy.downloadermiddlewares.retry.RetryMiddleware "Permalink to this definition")

A middleware to retry failed requests that are potentially caused by
temporary problems such as a connection timeout or HTTP 500 error.

Failed pages are collected on the scraping process and rescheduled at the
end, once the spider has finished crawling all regular (non failed) pages.

The [`RetryMiddleware`](#scrapy.downloadermiddlewares.retry.RetryMiddleware "scrapy.downloadermiddlewares.retry.RetryMiddleware") can be configured through the following
settings (see the settings documentation for more info):

* [`RETRY_ENABLED`](#std-setting-RETRY_ENABLED)
* [`RETRY_TIMES`](#std-setting-RETRY_TIMES)
* [`RETRY_HTTP_CODES`](#std-setting-RETRY_HTTP_CODES)
* [`RETRY_EXCEPTIONS`](#std-setting-RETRY_EXCEPTIONS)

If `Request.meta` has `dont_retry` key
set to True, the request will be ignored by this middleware.

To retry requests from a spider callback, you can use the
[`get_retry_request()`](#scrapy.downloadermiddlewares.retry.get_retry_request "scrapy.downloadermiddlewares.retry.get_retry_request") function:

scrapy.downloadermiddlewares.retry.get\_retry\_request(*request: Request*, *\**, *spider: Spider*, *reason: str | Exception | type[Exception] = 'unspecified'*, *max\_retry\_times: int | None = None*, *priority\_adjust: int | None = None*, *logger: Logger = <Logger scrapy.downloadermiddlewares.retry (WARNING)>*, *stats\_base\_key: str = 'retry'*) → [Request](request-response.html#scrapy.http.Request "scrapy.http.Request") | [None](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)")[[source]](../_modules/scrapy/downloadermiddlewares/retry.html#get_retry_request)[¶](#scrapy.downloadermiddlewares.retry.get_retry_request "Permalink to this definition")

Returns a new `Request` object to retry the specified
request, or `None` if retries of the specified request have been
exhausted.

For example, in a [`Spider`](spiders.html#scrapy.Spider "scrapy.Spider") callback, you could use it as
follows:

```
def parse(self, response):
    if not response.text:
        new_request_or_none = get_retry_request(
            response.request,
            spider=self,
            reason='empty',
        )
        return new_request_or_none

```

*spider* is the [`Spider`](spiders.html#scrapy.Spider "scrapy.Spider") instance which is asking for the
retry request. It is used to access the [settings](settings.html#topics-settings)
and [stats](stats.html#topics-stats), and to provide extra logging context (see
[`logging.debug()`](https://docs.python.org/3/library/logging.html#logging.debug "(in Python v3.13)")).

*reason* is a string or an [`Exception`](https://docs.python.org/3/library/exceptions.html#Exception "(in Python v3.13)") object that indicates the
reason why the request needs to be retried. It is used to name retry stats.

*max\_retry\_times* is a number that determines the maximum number of times
that *request* can be retried. If not specified or `None`, the number is
read from the [`max_retry_times`](request-response.html#std-reqmeta-max_retry_times) meta key of the request. If the
[`max_retry_times`](request-response.html#std-reqmeta-max_retry_times) meta key is not defined or `None`, the number
is read from the [`RETRY_TIMES`](#std-setting-RETRY_TIMES) setting.

*priority\_adjust* is a number that determines how the priority of the new
request changes in relation to *request*. If not specified, the number is
read from the [`RETRY_PRIORITY_ADJUST`](#std-setting-RETRY_PRIORITY_ADJUST) setting.

*logger* is the logging.Logger object to be used when logging messages

*stats\_base\_key* is a string to be used as the base key for the
retry-related job stats

#### RetryMiddleware Settings[¶](#retrymiddleware-settings "Permalink to this heading")

##### RETRY\_ENABLED[¶](#retry-enabled "Permalink to this heading")

Default: `True`

Whether the Retry middleware will be enabled.

##### RETRY\_TIMES[¶](#retry-times "Permalink to this heading")

Default: `2`

Maximum number of times to retry, in addition to the first download.

Maximum number of retries can also be specified per-request using
[`max_retry_times`](request-response.html#std-reqmeta-max_retry_times) attribute of `Request.meta`.
When initialized, the [`max_retry_times`](request-response.html#std-reqmeta-max_retry_times) meta key takes higher
precedence over the [`RETRY_TIMES`](#std-setting-RETRY_TIMES) setting.

##### RETRY\_HTTP\_CODES[¶](#retry-http-codes "Permalink to this heading")

Default: `[500, 502, 503, 504, 522, 524, 408, 429]`

Which HTTP response codes to retry. Other errors (DNS lookup issues,
connections lost, etc) are always retried.

In some cases you may want to add 400 to [`RETRY_HTTP_CODES`](#std-setting-RETRY_HTTP_CODES) because
it is a common code used to indicate server overload. It is not included by
default because HTTP specs say so.

##### RETRY\_EXCEPTIONS[¶](#retry-exceptions "Permalink to this heading")

Default:

```
[
    'twisted.internet.defer.TimeoutError',
    'twisted.internet.error.TimeoutError',
    'twisted.internet.error.DNSLookupError',
    'twisted.internet.error.ConnectionRefusedError',
    'twisted.internet.error.ConnectionDone',
    'twisted.internet.error.ConnectError',
    'twisted.internet.error.ConnectionLost',
    'twisted.internet.error.TCPTimedOutError',
    'twisted.web.client.ResponseFailed',
    IOError,
    'scrapy.core.downloader.handlers.http11.TunnelError',
]

```

List of exceptions to retry.

Each list entry may be an exception type or its import path as a string.

An exception will not be caught when the exception type is not in
[`RETRY_EXCEPTIONS`](#std-setting-RETRY_EXCEPTIONS) or when the maximum number of retries for a request
has been exceeded (see [`RETRY_TIMES`](#std-setting-RETRY_TIMES)). To learn about uncaught
exception propagation, see
[`process_exception()`](#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception "scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception").

##### RETRY\_PRIORITY\_ADJUST[¶](#retry-priority-adjust "Permalink to this heading")

Default: `-1`

Adjust retry request priority relative to original request:

* a positive priority adjust means higher priority.
* **a negative priority adjust (default) means lower priority.**

### RobotsTxtMiddleware[¶](#module-scrapy.downloadermiddlewares.robotstxt "Permalink to this heading")

*class* scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware[[source]](../_modules/scrapy/downloadermiddlewares/robotstxt.html#RobotsTxtMiddleware)[¶](#scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware "Permalink to this definition")

This middleware filters out requests forbidden by the robots.txt exclusion
standard.

To make sure Scrapy respects robots.txt make sure the middleware is enabled
and the [`ROBOTSTXT_OBEY`](settings.html#std-setting-ROBOTSTXT_OBEY) setting is enabled.

The [`ROBOTSTXT_USER_AGENT`](settings.html#std-setting-ROBOTSTXT_USER_AGENT) setting can be used to specify the
user agent string to use for matching in the [robots.txt](https://www.robotstxt.org/) file. If it
is `None`, the User-Agent header you are sending with the request or the
[`USER_AGENT`](settings.html#std-setting-USER_AGENT) setting (in that order) will be used for determining
the user agent to use in the [robots.txt](https://www.robotstxt.org/) file.

This middleware has to be combined with a [robots.txt](https://www.robotstxt.org/) parser.

Scrapy ships with support for the following [robots.txt](https://www.robotstxt.org/) parsers:

* [Protego](#protego-parser) (default)
* [RobotFileParser](#python-robotfileparser)
* [Robotexclusionrulesparser](#rerp-parser)

You can change the [robots.txt](https://www.robotstxt.org/) parser with the [`ROBOTSTXT_PARSER`](settings.html#std-setting-ROBOTSTXT_PARSER)
setting. Or you can also [implement support for a new parser](#support-for-new-robots-parser).

If `Request.meta` has
`dont_obey_robotstxt` key set to True
the request will be ignored by this middleware even if
[`ROBOTSTXT_OBEY`](settings.html#std-setting-ROBOTSTXT_OBEY) is enabled.

Parsers vary in several aspects:

* Language of implementation
* Supported specification
* Support for wildcard matching
* Usage of [length based rule](https://developers.google.com/search/docs/crawling-indexing/robots/robots_txt#order-of-precedence-for-rules):
  in particular for `Allow` and `Disallow` directives, where the most
  specific rule based on the length of the path trumps the less specific
  (shorter) rule

Performance comparison of different parsers is available at [the following link](https://github.com/scrapy/scrapy/issues/3969).

#### Protego parser[¶](#protego-parser "Permalink to this heading")

Based on [Protego](https://github.com/scrapy/protego):

* implemented in Python
* is compliant with [Google’s Robots.txt Specification](https://developers.google.com/search/docs/crawling-indexing/robots/robots_txt)
* supports wildcard matching
* uses the length based rule

Scrapy uses this parser by default.

#### RobotFileParser[¶](#robotfileparser "Permalink to this heading")

Based on [`RobotFileParser`](https://docs.python.org/3/library/urllib.robotparser.html#urllib.robotparser.RobotFileParser "(in Python v3.13)"):

* is Python’s built-in [robots.txt](https://www.robotstxt.org/) parser
* is compliant with [Martijn Koster’s 1996 draft specification](https://www.robotstxt.org/norobots-rfc.txt)
* lacks support for wildcard matching
* doesn’t use the length based rule

It is faster than Protego and backward-compatible with versions of Scrapy before 1.8.0.

In order to use this parser, set:

* [`ROBOTSTXT_PARSER`](settings.html#std-setting-ROBOTSTXT_PARSER) to `scrapy.robotstxt.PythonRobotParser`

#### Robotexclusionrulesparser[¶](#robotexclusionrulesparser "Permalink to this heading")

Based on [Robotexclusionrulesparser](https://pypi.org/project/robotexclusionrulesparser/):

* implemented in Python
* is compliant with [Martijn Koster’s 1996 draft specification](https://www.robotstxt.org/norobots-rfc.txt)
* supports wildcard matching
* doesn’t use the length based rule

In order to use this parser:

* Install `Robotexclusionrulesparser` by running
  `pip install robotexclusionrulesparser`
* Set [`ROBOTSTXT_PARSER`](settings.html#std-setting-ROBOTSTXT_PARSER) setting to
  `scrapy.robotstxt.RerpRobotParser`

#### Implementing support for a new parser[¶](#implementing-support-for-a-new-parser "Permalink to this heading")

You can implement support for a new [robots.txt](https://www.robotstxt.org/) parser by subclassing
the abstract base class [`RobotParser`](#scrapy.robotstxt.RobotParser "scrapy.robotstxt.RobotParser") and
implementing the methods described below.

*class* scrapy.robotstxt.RobotParser[[source]](../_modules/scrapy/robotstxt.html#RobotParser)[¶](#scrapy.robotstxt.RobotParser "Permalink to this definition")
*abstract* allowed(*url: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [bytes](https://docs.python.org/3/library/stdtypes.html#bytes "(in Python v3.13)")*, *user\_agent: [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") | [bytes](https://docs.python.org/3/library/stdtypes.html#bytes "(in Python v3.13)")*) → [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)")[[source]](../_modules/scrapy/robotstxt.html#RobotParser.allowed)[¶](#scrapy.robotstxt.RobotParser.allowed "Permalink to this definition")

Return `True` if `user_agent` is allowed to crawl `url`, otherwise return `False`.

Parameters:

* **url** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") *or* [*bytes*](https://docs.python.org/3/library/stdtypes.html#bytes "(in Python v3.13)")) – Absolute URL
* **user\_agent** ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") *or* [*bytes*](https://docs.python.org/3/library/stdtypes.html#bytes "(in Python v3.13)")) – User agent

*abstract classmethod* from\_crawler(*crawler: [Crawler](api.html#scrapy.crawler.Crawler "scrapy.crawler.Crawler")*, *robotstxt\_body: [bytes](https://docs.python.org/3/library/stdtypes.html#bytes "(in Python v3.13)")*) → Self[[source]](../_modules/scrapy/robotstxt.html#RobotParser.from_crawler)[¶](#scrapy.robotstxt.RobotParser.from_crawler "Permalink to this definition")

Parse the content of a [robots.txt](https://www.robotstxt.org/) file as bytes. This must be a class method.
It must return a new instance of the parser backend.

Parameters:

* **crawler** ([`Crawler`](api.html#scrapy.crawler.Crawler "scrapy.crawler.Crawler") instance) – crawler which made the request
* **robotstxt\_body** ([*bytes*](https://docs.python.org/3/library/stdtypes.html#bytes "(in Python v3.13)")) – content of a [robots.txt](https://www.robotstxt.org/) file.

### DownloaderStats[¶](#module-scrapy.downloadermiddlewares.stats "Permalink to this heading")

*class* scrapy.downloadermiddlewares.stats.DownloaderStats[[source]](../_modules/scrapy/downloadermiddlewares/stats.html#DownloaderStats)[¶](#scrapy.downloadermiddlewares.stats.DownloaderStats "Permalink to this definition")

Middleware that stores stats of all requests, responses and exceptions that
pass through it.

To use this middleware you must enable the [`DOWNLOADER_STATS`](settings.html#std-setting-DOWNLOADER_STATS)
setting.

### UserAgentMiddleware[¶](#module-scrapy.downloadermiddlewares.useragent "Permalink to this heading")

*class* scrapy.downloadermiddlewares.useragent.UserAgentMiddleware[[source]](../_modules/scrapy/downloadermiddlewares/useragent.html#UserAgentMiddleware)[¶](#scrapy.downloadermiddlewares.useragent.UserAgentMiddleware "Permalink to this definition")

Middleware that allows spiders to override the default user agent.

In order for a spider to override the default user agent, its `user_agent`
attribute must be set.

### AjaxCrawlMiddleware[¶](#module-scrapy.downloadermiddlewares.ajaxcrawl "Permalink to this heading")

*class* scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware[[source]](../_modules/scrapy/downloadermiddlewares/ajaxcrawl.html#AjaxCrawlMiddleware)[¶](#scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware "Permalink to this definition")

Middleware that finds ‘AJAX crawlable’ page variants based
on meta-fragment html tag.

Note

Scrapy finds ‘AJAX crawlable’ pages for URLs like
`'http://example.com/!#foo=bar'` even without this middleware.
AjaxCrawlMiddleware is necessary when URL doesn’t contain `'!#'`.
This is often a case for ‘index’ or ‘main’ website pages.

#### AjaxCrawlMiddleware Settings[¶](#ajaxcrawlmiddleware-settings "Permalink to this heading")

##### AJAXCRAWL\_ENABLED[¶](#ajaxcrawl-enabled "Permalink to this heading")

Default: `False`

Whether the AjaxCrawlMiddleware will be enabled. You may want to
enable it for [broad crawls](broad-crawls.html#topics-broad-crawls).

#### HttpProxyMiddleware settings[¶](#httpproxymiddleware-settings "Permalink to this heading")

##### HTTPPROXY\_ENABLED[¶](#httpproxy-enabled "Permalink to this heading")

Default: `True`

Whether or not to enable the `HttpProxyMiddleware`.

##### HTTPPROXY\_AUTH\_ENCODING[¶](#httpproxy-auth-encoding "Permalink to this heading")

Default: `"latin-1"`

The default encoding for proxy authentication on `HttpProxyMiddleware`.

 [Previous](addons.html "Add-ons")
[Next](spider-middleware.html "Spider Middleware")

---

© Copyright Scrapy developers.
Last updated on Nov 19, 2024.

Built with [Sphinx](https://www.sphinx-doc.org/) using a
[theme](https://github.com/readthedocs/sphinx_rtd_theme)
provided by [Read the Docs](https://readthedocs.org).



=== Content from github.com_2a79b1ee_20250115_080056.html ===

[Skip to content](#start-of-content)

## Navigation Menu

Toggle navigation

[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fscrapy%2Fscrapy%2Fsecurity%2Fadvisories%2FGHSA-jwqp-28gf-p498)

* Product

  + [GitHub Copilot
    Write better code with AI](https://github.com/features/copilot)
  + [Security
    Find and fix vulnerabilities](https://github.com/features/security)
  + [Actions
    Automate any workflow](https://github.com/features/actions)
  + [Codespaces
    Instant dev environments](https://github.com/features/codespaces)
  + [Issues
    Plan and track work](https://github.com/features/issues)
  + [Code Review
    Manage code changes](https://github.com/features/code-review)
  + [Discussions
    Collaborate outside of code](https://github.com/features/discussions)
  + [Code Search
    Find more, search less](https://github.com/features/code-search)

  Explore
  + [All features](https://github.com/features)
  + [Documentation](https://docs.github.com)
  + [GitHub Skills](https://skills.github.com)
  + [Blog](https://github.blog)
* Solutions

  By company size
  + [Enterprises](https://github.com/enterprise)
  + [Small and medium teams](https://github.com/team)
  + [Startups](https://github.com/enterprise/startups)
  By use case
  + [DevSecOps](/solutions/use-case/devsecops)
  + [DevOps](/solutions/use-case/devops)
  + [CI/CD](/solutions/use-case/ci-cd)
  + [View all use cases](/solutions/use-case)

  By industry
  + [Healthcare](/solutions/industry/healthcare)
  + [Financial services](/solutions/industry/financial-services)
  + [Manufacturing](/solutions/industry/manufacturing)
  + [Government](/solutions/industry/government)
  + [View all industries](/solutions/industry)

  [View all solutions](/solutions)
* Resources

  Topics
  + [AI](/resources/articles/ai)
  + [DevOps](/resources/articles/devops)
  + [Security](/resources/articles/security)
  + [Software Development](/resources/articles/software-development)
  + [View all](/resources/articles)

  Explore
  + [Learning Pathways](https://resources.github.com/learn/pathways)
  + [White papers, Ebooks, Webinars](https://resources.github.com)
  + [Customer Stories](https://github.com/customer-stories)
  + [Partners](https://partner.github.com)
  + [Executive Insights](https://github.com/solutions/executive-insights)
* Open Source

  + [GitHub Sponsors
    Fund open source developers](/sponsors)
  + [The ReadME Project
    GitHub community articles](https://github.com/readme)
  Repositories
  + [Topics](https://github.com/topics)
  + [Trending](https://github.com/trending)
  + [Collections](https://github.com/collections)
* Enterprise

  + [Enterprise platform
    AI-powered developer platform](/enterprise)
  Available add-ons
  + [Advanced Security
    Enterprise-grade security features](https://github.com/enterprise/advanced-security)
  + [GitHub Copilot
    Enterprise-grade AI features](/features/copilot#enterprise)
  + [Premium Support
    Enterprise-grade 24/7 support](/premium-support)
* [Pricing](https://github.com/pricing)

Search or jump to...

# Search code, repositories, users, issues, pull requests...

Search

Clear

[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)

# Provide feedback

We read every piece of feedback, and take your input very seriously.

Include my email address so I can be contacted

  Cancel

 Submit feedback

# Saved searches

## Use saved searches to filter your results more quickly

Name

Query

To see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).

  Cancel

 Create saved search

[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fscrapy%2Fscrapy%2Fsecurity%2Fadvisories%2FGHSA-jwqp-28gf-p498)

[Sign up](/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E%2Frepos%2Fadvisories%2Fshow&source=header-repo&source_repo=scrapy%2Fscrapy)
Reseting focus

You signed in with another tab or window. Reload to refresh your session.
You signed out in another tab or window. Reload to refresh your session.
You switched accounts on another tab or window. Reload to refresh your session.

Dismiss alert

{{ message }}

[scrapy](/scrapy)
/
**[scrapy](/scrapy/scrapy)**
Public

* [Notifications](/login?return_to=%2Fscrapy%2Fscrapy) You must be signed in to change notification settings
* [Fork
  10.6k](/login?return_to=%2Fscrapy%2Fscrapy)
* [Star
   53.8k](/login?return_to=%2Fscrapy%2Fscrapy)

* [Code](/scrapy/scrapy)
* [Issues
  427](/scrapy/scrapy/issues)
* [Pull requests
  182](/scrapy/scrapy/pulls)
* [Discussions](/scrapy/scrapy/discussions)
* [Actions](/scrapy/scrapy/actions)
* [Projects
  0](/scrapy/scrapy/projects)
* [Wiki](/scrapy/scrapy/wiki)
* [Security](/scrapy/scrapy/security)
* [Insights](/scrapy/scrapy/pulse)

Additional navigation options

* [Code](/scrapy/scrapy)
* [Issues](/scrapy/scrapy/issues)
* [Pull requests](/scrapy/scrapy/pulls)
* [Discussions](/scrapy/scrapy/discussions)
* [Actions](/scrapy/scrapy/actions)
* [Projects](/scrapy/scrapy/projects)
* [Wiki](/scrapy/scrapy/wiki)
* [Security](/scrapy/scrapy/security)
* [Insights](/scrapy/scrapy/pulse)

# HTTP authentication credentials potentially leaked to target websites

Moderate

[Gallaecio](/Gallaecio)
published
GHSA-jwqp-28gf-p498
Oct 6, 2021

## Package

pip

Scrapy
([pip](/advisories?query=ecosystem%3Apip))

## Affected versions

>=2,<2.5.1
<1.8.1

## Patched versions

>=2.5.1
>=1.8.1,<2

## Description

### Impact

If you use [`HttpAuthMiddleware`](http://doc.scrapy.org/en/latest/topics/downloader-middleware.html#module-scrapy.downloadermiddlewares.httpauth) (i.e. the `http_user` and `http_pass` spider attributes) for HTTP authentication, all requests will expose your credentials to the request target.

This includes requests generated by Scrapy components, such as `robots.txt` requests sent by Scrapy when the `ROBOTSTXT_OBEY` setting is set to `True`, or as requests reached through redirects.

### Patches

Upgrade to Scrapy 2.5.1 and use the new `http_auth_domain` spider attribute to control which domains are allowed to receive the configured HTTP authentication credentials.

If you are using Scrapy 1.8 or a lower version, and upgrading to Scrapy 2.5.1 is not an option, you may upgrade to Scrapy 1.8.1 instead.

### Workarounds

If you cannot upgrade, set your HTTP authentication credentials on a per-request basis, using for example the [`w3lib.http.basic_auth_header`](https://w3lib.readthedocs.io/en/latest/w3lib.html#w3lib.http.basic_auth_header) function to convert your credentials into a value that you can assign to the `Authorization` header of your request, instead of defining your credentials globally using [`HttpAuthMiddleware`](http://doc.scrapy.org/en/latest/topics/downloader-middleware.html#module-scrapy.downloadermiddlewares.httpauth).

### For more information

If you have any questions or comments about this advisory:

* [Open an issue](https://github.com/scrapy/scrapy/issues)
* Email us

### Severity

Moderate

### CVE ID

CVE-2021-41125

### Weaknesses

No CWEs

## Footer

© 2025 GitHub, Inc.

### Footer navigation

* [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)
* [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)
* [Security](https://github.com/security)
* [Status](https://www.githubstatus.com/)
* [Docs](https://docs.github.com/)
* [Contact](https://support.github.com?tags=dotcom-footer)
* Manage cookies
* Do not share my personal information

You can’t perform that action at this time.


