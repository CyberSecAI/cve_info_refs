=== Content from docs.vmware.com_18d758c5_20250115_210152.html ===
VMware ESXi670-202111001 Release Notes

Release Date: NOV 23, 2021

Build Details

Download Filename:

ESXi670-202111001.zip

Build:

Download Size:

md5sum:

sha256checksum:

18828794

478.2 MB

faa7493ac44f766c307bfa1a92f2bc03

92fb8d22d012c34195a39e6ccca2f123dfaf6db64fafcead131e9d71ccb2b146

Host Reboot Required:

Yes

Virtual Machine Migration or Shutdown Required: Yes

Bulletins

Bulletin ID

Category Severity

ESXi670-202111401-BG Bugfix

Critical

ESXi670-202111402-BG Bugfix

Important

ESXi670-202111403-BG Bugfix

Important

ESXi670-202111404-BG Bugfix

Important

ESXi670-202111405-BG Bugfix

Important

ESXi670-202111101-SG Security

Important

ESXi670-202111102-SG Security

Important

ESXi670-202111103-SG Security

Important

ESXi670-202111104-SG Security

Important

Rollup Bulletin

This rollup bulletin contains the latest VIBs with all the fixes since the initial release of ESXi 6.7.

Bulletin ID

ESXi670-202111001

Category

Bugfix

Severity

Critical

IMPORTANT: For clusters using VMware vSAN, you must first upgrade the vCenter Server system. Upgrading only the ESXi hosts is not supported.

Before an upgrade, always verify in the VMware Product Interoperability Matrix compatible upgrade paths from earlier versions of ESXi, vCenter Server and vSAN to

the current version.

Image Profiles

VMware patch and update releases contain general and critical image profiles. Application of the general release image profile applies to new bug fixes.

Image Profile Name

ESXi-6.7.0-20211104001-standard

ESXi-6.7.0-20211104001-no-tools

ESXi-6.7.0-20211101001s-standard

ESXi-6.7.0-20211101001s-no-tools

For more information about the individual bulletins, see the Product Patches page and the Resolved Issues section.

Patch Download and Installation

The typical way to apply patches to ESXi hosts is by using the VMware vSphere Update Manager. For details, see the About Installing and Administering VMware

vSphere Update Manager.

ESXi hosts can be updated by manually downloading the patch ZIP file from VMware Customer Connect. From the Select a Product drop-down menu, select ESXi

(Embedded and Installable) and from the Select a Version drop-down menu, select 6.7.0. Install VIBs by using the esxcli software vib update command. Additionally,

you can update the system by using the image profile and the esxcli software profile update command.

For more information, see the vSphere Command-Line Interface Concepts and Examples and the vSphere Upgrade Guide.

Resolved Issues

The resolved issues are grouped as follows.

ESXi670-202111401-BG

ESXi670-202111402-BG

ESXi670-202111403-BG

ESXi670-202111404-BG

ESXi670-202111405-BG

ESXi670-202111101-SG

ESXi670-202111102-SG

ESXi670-202111103-SG

ESXi670-202111104-SG

ESXi-6.7.0-20211104001-standard

ESXi-6.7.0-20211104001-no-tools

ESXi-6.7.0-20211101001s-standard

ESXi-6.7.0-20211101001s-no-tools

ESXi670-202111401-BG

Patch Category

Patch Severity

Host Reboot Required

Virtual Machine Migration or Shutdown
Required

Affected Hardware

Affected Software

VIBs Included

PRs Fixed

Bugfix

Critical

Yes

Yes

N/A

N/A

VMware_bootbank_vsan_6.7.0-3.159.18811818
VMware_bootbank_esx-base_6.7.0-3.159.18828794
VMware_bootbank_esx-update_6.7.0-3.159.18828794
VMware_bootbank_vsanhealth_6.7.0-3.159.18811819

2731317, 2727062, 2704831, 2764894, 2720189, 2761940, 2757263,
2752542, 2772048, 2768356, 2741112, 2786864, 2725025, 2776791,
2724166, 2790353, 2787866, 2790502, 2834968, 2832090, 2841163,
2804742, 2732482, 2793380, 2813328, 2792504, 2749902, 2765136,
2697256, 2731139, 2733489, 2755232, 2761131, 2763933, 2766401,
2744211, 2771730, 2800655, 2839716, 2847812, 2726678, 2789302,
2750034, 2739223, 2794562, 2857511, 2847091, 2765679, 2731643,
2774667, 2778008, 2833428, 2835990, 2852099

CVE numbers

N/A

Updates esx-base, esx-update, vsan, and vsanhealth VIBs to resolve the following issues:

PR 2731317: During an USB passthrough, virtual machines might become unresponsive

If the USB device attached to an ESXi host has descriptors that are not compliant with the standard USB specifications, the virtual USB stack might fail to pass

through a USB device into a virtual machine. As a result, virtual machines become unresponsive, and you must either power off the virtual machine by using an

ESXCLI command, or restart the ESXi host.

This issue is resolved in this release.

PR 2727062: Editing an advanced options parameter in a host profile and setting a value to false, results in setting the value to true

When attempting to set a value to false for an advanced option parameter in a host profile, the user interface creates a non-empty string value. Values that are

not empty are interpreted as true and the advanced option parameter receives a true value in the host profile.

This issue is resolved in this release.

PR 2704831: After vSphere Replication appliance powers on, ESXi hosts intermittently lose connectivity to vCenter Server

A rare race condition with static map initialization might cause ESXi hosts to temporarily lose connectivity to vCenter Server after the vSphere Replication

appliance powers on. However, the hostd service automatically restarts and the ESXi hosts restore connectivity.

This issue is resolved in this release.

PR 2764894: The hostd service might fail and ESXi hosts lose connectivity to vCenter Server due to an empty or unset property of a Virtual Infrastructure

Management (VIM) API array

In rare cases, an empty or unset property of a VIM API data array might cause the hostd service to fail. As a result, ESXi hosts lose connectivity to vCenter Server

and you must manually reconnect the hosts.

This issue is resolved in this release.

PR 2720189: ESXi hosts intermittently fail with a BSVMM_Validate@vmkernel error in the backtrace

A rare error condition in the VMKernel might cause ESXi hosts to fail when powering on a virtual machine with more than 1 virtual CPU. A BSVMM_Validate@vmkernel

error is indicative for the problem.

This issue is resolved in this release.

PR 2761940: An ESXi host might fail with a purple diagnostic screen due to locks during a vSphere vMotion operation

During a vSphere vMotion operation, if the calling context holds any locks, ESXi hosts might fail with a purple diagnostic screen and an error such as PSOD: Panic

at bora/vmkernel/core/lock.c:2070.

This issue is resolved in this release.

PR 2757263: ESXi hosts fail with a purple diagnostic screen with a kernel context error, but not in the correct context

ESXi hosts might intermittently fail with a purple diagnostic screen with an error such as @BlueScreen: VERIFY bora/vmkernel/sched/cpusched.c that suggests a

preemption anomaly. However, the VMkernel preemption anomaly detection logic might fail to identify the correct kernel context and show a warning for the

wrong context.

This issue is resolved in this release. The fix covers corner cases that might cause a preemption anomaly, such as when a system world exits while holding a

spinlock. The preemption anomaly detection warning remains @BlueScreen: VERIFY bora/vmkernel/sched/cpusched.c, but in the correct context.

PR 2752542: If you lower the value of the DiskMaxIOSize advanced config option, ESXi hosts I/O operations might fail

If you change the DiskMaxIOSize advanced config option to a lower value, I/Os with large block sizes might get incorrectly split and queue at the PSA path. As a

result, ESXi hosts I/O operations might time out and fail.

This issue is resolved in this release.

PR 2772048: ESXi hosts might fail with a purple diagnostic screen due to a memory allocation failure in the vmkapi character device driver

Some poll requests might exceed the metadata heap of the vmkapi character device driver and cause ESXi hosts to fail with a purple diagnostic screen and an

error such as:

#PF Exception 14 in world 2099138:IPMI Response IP 0x41800b922ab0 addr 0x18 PTEs:0x0;

In the VMkernel logs, you might see messages such as:

WARNING: Heap: 3571: Heap VMKAPI-char-metadata already at its maximum size. Cannot expand.

and

WARNING: Heap: 4079: Heap_Align(VMKAPI-char-metadata, 40/40 bytes, 8 align) failed. caller: 0x418029922489

This issue is resolved in this release.

PR 2741112: Failure of some Intel CPUs to forward Debug Exception (#DB) traps might result in virtual machine triple fault

In rare cases, some Intel CPUs might fail to forward #DB traps and if a timer interrupt happens during the Windows system call, virtual machine might triple fault.

In the vmware.log file, you see an error such as msg.monitorEvent.tripleFault. This issue is not consistent and virtual machines that consistently triple fault or triple

fault after boot are impacted by another issue.

This issue is resolved in this release. The fix forwards all #DB traps from CPUs into the guest operating system, except when the DB trap comes from a debugger

that is attached to the virtual machine.

PR 2786864: An ESXi host fails with a purple diagnostic screen due to a call stack corruption in the ESXi storage stack

While handling the SCSI command READ CAPACITY (10), ESXi might copy excess data from the response and corrupt the call stack. As a result, the ESXi host fails

with a purple diagnostic screen.

This issue is resolved in this release.

PR 2725025: You see alarms that a NIC link is down even after the uplink is restored

If you plug in and out a physical NIC in your vCenter Sever system, after the uplink is restored, you still see an alarm in the vSphere Client or the vSphere Web

Client that a NIC link on some ESXi hosts is down. The VOBD daemon might not create the event esx.clear.net.redundancy.restored to remove such alarms, which

causes the issue.

This issue is resolved in this release.

PR 2776791: ESXi installation by using a ks.cfg installation script and a USB or SD booting device might fail

In some cases, when multiple USB or SD devices with different file systems are connected to your vCenter Server system, ESXi installation by using a ks.cfg

installation script and a USB or SD booting device might fail.

This issue is resolved in this release. The fix logs an exception and allows the installation to continue, instead of failing.

PR 2724166: You might see higher than configured rate of automatic unmap operations on a newly mounted VMFS6 volumes

In the first minutes after mounting a VMFS6 volume, you might see higher than configured rate of automatic unmap operations. For example, if you set the

unmap bandwidth to 2000 MB/s, you see unmap operations running at more than 3000 MB/s.

This issue is resolved in this release.

PR 2790353: Old data on an independent nonpersistent mode virtual disk remains after a virtual machine reset

For independent nonpersistent mode virtual disks, all writes are stored in a redo log, which is a temporary file with extension .REDO_XXXXXX in the VM directory.

However, a reset of a virtual machine does not clear the old redo log on such disks.

This issue is resolved in this release.

PR 2787866: A replay of a pre-recorded support bundle by using the esxtop utility might fail with a segmentation fault

When you run the esxtop utility by using the ESXi Shell to replay a pre-recorded support bundle, the operation might fail with a segmentation fault.

This issue is resolved in this release. The fix makes sure all local variables of the esxtop utility are initialized properly.

PR 2790502: Shutdown or reboot of ESXi hosts takes very long

Rarely, in certain configurations, the shutdown or reboot of ESXi hosts might stop at the step Shutting down device drivers for a long time, in the order of 20

minutes, but the operation eventually completes.

This issue is resolved in this release.

PR 2834968: Virtual machines fail with a message for too many sticky pages

The virtual network adapter VMXNET Generation 3 (VMXNET3) uses buffers to process rx packets. Such buffers are either pre-pinned or pinned and mapped

during runtime. In rare occasions, some buffers might not un-pin and get re-pinned later, resulting in a higher-than-expected pin count of such a buffer. As a

result, virtual machines might fail with an error such as MONITOR PANIC: vmk: vcpu-0:Too many sticky pages..

This issue is resolved in this release.

PR 2832090: ESXi hosts might fail with a purple diagnostic screen when vSphere Replication is enabled due to a rare lock rank violation

Due to a rare lock rank violation in the vSphere Replication I/O filter, some ESXi hosts might fail with a purple diagnostic screen when vSphere Replication is

enabled. You see an error such as VERIFY bora/vmkernel/main/bh.c:978 on the screen.

This issue is resolved in this release.

PR 2841163: You see packet drops for virtual machines with VMware Network Extensibility (NetX) redirection enabled

In vCenter Server advanced performance charts, you see an increasing number of packet drop count for all virtual machines that have NetX redirection enabled.

However, if you disable NetX redirection, the count becomes 0.

This issue is resolved in this release.

PR 2804742: A virtual switch might drop IGMPv3 SSM specific queries during snooping

Virtual switches processing IGMPv3 SSM specific queries might drop some queries when an IP is not included in the list of available ports as a result of the source

IP check.

This issue is resolved in this release. The fix makes sure a group specific query is not processed like a normal multicast group and does not check the source IP.

PR 2732482: You see higher than usual scrub activity on vSAN

An integer overflow bug might cause vSAN DOM to issue scrubs more frequently than the configured setting.

This issue is resolved in this release.

PR 2793380: ESXi hosts become unresponsive when you power on a virtual machine

In particular circumstances, when you power on a virtual machine with a corrupted VMware Tools manifest file, the hostd service might fail. As a result, the ESXi

host becomes unresponsive. You can backtrace the issue in the hostd dump file that is usually generated in such cases at the time a VM is powered on.

This issue is resolved in this release.

PR 2813328: Encrypted virtual machines do not auto power on even when the Start delay option is configured

After a reboot of an ESXi host, encrypted virtual machines might not auto power on even when Autostart is configured with the Start delay option to set a

specific start time of the host. The issue affects only encrypted virtual machines due to a delay in the distribution of keys from standard key providers.

This issue is resolved in this release. The fix makes sure that the Start delay option for encrypted VMs considers the delay for getting a key after an ESXi host

reboot.

PR 2792504: Disabling the SLP service might cause failures during operations with host profiles

When the SLP service is disabled to prevent potential security vulnerabilities, the sfcbd-watchdog service might remain enabled and cause compliance check

failures when you perform updates by using a host profile.

This issue is resolved in this release.

PR 2749902: vSphere vMotion operations on ESXi 6.7.x hosts with more than 120 virtual machines might get slow or occasionally fail

When an ESXi 6.7.x host has more than 120 VMs on it, vSphere vMotion operations to that host or from that host might become slow or occasionally fail.

This issue is resolved in this release.

PR 2765136: vSphere Client might not meet all current browser security standards

In certain environments, vSphere Client might not fully protect the underlying websites through modern browser security mechanisms.

This issue is resolved in this release.

PR 2697256: ESXi updates and upgrades might fail on Mac Pro servers

ESXi updates and upgrades might fail on Mac Pro servers with a purple diagnostic screen and a message such as PSOD - NOT IMPLEMENTED

bora/vmkernel/hardware/pci/bridge.c:372 PCPU0:2097152/bootstrap. The issue occurs when Mac Pro BIOS does not assign resources to some devices.

This issue is resolved in this release.

PR 2731139: Packets with "IPv6 Tunnel Encapsulation Limit option" are lost in traffic between virtual machines with ENS enabled

Some Linux kernels add the IPv6 Tunnel Encapsulation Limit option to IPv6 tunnel packets as described in the RFC 2473, par. 5.1. As a result, IPv6 tunnel packets

are dropped in traffic between virtual machines with ENS enabled, because of the IPv6 extension header.

This issue is resolved in this release. The fix is to correctly parse packets with the IPv6 Tunnel Encapsulation Limit option enabled.

PR 2733489: Taking a snapshot of a large virtual machine on a large VMFS6 datastore might take long

Resource allocation for a delta disk to create a snapshot of a large virtual machine, with virtual disks equal to or exceeding 1TB, on large VMFS6 datastores of

30TB or more, might take significant time. As a result, the virtual machine might temporarily lose connectivity. The issue affects primarily VMFS6 filesystems.

This issue is resolved in this release.

PR 2755232: An ESXi host might not discover devices configured with the VMW_SATP_INV plug-in

In case of temporary connectivity issues, ESXi hosts might not discover devices configured with the VMW_SATP_INV plug-in after connectivity restores, because SCSI

commands that fail during the device discovery stage cause an out-of-memory condition for the plug-in.

This issue is resolved in this release.

PR 2761131: If you import virtual machines with a virtual USB device to a vCenter Server system, the virtual machines might fail to power on

If you import a virtual machine with a virtual USB device that is not supported by vCenter Server, such as a virtual USB camera, to a vCenter Server system, the

VM might fail to power on. The start of such virtual machines fails with an error message similar to: PANIC: Unexpected signal: 11. The issue affects mostly VM

import operations from VMware Fusion or VMware Workstation systems, which support a wide range of virtual USB devices.

This issue is resolved in this release. The fix ignores any unsupported virtual USB devices in virtual machines imported to a vCenter Server system.

PR 2763933: In the vmkernel.log, you see an error for an unexpected result while waiting for pointer block cluster (PBC) to clear

A non-blocking I/O operation might block a PBC file operation in a VMFS volume. In the vmkernel.log, you see an error such as:

cpu2:2098100)WARNING: PB3: 5020: Unexpected result (Would block) while waiting for PBC to clear -FD c63 r24

However, this is expected behavior.

This issue is resolved in this release. The fix removes the error message in cases when an I/O operation blocks a PBC file operation.

PR 2766401: When an overlay tunnel is configured from a guest VM with different default VXLAN port than 4789, vmxnet3 might drop packets

By default, the vmxnet3 driver uses port 4789 when a VM uses ENS and port 8472 when ENS is not enabled. As a result, when an overlay tunnel is configured

from a guest VM with different default VXLAN port, vmxnet3 might drop packets.

This issue is resolved in this release. The fix makes sure vmxnet3 delivers packets when a VM uses either of ports 4789 or 8472. However, the fix works only on

VMs with version ESXi 6.7 and later, hardware versions later than 14, and if the overlay tunnel is configured from a guest VM.

PR 2744211: A static MAC cannot be learned on another VNIC port across a different VLAN

In some cases, MAC learning does not work as expected and affects some virtual machine operations. For example, cloning a VM with the same MAC address on

a different VLAN causes a traffic flood to the cloned VM in the ESXi host where the original VM is present.

This issue is resolved in this release. The fix makes sure that a static MAC can be learned on another port, on a different VLAN. For example, a static MAC x on

VNIC port p1 with on VLAN 1, can be learned on VNIC port p2, VLAN 2.

PR 2771730: You cannot create a vCenter Host Profiles CIM Indication Subscription due to an out of memory error

Attempts to create a vCenter Host Profiles CIM Indication Subscription might fail due to a cim-xml parsing request that returns an out of memory error.

This issue is resolved in this release.

PR 2800655: You might see packet drops on uplink on heavy traffic

The packet scheduler that controls network I/O in a vCenter Server system has a fixed length queue that might quickly fill up. As a result, you might see packet

drops on uplink on heavy traffic.

This issue is resolved in this release. The fix makes the Network I/O Control queue size dynamic to allow expansion under certain conditions.

PR 2839716: You cannot add Intel Ice Lake servers to a Skylake cluster with enabled Enhanced vMotion Compatibility (EVC) mode

In a vSphere 6.7.x system, you cannot add an Intel Ice Lake server to a Skylake cluster with enabled EVC mode. In the vSphere Web Client or the vSphere Client,

you see the following message:

Host CPU lacks features required by that mode. XSAVE of BND0-BND3 bounds registers (BNDREGS) is unsupported. XSAVE of BNDCFGU and BNDSTATUS registers (BNDCSR)

is unsupported.

This issue is resolved in this release. You can update your ESXi hosts to ESXi 670-202111001, or upgrade to ESXi 7.0.x.

PR 2847812: Active Directory latency issues might impact performance across the entire vCenter Server system

The VIM API server deletes the Active Directory cache on certain intervals to check the validity of permissions of Active Directory domain accounts. After the

cache delete operation, daily by default, the VIM API server re-populates the cache. However, the cache re-populate operation might be slow on domains with

many accounts. As a result, such operations might impact performance across the entire vCenter Server system.

This issue is resolved in this release. The fix skips the step of re-populating the cache.

PR 2726678: ESXi hosts might become unresponsive after an abrupt power cycling or reboot

An abrupt power cycling or reboot of an ESX host might cause a race between subsequent journal replays because other hosts might try to access the same

resources. If a race condition happens, the journal replay cannot complete. As a result, virtual machines on a VMFS6 datastore become inaccessible or any

operations running on the VMs fail or stop.

This issue is resolved in this release. The fix enhances local and remote journal replay for VMFS6.

PR 2789302: Multiple cron instances run after using the reboot_helper.py script to shut down a vSAN cluster

If you manually run the reboot_helper.py script to shut down a vSAN cluster, the process cleans unused cron jobs registered during the shutdown. However, the

script might start a new cron instance instead of restarting the cron daemon after cleaning the unused cron jobs. As a result, multiple cron instances might

accumulate and you see cron jobs executed multiple times.

This issue is resolved in this release.

PR 2750034: The hostd service might fail with a “Memory exceeds hard limit” error due to a memory leak

If you run a recursive call on a large datastore, such as DatastoreBrowser::SearchSubFolders, a memory leak condition might cause the hostd service to fail with an

out-of-memory error.

This issue is resolved in this release. The fix prevents the memory leak condition.

PR 2739223: vSAN health reports certificate errors in the configured KMIP provider

When you enable encryption for a vSAN cluster with a Key Management Interoperability Protocol (KMIP) provider, the following health check might report status

errors: vCenter KMS status.

This issue is resolved in this release.

PR 2794562: The recommended size of the core dump partition for an all-flash vSAN cluster might be larger than required

For all-flash vSAN configurations, you might see larger than required recommended size for the core dump partition. As a result, the core dump configuration

check might fail.

This issue is resolved in this release.

PR 2857511: Virtual machines appear as inaccessible in the vSphere Client and you might see some downtime for applications

In rare cases, hardware issues might cause an SQlite DB corruption that makes multiple VMs become inaccessible and lead to some downtime for applications.

This issue is resolved in this release.

PR 2847091: Hosts in a vSAN cluster might fail due to a memory block attributes issue

When a vSAN host has memory pressure, adding disks to a disk group might cause failure of the memory block attributes (blkAttr) component initialization.

Without the blkAttr component, commit and flush tasks stop, which causes log entries to build up in the SSD and eventually cause а congestion. As a result, a

NMI failure might occur due to the CPU load for processing the large number of log entries.

This issue is resolved in this release.

PR 2765679: Cannot add hosts to a vSAN cluster with large cluster support enabled

In some cases, the host designated as the leader cannot send heartbeat messages to other hosts in a large vSAN cluster. This issue occurs when the leader uses

an insufficient size of the TX buffer. The result is that new hosts cannot join the cluster with large cluster support (up to 64 hosts) enabled.

This issue is resolved in this release.

PR 2731643: You see memory congestion on vSAN cluster hosts with large number of idle VMs

On a vSAN cluster with a large number of idle virtual machines, vSAN hosts might experience memory congestion due to higher scrub frequency.

This issue is resolved in this release.

PR 2774667: vSAN automatic rebalance does not occur even after capacity reaches 80%

An issue with trim space operations count might cause a large number of pending delete operations in vSAN. If an automatic rebalancing operation triggers, it

cannot progress due to such pending delete operations. For example, if you set the rebalancing threshold at 30%, you do not see any  progress on rebalancing

even when the storage capacity reaches 80%.

This issue is resolved in this release.

PR 2778008: ESXi hosts might fail with a purple diagnostic screen due to a lock spinout on a virtual machine configured to use persistent Memory (PMem)

with large number of vCPUs

Due to a dependency between the management of virtual RAM and virtual NVDIMM within a virtual machine, excessive access to the virtual NVDIMM device

might lead to a lock spinout while accessing virtual RAM and cause the ESXi host to fail with a purple diagnostic screen. In the backtrace, you see the following:

SP_WaitLock

SPLockWork

AsyncRemapPrepareRemapListVM

AsyncRemap_AddOrRemapVM

LPageSelectLPageToDefrag

VmAssistantProcessTasks

This issue is resolved in this release.

PR 2833428: ESXi hosts with virtual machines with Latency Sensitivity enabled might randomly become unresponsive due to CPU starvation

When you enable Latency Sensitivity on virtual machines, some threads of the Likewise Service Manager (lwsmd), which sets CPU affinity explicitly, might

compete for CPU resources on such virtual machines. As a result, you might see the ESXi host and the hostd service to become unresponsive.

This issue is resolved in this release. The fix makes sure lwsmd does not set CPU affinity explicitly.

PR 2835990: Adding ESXi hosts to an Active Directory domain might take long

Some LDAP queries that have no specified timeouts might cause a significant delay in domain join operations for adding an ESXi host to an Active Directory

domain.

This issue is resolved in this release. The fix adds a 15 seconds standard timeout with additional logging around the LDAP calls during domain join workflow.

PR 2852099: When vSphere Replication is enabled on a virtual machine, many other VMs might become unresponsive

When vSphere Replication is enabled on a virtual machine, you might see higher datastore and in-guest latencies that in certain cases might lead to ESXi hosts

becoming unresponsive to vCenter Server. The increased latency comes from vSphere Replication computing MD5 checksums on the I/O completion path, which

delays all other I/Os.

This issue is resolved in this release. The fix offloads the vSphere Replication MD5 calculation from the I/O completion path to a work pool and reduces the

amount of outstanding I/O that vSphere Replication issues.

ESXi670-202111402-BG

Patch Category

Patch Severity

Host Reboot Required

Virtual Machine Migration or Shutdown
Required

Affected Hardware

Affected Software

VIBs Included

PRs Fixed

CVE numbers

Updates the vmkusb VIB to resolve the following issue:

Bugfix

Important

Yes

Yes

N/A

N/A

VMW_bootbank_vmkusb_0.1-4vmw.670.3.159.18828794

2761169

N/A

PR 2761169: Connection to the /bootbank partition intermittently breaks when you use USB or SD devices

ESXi670-202111001 adds two fixes to the USB host controller for cases when a USB or SD device lose connectivity to the /bootbank partition:

1. Adds a parameter usbStorageRegisterDelaySecs.

The usbStorageRegisterDelaySecs parameter is part of the vmkusb module and it delays to register cached USB storage device within the limit of 0 to 600

seconds. The default delay is 10 seconds. The parametric delay makes sure that if the USB host controller disconnects from your vSphere system and any

resource on the device, such as dump files, is still in use by ESXi, the USB or SD device does not get a new path that might break connection to the

/bootbank partition or corrupt the VMFS-L LOCKER partition.

2. Enhances the USB host controller driver to tolerate command timeout failures.

In cases when a device, such as Dell IDSDM, reconnects by a hardware signal, or a USB device resets due to a heavy workload, the USB host controller

driver might make only one retry to restore connectivity. With the fix, the controller makes multiple retries and extends tolerance to timeout failures.

This issue is resolved in this release. As a best practice, do not set dump partition on USB storage device and do not set USB devices under a heavy workload.

For more information, see VMware knowledge base articles 2077516 and 2149257.

ESXi670-202111403-BG

Patch Category

Patch Severity

Host Reboot Required

Virtual Machine Migration or Shutdown
Required

Affected Hardware

Affected Software

VIBs Included

PRs Fixed

CVE numbers

Updates the nvme VIB.

ESXi670-202111404-BG

Bugfix

Important

Yes

Yes

N/A

N/A

VMW_bootbank_nvme_1.2.2.28-5vmw.670.3.159.18828794

2555789

N/A

Patch Category

Patch Severity

Host Reboot Required

Virtual Machine Migration or Shutdown
Required

Affected Hardware

Affected Software

VIBs Included

PRs Fixed

CVE numbers

Bugfix

Important

Yes

Yes

N/A

N/A

VMW_bootbank_brcmfcoe_11.4.1078.26-
14vmw.670.3.159.18828794

2775375

N/A

Updates the brcmfcoe VIB to resolve the following issue:

PR 2775375: ESXi hosts might lose connectivity after brcmfcoe driver upgrade on Hitachi storage arrays

After an upgrade of the brcmfcoe driver on Hitachi storage arrays, ESXi hosts might fail to boot and lose connectivity.

This issue is resolved in this release.

ESXi670-202111405-BG

Patch Category

Patch Severity

Host Reboot Required

Virtual Machine Migration or Shutdown
Required

Affected Hardware

Affected Software

VIBs Included

PRs Fixed

CVE numbers

Updates the lsu-smartpqi-plugin  VIB.

Bugfix

Important

Yes

No

N/A

N/A

VMware_bootbank_lsu-smartpqi-plugin_1.0.0-
4vmw.670.3.159.18828794

2820768

N/A

PR 2820768: OEM smartpqi drivers of version 1.0.3.2323 and earlier fail to blink LEDs or get the physical locations on logical drives

The OEM smartpqi driver in ESXi 670-202111001 uses a new logical LUN ID assignment rule to become compatible with the drivers of versions later than

1.0.3.2323. As a result, OEM smartpqi drivers of version 1.0.3.2323 and earlier might fail to blink LEDs or get the physical locations on logical drives.

This issue is resolved in this release. However, if you use an older version of the OEM smartpqi driver, upgrade to a version later than 1.0.3.2323 to avoid the

issue.

ESXi670-202111101-SG

Patch Category

Patch Severity

Host Reboot Required

Virtual Machine Migration or Shutdown
Required

Affected Hardware

Affected Software

Security

Important

Yes

Yes

N/A

N/A

VIBs Included

PRs Fixed

CVE numbers

VMware_bootbank_vsanhealth_6.7.0-3.155.18811783
VMware_bootbank_vsan_6.7.0-3.155.18811780
VMware_bootbank_esx-update_6.7.0-3.155.18812553
VMware_bootbank_esx-base_6.7.0-3.155.18812553

2704746, 2765136, 2794798, 2794804, 2794933, 2794943,
2795965, 2797008, 2830222, 2830671

CVE-2015-5180, CVE-2015-8777, CVE-2015-8982, CVE-2016-3706,
CVE-2017-1000366, CVE-2018-1000001, CVE-2018-19591, CVE-2019-
19126, CVE-2020-10029, CVE-2021-28831

Updates esx-base, esx-update, vsan, and vsanhealth VIBs to resolve the following issues:

Update of the SQLite database

The SQLite database is updated to version 3.34.1.

Update to OpenSSL

The OpenSSL package is updated to version openssl-1.0.2za.

Update to cURL

The cURL library is updated to 7.78.0.

Update to the OpenSSH

The OpenSSH is updated to version 8.6p1.

Update to the libarchive library

The libarchive library is updated to libarchive - 3.5.1.

Update to the BusyBox package

The Busybox package is updated to address CVE-2021-28831.

Update to the GNU C Library (glibc)

The glibc library is updated tо address the following CVEs: CVE-2015-5180, CVE-2015-8777, CVE-2015-8982, CVE-2016-3706, CVE-2017-1000366, CVE-2018-

1000001, CVE-2018-19591, CVE-2019-19126, CVE-2020-10029.

ESXi670-202111102-SG

Patch Category

Patch Severity

Host Reboot Required

Virtual Machine Migration or Shutdown
Required

Affected Hardware

Affected Software

VIBs Included

PRs Fixed

CVE numbers

Updates the vmkusb VIB.

ESXi670-202111103-SG

Patch Category

Patch Severity

Host Reboot Required

Virtual Machine Migration or
Shutdown Required

Affected Hardware

Affected Software

VIBs Included

PRs Fixed

Security

Important

Yes

Yes

N/A

N/A

VMW_bootbank_vmkusb_0.1-1vmw.670.3.155.18812553

2762034

N/A

Security

Important

No

No

N/A

N/A

VMware_locker_tools-light_11.3.5.18557794-18812553

2762034, 2785530, 2816541

Related CVE numbers

N/A

This patch updates the tools-light VIB.

The following VMware Tools ISO images are bundled with ESXi 670-202111001:

windows.iso: VMware Tools 11.3.5 supports Windows 7 SP1 or Windows Server 2008 R2 SP1 and later.

linux.iso: VMware Tools 10.3.23 ISO image for Linux OS with glibc 2.11 or later.

The following VMware Tools ISO images are available for download:

VMware Tools 11.0.6:

windows.iso: for Windows Vista (SP2) and Windows Server 2008 Service Pack 2 (SP2).

VMware Tools 10.0.12:

winPreVista.iso: for Windows 2000, Windows XP, and Windows 2003.

linuxPreGLibc25.iso: supports Linux guest operating systems earlier than Red Hat Enterprise Linux (RHEL) 5, SUSE Linux Enterprise Server (SLES) 11,

Ubuntu 7.04, and other distributions with glibc version earlier than 2.5.

solaris.iso: VMware Tools image 10.3.10 for Solaris.

darwin.iso: Supports Mac OS X versions 10.11 and later.

Follow the procedures listed in the following documents to download VMware Tools for platforms not bundled with ESXi:

VMware Tools 11.3.5 Release Notes

Earlier versions of VMware Tools

What Every vSphere Admin Must Know About VMware Tools

VMware Tools for hosts provisioned with Auto Deploy

Updating VMware Tools

ESXi670-202111104-SG

Patch Category

Patch Severity

Host Reboot Required

Virtual Machine Migration or
Shutdown Required

Affected Hardware

Affected Software

VIBs Included

PRs Fixed

Related CVE numbers

This patch updates the cpu-microcode VIB.

Security

Important

Yes

Yes

N/A

N/A

VMware_bootbank_cpu-microcode_6.7.0-3.155.18812553

2776028

N/A

The cpu-microcode VIB includes the following Intel microcode:
Code Name

Plt ID MCU Rev

FMS

MCU Date

Brand Names

Clarkdale

0x20652

0x12

0x00000011

5/8/2018

Intel i3/i5 Clarkdale Series;
Intel Xeon 34xx Clarkdale Series

Arrandale

0x20655

0x92 0x00000007

4/23/2018

Intel Core i7-620LE Processor

Sandy Bridge DT

0x206a7

0x12

0x0000002f

2/17/2019

Westmere EP

0x206c2

0x03 0x0000001f

5/8/2018

Sandy Bridge EP

0x206d6

0x6d 0x00000621

3/4/2020

Sandy Bridge EP

0x206d7

0x6d 0x0000071a

3/24/2020

Westmere EX

0x206f2

0x05 0x0000003b

5/16/2018

Ivy Bridge DT

0x306a9

0x12

0x00000021

2/13/2019

Haswell DT

0x306c3

0x32 0x00000028

11/12/2019

Ivy Bridge EP

0x306e4

0xed 0x0000042e

3/14/2019

Intel Xeon E3-1100 Series;
Intel Xeon E3-1200 Series;
Intel i7-2655-LE Series;
Intel i3-2100 Series

Intel Xeon 56xx Series;
Intel Xeon 36xx Series

Intel Pentium 1400 Series;
Intel Xeon E5-1400 Series;
Intel Xeon E5-1600 Series;
Intel Xeon E5-2400 Series;
Intel Xeon E5-2600 Series;
Intel Xeon E5-4600 Series

Intel Pentium 1400 Series;
Intel Xeon E5-1400 Series;
Intel Xeon E5-1600 Series;
Intel Xeon E5-2400 Series;
Intel Xeon E5-2600 Series;
Intel Xeon E5-4600 Series

Intel Xeon E7-8800 Series;
Intel Xeon E7-4800 Series;
Intel Xeon E7-2800 Series

Intel i3-3200 Series;
Intel i7-3500-LE/UE;
Intel i7-3600-QE;
Intel Xeon E3-1200-v2 Series;
Intel Xeon E3-1100-C-v2 Series;
Intel Pentium B925C

Intel Xeon E3-1200-v3 Series;
Intel i7-4700-EQ Series;
Intel i5-4500-TE Series;
Intel i3-4300 Series

Intel Xeon E5-4600-v2 Series;
Intel Xeon E5-2600-v2 Series;
Intel Xeon E5-2400-v2 Series;
Intel Xeon E5-1600-v2 Series;
Intel Xeon E5-1400-v2 Series

Ivy Bridge EX

0x306e7

0xed 0x00000715

3/14/2019

Intel Xeon E7-8800/4800/2800-v2
Series

Code Name

FMS

Plt ID MCU Rev

MCU Date

Brand Names

Haswell EP

0x306f2

0x6f

0x00000046

1/27/2021

Intel Xeon E5-4600-v3 Series;
Intel Xeon E5-2600-v3 Series;
Intel Xeon E5-2400-v3 Series;
Intel Xeon E5-1600-v3 Series;
Intel Xeon E5-1400-v3 Series

Haswell EX

0x306f4

0x80 0x00000019

2/5/2021

Intel Xeon E7-8800/4800-v3 Series

Broadwell H

0x40671

0x22 0x00000022

11/12/2019

Avoton

0x406d8

0x01

0x0000012d

9/16/2019

Broadwell EP/EX

0x406f1

0xef

0x0b00003e

2/6/2021

Skylake SP

0x50654

0xb7 0x02006b06

3/8/2021

Cascade Lake B-0 0x50656

0xbf

0x04003103

4/20/2021

Cascade Lake

0x50657

0xbf

0x05003103

4/8/2021

Cooper Lake

0x5065b

0xbf

0x07002302

4/23/2021

Intel Core i7-5700EQ;
Intel Xeon E3-1200-v4 Series

Intel Atom C2300 Series;
Intel Atom C2500 Series;
Intel Atom C2700 Series

Intel Xeon E7-8800/4800-v4 Series;
Intel Xeon E5-4600-v4 Series;
Intel Xeon E5-2600-v4 Series;
Intel Xeon E5-1600-v4 Series

Intel Xeon Platinum 8100 Series;
Intel Xeon Gold 6100/5100, Silver
4100, Bronze 3100 Series;
Intel Xeon D-2100 Series;
Intel Xeon D-1600 Series;
Intel Xeon W-3100 Series;
Intel Xeon W-2100 Series

Intel Xeon Platinum 9200/8200
Series;
Intel Xeon Gold 6200/5200;
Intel Xeon Silver 4200/Bronze 3200;
Intel Xeon W-3200

Intel Xeon Platinum 9200/8200
Series;
Intel Xeon Gold 6200/5200;
Intel Xeon Silver 4200/Bronze 3200;
Intel Xeon W-3200

Intel Xeon Platinum 8300 Series;
Intel Xeon Gold 6300/5300

Broadwell DE

0x50662

0x10

0x0000001c

6/17/2019

Intel Xeon D-1500 Series

Broadwell DE

0x50663

0x10

0x0700001b

2/4/2021

Intel Xeon D-1500 Series

Broadwell DE

0x50664

0x10

0x0f000019

2/4/2021

Intel Xeon D-1500 Series

Broadwell NS

0x50665

0x10

0x0e000012

2/4/2021

Intel Xeon D-1600 Series

Skylake H/S

0x506e3

0x36 0x000000ea

1/25/2021

Intel Xeon E3-1500-v5 Series;
Intel Xeon E3-1200-v5 Series

Denverton

0x506f1

0x01

0x00000034

10/23/2020

Intel Atom C3000 Series

Ice Lake SP

0x606a6

0x87 0x0d0002a0

4/25/2021

Intel Xeon Silver 4300 Series;
Intel Xeon Gold 6300/5300 Series;
Intel Xeon Platinum 8300 Series

Snow Ridge

0x80665

0x01

0x0b00000f

2/17/2021

Intel Atom P5000 Series

Kaby Lake H/S/X

0x906e9

0x2a 0x000000ea

1/5/2021

Coffee Lake

0x906ea

0x22 0x000000ea

1/5/2021

Intel Xeon E3-1200-v6 Series;
Intel Xeon E3-1500-v6 Series

Intel Xeon E-2100 Series;
Intel Xeon E-2200 Series (4 or 6
core)

Coffee Lake

0x906eb

0x02 0x000000ea

1/5/2021

Intel Xeon E-2100 Series

Coffee Lake

0x906ec

0x22 0x000000ea

1/5/2021

Intel Xeon E-2100 Series

Coffee Lake
Refresh

0x906ed

0x22 0x000000ea

1/5/2021

Intel Xeon E-2200 Series (8 core)

Rocket Lake S

0xa0671

0x02 0x00000040

4/11/2021

Intel Xeon E-2300 Series

ESXi-6.7.0-20211104001-standard

Profile Name

ESXi-6.7.0-20211104001-standard

Build

Vendor

Release Date

Acceptance Level

For build information, see Patches Contained in this Release.

VMware, Inc.

November 23, 2021

PartnerSupported

Affected Hardware

Affected Software

N/A

N/A

Affected VIBs

PRs Fixed

VMware_bootbank_vsan_6.7.0-3.159.18811818
VMware_bootbank_esx-base_6.7.0-3.159.18828794
VMware_bootbank_esx-update_6.7.0-3.159.18828794
VMware_bootbank_vsanhealth_6.7.0-3.159.18811819
VMW_bootbank_vmkusb_0.1-4vmw.670.3.159.18828794
VMW_bootbank_nvme_1.2.2.28-5vmw.670.3.159.18828794
VMW_bootbank_brcmfcoe_11.4.1078.26-14vmw.670.3.159.18828794
VMware_bootbank_lsu-smartpqi-plugin_1.0.0-
4vmw.670.3.159.18828794

2731317, 2727062, 2704831, 2764894, 2720189, 2761940, 2757263,
2752542, 2772048, 2768356, 2741112, 2786864, 2725025, 2776791,
2724166, 2790353, 2787866, 2790502, 2834968, 2832090, 2841163,
2804742, 2732482, 2793380, 2813328, 2792504, 2749902, 2765136,
2697256, 2731139, 2733489, 2755232, 2761131, 2763933, 2766401, 2744211,
2771730, 2800655, 2839716, 2847812, 2726678, 2789302, 2750034,
2739223, 2794562, 2857511, 2847091, 2765679, 2731643, 2774667,
2778008, 2833428, 2761169, 2775375, 2820768, 2835990, 2852099

Related CVE numbers

N/A

This patch resolves the following issues:

If the USB device attached to an ESXi host has descriptors that are not compliant with the standard USB specifications, the virtual USB stack might fail to pass

through a USB device into a virtual machine. As a result, virtual machines become unresponsive, and you must either power off the virtual machine by using an

ESXCLI command, or restart the ESXi host.

When attempting to set a value to false for an advanced option parameter in a host profile, the user interface creates a non-empty string value. Values that

are not empty are interpreted as true and the advanced option parameter receives a true value in the host profile.

A rare race condition with static map initialization might cause ESXi hosts to temporarily lose connectivity to vCenter Server after the vSphere Replication

appliance powers on. However, the hostd service automatically restarts and the ESXi hosts restore connectivity.

In rare cases, an empty or unset property of a VIM API data array might cause the hostd service to fail. As a result, ESXi hosts lose connectivity to vCenter

Server and you must manually reconnect the hosts.

A rare error condition in the VMKernel might cause ESXi hosts to fail when powering on a virtual machine with more than 1 virtual CPU.

A BSVMM_Validate@vmkernel error is indicative for the problem.

During a vSphere vMotion operation, if the calling context holds any locks, ESXi hosts might fail with a purple diagnostic screen and an error such as PSOD:

Panic at bora/vmkernel/core/lock.c:2070.

ESXi hosts might intermittently fail with a purple diagnostic screen with an error such as @BlueScreen: VERIFY bora/vmkernel/sched/cpusched.c that suggests a

preemption anomaly. However, the VMkernel preemption anomaly detection logic might fail to identify the correct kernel context and show a warning for the

wrong context.

If you change the DiskMaxIOSize advanced config option to a lower value, I/Os with large block sizes might get incorrectly split and queue at the PSA path. As a

result, ESXi hosts I/O operations might time out and fail.

Some poll requests might exceed the metadata heap of the vmkapi character device driver and cause ESXi hosts to fail with a purple diagnostic screen and an

error such as:

#PF Exception 14 in world 2099138:IPMI Response IP 0x41800b922ab0 addr 0x18 PTEs:0x0;

In the VMkernel logs, you might see messages such as:

WARNING: Heap: 3571: Heap VMKAPI-char-metadata already at its maximum size. Cannot expand.

and

WARNING: Heap: 4079: Heap_Align(VMKAPI-char-metadata, 40/40 bytes, 8 align) failed. caller: 0x418029922489

In rare cases, some Intel CPUs might fail to forward #DB traps and if a timer interrupt happens during the Windows system call, virtual machine might triple

fault. In the vmware.log file, you see an error such as msg.monitorEvent.tripleFault. This issue is not consistent and virtual machines that consistently triple fault

or triple fault after boot are impacted by another issue.

While handling the SCSI command READ CAPACITY (10), ESXi might copy excess data from the response and corrupt the call stack. As a result, the ESXi host fails

with a purple diagnostic screen.

If you plug in and out a physical NIC in your vCenter Sever system, after the uplink is restored, you still see an alarm in the vmkernel.log of some ESXi hosts that

a NIC link is down. The VOBD daemon might not create the event esx.clear.net.redundancy.restored to remove such alarms, which causes the issue.

In some cases, when multiple USB or SD devices with different file systems are connected to your vCenter Server system, ESXi installation by using a ks.cfg

installation script and a USB or SD booting device might fail.

In the first minutes after mounting a VMFS6 volume, you might see higher than configured rate of automatic unmap operations. For example, if you set the

unmap bandwidth to 2000 MB/s, you see unmap operations running at more than 3000 MB/s.

For independent nonpersistent mode virtual disks, all writes are stored in a redo log, which is a temporary file with extension .REDO_XXXXXX in the VM directory.

However, a reset of a virtual machine does not clear the old redo log on such disks.

When you run the esxtop utility by using the ESXi Shell to replay a pre-recorded support bundle, the operation might fail with a segmentation fault.

Rarely, in certain configurations, the shutdown or reboot of ESXi hosts might stop at the step Shutting down device drivers for a long time, in the order of 20

minutes, but the operation eventually completes.

The virtual network adapter VMXNET Generation 3 (VMXNET3) uses buffers to process rx packets. Such buffers are either pre-pinned or pinned and mapped

during runtime. In rare occasions, some buffers might not un-pin and get re-pinned later, resulting in a higher-than-expected pin count of such a buffer. As a

result, virtual machines might fail with an error such as MONITOR PANIC: vmk: vcpu-0:Too many sticky pages..

Due to a rare lock rank violation in the vSphere Replication I/O filter, some ESXi hosts might fail with a purple diagnostic screen when vSphere Replication is

enabled. You see an error such as VERIFY bora/vmkernel/main/bh.c:978 on the screen.

In vCenter Server advanced performance charts, you see an increasing number of packet drop count for all virtual machines that have NetX redirection

enabled. However, if you disable NetX redirection, the count becomes 0.

Virtual switches processing IGMPv3 SSM specific queries might drop some queries when an IP is not included in the list of available ports as a result of the

source IP check.

An integer overflow bug might cause vSAN DOM to issue scrubs more frequently than the configured setting.

In particular circumstances, when you power on a virtual machine with a corrupted VMware Tools manifest file, the hostd service might fail. As a result, the

ESXi host becomes unresponsive. You can backtrace the issue in the hostd dump file that is usually generated in such cases at the time a VM is powered on.

After a reboot of an ESXi host, encrypted virtual machines might not auto power on even when Autostart is configured with the Start delay option to set a

specific start time of the host. The issue affects only encrypted virtual machines due to a delay in the distribution of keys from standard key providers.

When the SLP service is disabled to prevent potential security vulnerabilities, the sfcbd-watchdog service might remain enabled and cause compliance check

failures when you perform updates by using a host profile.

When an ESXi 6.7.x host has more than 120 VMs on it, vSphere vMotion operations to that host or from that host might become slow or occasionally fail.

In certain environments, vSphere Client might not fully protect the underlying websites through modern browser security mechanisms.

ESXi updates and upgrades might fail on Mac Pro servers with a purple diagnostic screen and a message such as PSOD - NOT IMPLEMENTED

bora/vmkernel/hardware/pci/bridge.c:372 PCPU0:2097152/bootstrap. The issue occurs when Mac Pro BIOS does not assign resources to some devices.

Some Linux kernels add the IPv6 Tunnel Encapsulation Limit option to IPv6 tunnel packets as described in the RFC 2473, par. 5.1. As a result, IPv6 tunnel

packets are dropped in traffic between virtual machines with ENS enabled, because of the IPv6 extension header.

Resource allocation for a delta disk to create a snapshot of a large virtual machine, with virtual disks equal to or exceeding 1TB, on large VMFS6 datastores of

30TB or more, might take significant time. As a result, the virtual machine might temporarily lose connectivity. The issue affects primarily VMFS6 filesystems.

In case of temporary connectivity issues, ESXi hosts might not discover devices configured with the VMW_SATP_INV plug-in after connectivity restores, because

SCSI commands that fail during the device discovery stage cause an out-of-memory condition for the plug-in.

If you import a virtual machine with a virtual USB device that is not supported by vCenter Server, such as a virtual USB camera, to a vCenter Server system,

the VM might fail to power on. The start of such virtual machines fails with an error message similar to: PANIC: Unexpected signal: 11. The issue affects mostly

VM import operations from VMware Fusion or VMware Workstation systems, which support a wide range of virtual USB devices.

A non-blocking I/O operation might block a PBC file operation in a VMFS volume. In the vmkernel.log, you see an error such as:

cpu2:2098100)WARNING: PB3: 5020: Unexpected result (Would block) while waiting for PBC to clear -FD c63 r24

However, this is expected behavior.

By default, the vmxnet3 driver uses port 4789 when a VM uses ENS and port 8472 when ENS is not enabled. As a result, when an overlay tunnel is configured

from a guest VM with different default VXLAN port, vmxnet3 might drop packets.

In some cases, MAC learning does not work as expected and affects some virtual machine operations. For example, cloning a VM with the same MAC address

on a different VLAN causes a traffic flood to the cloned VM in the ESXi host where the original VM is present.

Attempts to create a vCenter Host Profiles CIM Indication Subscription might fail due to a cim-xml parsing request that returns an out of memory error.

The packet scheduler that controls network I/O in a vCenter Server system has a fixed length queue that might quickly fill up. As a result, you might see packet

drops on uplink on heavy traffic.

In a vSphere 6.7.x system, you cannot add an Intel Ice Lake server to a Skylake cluster with enabled EVC mode. In the vSphere Web Client or the vSphere

Client, you see the following message:

Host CPU lacks features required by that mode. XSAVE of BND0-BND3 bounds registers (BNDREGS) is unsupported. XSAVE of BNDCFGU and BNDSTATUS registers (BNDCSR)

is unsupported.

The VIM API server deletes the Active Directory cache on certain intervals to check the validity of permissions of Active Directory domain accounts. After the

cache delete operation, daily by default, the VIM API server re-populates the cache. However, the cache re-populate operation might be slow on domains with

many accounts. As a result, such operations might impact performance across the entire vCenter Server system.

An abrupt power cycling or reboot of an ESX host might cause a race between subsequent journal replays because other hosts might try to access the same

resources. If a race condition happens, the journal replay cannot complete. As a result, virtual machines on a VMFS6 datastore become inaccessible or any

operations running on the VMs fail or stop.

If you manually run the reboot_helper.py script to shut down a vSAN cluster, the process cleans unused cron jobs registered during the shutdown. However, the

script might start a new cron instance instead of restarting the cron daemon after cleaning the unused cron jobs. As a result, multiple cron instances might

accumulate and you see cron jobs executed multiple times.

If you run a recursive call on a large datastore, such as DatastoreBrowser::SearchSubFolders, a memory leak condition might cause the hostd service to fail with

an out-of-memory error.

When you enable encryption for a vSAN cluster with a Key Management Interoperability Protocol (KMIP) provider, the following health check might report

status errors: vCenter KMS status.

For all-flash vSAN configurations, you might see larger than required recommended size for the core dump partition. As a result, the core dump configuration

check might fail.

In rare cases, hardware issues might cause an SQlite DB corruption that makes multiple VMs become inaccessible and lead to some downtime for applications.

When a vSAN host has memory pressure, adding disks to a disk group might cause failure of the memory block attributes (blkAttr) component initialization.

Without the blkAttr component, commit and flush tasks stop, which causes log entries to build up in the SSD and eventually cause а congestion. As a result, a

NMI failure might occur due to the CPU load for processing the large number of log entries.

In some cases, the host designated as the leader cannot send heartbeat messages to other hosts in a large vSAN cluster. This issue occurs when the leader

uses an insufficient size of the TX buffer. The result is that new hosts cannot join the cluster with large cluster support (up to 64 hosts) enabled.

On a vSAN cluster with a large number of idle virtual machines, vSAN hosts might experience memory congestion due to higher scrub frequency.

An issue with trim space operations count might cause a large number of pending delete operations in vSAN. If an automatic rebalancing operation triggers, it

cannot progress due to such pending delete operations. For example, if you set the rebalancing threshold at 30%, you do not see any  progress on

rebalancing even when the storage capacity reaches 80%.

Due to a dependency between the management of virtual RAM and virtual NVDIMM within a virtual machine, excessive access to the virtual NVDIMM device

might lead to a lock spinout while accessing virtual RAM and cause the ESXi host to fail with a purple diagnostic screen. In the backtrace, you see the

following:

SP_WaitLock

SPLockWork

AsyncRemapPrepareRemapListVM

AsyncRemap_AddOrRemapVM

LPageSelectLPageToDefrag

VmAssistantProcessTasks

When you enable Latency Sensitivity on virtual machines, some threads of the Likewise Service Manager (lwsmd), which sets CPU affinity explicitly, might

compete for CPU resources on such virtual machines. As a result, you might see the ESXi host and the hostd service to become unresponsive.

ESXi670-202111001 adds two fixes to the USB host controller for cases when a USB or SD device lose connectivity to the /bootbank partition:

1. Adds a parameter usbStorageRegisterDelaySecs.

The usbStorageRegisterDelaySecs parameter is part of the vmkusb module and it delays to register cached USB storage device within the limit of 0 to 600

seconds. The default delay is 10 seconds. The parametric delay makes sure that if the USB host controller disconnects from your vSphere system and

any resource on the device, such as dump files, is still in use by ESXi, the USB or SD device does not get a new path that might break connection to

the /bootbank partition or corrupt the VMFS-L LOCKER partition.

2. Enhances the USB host controller driver to tolerate command timeout failures.

In cases when a device, such as Dell IDSDM, reconnects by a hardware signal, or a USB device resets due to a heavy workload, the USB host controller

driver might make only one retry to restore connectivity. With the fix, the controller makes multiple retries and extends tolerance to timeout failures.

Some LDAP queries that have no specified timeouts might cause a significant delay in domain join operations for adding an ESXi host to an Active Directory

domain.

After an upgrade of the brcmfcoe driver on Hitachi storage arrays, ESXi hosts might fail to boot and lose connectivity.

The OEM smartpqi driver in ESXi 670-202111001 uses a new logical LUN ID assignment rule to become compatible with the drivers of versions later than

1.0.3.2323. As a result, OEM smartpqi drivers of version 1.0.3.2323 and earlier might fail to blink LEDs or get the physical locations on logical drives.

When vSphere Replication is enabled on a virtual machine, you might see higher datastore and in-guest latencies that in certain cases might lead to ESXi hosts

becoming unresponsive to vCenter Server. The increased latency comes from vSphere Replication computing MD5 checksums on the I/O completion path,

which delays all other I/Os.

ESXi-6.7.0-20211104001-no-tools

Profile Name

ESXi-6.7.0-20211104001-no-tools

Build

Vendor

Release Date

Acceptance Level

Affected Hardware

Affected Software

Affected VIBs

PRs Fixed

For build information, see Patches Contained in this Release.

VMware, Inc.

November 23, 2021

PartnerSupported

N/A

N/A

VMware_bootbank_vsan_6.7.0-3.159.18811818
VMware_bootbank_esx-base_6.7.0-3.159.18828794
VMware_bootbank_esx-update_6.7.0-3.159.18828794
VMware_bootbank_vsanhealth_6.7.0-3.159.18811819
VMW_bootbank_vmkusb_0.1-4vmw.670.3.159.18828794
VMW_bootbank_nvme_1.2.2.28-5vmw.670.3.159.18828794
VMW_bootbank_brcmfcoe_11.4.1078.26-14vmw.670.3.159.18828794
VMware_bootbank_lsu-smartpqi-plugin_1.0.0-
4vmw.670.3.159.18828794

2731317, 2727062, 2704831, 2764894, 2720189, 2761940, 2757263,
2752542, 2772048, 2768356, 2741112, 2786864, 2725025, 2776791,
2724166, 2790353, 2787866, 2790502, 2834968, 2832090, 2841163,
2804742, 2732482, 2793380, 2813328, 2792504, 2749902, 2765136,
2697256, 2731139, 2733489, 2755232, 2761131, 2763933, 2766401, 2744211,
2771730, 2800655, 2839716, 2847812, 2726678, 2789302, 2750034,
2739223, 2794562, 2857511, 2847091, 2765679, 2731643, 2774667,
2778008, 2833428, 2761169, 2775375, 2820768, 2835990, 2852099

Related CVE numbers

N/A

This patch resolves the following issues:

If the USB device attached to an ESXi host has descriptors that are not compliant with the standard USB specifications, the virtual USB stack might fail to pass

through a USB device into a virtual machine. As a result, virtual machines become unresponsive, and you must either power off the virtual machine by using an

ESXCLI command, or restart the ESXi host.

When attempting to set a value to false for an advanced option parameter in a host profile, the user interface creates a non-empty string value. Values that

are not empty are interpreted as true and the advanced option parameter receives a true value in the host profile.

A rare race condition with static map initialization might cause ESXi hosts to temporarily lose connectivity to vCenter Server after the vSphere Replication

appliance powers on. However, the hostd service automatically restarts and the ESXi hosts restore connectivity.

In rare cases, an empty or unset property of a VIM API data array might cause the hostd service to fail. As a result, ESXi hosts lose connectivity to vCenter

Server and you must manually reconnect the hosts.

A rare error condition in the VMKernel might cause ESXi hosts to fail when powering on a virtual machine with more than 1 virtual CPU.

A BSVMM_Validate@vmkernel error is indicative for the problem.

During a vSphere vMotion operation, if the calling context holds any locks, ESXi hosts might fail with a purple diagnostic screen and an error such as PSOD:

Panic at bora/vmkernel/core/lock.c:2070.

ESXi hosts might intermittently fail with a purple diagnostic screen with an error such as @BlueScreen: VERIFY bora/vmkernel/sched/cpusched.c that suggests a

preemption anomaly. However, the VMkernel preemption anomaly detection logic might fail to identify the correct kernel context and show a warning for the

wrong context.

If you change the DiskMaxIOSize advanced config option to a lower value, I/Os with large block sizes might get incorrectly split and queue at the PSA path. As a

result, ESXi hosts I/O operations might time out and fail.

Some poll requests might exceed the metadata heap of the vmkapi character device driver and cause ESXi hosts to fail with a purple diagnostic screen and an

error such as:

#PF Exception 14 in world 2099138:IPMI Response IP 0x41800b922ab0 addr 0x18 PTEs:0x0;

In the VMkernel logs, you might see messages such as:

WARNING: Heap: 3571: Heap VMKAPI-char-metadata already at its maximum size. Cannot expand.

and

WARNING: Heap: 4079: Heap_Align(VMKAPI-char-metadata, 40/40 bytes, 8 align) failed. caller: 0x418029922489

In rare cases, some Intel CPUs might fail to forward #DB traps and if a timer interrupt happens during the Windows system call, virtual machine might triple

fault. In the vmware.log file, you see an error such as msg.monitorEvent.tripleFault. This issue is not consistent and virtual machines that consistently triple fault

or triple fault after boot are impacted by another issue.

While handling the SCSI command READ CAPACITY (10), ESXi might copy excess data from the response and corrupt the call stack. As a result, the ESXi host fails

with a purple diagnostic screen.

If you plug in and out a physical NIC in your vCenter Sever system, after the uplink is restored, you still see an alarm in the vmkernel.log of some ESXi hosts that

a NIC link is down. The VOBD daemon might not create the event esx.clear.net.redundancy.restored to remove such alarms, which causes the issue.

In some cases, when multiple USB or SD devices with different file systems are connected to your vCenter Server system, ESXi installation by using a ks.cfg

installation script and a USB or SD booting device might fail.

In the first minutes after mounting a VMFS6 volume, you might see higher than configured rate of automatic unmap operations. For example, if you set the

unmap bandwidth to 2000 MB/s, you see unmap operations running at more than 3000 MB/s.

For independent nonpersistent mode virtual disks, all writes are stored in a redo log, which is a temporary file with extension .REDO_XXXXXX in the VM directory.

However, a reset of a virtual machine does not clear the old redo log on such disks.

When you run the esxtop utility by using the ESXi Shell to replay a pre-recorded support bundle, the operation might fail with a segmentation fault.

Rarely, in certain configurations, the shutdown or reboot of ESXi hosts might stop at the step Shutting down device drivers for a long time, in the order of 20

minutes, but the operation eventually completes.

The virtual network adapter VMXNET Generation 3 (VMXNET3) uses buffers to process rx packets. Such buffers are either pre-pinned or pinned and mapped

during runtime. In rare occasions, some buffers might not un-pin and get re-pinned later, resulting in a higher-than-expected pin count of such a buffer. As a

result, virtual machines might fail with an error such as MONITOR PANIC: vmk: vcpu-0:Too many sticky pages..

Due to a rare lock rank violation in the vSphere Replication I/O filter, some ESXi hosts might fail with a purple diagnostic screen when vSphere Replication is

enabled. You see an error such as VERIFY bora/vmkernel/main/bh.c:978 on the screen.

In vCenter Server advanced performance charts, you see an increasing number of packet drop count for all virtual machines that have NetX redirection

enabled. However, if you disable NetX redirection, the count becomes 0.

Virtual switches processing IGMPv3 SSM specific queries might drop some queries when an IP is not included in the list of available ports as a result of the

source IP check.

An integer overflow bug might cause vSAN DOM to issue scrubs more frequently than the configured setting.

In particular circumstances, when you power on a virtual machine with a corrupted VMware Tools manifest file, the hostd service might fail. As a result, the

ESXi host becomes unresponsive. You can backtrace the issue in the hostd dump file that is usually generated in such cases at the time a VM is powered on.

After a reboot of an ESXi host, encrypted virtual machines might not auto power on even when Autostart is configured with the Start delay option to set a

specific start time of the host. The issue affects only encrypted virtual machines due to a delay in the distribution of keys from standard key providers.

When the SLP service is disabled to prevent potential security vulnerabilities, the sfcbd-watchdog service might remain enabled and cause compliance check

failures when you perform updates by using a host profile.

When an ESXi 6.7.x host has more than 120 VMs on it, vSphere vMotion operations to that host or from that host might become slow or occasionally fail.

In certain environments, vSphere Client might not fully protect the underlying websites through modern browser security mechanisms.

ESXi updates and upgrades might fail on Mac Pro servers with a purple diagnostic screen and a message such as PSOD - NOT IMPLEMENTED

bora/vmkernel/hardware/pci/bridge.c:372 PCPU0:2097152/bootstrap. The issue occurs when Mac Pro BIOS does not assign resources to some devices.

Some Linux kernels add the IPv6 Tunnel Encapsulation Limit option to IPv6 tunnel packets as described in the RFC 2473, par. 5.1. As a result, IPv6 tunnel

packets are dropped in traffic between virtual machines with ENS enabled, because of the IPv6 extension header.

Resource allocation for a delta disk to create a snapshot of a large virtual machine, with virtual disks equal to or exceeding 1TB, on large VMFS6 datastores of

30TB or more, might take significant time. As a result, the virtual machine might temporarily lose connectivity. The issue affects primarily VMFS6 filesystems.

In case of temporary connectivity issues, ESXi hosts might not discover devices configured with the VMW_SATP_INV plug-in after connectivity restores, because

SCSI commands that fail during the device discovery stage cause an out-of-memory condition for the plug-in.

If you import a virtual machine with a virtual USB device that is not supported by vCenter Server, such as a virtual USB camera, to a vCenter Server system,

the VM might fail to power on. The start of such virtual machines fails with an error message similar to: PANIC: Unexpected signal: 11. The issue affects mostly

VM import operations from VMware Fusion or VMware Workstation systems, which support a wide range of virtual USB devices.

A non-blocking I/O operation might block a PBC file operation in a VMFS volume. In the vmkernel.log, you see an error such as:

cpu2:2098100)WARNING: PB3: 5020: Unexpected result (Would block) while waiting for PBC to clear -FD c63 r24

However, this is expected behavior.

By default, the vmxnet3 driver uses port 4789 when a VM uses ENS and port 8472 when ENS is not enabled. As a result, when an overlay tunnel is configured

from a guest VM with different default VXLAN port, vmxnet3 might drop packets.

In some cases, MAC learning does not work as expected and affects some virtual machine operations. For example, cloning a VM with the same MAC address

on a different VLAN causes a traffic flood to the cloned VM in the ESXi host where the original VM is present.

Attempts to create a vCenter Host Profiles CIM Indication Subscription might fail due to a cim-xml parsing request that returns an out of memory error.

The packet scheduler that controls network I/O in a vCenter Server system has a fixed length queue that might quickly fill up. As a result, you might see packet

drops on uplink on heavy traffic.

In a vSphere 6.7.x system, you cannot add an Intel Ice Lake server to a Skylake cluster with enabled EVC mode. In the vSphere Web Client or the vSphere

Client, you see the following message:

Host CPU lacks features required by that mode. XSAVE of BND0-BND3 bounds registers (BNDREGS) is unsupported. XSAVE of BNDCFGU and BNDSTATUS registers (BNDCSR)

is unsupported.

The VIM API server deletes the Active Directory cache on certain intervals to check the validity of permissions of Active Directory domain accounts. After the

cache delete operation, daily by default, the VIM API server re-populates the cache. However, the cache re-populate operation might be slow on domains with

many accounts. As a result, such operations might impact performance across the entire vCenter Server system.

An abrupt power cycling or reboot of an ESX host might cause a race between subsequent journal replays because other hosts might try to access the same

resources. If a race condition happens, the journal replay cannot complete. As a result, virtual machines on a VMFS6 datastore become inaccessible or any

operations running on the VMs fail or stop.

If you manually run the reboot_helper.py script to shut down a vSAN cluster, the process cleans unused cron jobs registered during the shutdown. However, the

script might start a new cron instance instead of restarting the cron daemon after cleaning the unused cron jobs. As a result, multiple cron instances might

accumulate and you see cron jobs executed multiple times.

If you run a recursive call on a large datastore, such as DatastoreBrowser::SearchSubFolders, a memory leak condition might cause the hostd service to fail with

an out-of-memory error.

When you enable encryption for a vSAN cluster with a Key Management Interoperability Protocol (KMIP) provider, the following health check might report

status errors: vCenter KMS status.

For all-flash vSAN configurations, you might see larger than required recommended size for the core dump partition. As a result, the core dump configuration

check might fail.

In rare cases, hardware issues might cause an SQlite DB corruption that makes multiple VMs become inaccessible and lead to some downtime for applications.

When a vSAN host has memory pressure, adding disks to a disk group might cause failure of the memory block attributes (blkAttr) component initialization.

Without the blkAttr component, commit and flush tasks stop, which causes log entries to build up in the SSD and eventually cause а congestion. As a result, a

NMI failure might occur due to the CPU load for processing the large number of log entries.

In some cases, the host designated as the leader cannot send heartbeat messages to other hosts in a large vSAN cluster. This issue occurs when the leader

uses an insufficient size of the TX buffer. The result is that new hosts cannot join the cluster with large cluster support (up to 64 hosts) enabled.

On a vSAN cluster with a large number of idle virtual machines, vSAN hosts might experience memory congestion due to higher scrub frequency.

An issue with trim space operations count might cause a large number of pending delete operations in vSAN. If an automatic rebalancing operation triggers, it

cannot progress due to such pending delete operations. For example, if you set the rebalancing threshold at 30%, you do not see any  progress on

rebalancing even when the storage capacity reaches 80%.

Due to a dependency between the management of virtual RAM and virtual NVDIMM within a virtual machine, excessive access to the virtual NVDIMM device

might lead to a lock spinout while accessing virtual RAM and cause the ESXi host to fail with a purple diagnostic screen. In the backtrace, you see the

following:

SP_WaitLock

SPLockWork

AsyncRemapPrepareRemapListVM

AsyncRemap_AddOrRemapVM

LPageSelectLPageToDefrag

VmAssistantProcessTasks

When you enable Latency Sensitivity on virtual machines, some threads of the Likewise Service Manager (lwsmd), which sets CPU affinity explicitly, might

compete for CPU resources on such virtual machines. As a result, you might see the ESXi host and the hostd service to become unresponsive.

ESXi670-202111001 adds two fixes to the USB host controller for cases when a USB or SD device lose connectivity to the /bootbank partition:

1. Adds a parameter usbStorageRegisterDelaySecs.

The usbStorageRegisterDelaySecs parameter is part of the vmkusb module and it delays to register cached USB storage device within the limit of 0 to 600

seconds. The default delay is 10 seconds. The parametric delay makes sure that if the USB host controller disconnects from your vSphere system and

any resource on the device, such as dump files, is still in use by ESXi, the USB or SD device does not get a new path that might break connection to

the /bootbank partition or corrupt the VMFS-L LOCKER partition.

2. Enhances the USB host controller driver to tolerate command timeout failures.

In cases when a device, such as Dell IDSDM, reconnects by a hardware signal, or a USB device resets due to a heavy workload, the USB host controller

driver might make only one retry to restore connectivity. With the fix, the controller makes multiple retries and extends tolerance to timeout failures.

Some LDAP queries that have no specified timeouts might cause a significant delay in domain join operations for adding an ESXi host to an Active Directory

domain.

After an upgrade of the brcmfcoe driver on Hitachi storage arrays, ESXi hosts might fail to boot and lose connectivity.

The OEM smartpqi driver in ESXi 670-202111001 uses a new logical LUN ID assignment rule to become compatible with the drivers of versions later than

1.0.3.2323. As a result, OEM smartpqi drivers of version 1.0.3.2323 and earlier might fail to blink LEDs or get the physical locations on logical drives.

When vSphere Replication is enabled on a virtual machine, you might see higher datastore and in-guest latencies that in certain cases might lead to ESXi hosts

becoming unresponsive to vCenter Server. The increased latency comes from vSphere Replication computing MD5 checksums on the I/O completion path,

which delays all other I/Os.

ESXi-6.7.0-20211101001s-standard

Profile Name

ESXi-6.7.0-20211101001s-standard

Build

Vendor

Release Date

Acceptance Level

Affected Hardware

Affected Software

Affected VIBs

For build information, see Patches Contained in this Release.

VMware, Inc.

November 23, 2021

PartnerSupported

N/A

N/A

VMware_bootbank_vsanhealth_6.7.0-3.155.18811783
VMware_bootbank_vsan_6.7.0-3.155.18811780
VMware_bootbank_esx-update_6.7.0-3.155.18812553
VMware_bootbank_esx-base_6.7.0-3.155.18812553
VMW_bootbank_vmkusb_0.1-1vmw.670.3.155.18812553
VMware_locker_tools-light_11.3.5.18557794-18812553
VMware_bootbank_cpu-microcode_6.7.0-3.155.18812553

PRs Fixed

Related CVE numbers

2704746, 2765136, 2794798, 2794804, 2794933, 2794943, 2795965,
2797008, 2830222, 2830671, 2762034, 2762034, 2785530, 2816541,
2776028

CVE-2015-5180, CVE-2015-8777, CVE-2015-8982, CVE-2016-3706, CVE-
2017-1000366, CVE-2018-1000001, CVE-2018-19591, CVE-2019-19126, CVE-
2020-10029, CVE-2021-28831

This patch resolves the following issues:

The SQLite database is updated to version 3.34.1.

The OpenSSL package is updated to version openssl-1.0.2za.

The cURL library is updated to 7.78.0.

The OpenSSH is updated to version 8.6p1.

The libarchive library is updated to libarchive - 3.5.1.

The Busybox package is updated to address CVE-2021-28831.

The glibc library is updated tо address the following CVEs: CVE-2015-5180, CVE-2015-8777, CVE-2015-8982, CVE-2016-3706, CVE-2017-1000366, CVE-2018-

1000001, CVE-2018-19591, CVE-2019-19126, CVE-2020-10029.

The following VMware Tools ISO images are bundled with ESXi 670-202111001:

windows.iso: VMware Tools 11.3.5 supports Windows 7 SP1 or Windows Server 2008 R2 SP1 and later.

linux.iso: VMware Tools 10.3.23 ISO image for Linux OS with glibc 2.11 or later.

The following VMware Tools ISO images are available for download:

VMware Tools 11.0.6:

windows.iso: for Windows Vista (SP2) and Windows Server 2008 Service Pack 2 (SP2).

VMware Tools 10.0.12:

winPreVista.iso: for Windows 2000, Windows XP, and Windows 2003.

linuxPreGLibc25.iso: supports Linux guest operating systems earlier than Red Hat Enterprise Linux (RHEL) 5, SUSE Linux Enterprise Server (SLES) 11,

Ubuntu 7.04, and other distributions with glibc version earlier than 2.5.

solaris.iso: VMware Tools image 10.3.10 for Solaris.

darwin.iso: Supports Mac OS X versions 10.11 and later.

Follow the procedures listed in the following documents to download VMware Tools for platforms not bundled with ESXi:

VMware Tools 11.3.5 Release Notes

Earlier versions of VMware Tools

What Every vSphere Admin Must Know About VMware Tools

VMware Tools for hosts provisioned with Auto Deploy

Updating VMware Tools

This patch updates the cpu-microcode VIB.

Code Name FMS

Plt ID MCU Rev

MCU Date Brand Names

Clarkdale

0x20652 0x12 0x00000011

5/8/2018

Arrandale

0x20655 0x92 0x00000007 4/23/2018

Sandy
Bridge DT

Westmere
EP

0x206a7 0x12 0x0000002f 2/17/2019

0x206c2 0x03 0x0000001f

5/8/2018

Sandy
Bridge EP

0x206d6 0x6d 0x00000621 3/4/2020

Sandy
Bridge EP

0x206d7 0x6d 0x0000071a 3/24/2020

Westmere
EX

0x206f2 0x05 0x0000003b 5/16/2018

Ivy Bridge
DT

0x306a9 0x12 0x00000021 2/13/2019

Haswell DT 0x306c3 0x32 0x00000028 11/12/2019

Intel i3/i5 Clarkdale
Series;
Intel Xeon 34xx
Clarkdale Series

Intel Core i7-620LE
Processor

Intel Xeon E3-1100
Series;
Intel Xeon E3-1200
Series;
Intel i7-2655-LE
Series;
Intel i3-2100 Series

Intel Xeon 56xx
Series;
Intel Xeon 36xx Series

Intel Pentium 1400
Series;
Intel Xeon E5-1400
Series;
Intel Xeon E5-1600
Series;
Intel Xeon E5-2400
Series;
Intel Xeon E5-2600
Series;
Intel Xeon E5-4600
Series

Intel Pentium 1400
Series;
Intel Xeon E5-1400
Series;
Intel Xeon E5-1600
Series;
Intel Xeon E5-2400
Series;
Intel Xeon E5-2600
Series;
Intel Xeon E5-4600
Series

Intel Xeon E7-8800
Series;
Intel Xeon E7-4800
Series;
Intel Xeon E7-2800
Series

Intel i3-3200 Series;
Intel i7-3500-LE/UE;
Intel i7-3600-QE;
Intel Xeon E3-1200-v2
Series;
Intel Xeon E3-1100-C-
v2 Series;
Intel Pentium B925C

Intel Xeon E3-1200-v3
Series;
Intel i7-4700-EQ
Series;
Intel i5-4500-TE
Series;
Intel i3-4300 Series

Code Name FMS

Plt ID MCU Rev

MCU Date Brand Names

Ivy Bridge
EP

0x306e4 0xed 0x0000042e 3/14/2019

Ivy Bridge
EX

0x306e7 0xed 0x00000715 3/14/2019

Haswell EP 0x306f2 0x6f 0x00000046 1/27/2021

Haswell EX 0x306f4 0x80 0x00000019 2/5/2021

Broadwell H 0x40671 0x22 0x00000022 11/12/2019

Avoton

0x406d8 0x01 0x0000012d 9/16/2019

Broadwell
EP/EX

0x406f1 0xef 0x0b00003e 2/6/2021

Skylake SP 0x50654 0xb7 0x02006b06 3/8/2021

Cascade
Lake B-0

0x50656 0xbf 0x04003103 4/20/2021

Cascade
Lake

0x50657 0xbf 0x05003103

4/8/2021

Intel Xeon E5-4600-
v2 Series;
Intel Xeon E5-2600-
v2 Series;
Intel Xeon E5-2400-
v2 Series;
Intel Xeon E5-1600-v2
Series;
Intel Xeon E5-1400-v2
Series

Intel Xeon E7-
8800/4800/2800-v2
Series

Intel Xeon E5-4600-
v3 Series;
Intel Xeon E5-2600-
v3 Series;
Intel Xeon E5-2400-
v3 Series;
Intel Xeon E5-1600-v3
Series;
Intel Xeon E5-1400-v3
Series

Intel Xeon E7-
8800/4800-v3 Series

Intel Core i7-5700EQ;
Intel Xeon E3-1200-v4
Series

Intel Atom C2300
Series;
Intel Atom C2500
Series;
Intel Atom C2700
Series

Intel Xeon E7-
8800/4800-v4 Series;
Intel Xeon E5-4600-
v4 Series;
Intel Xeon E5-2600-
v4 Series;
Intel Xeon E5-1600-v4
Series

Intel Xeon Platinum
8100 Series;
Intel Xeon Gold
6100/5100, Silver
4100, Bronze 3100
Series;
Intel Xeon D-2100
Series;
Intel Xeon D-1600
Series;
Intel Xeon W-3100
Series;
Intel Xeon W-2100
Series

Intel Xeon Platinum
9200/8200 Series;
Intel Xeon Gold
6200/5200;
Intel Xeon Silver
4200/Bronze 3200;
Intel Xeon W-3200

Intel Xeon Platinum
9200/8200 Series;
Intel Xeon Gold
6200/5200;
Intel Xeon Silver
4200/Bronze 3200;
Intel Xeon W-3200

Code Name FMS

Plt ID MCU Rev

MCU Date Brand Names

Cooper
Lake

Broadwell
DE

Broadwell
DE

Broadwell
DE

Broadwell
NS

0x5065b 0xbf 0x07002302 4/23/2021

0x50662 0x10 0x0000001c 6/17/2019

0x50663 0x10 0x0700001b 2/4/2021

0x50664 0x10 0x0f000019

2/4/2021

0x50665 0x10 0x0e000012 2/4/2021

Skylake H/S 0x506e3 0x36 0x000000ea 1/25/2021

Denverton

0x506f1 0x01 0x00000034 10/23/2020

Ice Lake SP 0x606a6 0x87 0x0d0002a0 4/25/2021

Snow Ridge 0x80665 0x01 0x0b00000f 2/17/2021

Kaby Lake
H/S/X

0x906e9 0x2a 0x000000ea 1/5/2021

Coffee Lake 0x906ea 0x22 0x000000ea 1/5/2021

Coffee Lake 0x906eb 0x02 0x000000ea 1/5/2021

Coffee Lake 0x906ec 0x22 0x000000ea 1/5/2021

Coffee Lake
Refresh

Rocket
Lake S

0x906ed 0x22 0x000000ea 1/5/2021

0xa0671 0x02 0x00000040 4/11/2021

Intel Xeon Platinum
8300 Series;
Intel Xeon Gold
6300/5300

Intel Xeon D-1500
Series

Intel Xeon D-1500
Series

Intel Xeon D-1500
Series

Intel Xeon D-1600
Series

Intel Xeon E3-1500-v5
Series;
Intel Xeon E3-1200-v5
Series

Intel Atom C3000
Series

Intel Xeon Silver 4300
Series;
Intel Xeon Gold
6300/5300 Series;
Intel Xeon Platinum
8300 Series

Intel Atom P5000
Series

Intel Xeon E3-1200-v6
Series;
Intel Xeon E3-1500-v6
Series

Intel Xeon E-2100
Series;
Intel Xeon E-2200
Series (4 or 6 core)

Intel Xeon E-2100
Series

Intel Xeon E-2100
Series

Intel Xeon E-2200
Series (8 core)

Intel Xeon E-2300
Series

ESXi-6.7.0-20211101001s-no-tools

Profile Name

ESXi-6.7.0-20211101001s-no-tools

Build

Vendor

Release Date

Acceptance Level

Affected Hardware

Affected Software

Affected VIBs

PRs Fixed

Related CVE numbers

For build information, see Patches Contained in this Release.

VMware, Inc.

November 23, 2021

PartnerSupported

N/A

N/A

VMware_bootbank_vsanhealth_6.7.0-3.155.18811783
VMware_bootbank_vsan_6.7.0-3.155.18811780
VMware_bootbank_esx-update_6.7.0-3.155.18812553
VMware_bootbank_esx-base_6.7.0-3.155.18812553
VMW_bootbank_vmkusb_0.1-1vmw.670.3.155.18812553
VMware_bootbank_cpu-microcode_6.7.0-3.155.18812553

2704746, 2765136, 2794798, 2794804, 2794933, 2794943, 2795965,
2797008, 2830222, 2830671, 2762034, 2762034, 2785530, 2816541,
2776028

CVE-2015-5180, CVE-2015-8777, CVE-2015-8982, CVE-2016-3706, CVE-
2017-1000366, CVE-2018-1000001, CVE-2018-19591, CVE-2019-19126, CVE-
2020-10029, CVE-2021-28831

This patch updates the following issues:

The SQLite database is updated to version 3.34.1.

The OpenSSL package is updated to version openssl-1.0.2za.

The cURL library is updated to 7.78.0.

The OpenSSH is updated to version 8.6p1.

The libarchive library is updated to libarchive - 3.5.1.

The Busybox package is updated to address CVE-2021-28831.

The glibc library is updated tо address the following CVEs: CVE-2015-5180, CVE-2015-8777, CVE-2015-8982, CVE-2016-3706, CVE-2017-1000366, CVE-2018-

1000001, CVE-2018-19591, CVE-2019-19126, CVE-2020-10029.

The following VMware Tools ISO images are bundled with ESXi 670-202111001:

windows.iso: VMware Tools 11.3.5 supports Windows 7 SP1 or Windows Server 2008 R2 SP1 and later.

linux.iso: VMware Tools 10.3.23 ISO image for Linux OS with glibc 2.11 or later.

The following VMware Tools ISO images are available for download:

VMware Tools 11.0.6:

windows.iso: for Windows Vista (SP2) and Windows Server 2008 Service Pack 2 (SP2).

VMware Tools 10.0.12:

winPreVista.iso: for Windows 2000, Windows XP, and Windows 2003.

linuxPreGLibc25.iso: supports Linux guest operating systems earlier than Red Hat Enterprise Linux (RHEL) 5, SUSE Linux Enterprise Server (SLES) 11,

Ubuntu 7.04, and other distributions with glibc version earlier than 2.5.

solaris.iso: VMware Tools image 10.3.10 for Solaris.

darwin.iso: Supports Mac OS X versions 10.11 and later.

Follow the procedures listed in the following documents to download VMware Tools for platforms not bundled with ESXi:

VMware Tools 11.3.5 Release Notes

Earlier versions of VMware Tools

What Every vSphere Admin Must Know About VMware Tools

VMware Tools for hosts provisioned with Auto Deploy

Updating VMware Tools

This patch updates the cpu-microcode VIB.

Code Name FMS

Plt ID MCU Rev

MCU Date Brand Names

Clarkdale

0x20652 0x12 0x00000011

5/8/2018

Arrandale

0x20655 0x92 0x00000007 4/23/2018

Sandy
Bridge DT

Westmere
EP

0x206a7 0x12 0x0000002f 2/17/2019

0x206c2 0x03 0x0000001f

5/8/2018

Sandy
Bridge EP

0x206d6 0x6d 0x00000621 3/4/2020

Intel i3/i5 Clarkdale
Series;
Intel Xeon 34xx
Clarkdale Series

Intel Core i7-620LE
Processor

Intel Xeon E3-1100
Series;
Intel Xeon E3-1200
Series;
Intel i7-2655-LE
Series;
Intel i3-2100 Series

Intel Xeon 56xx
Series;
Intel Xeon 36xx Series

Intel Pentium 1400
Series;
Intel Xeon E5-1400
Series;
Intel Xeon E5-1600
Series;
Intel Xeon E5-2400
Series;
Intel Xeon E5-2600
Series;
Intel Xeon E5-4600
Series

Code Name FMS

Plt ID MCU Rev

MCU Date Brand Names

Sandy
Bridge EP

0x206d7 0x6d 0x0000071a 3/24/2020

Westmere
EX

0x206f2 0x05 0x0000003b 5/16/2018

Ivy Bridge
DT

0x306a9 0x12 0x00000021 2/13/2019

Haswell DT 0x306c3 0x32 0x00000028 11/12/2019

Ivy Bridge
EP

0x306e4 0xed 0x0000042e 3/14/2019

Ivy Bridge
EX

0x306e7 0xed 0x00000715 3/14/2019

Haswell EP 0x306f2 0x6f 0x00000046 1/27/2021

Haswell EX 0x306f4 0x80 0x00000019 2/5/2021

Broadwell H 0x40671 0x22 0x00000022 11/12/2019

Avoton

0x406d8 0x01 0x0000012d 9/16/2019

Intel Pentium 1400
Series;
Intel Xeon E5-1400
Series;
Intel Xeon E5-1600
Series;
Intel Xeon E5-2400
Series;
Intel Xeon E5-2600
Series;
Intel Xeon E5-4600
Series

Intel Xeon E7-8800
Series;
Intel Xeon E7-4800
Series;
Intel Xeon E7-2800
Series

Intel i3-3200 Series;
Intel i7-3500-LE/UE;
Intel i7-3600-QE;
Intel Xeon E3-1200-v2
Series;
Intel Xeon E3-1100-C-
v2 Series;
Intel Pentium B925C

Intel Xeon E3-1200-v3
Series;
Intel i7-4700-EQ
Series;
Intel i5-4500-TE
Series;
Intel i3-4300 Series

Intel Xeon E5-4600-
v2 Series;
Intel Xeon E5-2600-
v2 Series;
Intel Xeon E5-2400-
v2 Series;
Intel Xeon E5-1600-v2
Series;
Intel Xeon E5-1400-v2
Series

Intel Xeon E7-
8800/4800/2800-v2
Series

Intel Xeon E5-4600-
v3 Series;
Intel Xeon E5-2600-
v3 Series;
Intel Xeon E5-2400-
v3 Series;
Intel Xeon E5-1600-v3
Series;
Intel Xeon E5-1400-v3
Series

Intel Xeon E7-
8800/4800-v3 Series

Intel Core i7-5700EQ;
Intel Xeon E3-1200-v4
Series

Intel Atom C2300
Series;
Intel Atom C2500
Series;
Intel Atom C2700
Series

Code Name FMS

Plt ID MCU Rev

MCU Date Brand Names

Broadwell
EP/EX

0x406f1 0xef 0x0b00003e 2/6/2021

Skylake SP 0x50654 0xb7 0x02006b06 3/8/2021

Cascade
Lake B-0

0x50656 0xbf 0x04003103 4/20/2021

Cascade
Lake

Cooper
Lake

Broadwell
DE

Broadwell
DE

Broadwell
DE

Broadwell
NS

0x50657 0xbf 0x05003103

4/8/2021

0x5065b 0xbf 0x07002302 4/23/2021

0x50662 0x10 0x0000001c 6/17/2019

0x50663 0x10 0x0700001b 2/4/2021

0x50664 0x10 0x0f000019

2/4/2021

0x50665 0x10 0x0e000012 2/4/2021

Skylake H/S 0x506e3 0x36 0x000000ea 1/25/2021

Denverton

0x506f1 0x01 0x00000034 10/23/2020

Ice Lake SP 0x606a6 0x87 0x0d0002a0 4/25/2021

Snow Ridge 0x80665 0x01 0x0b00000f 2/17/2021

Kaby Lake
H/S/X

0x906e9 0x2a 0x000000ea 1/5/2021

Coffee Lake 0x906ea 0x22 0x000000ea 1/5/2021

Intel Xeon E7-
8800/4800-v4 Series;
Intel Xeon E5-4600-
v4 Series;
Intel Xeon E5-2600-
v4 Series;
Intel Xeon E5-1600-v4
Series

Intel Xeon Platinum
8100 Series;
Intel Xeon Gold
6100/5100, Silver
4100, Bronze 3100
Series;
Intel Xeon D-2100
Series;
Intel Xeon D-1600
Series;
Intel Xeon W-3100
Series;
Intel Xeon W-2100
Series

Intel Xeon Platinum
9200/8200 Series;
Intel Xeon Gold
6200/5200;
Intel Xeon Silver
4200/Bronze 3200;
Intel Xeon W-3200

Intel Xeon Platinum
9200/8200 Series;
Intel Xeon Gold
6200/5200;
Intel Xeon Silver
4200/Bronze 3200;
Intel Xeon W-3200

Intel Xeon Platinum
8300 Series;
Intel Xeon Gold
6300/5300

Intel Xeon D-1500
Series

Intel Xeon D-1500
Series

Intel Xeon D-1500
Series

Intel Xeon D-1600
Series

Intel Xeon E3-1500-v5
Series;
Intel Xeon E3-1200-v5
Series

Intel Atom C3000
Series

Intel Xeon Silver 4300
Series;
Intel Xeon Gold
6300/5300 Series;
Intel Xeon Platinum
8300 Series

Intel Atom P5000
Series

Intel Xeon E3-1200-v6
Series;
Intel Xeon E3-1500-v6
Series

Intel Xeon E-2100
Series;
Intel Xeon E-2200
Series (4 or 6 core)

Code Name FMS

Plt ID MCU Rev

MCU Date Brand Names

Coffee Lake 0x906eb 0x02 0x000000ea 1/5/2021

Coffee Lake 0x906ec 0x22 0x000000ea 1/5/2021

Coffee Lake
Refresh

Rocket
Lake S

0x906ed 0x22 0x000000ea 1/5/2021

0xa0671 0x02 0x00000040 4/11/2021

Intel Xeon E-2100
Series

Intel Xeon E-2100
Series

Intel Xeon E-2200
Series (8 core)

Intel Xeon E-2300
Series

Known Issues

The known issues are grouped as follows.

Miscellaneous Issues

Known Issues from Prior Releases

Miscellaneous Issues

The sensord daemon fails to report ESXi host hardware status

A logic error in the IPMI SDR validation might cause sensord to fail to identify a source for power supply information. As a result, when you run the command

vsish -e get /power/hostStats, you might not see any output.

Workaround: None

The ESXi SNMP service intermittently stops working and ESXi hosts become unresponsive

If your environment has IPv6 disabled, the ESXi SNMP agent might stop responding. As a result, ESXi hosts also become unresponsive. In the VMkernel logs, you

see multiple messages such as Admission failure in path: snmpd/snmpd.3955682/uw.3955682.

Workaround: Manually restart the SNMP service or use a cron job to periodically restart the service.

Stateful ESXi installation might fail on hosts with a hardware iSCSI disk connected to a Emulex OneConnect OCe11100 or 14000 NIC device

An issue with the IMA elx-esx-libelxima plug-in might cause the hostd service to fail and statefull ESXi installation cannot complete on hosts with hardware iSCSI

disk connected to a Emulex OneConnect OCe11100 or 14000 NIC device. After a network boot, the ESXi server does not reboot, but you see no errors in the

vSphere Client or vSphere Web Client.

Workaround: Install a vendor-provided async version of the elx-esx-libelxima plug-in.

The Windows guest OS of a virtual machine configured with a virtual NVDIMM of size less than 16MB might fail while initializing a new disk

If you configure a Windows virtual machine with a NVDIMM of size less than 16MB, when you try to initialize a new disk, you might see either the guest OS failing

with a blue diagnostic screen or an error message in a pop-up window in the Disk Management screen. The blue diagnostic screen issue occurs in Windows 10,

Windows Server 2022, and Windows 11 v21H2 guest operating systems.

Workaround: Increase the size of the virtual NVDIMM to 16MB or larger.

Known Issues from Prior Releases

To view a list of previous known issues, click here.

Copyright © Broadcom



=== Content from kb.vmware.com_36b2f2fb_20250115_210159.html ===


search

cancel

Search

### ASync BOM patching procedure to patch ESXi 7.0.U1E for VCF 4.1.x and VCF 4.2.x, and 7.0.U2E for 4.3.x environments

book
#### Article ID: 313492

calendar\_today
#### Updated On:

#### Products

VMware Cloud Foundation

Show More
Show Less

#### Issue/Introduction

[VMSA-2022-0004](https://www.vmware.com/security/advisories/VMSA-2022-0004.html) details numerous vulnerabilities in VMware ESXi.
As part of the response, VMware released "back in time" patches for VMware ESXi 7.0U1 and VMware ESX 7.0U2.
This KB documents the process should customers running VMware Cloud Foundation 4.1.x, 4.2.x or 4.3.x wish to update their ESXi hosts to these releases.

[Release Notes](https://docs.vmware.com/en/VMware-vSphere/7.0/rn/vsphere-esxi-70u1e.html) for VMware ESXi 7.0U1e
[Release Notes](https://docs.vmware.com/en/VMware-vSphere/7.0/rn/vsphere-esxi-70u2e-release-notes.html)  for VMware ESXi 7.0U2e

**NOTE: This KB is not applicable for VCF on Dell EMC VxRail Release**

Symptoms:
As documented in [VMSA-2022-0004](https://www.vmware.com/security/advisories/VMSA-2022-0004.html) , all versions of the VMware ESXi 7.0 prior to 7.0 Update 3c are affected by the vulnerabilities listed in the advisory.
Since the VMware Cloud Foundation(VCF) 4.x versions, prior to VCF 4.4, bundle impacted releases of VMware ESXi, the VCF versions VCF 4.1.x, VCF 4.2.x and VCF 4.3.x releases, are similarly impacted by the vulnerabilities listed in the advisory.

#### Environment

VMware Cloud Foundation 4.3.x
VMware Cloud Foundation 4.2.x
VMware Cloud Foundation 4.1

#### Resolution

#### **Guidance Steps:**

1. Verify the VCF release to be able to apply ESXi Async patch.

| **VCF Release** | **Upgrade Steps** |
| --- | --- |
| VCF 4.1.0 | Upgrade to VCF 4.1.0.1, Apply [KB 87050](https://kb.vmware.com/s/article/87050)  and then move to Step 2. |
| VCF 4.1.0.1 | Apply [KB 87050](https://kb.vmware.com/s/article/87050) and then move to Step 2. |
| VCF 4.2.0 | Upgrade to VCF 4.2.1, Apply [KB 87050](https://kb.vmware.com/s/article/87050)  and then apply [KB 88287](https://kb.vmware.com/s/article/88287). |
| VCF 4.2.1 | Apply [KB 87050](https://kb.vmware.com/s/article/87050)  and then apply [KB 88287](https://kb.vmware.com/s/article/88287). |
| VCF 4.3.0 | Upgrade to VCF 4.3.1.1 as documented in [VCF 4.3.1.1 Release Notes](https://docs.vmware.com/en/VMware-Cloud-Foundation/4.3.1/rn/vmware-cloud-foundation-431-release-notes/index.html) and apply [KB 88287](https://kb.vmware.com/s/article/88287). |
| VCF 4.3.1 | Upgrade to VCF 4.3.1.1 as documented in [VCF 4.3.1.1 Release Notes](https://docs.vmware.com/en/VMware-Cloud-Foundation/4.3.1/rn/vmware-cloud-foundation-431-release-notes/index.html) and apply [KB 88287](https://kb.vmware.com/s/article/88287). |
| VCF 4.3.1.1 | Apply [KB 88287](https://kb.vmware.com/s/article/88287). |

2. Apply ESXi fix patch async outside of VCF.

    Release Notes:
    For VCF 4.1.x and 4.2.x, refer [VMware ESXi 7.0 Update 1e](https://docs.vmware.com/en/VMware-vSphere/7.0/rn/vsphere-esxi-70u1e.html)
    For VCF 4.3.x, refer [VMware ESXi 7.0 Update 2e](https://docs.vmware.com/en/VMware-vSphere/7.0/rn/vsphere-esxi-70u2e-release-notes.html)

    Using vSphere product documentation apply ESXi patch directly without using SDDC Manager.

    **Note: After ESXi hosts are async patched. Please make sure to cleanup the baselines and ISOs on vSphere UI which were created as part of the upgrade through vSphere. If not deleted, the future upgrades from VCF LCM are prone to failures.**
3. Update the SDDC Manager Inventory.

Following steps can be used to update SDDC Manager inventory:
3.1 Download the tar file from attachments.
3.2 SCP the file to SDDC Manager in "/tmp" directory.
3.3 SSH to SDDC Manager using "vcf" user and switch to root using "su" command.
3.4 On the SDDC manager, cd  to "/tmp" directory and extract the tar file.

Extract the tar file on the SDDC Manager
root@sddc-manager [~]# cd /tmp
root@sddc-manager [/tmp]# tar xvf lcm-tools-prod.tar.gz

3.5 The executable script can be found in the bin directory.

usage: bin/inventory-sync <SDDC SSO USER> <SDDC SSH USER>
Once the above command is executed with the appropriate parameters, the script will prompt as below. Please provide the credentials when prompted.

Enter SDDC Manager SSH Password:
Enter SDDC Manager Root User's Password:
Enter SDDC Manager SSO Password:

Below is a sample run:
---
vcf@sddc-manager [ /tmp ]$ bin/inventory-sync [[email protected]](/cdn-cgi/l/email-protection) vcf
2022-02-18 16:50:19.130 [WARN ] Cloud Sleuth not configured.
2022-02-18 16:50:20.254 [INFO ] VCF Async Patch Tool - Version: 4.4.0-vcf4400RELEASE-19311902
2022-02-18 16:50:20.308 [INFO ] Log file is generated at /home/vcf/tmp/bin/async\_patch\_tool.log
Enter SDDC Manager SSH Password:
Enter SDDC Manager Root User's Password:
Enter SDDC Manager SSO Password:
2022-02-18 16:50:41.284 [INFO ] Performing global inventory sync.
2022-02-18 16:50:41.284 [INFO ] Validating inventory sync input spec
2022-02-18 16:50:41.308 [INFO ] Performing inventory sync for entities [VCENTER, ESXI, NSXT\_CLUSTER]
2022-02-18 16:50:41.308 [INFO ] Constructing version diff for vcenters
2022-02-18 16:50:42.767 [INFO ] Retrieved 1 vcenter(s) from the inventory.
2022-02-18 16:50:43.311 [INFO ] Creating new session on vcenter-1.vrack.vsphere.local for [[email protected]](/cdn-cgi/l/email-protection)
2022-02-18 16:50:43.821 [INFO ] Get the current appliance system version
---

3.6 After the inventory-sync finishes executing, the SDDC manager inventory will be in sync with all the hosts that were updated.

4. Steps to enable future upgrades from VCF (one time activity per SDDC manager instance).

   | **Patch Applied** | **Recommended Upgrade to ESXi Version** | **Corresponding VCF Version** |
   | --- | --- | --- |
   | 7.0Update 1e | 7.0 Update 3c | VCF 4.4 |
   | 7.0Update 2e | 7.0 Update 3c | VCF 4.4 |

For offline customers, upload the VCF 4.4 patch bundles following [VCF 4.4 release documentation](https://docs.vmware.com/en/VMware-Cloud-Foundation/4.4/vcf-lifecycle/GUID-C82BB0D4-FD52-4CC9-A6BB-3A6CF35E380E.html).

**4.1 Get access token:**

Login to SDDC Manager via SSH and run following command

curl '<SDDC Manager FQDN>/v1/tokens' -i -X POST -H 'Content-Type: application/json' -H 'Accept: application/json' -d '{"username" : "<SSO User ID>","password" : "<SSO Password>"}'

<SDDC Manager FQDN > -   Fully qualified domain name of SDDC manager.
<SSO User ID> - SSO user id of SDDC VM
<SSO Password> - SSO password for SDDC VM

**Example :**

Request:
curl 'http://sddc-manager.vrack.vsphere.local/v1/tokens' -i -X POST -H 'Content-Type: application/json' -H 'Accept: application/json' -d '{"username" : "[[email protected]](/cdn-cgi/l/email-protection)","password" : "password@123"}'

Response:{"accessToken":"eyJhbG...bWluaXN","refreshToken":{"id":"8702601b-cc1d-4ed2-acc4-8ccf8f849999"}}

**4.2 Enable forward upgrades by following below steps based on VCF Version**

#### [Steps for VCF 4.1.xSteps for VCF 4.2.x](#forward_upgrade_42)[Steps for VCF 4.3.x](#forward_upgrade_43)

#### **Forward upgrade path for customers on VCF 4.1.x and VCF 4.2.x:**

To enable forward upgrades, please follow below steps:

1. Create the API request payload:

* Open a vi editor with file name request.json

vi request.json

* Create a file request.json with below contents:

{
    "forceUpdate": true,
    "versionAliasesForBundleComponentTypes": [ {
        "bundleComponentType": "ESX\_HOST",
        "versionAliases": [ {
            "aliases": ["7.0.1-19324898"],
            "version": "7.0.2-18426014"
        } ]
    } ]
}

2. Trigger the PUT api call using the request.json payload and the output for the API call is as below.

API: curl -k '<SDDC Manager FQDN>/v1/system/settings/version-aliases' -X PUT -d @request.json  -H 'Content-Type:application/json'  -H 'Authorization: Bearer <AUTH TOKEN>'

<SDDC Manager FQDN >  -   Fully qualified domain name of SDDC manager.
<AUTH TOKEN> -   Access Token retrieved from Step 4.1.

Sample Output:

{
    "elements": [{
            "bundleComponentType": "ESX\_HOST",
            "versionAliases": [{
                "version": "7.0.2-18426014",
                "aliases": [
                    "7.0.1-19324898"
                ]
            }]
        }
    ]
}

#### **Forward upgrade path for customers on VCF 4.3.x:**

To enable forward upgrades, please follow below steps:

1. Create the API request payload:

* Open a vi editor with file name request.json

vi request.json

* Create a file request.json with below contents:

{
    "forceUpdate": true,
    "versionAliasesForBundleComponentTypes": [ {
        "bundleComponentType": "ESX\_HOST",
        "versionAliases": [ {
            "aliases": ["7.0.2-19290878"],
            "version": "7.0.2-18426014"
        } ]
    } ]
}

2. Trigger the PUT api call using the request.json payload and the output for the API call is as below.

API: curl -k '<SDDC Manager FQDN>/v1/system/settings/version-aliases' -X PUT -d @request.json  -H 'Content-Type:application/json'  -H 'Authorization: Bearer <AUTH TOKEN>'

<SDDC Manager FQDN >  -   Fully qualified domain name of SDDC manager.
<AUTH TOKEN> -   Access Token retrieved from Step 4.1.

Sample Output:
{
    "elements": [{
            "bundleComponentType": "ESX\_HOST",
            "versionAliases": [{
                "version": "7.0.2-18426014",
                "aliases": [
                    "7.0.2-19290878"
                ]
            }]
        }
    ]
}

**Note:**
VI domain creation will still be based on the VCF release BOM. For example: VI domain creation on VCF 4.2.1 will be based on the ESXi version(7.0.1-17551050) i.e [4.2.1 release BOM](https://docs.vmware.com/en/VMware-Cloud-Foundation/4.2.1/rn/VMware-Cloud-Foundation-421-Release-Notes.html#swversions). Follow the Guidance Steps above to patch ESXi for a new VI.

#### Attachments

lcm-tools-prod.tar
get\_app

#### Feedback

thumb\_up
Yes

thumb\_down
No

Powered by
[![Wolken Software](https://cdn.wolkenservicedesk.com/wolken-footer-image.png)](https://www.wolkensoftware.com/)



=== Content from docs.vmware.com_edd5010b_20250115_210157.html ===
VMware ESXi 7.0 Update 3c Release Notes

ESXi 7.0 Update 3c | 27 JAN 2022  | ISO Build 19193900

Check for additions and updates to these release notes.

What's in the Release Notes

The release notes cover the following topics:

What's New

Earlier Releases of ESXi 7.0

Patches Contained in this Release

Product Support Notices

Resolved Issues

Known Issues

Known Issues from Previous Releases

IMPORTANT: VMware removed ESXi 7.0 Update 3, 7.0 Update 3a and 7.0 Update 3b from all sites on November 19, 2021 due to an upgrade-impacting issue. Build

19193900 for ESXi 7.0 Update 3c ISO replaces build 18644231, 18825058, and 18905247 for ESXi 7.0 Update 3, 7.0 Update 3a, and 7.0 Update 3b respectively. To make

sure you run a smooth upgrade to vSphere 7.0 Update 3c, see VMware knowledge base articles 86447 and 87327.

What's New

If your source system contains the ESXi 7.0 Update 2 release (build number 17630552) or later builds with Intel drivers, before upgrading to ESXi 7.0 Update 3c,

see the Resolved Issues and Known Issues sections, and the VMware vCenter Server 7.0 Update 3c Release Notes.

ESXi 7.0 Update 3c delivers bug and security fixes documented in the Resolved Issues section, and VMware knowledge base articles 86255, 86158,

85982, 86283, and 86100.

For new features in the rolled back releases, see this list:

vSphere Memory Monitoring and Remediation, and support for snapshots of PMem VMs: vSphere Memory Monitoring and Remediation collects data and

provides visibility of performance statistics to help you determine if your application workload is regressed due to Memory Mode. vSphere 7.0 Update 3 also

adds support for snapshots of PMem VMs. For more information, see vSphere Memory Monitoring and Remediation.

Extended support for disk drives types: Starting with vSphere 7.0 Update 3, vSphere Lifecycle Manager validates the following types of disk drives and storage

device configurations:

• HDD (SAS/SATA)

• SSD (SAS/SATA)

• SAS/SATA disk drives behind single-disk RAID-0 logical volumes

For more information, see Cluster-Level Hardware Compatibility Checks.

Use vSphere Lifecycle Manager images to manage a vSAN stretched cluster and its witness host: Starting with vSphere 7.0 Update 3, you can use vSphere

Lifecycle Manager images to manage a vSAN stretched cluster and its witness host. For more information, see Using vSphere Lifecycle Manager Images to

Remediate vSAN Stretched Clusters.

vSphere Cluster Services (vCLS) enhancements: With vSphere 7.0 Update 3, vSphere admins can configure vCLS virtual machines to run on specific datastores

by configuring the vCLS VM datastore preference per cluster. Admins can also define compute policies to specify how the vSphere Distributed Resource

Scheduler (DRS) should place vCLS agent virtual machines (vCLS VMs) and other groups of workload VMs.

Improved interoperability between vCenter Server and ESXi versions: Starting with vSphere 7.0 Update 3, vCenter Server can manage ESXi hosts from the

previous two major releases and any ESXi host from version 7.0 and 7.0 updates. For example, vCenter Server 7.0 Update 3 can manage ESXi hosts of versions

6.5, 6.7 and 7.0, all 7.0 update releases, including later than Update 3, and a mixture of hosts between major and update versions.

New VMNIC tag for NVMe-over-RDMA storage traffic: ESXi 7.0 Update 3 adds a new VMNIC tag for NVMe-over-RDMA storage traffic. This VMkernel port

setting enables NVMe-over-RDMA traffic to be routed over the tagged interface. You can also use the ESXCLI command esxcli network ip interface tag add -i

<interface name> -t NVMeRDMA to enable the NVMeRDMA VMNIC tag.

NVMe over TCP support: vSphere 7.0 Update 3 extends the NVMe-oF suite with the NVMe over TCP storage protocol to enable high performance and

parallelism of NVMe devices over a wide deployment of TCP/IP networks.

Zero downtime, zero data loss for mission critical VMs in case of Machine Check Exception (MCE) hardware failure: With vSphere 7.0 Update 3, mission critical

VMs protected by VMware vSphere Fault Tolerance can achieve zero downtime, zero data loss in case of Machine Check Exception (MCE) hardware failure,

because VMs fallback to the secondary VM, instead of failing. For more information, see How Fault Tolerance Works.

Micro-second level time accuracy for workloads: ESXi 7.0 Update 3 adds the hardware timestamp Precision Time Protocol (PTP) to enable micro-second level

time accuracy. For more information, see Use PTP for Time and Date Synchronization of a Host.

Improved ESXi host timekeeping configuration:  ESXi 7.0 Update 3 enhances the workflow and user experience for setting an ESXi host timekeeping

configuration. For more information, see Editing the Time Configuration Settings of a Host.

Earlier Releases of ESXi 7.0

New features, resolved, and known issues of ESXi are described in the release notes for each release. Release notes for earlier releases of ESXi 7.0 are:

VMware ESXi 7.0, ESXi 7.0 Update 2d Release Notes

VMware ESXi 7.0, ESXi 7.0 Update 2c Release Notes

VMware ESXi 7.0, ESXi 7.0 Update 2a Release Notes

VMware ESXi 7.0, ESXi 7.0 Update 2 Release Notes

VMware ESXi 7.0, ESXi 7.0 Update 1d Release Notes

VMware ESXi 7.0, ESXi 7.0 Update 1c Release Notes

VMware ESXi 7.0, ESXi 7.0 Update 1b Release Notes

VMware ESXi 7.0, ESXi 7.0 Update 1a Release Notes

VMware ESXi 7.0, ESXi 7.0 Update 1 Release Notes

VMware ESXi 7.0, ESXi 7.0b Release Notes

For internationalization, compatibility, and open source components, see the VMware vSphere 7.0 Release Notes.

Patches Contained in This Release

This release of ESXi 7.0 Update 3c delivers the following patches:

Build Details

Download Filename:

VMware-ESXi-7.0U3c-19193900-depot

Build:

Download Size:

md5sum:

sha256checksum:

19193900

395.8 MB

e39a951f4e96e92eae41c94947e046ec

20cdcd6fd8f22f5f8a848b45db67316a3ee630b31a152312f4beab737f2b3cdc

Host Reboot Required:

Virtual Machine Migration or
Shutdown Required:

Yes

Yes

For a table of build numbers and versions of VMware ESXi, see VMware knowledge base article 2143832.

IMPORTANT:

Starting with vSphere 7.0, VMware uses components for packaging VIBs along with bulletins. The ESXi and esx-update bulletins depend on each other. Always

include both in a single ESXi host patch baseline or include the rollup bulletin in the baseline to avoid failure during host patching.

When patching ESXi hosts by using VMware Update Manager from a version prior to ESXi 7.0 Update 2, it is strongly recommended to use the rollup bulletin in

the patch baseline. If you cannot use the rollup bulletin, be sure to include all of the following packages in the patching baseline. If the following packages are not

included in the baseline, the update operation fails:

VMware-vmkusb_0.1-1vmw.701.0.0.16850804 or higher

VMware-vmkata_0.1-1vmw.701.0.0.16850804 or higher

VMware-vmkfcoe_1.0.0.2-1vmw.701.0.0.16850804 or higher

VMware-NVMeoF-RDMA_1.0.1.2-1vmw.701.0.0.16850804 or higher

Rollup Bulletin

This rollup bulletin contains the latest VIBs with all the fixes after the initial release of ESXi 7.0.

Bulletin ID

ESXi70U3c-19193900

Image Profiles

Category

Bugfix

Severity

Critical

VMware patch and update releases contain general and critical image profiles. Application of the general release image profile applies to new bug fixes.

Image Profile Name

ESXi70U3c-19193900-standard

ESXi70U3c-19193900-no-tools

ESXi Image

Name and Version

ESXi70U3c-19193900

Release Date

27 JAN 2022

Category

Detail

Bugfix

Bugfix image

For information about the individual components and bulletins, see the Product Patches page and the Resolved Issues section.

Patch Download and Installation

In vSphere 7.x, the Update Manager plug-in, used for administering vSphere Update Manager, is replaced with the Lifecycle Manager plug-in. Administrative operations

for vSphere Update Manager are still available under the Lifecycle Manager plug-in, along with new capabilities for vSphere Lifecycle Manager.

The typical way to apply patches to ESXi 7.x hosts is by using the vSphere Lifecycle Manager. For details, see About vSphere Lifecycle Manager and vSphere Lifecycle

Manager Baselines and Images.

You can also update ESXi hosts without using the Lifecycle Manager plug-in, and use an image profile instead. To do this, you must manually download the patch

offline bundle ZIP file after you log in to VMware Customer Connect. From the Select a Product drop-down menu, select ESXi (Embedded and Installable) and from

the Select a Version drop-down menu, select 7.0. For more information, see the Upgrading Hosts by Using ESXCLI Commands and the VMware ESXi Upgrade guide.

Product Support Notices

Deprecation of localos accounts: Support for use of localos accounts as an identity source is deprecated. VMware plans to discontinue support for use of the

local operating system as an identity source. This functionality will be removed in a future release of vSphere.

The cURL version in ESXi650-202110001 and ESXi670-202111001 is later than the cURL version in ESXi 7.0 Update 3c: The cURL version in ESXi 7.0 Update

3c is 7.77.0, while ESXi650-202110001 and ESXi670-202111001 have the newer fixed version 7.78.0. As a result, if you upgrade from ESXi650-202110001 or

ESXi670-202111001 to ESXi 7.0 Update 3c, cURL 7.7.0 might expose your system to the following vulnerabilities:

CVE-2021-22926: CVSS 7.5

CVE-2021-22925: CVSS 5.3

CVE-2021-22924: CVSS 3.7

CVE-2021-22923: CVSS 5.3

CVE-2021-22922: CVSS 6.5

cURL version 7.78.0 comes with a future ESXi 7.x release.

Merging the lpfc and brcmnvmefc drivers: Starting with vSphere 7.0 Update 3c, the brcmnvmefc driver is no longer available. The NVMe over Fibre Channel

functionality previously delivered with the brcmnvmefc driver is now included in the lpfc driver.

Deprecation of RDMA over Converged Ethernet (RoCE) v1: VMware intends in a future major vSphere release to discontinue support for the network protocol

RoCE v1. You must migrate drivers that rely on the RoCEv1 protocol to RoCEv2. In addition, you must migrate paravirtualized remote direct memory access

(PVRDMA) network adapters for virtual machines and guest operating systems to an adapter that supports RoCEv2.

Deprecation of SD and USB devices for the ESX-OSData partition: The use of SD and USB devices for storing the ESX-OSData partition, which consolidates the

legacy scratch partition, locker partition for VMware Tools, and core dump destinations, is being deprecated. SD and USB devices are supported for boot bank

partitions. For warnings related to the use of SD and USB devices during ESXi 7.0 Update 3c update or installation, see VMware Knowledge Based Article 85615.

For more information, see VMware knowledge base article 85685.

Resolved Issues

The resolved issues are grouped as follows.

Miscellaneous Issues

Networking Issues

Installation, Upgrade and Migration Issues

Security Issues

vSphere Client Issues

Storage Issues

vSAN Issues

Virtual Machine Management Issues

Resolved Issues from Previous Releases

Miscellaneous Issues

NEW: In the vSphere Client, you might see the alarm Host connection and power state on xxx to change from green to red

Due to a rare issue with handling Asynchronous Input/Output (AIO) calls, hostd and vpxa services on an ESXi host might fail and trigger alarms in the vSphere

Client. In the backtrace, you see errors such as:

#0 0x0000000bd09dcbe5 in __GI_raise (sig=6) at ../nptl/sysdeps/unix/sysv/linux/raise.c:56

#1 0x0000000bd09de05b in __GI_abort () at abort.c:90

#2 0x0000000bc7d00b65 in Vmacore::System::SignalTerminateHandler (info=, ctx=) at bora/vim/lib/vmacore/posix/defSigHandlers.cpp:62

#3 <signal called="" handler="">

#4 NfcAioProcessCloseSessionMsg (closeMsg=0xbd9280420, session=0xbde2c4510) at bora/lib/nfclib/nfcAioServer.c:935

#5 NfcAioProcessMsg (session=session@entry=0xbde2c4510, aioMsg=aioMsg@entry=0xbd92804b0) at bora/lib/nfclib/nfcAioServer.c:4206

#6 0x0000000bd002cc8b in NfcAioGetAndProcessMsg (session=session@entry=0xbde2c4510) at bora/lib/nfclib/nfcAioServer.c:4324

#7 0x0000000bd002d5bd in NfcAioServerProcessMain (session=session@entry=0xbde2c4510, netCallback=netCallback@entry=0 '\000') at

bora/lib/nfclib/nfcAioServer.c:4805

#8 0x0000000bd002ea38 in NfcAioServerProcessClientMsg (session=session@entry=0xbde2c4510, done=done@entry=0xbd92806af "") at bora/lib/nfclib/nfcAioServer.c:5166

This issue is resolved in this release. The fix makes sure the AioSession object works as expected.

A very rare issue with NVIDIA vGPU-powered virtual machines might cause ESXi hosts to fail with a purple diagnostic screen

In very rare conditions, ESXi hosts with NVIDIA vGPU-powered virtual machines might intermittently fail with a purple diagnostic screen with a kernel panic error.

The issue might affect multiple ESXi hosts, but not at the same time. In the backlog, you see kernel reports about heartbeat timeouts against CPU for x seconds

and the stack informs about a P2M cache.

This issue is resolved in this release.

ESXi hosts with virtual machines with Latency Sensitivity enabled might randomly become unresponsive due to CPU starvation

When you enable Latency Sensitivity on virtual machines, some threads of the Likewise Service Manager (lwsmd), which sets CPU affinity explicitly, might

compete for CPU resources on such virtual machines. As a result, you might see the ESXi host and the hostd service to become unresponsive.

This issue is resolved in this release. The fix makes sure lwsmd does not set CPU affinity explicitly.

In very rare cases, the virtual NVME adapter (VNVME) retry logic in ESXi 7.0 Update 3 might potentially cause silent data corruption

The VNVME retry logic in ESXi 7.0 Update 3 has an issue that might potentially cause silent data corruption. Retries rarely occur and they can potentially, not

always, cause data errors. The issue affects only ESXi 7.0 Update 3.

This issue is resolved in this release.

ESXi hosts might fail with a purple diagnostic screen during shutdown due to stale metadata

In rare cases, when you delete a large component in an ESXi host, followed by a reboot, the reboot might start before all metadata of the component gets

deleted. The stale metadata might cause the ESXi host to fail with a purple diagnostic screen.

This issue is resolved in this release. The fix makes sure no pending metadata remains before a reboot of ESXi hosts.

Virtual desktop infrastructure (VDI) might become unresponsive due to a race condition in the VMKAPI driver

Event delivery to applications might delay indefinitely due to a race condition in the VMKAPI driver. As a result, the virtual desktop infrastructure in some

environments, such as systems using NVIDIA graphic cards, might become unresponsive or lose connection to the VDI client.

This issue is resolve in this release.

ESXi hosts might fail with a purple diagnostic screen due to issues with ACPI Component Architecture (ACPICA) semaphores

Several issues in the implementation of ACPICA semaphores in ESXi 7.0 Update 3 and earlier can result in VMKernel panics, typically during boot. An issue in the

semaphore implementation can cause starvation, and on several call paths the VMKernel might improperly try to acquire an ACPICA semaphore or to sleep

within ACPICA while holding a spinlock. Whether these issues cause problems on a specific machine depends on details of the ACPI firmware of the machine.

These issues are resolved in this release. The fix involves a rewrite of the ACPICA semaphores in ESXi, and correction of the code paths that try to enter ACPICA

while holding a spinlock.

ESXi hosts might fail with a purple diagnostic screen when I/O operations run on a software iSCSI adapter

I/O operations on a software iSCSI adapter might cause a rare race condition inside the iscsi_vmk driver. As a result, ESXi hosts might intermittently fail with a

purple diagnostic screen.

This issue is resolved in this release.

Networking Issues

If you use a vSphere Distributed Switch (VDS) of version earlier than 6.6 and change the LAG hash algorithm, ESXi hosts might fail with a purple diagnostic

screen

If you use a VDS of version earlier than 6.6 on a vSphere 7.0 Update 1 or later system, and you change the LAG hash algorithm, for example from L3 to L2

hashes, ESXi hosts might fail with a purple diagnostic screen.

This issue is resolved in this release.

You see packet drops for virtual machines with VMware Network Extensibility (NetX) redirection enabled

In vCenter Server advanced performance charts, you see an increasing number of packet drop count for all virtual machines that have NetX redirection enabled.

However, if you disable NetX redirection, the count becomes 0.

This issue is resolved in this release.

An ESXi host might fail with a purple diagnostic screen during booting due to incorrect CQ to EQ mapping in an Emulex FC HBA

In rare cases, incorrect mapping of completion queues (CQ) when the total number of I/O channels of a Emulex FC HBA is not an exact multiple of the number of

event queues (EQ), might cause booting of an ESXi host to fail with a purple diagnostic screen. In the backtrace, you can see an error in the lpfc_cq_create()

method.

This issue is resolved in this release. The fix ensures correct mapping of CQs to EQs.

ESXi hosts might fail with a purple diagnostic screen due to memory allocation issue in the UNIX domain sockets

During internal communication between UNIX domain sockets, a heap allocation might occur instead of cleaning ancillary data such as file descriptors. As a

result, in some cases, the ESXi host might report an out of memory condition and fail with a purple diagnostic screen with #PF Exception 14 and errors similar to

UserDuct_ReadAndRecvMsg().

This issue is resolved in this release. The fix cleans ancillary data to avoid buffer memory allocations.

NTP optional configurations do not persist on ESXi host reboot

When you set up optional configurations for NTP by using ESXCLI commands, the settings might not persist after the ESXi host reboots.

This issue is resolved in this release. The fix makes sure that optional configurations are restored into the local cache from ConfigStore during ESXi host bootup.

When you change the LACP hashing algorithm in systems with vSphere Distributed Switch of version 6.5.0, multiple ESXi hosts might fail with a purple

diagnostic screen

In systems with vSphere Distributed Switch of version 6.5.0 and ESXi hosts of version 7.0 or later, when you change the LACP hashing algorithm, this might

cause an unsupported LACP event error due to a temporary string array used to save the event type name. As a result, multiple ESXi hosts might fail with a

purple diagnostic screen.

This issue is resolved in this release. To avoid facing the issue, in vCenter Server systems of version 7.0 and later make sure you use a vSphere Distributed Switch

version later than 6.5.0.

Installation, Upgrade and Migration Issues

Remediation of clusters that you manage with vSphere Lifecycle Manager baselines might take long

Remediation of clusters that you manage with vSphere Lifecycle Manager baselines might take long after updates from ESXi 7.0 Update 2d and earlier to a

version later than ESXi 7.0 Update 2d.

This issue is resolved in this release.

After updating to ESXi 7.0 Update 3, virtual machines with physical RDM disks fail to migrate or power-on on destination ESXi hosts

In certain cases, for example virtual machines with RDM devices running on servers with SNMP, a race condition between device open requests might lead to

failing vSphere vMotion operations.

This issue is resolved in this release. The fix makes sure that device open requests are sequenced to avoid race conditions. For more information, see VMware

knowledge base article 86158.

After upgrading to ESXi 7.0 Update 2d and later, you see an NTP time sync error

In some environments, after upgrading to ESXi 7.0 Update 2d and later, in the vSphere Client you might see the error Host has lost time synchronization.

However, the alarm might not indicate an actual issue.

This issue is resolved in this release. The fix replaces the error message with a log function for backtracing but prevents false alarms.

Security Issues

Update to OpenSSL

The OpenSSL package is updated to version openssl-1.0.2zb.

Update to the Python package

The Python package is updated to address CVE-2021-29921.

You can connect to port 9080 by using restricted DES/3DES ciphers

With the OPENSSL command openssl s_client -cipher <CIPHER> -connect localhost:9080 you can connect to port 9080 by using restricted DES/3DES ciphers.

This issue is resolved in this release. You cannot connect to port 9080 by using the following ciphers: DES-CBC3-SHA, EDH-RSA-DES-CBC3-SHA, ECDHE-RSA-

DES-CBC3-SHA, and AECDH-DES-CBC3-SHA.

The following VMware Tools ISO images are bundled with ESXi 7.0 Update 3c:

windows.iso: VMware Tools 11.3.5 supports Windows 7 SP1 or Windows Server 2008 R2 SP1 and later.

linux.iso: VMware Tools 10.3.23 ISO image for Linux OS with glibc 2.11 or later.

The following VMware Tools ISO images are available for download:

VMware Tools 11.0.6:

windows.iso: for Windows Vista (SP2) and Windows Server 2008 Service Pack 2 (SP2).

VMware Tools 10.0.12:

winPreVista.iso: for Windows 2000, Windows XP, and Windows 2003.

linuxPreGLibc25.iso: supports Linux guest operating systems earlier than Red Hat Enterprise Linux (RHEL) 5, SUSE Linux Enterprise Server (SLES) 11,

Ubuntu 7.04, and other distributions with glibc version earlier than 2.5.

solaris.iso: VMware Tools image 10.3.10 for Solaris.

darwin.iso: Supports Mac OS X versions 10.11 and later.

Follow the procedures listed in the following documents to download VMware Tools for platforms not bundled with ESXi:

VMware Tools 11.3.5 Release Notes

Earlier versions of VMware Tools

What Every vSphere Admin Must Know About VMware Tools

VMware Tools for hosts provisioned with Auto Deploy

Updating VMware Tools

vSphere Client Issues

Virtual machines appear as inaccessible in the vSphere Client and you might see some downtime for applications

In rare cases, hardware issues might cause an SQlite DB corruption that makes multiple VMs become inaccessible and lead to some downtime for applications.

This issue is resolved in this release.

Storage Issues

Virtual machine operations fail with an error for insufficient disk space on datastore

A new datastore normally has a high number of large file block (LFB) resources and a lesser number of small file block (SFB) resources. For workflows that

consume SFBs, such as virtual machine operations, LFBs convert to SFBs. However, due to a delay in updating the conversion status, newly converted SFBs

might not be recognized as available for allocation. As a result, you see an error such as Insufficient disk space on datastore when you try to power on, clone, or

migrate a virtual machine.

This issue is resolved in this release.

vSphere Virtual Volume snapshot operations might fail on the source volume or the snapshot volume on Pure storage

Due to an issue that allows the duplication of the unique ID of vSphere Virtual Volumes, virtual machine snapshot operations might fail, or the source volume

might get deleted. The issue is specific to Pure storage and affects Purity release lines 5.3.13 and earlier, 6.0.5 and earlier, and 6.1.1 and earlier.

This issue is resolved in this release.

vSAN Issues

You might see vSAN health errors for cluster partition when data-in-transit encryption is enabled

In the vSphere Client, you might see vSAN health errors such as vSAN cluster partition or vSAN object health when data-in-transit encryption is enabled. The issue

occurs because when a rekey operation starts in a vSAN cluster, a temporary resource issue might cause key exchange between peers to fail.

This issue is resolved in this release.

Virtual Machine Management Issues

A race condition between live migration operations might cause the ESXi host to fail with a purple diagnostic screen

In environments with VMs of 575 GB or more reserved memory that do not use Encrypted vSphere vMotion, a live migration operation might race with another

live migration and cause the ESXi host to fail with a purple diagnostic screen.

This issue is resolved in this release. However, in very rare cases, the migration operation might still fail, regardless that the root cause for the purple diagnostic

screen condition is fixed. In such cases, retry the migration when no other live migration is in progress on the source host, or enable Encrypted vSphere vMotion

on the virtual machines.

Resolved Issues from Previous Releases

Networking Issues

RDMA traffic by using the iWARP protocol might not complete

RDMA traffic by using the iWARP protocol on Intel x722 cards might time out and not complete.

This issue is resolved in this release.

Installation, Upgrade and Migration Issues

The /locker partition might be corrupted when the partition is stored on a USB or SD device

Due to the I/O sensitivity of USB and SD devices, the VMFS-L locker partition on such devices that stores VMware Tools and core dump files might get

corrupted.

This issue is resolved in this release. By default, ESXi loads the locker packages to the RAM disk during boot.

ESXi hosts might lose connectivity after brcmfcoe driver upgrade on Hitachi storage arrays

After an upgrade of the brcmfcoe driver on Hitachi storage arrays, ESXi hosts might fail to boot and lose connectivity.

This issue is resolved in this release.

After upgrading to ESXi 7.0 Update 2, you see excessive storage read I/O load

ESXi 7.0 Update 2 introduced a system statistics provider interface that requires reading the datastore stats for every ESXi host on every 5 min. If a datastore is

shared by multiple ESXi hosts, such frequent reads might cause a read latency on the storage array and lead to excessive storage read I/O load.

This issue is resolved in this release.

Virtual Machine Management Issues

Virtual machines with enabled AMD Secure Encrypted Virtualization-Encrypted State (SEV-ES) cannot create Virtual Machine Communication Interface (VMCI)

sockets

Performance and functionality of features that require VMCI might be affected on virtual machines with enabled AMD SEV-ES, because such virtual machines

cannot create VMCI sockets.

This issue is resolved in this release.

Virtual machines might fail when rebooting a heavily loaded guest OS

In rare cases, when a guest OS reboot is initiated outside the guest, for example from the vSphere Client, virtual machines might fail, generating a VMX dump.

The issue might occur when the guest OS is heavily loaded. As a result, responses from the guest to VMX requests are delayed prior to the reboot. In such cases,

the vmware.log file of the virtual machines includes messages such as: I125: Tools: Unable to send state change 3: TCLO error. E105: PANIC: NOT_REACHED

bora/vmx/tools/toolsRunningStatus.c:953.

This issue is resolved in this release.

Miscellaneous Issues

Asynchronous read I/O containing a SCATTER_GATHER_ELEMENT array of more than 16 members with at least 1 member falling in the last partial block of a

file might lead to ESXi host panic

In rare cases, in an asynchronous read I/O containing a SCATTER_GATHER_ELEMENT array of more than 16 members, at least 1 member might fall in the last partial block

of a file. This might lead to corrupting VMFS memory heap, which in turn causes ESXi hosts to fail with a purple diagnostic screen.

This issue is resolved in this release.

If a guest OS issues UNMAP requests with large size on thin provisioned VMDKs, ESXi hosts might fail with a purple diagnostic screen

ESXi 7.0 Update 3 introduced an uniform UNMAP granularity for VMFS and SEsparse snapshots, and set the maximum UNMAP granularity reported by VMFS to

2GB. However, in certain environments, when the guest OS makes a trim or unmap request of 2GB, such a request might require the VMFS metadata transaction

to do lock acquisition of more than 50 resource clusters. VMFS might not handle such requests correctly. As a result, an ESXi host might fail with a purple

diagnostic screen. VMFS metadata transaction requiring lock actions on greater then 50 resource clusters is rare and can only happen on aged datastores. The

issue impacts only thin-provisioned VMDKs. Thick and eager zero thick VMDKs are not impacted.

Along with the purple diagnostic screen, in the /var/run/log/vmkernel file you see errors such as:

2021-10-20T03:11:41.679Z cpu0:2352732)@BlueScreen: NMI IPI: Panic requested by another PCPU. RIPOFF(base):RBP:CS [0x1404f8(0x420004800000):0x12b8:0xf48] (Src

0x1, CPU0)

2021-10-20T03:11:41.689Z cpu0:2352732)Code start: 0x420004800000 VMK uptime: 11:07:27:23.196

2021-10-20T03:11:41.697Z cpu0:2352732)Saved backtrace from: pcpu 0 Heartbeat NMI

2021-10-20T03:11:41.715Z cpu0:2352732)0x45394629b8b8:[0x4200049404f7]HeapVSIAddChunkInfo@vmkernel#nover+0x1b0 stack: 0x420005bd611e

This issue is resolved in this release.

The hostd service might fail due to a time service event monitoring issue

An issue in the time service event monitoring service, which is enabled by default, might cause the hostd service to fail. In the vobd.log file, you see errors such

as:

2021-10-21T18:04:28.251Z: [UserWorldCorrelator] 304957116us: [esx.problem.hostd.core.dumped] /bin/hostd crashed (1 time(s) so far) and a core file may have been

created at /var/core/hostd-zdump.000. This may have caused connections to the host to be dropped.

2021-10-21T18:04:28.251Z: An event (esx.problem.hostd.core.dumped) could not be sent immediately to hostd; queueing for retry. 2021-10-21T18:04:32.298Z:

[UserWorldCorrelator] 309002531us: [vob.uw.core.dumped] /bin/hostd(2103800) /var/core/hostd-zdump.001

2021-10-21T18:04:36.351Z: [UserWorldCorrelator] 313055552us: [vob.uw.core.dumped] /bin/hostd(2103967) /var/core/hostd-zdump.002.

This issue is resolved in this release.

Known Issues

The known issues are grouped as follows.

Networking Issues

Security Issues

Known Issues from Earlier Releases

Networking Issues

Stale NSX for vSphere properties in vSphere Distributed Switch 7.0 (VDS) or ESXi 7.x hosts might fail host updates

If you had NSX for vSphere with VXLAN enabled on a vSphere Distributed Switch (VDS) of version 7.0 and migrated to NSX-T Data Center by using NSX V2T

migration, stale NSX for vSphere properties in the VDS or some hosts might prevent ESXi 7.x hosts updates. Host update fails with a platform configuration error.

Workaround: Upload the CleanNSXV.py script to the /tmp dir in vCenter Server. Log in to the appliance shell as a user with super administrative privileges (for

example, root) and follow these steps:

1. Run CleanNSXV.py by using the command PYTHONPATH=$VMWARE_PYTHON_PATH python /tmp/CleanNSXV.py --user <vc_admin_user> --password <passwd>. The

<vc_admin_user> parameter is a vCenter Server user with super administrative privileges and <passwd> parameter is the user password.

For example:

PYTHONPATH=$VMWARE_PYTHON_PATH python /tmp/CleanNSXV.py --user 'administrator@vsphere.local' --password 'Admin123'

2. Verify if the following NSX for vSphere properties, com.vmware.netoverlay.layer0 and com.vmware.net.vxlan.udpport, are removed from the ESXi hosts:

1. Connect to a random ESXi host by using an SSH client.

2. Run the command net-dvs -l | grep "com.vmware.netoverlay.layer0\|com.vmware.net.vxlan.udpport".

If you see no output, then the stale properties are removed.

To download the CleanNSXV.py script and for more details, see VMware knowledge base article 87423.

Security Issues

The cURL version in ESXi650-202110001 and ESXi670-202111001 is later than the cURL version in ESXi 7.0 Update 3c

The cURL version in ESXi 7.0 Update 3c is 7.77.0, while ESXi650-202110001 and ESXi670-202111001 have the newer fixed version 7.78.0. As a result, if you

upgrade from ESXi650-202110001 or ESXi670-202111001 to ESXi 7.0 Update 3c, cURL 7.7.0 might expose your system to the following vulnerabilities:

CVE-2021-22926: CVSS 7.5

CVE-2021-22925: CVSS 5.3

CVE-2021-22924: CVSS 3.7

CVE-2021-22923: CVSS 5.3

CVE-2021-22922: CVSS 6.5

Workaround: None. cURL version 7.78.0 comes with a future ESXi 7.x release.

Known Issues from Earlier Releases

To view a list of previous known issues, click here.

Copyright © Broadcom



=== Content from www.vmware.com_b8aa0dcc_20250114_233410.html ===


Menu

* [Products](https://www.broadcom.com/products/)
* [Solutions](https://www.broadcom.com/solutions)
* [Support and Services](https://www.broadcom.com/support)
* [Company](https://www.broadcom.com/company/about-us/)
* [How To Buy](https://www.broadcom.com/how-to-buy/#sales)

* Log in

  [Log In](/c/portal/login)
  [Register](https://profile.broadcom.com/web/registration)

[Register](https://profile.broadcom.com/web/registration)
[Login](/c/portal/login)

VMSA-2022-0004:VMware ESXi, Workstation, and Fusion updates address multiple security vulnerabilities

Product/Component

VMware Cloud Foundation

2 more products

List of Products

3 Products

* VMware Cloud Foundation
* VMware Desktop Hypervisor
* VMware vSphere ESXi

Notification Id

23619

Last Updated

13 February 2022

Initial Publication Date

13 February 2022

Status

CLOSED

Severity

CRITICAL

CVSS Base Score

5.3-8.4

WorkAround

Affected CVE

CVE-2021-22040,CVE-2021-22041,CVE-2021-22042,CVE-2021-22043,CVE-2021-22050

             Advisory ID: VMSA-2022-0004   CVSSv3 Range: 5.3-8.4   Issue Date:2022-02-15   Updated On: 2022-02-15 (Initial Advisory)   CVE(s): CVE-2021-22040, CVE-2021-22041, CVE-2021-22042, CVE-2021-22043, CVE-2021-22050   Synopsis: VMware ESXi, Workstation, and Fusion updates address multiple security vulnerabilities (CVE-2021-22040, CVE-2021-22041, CVE-2021-22042, CVE-2021-22043, CVE-2021-22050)

 [RSS Feed](https://www.vmware.com/security/advisories/VMSA-2022-0004.xml)

 Download PDF

 Download Text File

Share this page on social media:

##### **1. Impacted Products**

* VMware ESXi
* VMware Workstation Pro / Player (Workstation)
* VMware Fusion Pro / Fusion (Fusion)
* VMware Cloud Foundation (Cloud Foundation)

##### **2. Introduction**

Multiple vulnerabilities in VMware ESXi, Workstation, and Fusion were privately reported to VMware. Updates are available to remediate these vulnerabilities in affected VMware products.

The individual vulnerabilities documented on this VMSA have severity Important/Moderate but combining these issues may result in higher severity, hence the severity of this VMSA is at severity level Critical.

##### **3a. Use-after-free vulnerability in XHCI USB controller (CVE-2021-22040)**

**Description**

VMware ESXi, Workstation, and Fusion contain a use-after-free vulnerability in the XHCI USB controller.VMware has evaluated the severity of this issue to be in the [Important severity range](https://www.vmware.com/support/policies/security_response.html) with a maximum CVSSv3 base score of [8.4](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:L/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H).

**Known Attack Vectors**

A malicious actor with local administrative privileges on a virtual machine may exploit this issue to execute code as the virtual machine's VMX process running on the host.

**Resolution**

To remediate CVE-2021-22040 apply the patches listed in the 'Fixed Version' column of the 'Response Matrix' found below.

**Workarounds**

Workarounds for CVE-2021-22040 have been listed in the 'Workarounds' column of the 'Response Matrix' below.

**Additional Documentation**

A supplemental blog post was created for additional clarification. Please see: <https://via.vmw.com/vmsa-2022-0004-qna>.

**Notes**

**[1]** VMware recommends taking ESXi670-202201001 released on January 25, 2022 over ESXi670-202111101-SG released on November 23, 2021 since ESXi670-202201001 also resolves non-security related issues (documented in <https://docs.vmware.com/en/VMware-vSphere/6.7/rn/esxi670-202201001.html>).

**Acknowledgements**

VMware would like to thank Wei of Kunlun Lab working with the 2021 Tianfu Cup Pwn Contest for reporting this issue to us.

##### **3b. Double-fetch vulnerability in UHCI USB controller (CVE-2021-22041)**

**Description**

VMware ESXi, Workstation, and Fusion contain a double-fetch vulnerability in the UHCI USB controller. VMware has evaluated the severity of this issue to be in the [Important severity range](https://www.vmware.com/support/policies/security_response.html) with a maximum CVSSv3 base score of [8.4](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:L/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H).

**Known Attack Vectors**

A malicious actor with local administrative privileges on a virtual machine may exploit this issue to execute code as the virtual machine's VMX process running on the host.

**Resolution**

To remediate CVE-2021-22041 apply the patches listed in the 'Fixed Version' column of the 'Response Matrix' found below.

**Workarounds**

Workarounds for CVE-2021-22041 have been listed in the 'Workarounds' column of the 'Response Matrix' below.

**Additional Documentation**

A supplemental blog post was created for additional clarification. Please see: <https://via.vmw.com/vmsa-2022-0004-qna>.

**Notes**

Successful exploitation of this issue requires an isochronous USB endpoint to be made available to the virtual machine.

**[1]** VMware recommends taking ESXi670-202201001 released on January 25, 2022 over ESXi670-202111101-SG released on November 23, 2021 since ESXi670-202201001 also resolves non-security related issues (documented in <https://docs.vmware.com/en/VMware-vSphere/6.7/rn/esxi670-202201001.html>).

**Acknowledgements**

VMware would like to thank VictorV of Kunlun Lab working with the 2021 Tianfu Cup Pwn Contest for reporting this issue to us.

**Response Matrix: - 3a & 3b**

| Product | Version | Running On | CVE Identifier | CVSSv3 | Severity | Fixed Version | Workarounds | Additional Documentation |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| ESXi | 7.0 U3 | Any | CVE-2021-22040, CVE-2021-22041 | [8.4](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:L/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H) | important | [ESXi70U3c-19193900](https://docs.vmware.com/en/VMware-vSphere/7.0/rn/vsphere-esxi-70u3c-release-notes.html) | [KB87349](https://kb.vmware.com/s/article/87349) | [FAQ](https://via.vmw.com/vmsa-2022-0004-qna) |
| ESXi | 7.0 U2 | Any | CVE-2021-22040, CVE-2021-22041 | [8.4](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:L/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H) | important | [ESXi70U2e-19290878](https://docs.vmware.com/en/VMware-vSphere/7.0/rn/vsphere-esxi-70u2e-release-notes.html) | [KB87349](https://kb.vmware.com/s/article/87349) | [FAQ](https://via.vmw.com/vmsa-2022-0004-qna) |
| ESXi | 7.0 U1 | Any | CVE-2021-22040, CVE-2021-22041 | [8.4](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:L/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H) | important | [ESXi70U1e-19324898](https://docs.vmware.com/en/VMware-vSphere/7.0/rn/vsphere-esxi-70u1e.html) | [KB87349](https://kb.vmware.com/s/article/87349) | [FAQ](https://via.vmw.com/vmsa-2022-0004-qna) |
| ESXi | 6.7 | Any | CVE-2021-22040, CVE-2021-22041 | [8.4](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:L/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H) | important | [[1] ESXi670-202111101-SG](https://docs.vmware.com/en/VMware-vSphere/6.7/rn/esxi670-202111001.html) | [KB87349](https://kb.vmware.com/s/article/87349) | [FAQ](https://via.vmw.com/vmsa-2022-0004-qna) |
| ESXi | 6.5 | Any | CVE-2021-22040, CVE-2021-22041 | [8.4](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:L/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H) | important | [ESXi650-202202401-SG](https://docs.vmware.com/en/VMware-vSphere/6.5/rn/esxi650-202202001.html) | [KB87349](https://kb.vmware.com/s/article/87349) | [FAQ](https://via.vmw.com/vmsa-2022-0004-qna) |
| Fusion | 12.x | OS X | CVE-2021-22040, CVE-2021-22041 | [8.4](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:L/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H) | important | [12.2.1](https://docs.vmware.com/en/VMware-Fusion/12.2.1/rn/VMware-Fusion-1221-Release-Notes.html) | [KB87349](https://kb.vmware.com/s/article/87349) | [FAQ](https://via.vmw.com/vmsa-2022-0004-qna) |
| Workstation | 16.x | Any | CVE-2021-22040, CVE-2021-22041 | [8.4](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:L/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H) | important | [16.2.1](https://docs.vmware.com/en/VMware-Workstation-Player/16.2.1/rn/VMware-Workstation-1621-Player-Release-Notes.html) | [KB87349](https://kb.vmware.com/s/article/87349) | [FAQ](https://via.vmw.com/vmsa-2022-0004-qna) |

**Impacted Product Suites that Deploy Response Matrix 3a & 3b Components:**

| Product | Version | Running On | CVE Identifier | CVSSv3 | Severity | Fixed Version | Workarounds | Additional Documentation |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| Cloud Foundation (ESXi) | 4.x | Any | CVE-2021-22040, CVE-2021-22041 | [8.4](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:L/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H) | important | [KB87646 (4.4)](https://kb.vmware.com/s/article/87646) | [KB87349](https://kb.vmware.com/s/article/87349) | [FAQ](https://via.vmw.com/vmsa-2022-0004-qna) |
| Cloud Foundation (ESXi) | 3.x | Any | CVE-2021-22040, CVE-2021-22041 | [8.4](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:L/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H) | important | [3.11](https://docs.vmware.com/en/VMware-Cloud-Foundation/3.11/rn/VMware-Cloud-Foundation-311-Release-Notes.html) | [KB87349](https://kb.vmware.com/s/article/87349) | [FAQ](https://via.vmw.com/vmsa-2022-0004-qna) |

##### **3c. ESXi settingsd unauthorized access vulnerability (CVE-2021-22042)**

**Description**

VMware ESXi contains an unauthorized access vulnerability due to VMX having access to settingsd authorization tickets. VMware has evaluated the severity of this issue to be in the [Important severity range](https://www.vmware.com/support/policies/security_response.html) with a maximum CVSSv3 base score of [8.2](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:L/AC:L/PR:H/UI:N/S:C/C:H/I:H/A:H).

**Known Attack Vectors**

A malicious actor with privileges within the VMX process only, may be able to access settingsd service running as a high privileged user.

**Resolution**

To remediate CVE-2021-22042 apply the patches listed in the 'Fixed Version' column of the 'Response Matrix' found below.

**Workarounds**

None.

**Additional Documentation**

A supplemental blog post was created for additional clarification. Please see: <https://via.vmw.com/vmsa-2022-0004-qna>.

**Notes**

None.

**Acknowledgements**

VMware would like to thank Wei of Kunlun Lab working with the 2021 Tianfu Cup Pwn Contest for reporting this issue to us.

##### **3d. ESXi settingsd TOCTOU vulnerability (CVE-2021-22043)**

**Description**

VMware ESXi contains a TOCTOU (Time-of-check Time-of-use) vulnerability that exists in the way temporary files are handled. VMware has evaluated the severity of this issue to be in the [Important severity range](https://www.vmware.com/support/policies/security_response.html) with a maximum CVSSv3 base score of [8.2](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:L/AC:L/PR:H/UI:N/S:C/C:H/I:H/A:H).

**Known Attack Vectors**

A malicious actor with access to settingsd, may exploit this issue to escalate their privileges by writing arbitrary files.

**Resolution**

To remediate CVE-2021-22043 apply the patches listed in the 'Fixed Version' column of the 'Response Matrix' found below.

**Workarounds**

None.

**Additional Documentation**

A supplemental blog post was created for additional clarification. Please see: <https://via.vmw.com/vmsa-2022-0004-qna>.

**Notes**

None.

**Acknowledgements**

VMware would like to thank Wei of Kunlun Lab working with the 2021 Tianfu Cup Pwn Contest for reporting this issue to us.

**Response Matrix: - 3c & 3d**

| Product | Version | Running On | CVE Identifier | CVSSv3 | Severity | Fixed Version | Workarounds | Additional Documentation |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| ESXi | 7.0 U3 | Any | CVE-2021-22042, CVE-2021-22043 | [8.2](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:L/AC:L/PR:H/UI:N/S:C/C:H/I:H/A:H) | important | [ESXi70U3c-19193900](https://docs.vmware.com/en/VMware-vSphere/7.0/rn/vsphere-esxi-70u3c-release-notes.html) | None | [FAQ](https://via.vmw.com/vmsa-2022-0004-qna) |
| ESXi | 7.0 U2 | Any | CVE-2021-22042, CVE-2021-22043 | [8.2](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:L/AC:L/PR:H/UI:N/S:C/C:H/I:H/A:H) | important | [ESXi70U2e-19290878](https://docs.vmware.com/en/VMware-vSphere/7.0/rn/vsphere-esxi-70u2e-release-notes.html) | None | [FAQ](https://via.vmw.com/vmsa-2022-0004-qna) |
| ESXi | 7.0 U1 | Any | CVE-2021-22042, CVE-2021-22043 | [8.2](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:L/AC:L/PR:H/UI:N/S:C/C:H/I:H/A:H) | important | [ESXi70U1e-19324898](https://docs.vmware.com/en/VMware-vSphere/7.0/rn/vsphere-esxi-70u1e.html) | None | [FAQ](https://via.vmw.com/vmsa-2022-0004-qna) |
| ESXi | 6.7 | Any | CVE-2021-22042, CVE-2021-22043 | N/A | N/A | Unaffected | N/A | N/A |
| ESXi | 6.5 | Any | CVE-2021-22042, CVE-2021-22043 | N/A | N/A | Unaffected | N/A | N/A |

**Impacted Product Suites that Deploy Response Matrix 3c & 3d Components:**

| Product | Version | Running On | CVE Identifier | CVSSv3 | Severity | Fixed Version | Workarounds | Additional Documentation |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| Cloud Foundation (ESXi) | 4.x | Any | CVE-2021-22042, CVE-2021-22043 | [8.2](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:L/AC:L/PR:H/UI:N/S:C/C:H/I:H/A:H) | important | [KB87646 (4.4)](https://kb.vmware.com/s/article/87646) | None | [FAQ](https://via.vmw.com/vmsa-2022-0004-qna) |
| Cloud Foundation (ESXi) | 3.x | Any | CVE-2021-22042, CVE-2021-22043 | N/A | N/A | Unaffected | N/A | N/A |

##### **3e. ESXi slow HTTP POST denial of service vulnerability (CVE-2021-22050)**

**Description**

ESXi contains a slow HTTP POST denial-of-service vulnerability in rhttpproxy. VMware has evaluated the severity of this issue to be in the [Moderate severity range](https://www.vmware.com/support/policies/security_response.html) with a maximum CVSSv3 base score of [5.3](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:L).

**Known Attack Vectors**

A malicious actor with network access to ESXi may exploit this issue to create a denial-of-service condition by overwhelming rhttpproxy service with multiple requests.

**Resolution**

To remediate CVE-2021-22050 apply the patches listed in the 'Fixed Version' column of the 'Response Matrix' found below.

**Workarounds**

None.

**Additional Documentation**

A supplemental blog post was created for additional clarification. Please see: <https://via.vmw.com/vmsa-2022-0004-qna>.

**Notes**

**[1]** VMware recommends taking ESXi670-202201001 released on January 25, 2022 over ESXi670-202111101-SG released on November 23, 2021 since ESXi670-202201001 also resolves non-security related issues (documented in <https://docs.vmware.com/en/VMware-vSphere/6.7/rn/esxi670-202201001.html>).

**Acknowledgements**

VMware would like to thank George Noseevich (@webpentest) and Sergey Gerasimov of SolidLab LLC for reporting this issue to us.

**Response Matrix**

| Product | Version | Running On | CVE Identifier | CVSSv3 | Severity | Fixed Version | Workarounds | Additional Documentation |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| ESXi | 7.0 | Any | CVE-2021-22050 | [5.3](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:L) | moderate | [ESXi70U3c-19193900](https://docs.vmware.com/en/VMware-vSphere/7.0/rn/vsphere-esxi-70u3c-release-notes.html) | None | [FAQ](https://via.vmw.com/vmsa-2022-0004-qna) |
| ESXi | 6.7 | Any | CVE-2021-22050 | [5.3](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:L) | moderate | [[1] ESXi670-202111101-SG](https://docs.vmware.com/en/VMware-vSphere/6.7/rn/esxi670-202111001.html) | None | [FAQ](https://via.vmw.com/vmsa-2022-0004-qna) |
| ESXi | 6.5 | Any | CVE-2021-22050 | [5.3](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:L) | moderate | [ESXi650-202110101-SG](https://docs.vmware.com/en/VMware-vSphere/6.5/rn/esxi650-202110001.html) | None | [FAQ](https://via.vmw.com/vmsa-2022-0004-qna) |

**Impacted Product Suites that Deploy Response Matrix 3e Components:**

| Product | Version | Running On | CVE Identifier | CVSSv3 | Severity | Fixed Version | Workarounds | Additional Documentation |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| Cloud Foundation (ESXi) | 4.x | Any | CVE-2021-22050 | [5.3](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:L) | moderate | [KB87646 (4.4)](https://kb.vmware.com/s/article/87646) | None | [FAQ](https://via.vmw.com/vmsa-2022-0004-qna) |
| Cloud Foundation (ESXi) | 3.x | Any | CVE-2021-22050 | [5.3](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:L) | moderate | [3.11](https://docs.vmware.com/en/VMware-Cloud-Foundation/3.11/rn/VMware-Cloud-Foundation-311-Release-Notes.html) | None | [FAQ](https://via.vmw.com/vmsa-2022-0004-qna) |

##### **4. References**

**VMware ESXi 7.0 ESXi70U3c-19193900**
<https://my.vmware.com/group/vmware/patch>
<https://docs.vmware.com/en/VMware-vSphere/7.0/rn/vsphere-esxi-70u3c-release-notes.html>

**VMware ESXi 7.0 ESXi70U2e-19290878**
<https://my.vmware.com/group/vmware/patch>
<https://docs.vmware.com/en/VMware-vSphere/7.0/rn/vsphere-esxi-70u2e-release-notes.html>

**VMware ESXi 7.0 ESXi70U1e-19324898**
<https://my.vmware.com/group/vmware/patch>
<https://docs.vmware.com/en/VMware-vSphere/7.0/rn/vsphere-esxi-70u1e.html>

**VMware ESXi 6.7 ESXi670-202111101-SG**
<https://my.vmware.com/group/vmware/patch>
<https://docs.vmware.com/en/VMware-vSphere/6.7/rn/esxi670-202111001.html>

**VMware ESXi 6.5 ESXi650-202202401-SG**
<https://my.vmware.com/group/vmware/patch>
<https://docs.vmware.com/en/VMware-vSphere/6.5/rn/esxi650-202202001.html>

**VMware ESXi 6.5 ESXi650-202110101-SG**
<https://my.vmware.com/group/vmware/patch>
<https://docs.vmware.com/en/VMware-vSphere/6.5/rn/esxi650-202110001.html>

**VMware Cloud Foundation 4.4**
Downloads and Documentation:
<https://docs.vmware.com/en/VMware-Cloud-Foundation/4.4/rn/VMware-Cloud-Foundation-44-Release-Notes.html>

**VMware Cloud Foundation 3.11**
Downloads and Documentation:
<https://docs.vmware.com/en/VMware-Cloud-Foundation/3.11/rn/VMware-Cloud-Foundation-311-Release-Notes.html>

**VMware Workstation Player 16.2.1**
<https://www.vmware.com/go/downloadplayer>
<https://docs.vmware.com/en/VMware-Workstation-Player/index.html>

**VMware Fusion 12.2.1**
Downloads and Documentation:
<https://www.vmware.com/go/downloadfusion>
<https://docs.vmware.com/en/VMware-Fusion/index.html>

**Mitre CVE Dictionary Links:**
<https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-22040>
<https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-22041>
<https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-22042>
<https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-22043>
<https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-22050>

**FIRST CVSSv3 Calculator:**
CVE-2021-22040: <https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:L/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H>
CVE-2021-22041: <https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:L/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H>
CVE-2021-22042: <https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:L/AC:L/PR:H/UI:N/S:C/C:H/I:H/A:H>
CVE-2021-22043: <https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:L/AC:L/PR:H/UI:N/S:C/C:H/I:H/A:H>
CVE-2021-22050: <https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:L>

##### **5. Change Log**

**2022-02-15 VMSA-2022-0004**
Initial security advisory.

##### **6. Contact**

E-mail list for product security notifications and announcements:

[https://lists.vmware.com/mailman/listinfo/security-announce](https://lists.vmware.com/cgi-bin/mailman/listinfo/security-announce)

This Security Advisory is posted to the following lists:

[[email protected]](/cdn-cgi/l/email-protection#4033252335322934396d212e2e2f352e2325002c293334336e362d372132256e232f2d)

[[email protected]](/cdn-cgi/l/email-protection#c8aabdafbcbaa9b988bbadabbdbaa1bcb1aea7abbdbbe6aba7a5)

[[email protected]](/cdn-cgi/l/email-protection#3a5c4f56565e5349595655494f485f7a495f595653494e491455485d)

E-mail: [[email protected]](/cdn-cgi/l/email-protection#4536202630372c313c053328322437206b262a28)

PGP key at:

<https://kb.vmware.com/kb/1055>

VMware Security Advisories

<https://www.vmware.com/security/advisories>

VMware Security Response Policy

<https://www.vmware.com/support/policies/security_response.html>

VMware Lifecycle Support Phases

<https://www.vmware.com/support/policies/lifecycle.html>

VMware Security & Compliance Blog

<https://blogs.vmware.com/security>

Twitter

<https://twitter.com/VMwareSRC>

Copyright 2022 VMware Inc. All rights reserved.

Hidden

#####

×

It appears your Broadcom Products and Services are

supported by one of our certified Support partners

Click below to be redirected to the appropriate Support

Partner Portal to request support

For non-product related issues (Support Portal / Licensing) Click HERE

Continue

#####

×

For **Technical Support** (issues with products or services)

1. Select **Technical** to be redirected to the My Entitlements page
2. Expand the product you require support on
3. Select the case icon from the case column
4. You will be redirected to the appropriate vendor portal where you can raise your technical request

For **Non-Technical Support** (issues with portal access, license keys, software downloads)

1. Select **Non-Technical** to be redirected to Broadcom's case management portal

Technical
Non-Technical

#####

×

# Access Denied

This feature has been disabled by your administrator.

#####

×

To prevent this message from showing again, please enable pop-up blockers for [support.broadcom.com](https://support.broadcom.com/)
or click Continue to proceed.

Continue

Top

* [Products](https://www.broadcom.com/products)
* [Solutions](https://www.broadcom.com/solutions)
* [Support and Services](https://www.broadcom.com/support)
* [Company](https://www.broadcom.com/)
* [How to Buy](https://www.broadcom.com/how-to-buy)

 Copyright © 2005-2024 Broadcom. All Rights Reserved. The term “Broadcom” refers to Broadcom Inc. and/or its subsidiaries.

* [Accessibility](https://www.broadcom.com/company/legal/accessibility)
* [Privacy](https://www.broadcom.com/company/legal/privacy)
* [Supplier Responsibility](https://www.broadcom.com/company/citizenship/supplier-responsibility)
* [Terms of Use](https://www.broadcom.com/company/legal/terms-of-use)
* [Site Map](https://www.broadcom.com/sitemap)



=== Content from docs.vmware.com_e8f60eb9_20250115_130829.html ===
VMware ESXi650-202110001 Release Notes

Release Date: 12 OCT, 2021

What's in the Release Notes

The release notes cover the following topics:

Patch Build Details

Resolved Issues

Known Issues from Previous Releases

Build Details

Download Filename:

ESXi650-202110001.zip

Build:

Download Size:

md5sum:

18678235

484.5 MB

9bbf065773dfff41eca0605936c98cfc

sha256checksum:

f403c5bf4ae81c61b3e12069c8cedaf297aa757cd977161643743bcd8e8f74dd

Host Reboot Required:

Virtual Machine Migration or
Shutdown Required:

Yes

Yes

Bulletins

Bulletin ID

ESXi650-202110401-BG

ESXi650-202110402-BG

ESXi650-202110403-BG

ESXi650-202110404-BG

ESXi650-202110101-SG

ESXi650-202110102-SG

ESXi650-202110103-SG

Rollup Bulletin

Category

Bugfix

Bugfix

Bugfix

Bugfix

Security

Security

Security

Severity

Important

Important

Important

Important

Important

Important

Important

This rollup bulletin contains the latest VIBs with all the fixes since the initial release of ESXi 6.5.

Bulletin ID

ESXi650-202110001

Category

Bugfix

Severity

Important

IMPORTANT: For clusters using VMware vSAN, you must first upgrade the vCenter Server system. Upgrading only ESXi is not supported.

Before an upgrade, always verify in the VMware Product Interoperability Matrix compatible upgrade paths from earlier versions of ESXi, vCenter Server and vSAN to

the current version.

Image Profiles

VMware patch and update releases contain general and critical image profiles. Application of the general release image profile applies to new bug fixes.

Image Profile Name

ESXi-6.5.0-20211004001-standard

ESXi-6.5.0-20211004001-no-tools

ESXi-6.5.0-20211001001s-standard

ESXi-6.5.0-20211001001s-no-tools

For more information about the individual bulletins, see the Download Patches page and the Resolved Issues section.

Patch Download and Installation

The typical way to apply patches to ESXi hosts is by using the VMware vSphere Update Manager. For details, see About Installing and Administering VMware vSphere

Update Manager.

ESXi hosts can be updated by manually downloading the patch ZIP file from VMware Customer Connect. Navigate to Products and Accounts > Product Patches. From

the Select a Product drop-down menu, select ESXi (Embedded and Installable) and from the Select a Version drop-down menu, select 6.5.0. Install VIBs by using

the esxcli software vib update command. Additionally, the system can be updated by using the image profile and the esxcli software profile update command. For

more information, see vSphere Command-Line Interface Concepts and Examples and vSphere Upgrade Guide.

Resolved Issues

The resolved issues are grouped as follows.

ESXi650-202110401-BG

ESXi650-202110402-BG

ESXi650-202110403-BG

ESXi650-202110404-BG

ESXi650-202110101-SG

ESXi650-202110102-SG

ESXi650-202110103-SG

ESXi-6.5.0-20211004001-standard

ESXi-6.5.0-20210704001-no-tools

ESXi-6.5.0-20211001001s-standard

ESXi-6.5.0-20211001001s-no-tools

ESXi650-202110401-BG

Patch Category

Patch Severity

Host Reboot Required

Virtual Machine Migration or
Shutdown Required

Affected Hardware

Affected Software

Bugfix

Important

Yes

Yes

N/A

N/A

VIBs Included

PRs Fixed

VMware_bootbank_esx-base_6.5.0-3.170.18678235
VMware_bootbank_vsan_6.5.0-3.170.18569621
VMware_bootbank_vsanhealth_6.5.0-3.170.18569625
VMware_bootbank_esx-tboot_6.5.0-3.170.18678235

2761135, 2685552, 2720188, 2793135, 2761972, 2755231, 2492991, 2710163,
2715132, 2701240, 2750814, 2635091, 2540214, 2749440, 2719215, 2603461,
2654781, 2603445, 2418383, 2796947, 2704429, 2765145, 2704412,
2731316, 2688189

Related CVE numbers

N/A

This patch updates esx-base, esx-tboot, vsan, and vsanhealth VIBs to resolve the following issues:

PR 2761135: If you import virtual machines with a virtual USB device to a vCenter Server system, the VMs might fail to power on

If you import a VM with a virtual USB device that is not supported by vCenter Server, such as a virtual USB camera, to a vCenter Server system, the VM might fail

to power on. The start of such virtual machines fails with an error message similar to: PANIC: Unexpected signal: 11. The issue affects mostly VM import operations

from VMware Fusion or VMware Workstation systems, which support a wide range of virtual USB devices.

This issue is resolved in this release. The fix ignores any unsupported virtual USB devices in virtual machines imported to a vCenter Server system.

PR 2685552: If you use large block sizes, I/O bandwidth might drop

In some configurations, if you use block sizes higher than the supported max transfer length of the storage device, you might see a drop in the I/O bandwidth.

The issue occurs due to buffer allocations in the I/O split layer in the storage stack that cause a lock contention.

This issue is resolved in this release. The fix optimizes the I/O split layer to avoid new buffer allocations. However, the optimization depends on the buffers that

the guest OS creates while issuing I/O and might not work in all cases.

PR 2720188: ESXi hosts intermittently fail with a BSVMM_Validate@vmkernel error in the backtrace.

A rare error condition in the VMKernel might cause ESXi hosts to fail when powering on a virtual machine with more than 1 virtual CPU. A BSVMM_Validate@vmkernel

error is indicative for the problem.

This issue is resolved in this release.

PR 2793135: The hostd service might fail and ESXi hosts lose connectivity to vCenter Server due to an empty or unset property of a Virtual Infrastructure

Management (VIM) API array

In rare cases, an empty or unset property of a VIM API data array might cause the hostd service to fail. As a result, ESXi hosts lose connectivity to vCenter Server

and you must manually reconnect the hosts.

This issue is resolved in this release.

PR 2761972: TPM 1.2 read operations might cause an ESXi host to fail with a purple diagnostic screen

An issue with the TPM 1.2 character device might cause ESXi hosts to fail with a page fault (#PF) purple diagnostic screen.

This issue is resolved in this release.

PR 2755231: An ESXi host might not discover devices configured with the VMW_SATP_INV plug-in

In case of temporary connectivity issues, ESXi hosts might not discover devices configured with the VMW_SATP_INV plug-in after connectivity restores, because SCSI

commands that fail during the device discovery stage cause an out-of-memory condition for the plug-in.

This issue is resolved in this release.

PR 2492991: You see multiple duplicate logs in the vSAN health service logs

Due to an issue with clearing some profile variables, you might see multiple duplicate logs in the /var/log/vmware/vsan-health/vmware-vsan-health-service.log file.

Such messages are very short and do not cause significant memory consumption.

This issue is resolved in this release.

PR 2710163: Claim rules must be manually added to ESXi for Fujitsu Eternus AB/HB series

This fix sets Storage Array Type Plug-in (SATP) to VMW_SATP_ALUA, Path Selection Policy (PSP) to VMW_PSP_RR and Claim Options to tpgs_on as default for the Fujitsu

Eternus AB/HB series.

This issue is resolved in this release.

PR 2715132: You cannot collect performance statistics by using the rvc.bat file

If you try to collect performance statistics by using the rvc.bat file in a vCenter Server system of version later than 6.5 Update 3k, the operation fails due to a

wrong library path.

This issue is resolved in this release.

PR 2701240: Virtual machines might power off during an NFS server failover

During an NFS server failover, the NFS client reclaims all open files. In rare cases, the reclaim operation fails and virtual machines power off, because the NFS

server rejects failed requests.

This issue is resolved in this release. The fix makes sure that reclaim requests succeed.

PR 2750814: Failure of some Intel CPUs to forward Debug Exception (#DB) traps might result in virtual machine triple fault

In rare cases, some Intel CPUs might fail to forward #DB traps and if a timer interrupt happens during the Windows system call, virtual machine might triple fault.

In the vmware.log file, you see an error such as msg.monitorEvent.tripleFault. This issue is not consistent and virtual machines that consistently triple fault or triple

fault after boot are impacted by another issue.

This issue is resolved in this release. The fix forwards all #DB traps from CPUs into the guest operating system, except when the DB trap comes from a debugger

that is attached to the virtual machine.

PR 2635091: ESXi hosts fail with a purple diagnostic screen with a memory region error

A checksum validation issue with newly created memory regions might cause ESXi hosts fail with a purple diagnostic screen. In the backtrace, you see errors

such as VMKernel: checksum BAD: 0xXXXX 0xXXXX . The issue is specific to the process of VMkernel module loading.

This issue is resolved in this release. The fix adds a check to avoid verifying checksum for an uninitialized memory region.

PR 2540214: Taking a snapshot of a large virtual machine on a large VMFS6 datastore might take long

Resource allocation for a delta disk to create a snapshot of a large virtual machine, with virtual disks equal to or exceeding 1TB, on large VMFS6 datastores of

30TB or more, might take significant time. As a result, the virtual machine might temporarily lose connectivity. The issue affects primarily VMFS6 filesystems.

This issue is resolved in this release.

PR 2749440: The ESXi SNMP service intermittently stops working and ESXi hosts become unresponsive

If your environment has IPv6 disabled, the ESXi SNMP agent might stop responding. As a result, ESXi hosts also become unresponsive. In the VMkernel logs, you

see multiple messages such as Admission failure in path: snmpd/snmpd.3955682/uw.3955682.

Workaround: Manually restart the SNMP service or use a cron job to periodically restart the service.

PR 2719215: You see health alarms for sensor entity ID 44 after upgrading the firmware of HPE Gen10 servers

After upgrading the firmware version on HP Gen10 servers, you might see health alarms for the I/O Module 2 ALOM_Link_P2 and NIC_Link_02P2 sensors, related to

the sensor entity ID 44.x. The alarms do not indicate an actual health issue and you can ignore them irrespective of the firmware version.

This issue is resolved in this release.

PR 2603461: In case of a non-UTF8 string in the name property of numeric sensors, the vpxa service fails

The vpxa service fails in case of a non-UTF8 string in the name property of numeric sensors, and ESXi hosts disconnect from the vCenter Server system.

This issue is resolved in this release.

PR 2465245: ESXi upgrades fail due to all-paths-down error of the vmkusb driver

In certain environments, ESXi upgrades might fail due to all-paths-down error of the vmkusb driver that prevents mounting images from external devices while

running scripted installations. If you try to use a legacy driver, the paths might display as active but the script still does not run.

This issue is resolved in this release. The fix makes sure no read or write operations run on LUNs with external drives if no media is currently available in that drive

until booting is complete.

PR 2654781: The advanced config option UserVars/HardwareHealthIgnoredSensors fails to ignore some sensors

If you use the advanced config option UserVars/HardwareHealthIgnoredSensors to ignore sensors with consecutive entries in a numeric list, such as 0.52 and 0.53,

the operation might fail to ignore some sensors. For example, if you run the command esxcfg-advcfg -s 52,53 /UserVars/HardwareHealthIgnoredSensors, only the

sensor 0.53 might be ignored.

This issue has been fixed in this release.

PR 2603445: The hostd service fails due to an invalid UTF8 string for numeric sensor base-unit property

If the getBaseUnitString function returns a non-UTF8 string for the max value of a unit description array, the hostd service fails with a core dump. You see an

error such as:  [Hostd-zdump] 0x00000083fb085957 in Vmacore::PanicExit (msg=msg@entry=0x840b57eee0 "Validation failure").

This issue is resolved in this release.

PR 2418383: You see Sensor -1 type hardware health alarms on ESXi hosts and receive excessive mail alerts

After upgrading to ESXi 6.5 Update 3, you might see Sensor -1 type hardware health alarms on ESXi hosts being triggered without an actual problem. This can

result in excessive email alerts if you have configured email notifications for hardware sensor state alarms in your vCenter Server system. These mails might

cause storage issues in the vCenter Server database if the Stats, Events, Alarms and Tasks (SEAT) directory goes above the 95% threshold.

This issue is resolved in this release.

PR 2796947: Disabling the SLP service might cause failures during operations with host profiles

When the SLP service is disabled to prevent potential security vulnerabilities, the sfcbd-watchdog service might remain enabled and cause compliance check

failures when you perform updates by using a host profile.

This issue is resolved in this release.

PR 2704429: The sfcb service fails and you see multiple core dumps while a reset of CIM providers

During a reset of some CIM providers, the sfcb service might make a shutdown call to close child processes that are already closed. As a result, the sfcb service

fails and you see multiple core dumps during the reset operation.

This issue is resolved in this release. Fix enhances tracking of sfcb service child processes during shutdown.

PR 2765145: vSphere Client might not meet all current browser security standards

In certain environments, vSphere Client might not fully protect the underlying websites through modern browser security mechanisms.

This issue is resolved in this release.

PR 2704412: You do not see vCenter Server alerts in the vSphere Client and the vSphere Web Client

If you use the /bin/services.sh restart command to restart vCenter Server management services, the vobd daemon, which is responsible for sending ESXi host

events to vCenter Server, might not restart. As a result, you do not see alerts in the vSphere Client and the vSphere Web Client.

This issue is resolved in this release. The fix makes sure than the vobd daemon does not shut down when using the /bin/services.sh restart.

PR 2731316: Virtual machines become unresponsive after pass through a USB device

Some devices, such as USB, might add invalid bytes to the configuration descriptor that ESXi reads and cause virtual machines that pass through such devices to

become unresponsive.

This issue is resolved in this release.

ESXi650-202110402-BG

Patch Category

Patch Severity

Host Reboot Required

Virtual Machine Migration or
Shutdown Required

Affected Hardware

Affected Software

VIBs Included

PRs Fixed

Related CVE numbers

Bugfix

Important

Yes

Yes

N/A

N/A

VMW_bootbank_vmkusb_0.1-1vmw.650.3.170.18678235

2688189

N/A

This patch updates the vmkusb VIB to resolve the following issue:

PR 2688189: Guest OS of virtual machines cannot recognize some USB passthrough devices

In rare cases, the guest OS of virtual machines might not recognize some USB passthrough devices, such as ExcelSecu USB devices, and you cannot use the

VMs.

This issue is resolved in this release. The fix makes sure that USB control transfer larger than 1KB are not split by using link transfer ring blocks (TRB), therefore

the subsequent data stage TRB does not fail.

ESXi650-202110403-BG

Patch Category

Patch Severity

Host Reboot Required

Virtual Machine Migration or
Shutdown Required

Affected Hardware

Affected Software

VIBs Included

PRs Fixed

Related CVE numbers

Bugfix

Important

Yes

Yes

N/A

N/A

VMW_bootbank_brcmfcoe_11.4.1078.26-14vmw.650.3.170.18678235

2775374

N/A

This patch updates the brcmfcoe VIB to resolve the following issue:

PR 2775374: ESXi hosts might lose connectivity after brcmfcoe driver upgrade on Hitachi storage arrays

After an upgrade of the brcmfcoe driver on Hitachi storage arrays, ESXi hosts might fail to boot and lose connectivity.

This issue is resolved in this release.

ESXi650-202110404-BG

Patch Category

Patch Severity

Host Reboot Required

Virtual Machine Migration or
Shutdown Required

Affected Hardware

Affected Software

VIBs Included

PRs Fixed

Related CVE numbers

Bugfix

Important

Yes

Yes

N/A

N/A

VMW_bootbank_nvme_1.2.2.28-5vmw.650.3.170.18678235

2805346

N/A

This patch updates the nvme VIB to resolve the following issue:

PR 2805346: ESXi hosts with a NVMe device might fail with a purple diagnostic screen while exporting a log bundle

When the ESXi NVMe driver receives an esxcli nvme device register get request, it copies chunks of 8 bytes at a time from NVMe controller registers to user

space. The widths of NVMe controller registers can be 4 bytes or 8 bytes. When drivers access 8 bytes on a 4-byte register, some NVMe devices might report

critical errors, which cause ESXi hosts to fail with a purple diagnostic screen. Since the vm-support command automatically calls esxcli nvme device register get on

NVMe controllers to dump device information, the issue might also occur while collecting vm-support log bundle.

This issue is resolved in this release.

ESXi650-202110101-SG

Patch Category

Patch Severity

Host Reboot Required

Virtual Machine Migration or
Shutdown Required

Affected Hardware

Affected Software

Security

Important

Yes

Yes

N/A

N/A

VIBs Included

PRs Fixed

VMware_bootbank_esx-base_6.5.0-3.166.18677441
VMware_bootbank_vsanhealth_6.5.0-3.166.18596722
VMware_bootbank_vsan_6.5.0-3.166.18596718
VMware_bootbank_esx-tboot_6.5.0-3.166.18677441

2761972, 2765145, 2801745, 2801963, 2819291, 2819327, 2819329, 2819366,
2821179, 2821577, 2825672, 2825674, 2831766

Related CVE numbers

CVE-2018-6485, CVE-2021-35942, CVE-2018-11236, CVE-2019-9169, CVE-
2017-1000366, CVE-2016-10196, CVE-2016-10197

This patch esx-base, esx-tboot, vsan and vsanhealth VIBs to resolve the following issues:

The ESXi userworld libcurl library is updated to version 7.78.0.

The glibc package is updated to address CVE-2018-6485, CVE-2021-35942, CVE-2018-11236, CVE-2019-9169, CVE-2017-1000366.

The ESXi userworld OpenSSL library is updated to version openssl- 1.0.2za.

The jansson library is updated to version 2.10.

The libarchive library is updated to version 3.3.1.

The Libxml2 library is updated to version 2.9.12.

The Sqlite library is updated to version 3.34.1.

The Libevent library is updated to address CVE-2016-10196 and CVE-2016-10197.

ESXi650-202110102-SG

Patch Category

Patch Severity

Host Reboot Required

Virtual Machine Migration or
Shutdown Required

Affected Hardware

Affected Software

VIBs Included

PRs Fixed

Related CVE numbers

This patch updates the vmkusb  VIB.

ESXi650-202110103-SG

Patch Category

Patch Severity

Host Reboot Required

Virtual Machine Migration or
Shutdown Required

Affected Hardware

Affected Software

VIBs Included

PRs Fixed

Related CVE numbers

This patch updates the tools-light  VIB.

Security

Important

Yes

Yes

N/A

N/A

N/A

N/A

VMW_bootbank_vmkusb_0.1-1vmw.650.3.166.18677441

Security

Important

No

No

N/A

N/A

VMware_locker_tools-light_6.5.0-3.166.18677441

2785533

N/A

The following VMware Tools ISO images are bundled with ESXi 650-202110001:

windows.iso: VMware Tools 11.3.0 supports Windows 7 SP1 or Windows Server 2008 R2 SP1 and later.

linux.iso: VMware Tools 10.3.23 ISO image for Linux OS with glibc 2.11 or later.

The following VMware Tools ISO images are available for download:

VMware Tools 10.0.12:

winPreVista.iso: for Windows 2000, Windows XP, and Windows 2003.

linuxPreGLibc25.iso: for Linux OS with a glibc version less than 2.5.

VMware Tools 11.0.6:

windows.iso: for Windows Vista (SP2) and Windows Server 2008 Service Pack 2 (SP2).

solaris.iso: VMware Tools image 10.3.10 for Solaris.

darwin.iso: Supports Mac OS X versions 10.11 and later.

Follow the procedures listed in the following documents to download VMware Tools for platforms not bundled with ESXi:

VMware Tools 11.3.0 Release Notes

Earlier versions of VMware Tools

What Every vSphere Admin Must Know About VMware Tools

VMware Tools for hosts provisioned with Auto Deploy

Updating VMware Tools

ESXi-6.5.0-20211004001-standard

Profile Name

ESXi-6.5.0-20211004001-standard

Build

Vendor

Release Date

Acceptance Level

Affected Hardware

Affected Software

Affected VIBs

PRs Fixed

For build information, see the top of the page.

VMware, Inc.

October 12, 2021

PartnerSupported

N/A

N/A

VMware_bootbank_esx-base_6.5.0-3.170.18678235
VMware_bootbank_vsan_6.5.0-3.170.18569621
VMware_bootbank_vsanhealth_6.5.0-3.170.18569625
VMware_bootbank_esx-tboot_6.5.0-3.170.18678235
VMW_bootbank_vmkusb_0.1-1vmw.650.3.170.18678235
VMW_bootbank_brcmfcoe_11.4.1078.26-14vmw.650.3.170.18678235
VMW_bootbank_nvme_1.2.2.28-5vmw.650.3.170.18678235
VMware_locker_tools-light_6.5.0-3.166.18677441

2761135, 2685552, 2720188, 2793135, 2761972, 2755231, 2492991, 2710163,
2715132, 2701240, 2750814, 2635091, 2540214, 2749440, 2719215, 2603461,
2654781, 2603445, 2418383, 2796947, 2704429, 2765145, 2704412, 2731316,
2688189, 2465245, 2775374, 2805346

Related CVE numbers

N/A

This patch resolves the following issues:

If you import a VM with a virtual USB device that is not supported by vCenter Server, such as a virtual USB camera, to a vCenter Server system, the VM might

fail to power on. The start of such virtual machines fails with an error message similar to: PANIC: Unexpected signal: 11. The issue affects mostly VM import

operations from VMware Fusion or VMware Workstation systems, which support a wide range of virtual USB devices.

In some configurations, if you use block sizes higher than the supported max transfer length of the storage device, you might see a drop in the I/O bandwidth.

The issue occurs due to buffer allocations in the I/O split layer in the storage stack that cause a lock contention.

A rare error condition in the VMKernel might cause ESXi hosts to fail when powering on a virtual machine with more than 1 virtual CPU.

A BSVMM_Validate@vmkernel error is indicative for the problem.

In rare cases, an empty or unset property of a VIM API data array might cause the hostd service to fail. As a result, ESXi hosts lose connectivity to vCenter

Server and you must manually reconnect the hosts.

An issue with the TPM 1.2 character device might cause ESXi hosts to fail with a page fault (#PF) purple diagnostic screen.

In case of temporary connectivity issues, ESXi hosts might not discover devices configured with the VMW_SATP_INV plug-in after connectivity restores, because

SCSI commands that fail during the device discovery stage cause an out-of-memory condition for the plug-in.

Due to an issue with clearing some profile variables, you might see multiple duplicate logs in the /var/log/vmware/vsan-health/vmware-vsan-health-service.log file.

Such messages are very short and do not cause significant memory consumption.

This fix sets Storage Array Type Plug-in (SATP) to VMW_SATP_ALUA, Path Selection Policy (PSP) to VMW_PSP_RR and Claim Options to tpgs_on as default for the

Fujitsu Eternus AB/HB series.

If you try to collect performance statistics by using the rvc.bat file in a vCenter Server system of version later than 6.5 Update 3k, the operation fails due to a

wrong library path.

During an NFS server failover, the NFS client reclaims all open files. In rare cases, the reclaim operation fails and virtual machines power off, because the NFS

server rejects failed requests.

In rare cases, some Intel CPUs might fail to forward #DB traps and if a timer interrupt happens during the Windows system call, virtual machine might triple

fault. In the vmware.log file, you see an error such as msg.monitorEvent.tripleFault. This issue is not consistent and virtual machines that consistently triple fault

or triple fault after boot are impacted by another issue.

A checksum validation issue with newly created memory regions might cause ESXi hosts fail with a purple diagnostic screen. In the backtrace, you see errors

such as VMKernel: checksum BAD: 0xXXXX 0xXXXX . The issue is specific to the process of VMkernel module loading.

Resource allocation for a delta disk to create a snapshot of a large virtual machine, with virtual disks equal to or exceeding 1TB, on large VMFS6 datastores of

30TB or more, might take significant time. As a result, the virtual machine might temporarily lose connectivity. The issue affects primarily VMFS6 filesystems.

If your environment has IPv6 disabled, the ESXi SNMP agent might stop responding. As a result, ESXi hosts also become unresponsive. In the VMkernel logs,

you see multiple messages such as Admission failure in path: snmpd/snmpd.3955682/uw.3955682.

After upgrading the firmware version on HP Gen10 servers, you might see health alarms for the I/O Module 2 ALOM_Link_P2 and NIC_Link_02P2 sensors, related to

the sensor entity ID 44.x. The alarms do not indicate an actual health issue and you can ignore them irrespective of the firmware version.

The vpxa service fails in case of a non-UTF8 string in the name property of numeric sensors, and ESXi hosts disconnect from the vCenter Server system.

If you use the advanced config option UserVars/HardwareHealthIgnoredSensors to ignore sensors with consecutive entries in a numeric list, such as 0.52 and 0.53,

the operation might fail to ignore some sensors. For example, if you run the command esxcfg-advcfg -s 52,53 /UserVars/HardwareHealthIgnoredSensors, only

the sensor 0.53 might be ignored.

If the getBaseUnitString function returns a non-UTF8 string for the max value of a unit description array, the hostd service fails with a core dump. You see an

error such as:  [Hostd-zdump] 0x00000083fb085957 in Vmacore::PanicExit (msg=msg@entry=0x840b57eee0 "Validation failure")

After upgrading to ESXi 6.5 Update 3, you might see Sensor -1 type hardware health alarms on ESXi hosts being triggered without an actual problem. This can

result in excessive email alerts if you have configured email notifications for hardware sensor state alarms in your vCenter Server system. These mails might

cause storage issues in the vCenter Server database if the Stats, Events, Alarms and Tasks (SEAT) directory goes above the 95% threshold.

When the SLP service is disabled to prevent potential security vulnerabilities, the sfcbd-watchdog service might remain enabled and cause compliance check

failures when you perform updates by using a host profile.

During a reset of some CIM providers, the sfcb service might make a shutdown call to close child processes that are already closed. As a result, the sfcb

service fails and you see multiple core dumps during the reset operation.

In certain environments, vSphere Client might not fully protect the underlying websites through modern browser security mechanisms.

If you use the /bin/services.sh restart command to restart vCenter Server management services, the vobd daemon, which is responsible for sending ESXi host

events to vCenter Server, might not restart. As a result, you do not see alerts in the vSphere Client and the vSphere Web Client.

Some devices, such as USB, might add invalid bytes to the configuration descriptor that ESXi reads and cause virtual machines that pass through such devices

to become unresponsive.

In rare cases, the guest OS of virtual machines might not recognize some USB passthrough devices, such as ExcelSecu USB devices, and you cannot use the

VMs.

In certain environments, ESXi upgrades might fail due to all-paths-down error of the vmkusb driver that prevents mounting images from external devices while

running scripted installations. If you try to use a legacy driver, the paths might display as active but the script still does not run.

After an upgrade of the brcmfcoe driver on Hitachi storage arrays, ESXi hosts might fail to boot and lose connectivity.

When the ESXi NVMe driver receives an esxcli nvme device register get request, it copies chunks of 8 bytes at a time from NVMe controller registers to user

space. The widths of NVMe controller registers can be 4 bytes or 8 bytes. When drivers access 8 bytes on a 4-byte register, some NVMe devices might report

critical errors, which cause ESXi hosts to fail with a purple diagnostic screen. Since the vm-support command automatically calls esxcli nvme device register

get on NVMe controllers to dump device information, the issue might also occur while collecting vm-support log bundle.

ESXi-6.5.0-20210704001-no-tools

Profile Name

ESXi-6.5.0-20211004001-no-tools

Build

Vendor

Release Date

Acceptance Level

Affected Hardware

Affected Software

Affected VIBs

PRs Fixed

For build information, see the top of the page.

VMware, Inc.

October 12, 2021

PartnerSupported

N/A

N/A

VMware_bootbank_esx-base_6.5.0-3.170.18678235
VMware_bootbank_vsan_6.5.0-3.170.18569621
VMware_bootbank_vsanhealth_6.5.0-3.170.18569625
VMware_bootbank_esx-tboot_6.5.0-3.170.18678235
VMW_bootbank_vmkusb_0.1-1vmw.650.3.170.18678235
VMW_bootbank_brcmfcoe_11.4.1078.26-14vmw.650.3.170.18678235
VMW_bootbank_nvme_1.2.2.28-5vmw.650.3.170.18678235

2761135, 2685552, 2720188, 2793135, 2761972, 2755231, 2492991, 2710163,
2715132, 2701240, 2750814, 2635091, 2540214, 2749440, 2719215, 2603461,
2654781, 2603445, 2418383, 2796947, 2704429, 2765145, 2704412, 2731316,
2688189, 2465245, 2775374, 2805346

Related CVE numbers

N/A

This patch resolves the following issues:

If you import a VM with a virtual USB device that is not supported by vCenter Server, such as a virtual USB camera, to a vCenter Server system, the VM might

fail to power on. The start of such virtual machines fails with an error message similar to: PANIC: Unexpected signal: 11. The issue affects mostly VM import

operations from VMware Fusion or VMware Workstation systems, which support a wide range of virtual USB devices.

In some configurations, if you use block sizes higher than the supported max transfer length of the storage device, you might see a drop in the I/O bandwidth.

The issue occurs due to buffer allocations in the I/O split layer in the storage stack that cause a lock contention.

A rare error condition in the VMKernel might cause ESXi hosts to fail when powering on a virtual machine with more than 1 virtual CPU.

A BSVMM_Validate@vmkernel error is indicative for the problem.

In rare cases, an empty or unset property of a VIM API data array might cause the hostd service to fail. As a result, ESXi hosts lose connectivity to vCenter

Server and you must manually reconnect the hosts.

An issue with the TPM 1.2 character device might cause ESXi hosts to fail with a page fault (#PF) purple diagnostic screen.

In case of temporary connectivity issues, ESXi hosts might not discover devices configured with the VMW_SATP_INV plug-in after connectivity restores, because

SCSI commands that fail during the device discovery stage cause an out-of-memory condition for the plug-in.

Due to an issue with clearing some profile variables, you might see multiple duplicate logs in the /var/log/vmware/vsan-health/vmware-vsan-health-service.log file.

Such messages are very short and do not cause significant memory consumption.

This fix sets Storage Array Type Plug-in (SATP) to VMW_SATP_ALUA, Path Selection Policy (PSP) to VMW_PSP_RR and Claim Options to tpgs_on as default for the

Fujitsu Eternus AB/HB series.

If you try to collect performance statistics by using the rvc.bat file in a vCenter Server system of version later than 6.5 Update 3k, the operation fails due to a

wrong library path.

During an NFS server failover, the NFS client reclaims all open files. In rare cases, the reclaim operation fails and virtual machines power off, because the NFS

server rejects failed requests.

In rare cases, some Intel CPUs might fail to forward #DB traps and if a timer interrupt happens during the Windows system call, virtual machine might triple

fault. In the vmware.log file, you see an error such as msg.monitorEvent.tripleFault. This issue is not consistent and virtual machines that consistently triple fault

or triple fault after boot are impacted by another issue.

A checksum validation issue with newly created memory regions might cause ESXi hosts fail with a purple diagnostic screen. In the backtrace, you see errors

such as VMKernel: checksum BAD: 0xXXXX 0xXXXX . The issue is specific to the process of VMkernel module loading.

Resource allocation for a delta disk to create a snapshot of a large virtual machine, with virtual disks equal to or exceeding 1TB, on large VMFS6 datastores of

30TB or more, might take significant time. As a result, the virtual machine might temporarily lose connectivity. The issue affects primarily VMFS6 filesystems.

If your environment has IPv6 disabled, the ESXi SNMP agent might stop responding. As a result, ESXi hosts also become unresponsive. In the VMkernel logs,

you see multiple messages such as Admission failure in path: snmpd/snmpd.3955682/uw.3955682.

After upgrading the firmware version on HP Gen10 servers, you might see health alarms for the I/O Module 2 ALOM_Link_P2 and NIC_Link_02P2 sensors, related to

the sensor entity ID 44.x. The alarms do not indicate an actual health issue and you can ignore them irrespective of the firmware version.

The vpxa service fails in case of a non-UTF8 string in the name property of numeric sensors, and ESXi hosts disconnect from the vCenter Server system.

If you use the advanced config option UserVars/HardwareHealthIgnoredSensors to ignore sensors with consecutive entries in a numeric list, such as 0.52 and 0.53,

the operation might fail to ignore some sensors. For example, if you run the command esxcfg-advcfg -s 52,53 /UserVars/HardwareHealthIgnoredSensors, only

the sensor 0.53 might be ignored.

If the getBaseUnitString function returns a non-UTF8 string for the max value of a unit description array, the hostd service fails with a core dump. You see an

error such as:  [Hostd-zdump] 0x00000083fb085957 in Vmacore::PanicExit (msg=msg@entry=0x840b57eee0 "Validation failure")

After upgrading to ESXi 6.5 Update 3, you might see Sensor -1 type hardware health alarms on ESXi hosts being triggered without an actual problem. This can

result in excessive email alerts if you have configured email notifications for hardware sensor state alarms in your vCenter Server system. These mails might

cause storage issues in the vCenter Server database if the Stats, Events, Alarms and Tasks (SEAT) directory goes above the 95% threshold.

When the SLP service is disabled to prevent potential security vulnerabilities, the sfcbd-watchdog service might remain enabled and cause compliance check

failures when you perform updates by using a host profile.

During a reset of some CIM providers, the sfcb service might make a shutdown call to close child processes that are already closed. As a result, the sfcb

service fails and you see multiple core dumps during the reset operation.

In certain environments, vSphere Client might not fully protect the underlying websites through modern browser security mechanisms.

If you use the /bin/services.sh restart command to restart vCenter Server management services, the vobd daemon, which is responsible for sending ESXi host

events to vCenter Server, might not restart. As a result, you do not see alerts in the vSphere Client and the vSphere Web Client.

Some devices, such as USB, might add invalid bytes to the configuration descriptor that ESXi reads and cause virtual machines that pass through such devices

to become unresponsive.

In rare cases, the guest OS of virtual machines might not recognize some USB passthrough devices, such as ExcelSecu USB devices, and you cannot use the

VMs.

In certain environments, ESXi upgrades might fail due to all-paths-down error of the vmkusb driver that prevents mounting images from external devices while

running scripted installations. If you try to use a legacy driver, the paths might display as active but the script still does not run.

After an upgrade of the brcmfcoe driver on Hitachi storage arrays, ESXi hosts might fail to boot and lose connectivity.

When the ESXi NVMe driver receives an esxcli nvme device register get request, it copies chunks of 8 bytes at a time from NVMe controller registers to user

space. The widths of NVMe controller registers can be 4 bytes or 8 bytes. When drivers access 8 bytes on a 4-byte register, some NVMe devices might report

critical errors, which cause ESXi hosts to fail with a purple diagnostic screen. Since the vm-support command automatically calls esxcli nvme device register

get on NVMe controllers to dump device information, the issue might also occur while collecting vm-support log bundle.

ESXi-6.5.0-20211001001s-standard

Profile Name

ESXi-6.5.0-20211001001s-standard

Build

Vendor

Release Date

Acceptance Level

Affected Hardware

Affected Software

Affected VIBs

For build information, see the top of the page.

VMware, Inc.

October 12, 2021

PartnerSupported

N/A

N/A

VMware_bootbank_esx-base_6.5.0-3.166.18677441
VMware_bootbank_vsanhealth_6.5.0-3.166.18596722
VMware_bootbank_vsan_6.5.0-3.166.18596718
VMware_bootbank_esx-tboot_6.5.0-3.166.18677441
VMW_bootbank_vmkusb_0.1-1vmw.650.3.166.18677441
VMware_locker_tools-light_6.5.0-3.166.18677441

PRs Fixed

Related CVE numbers

2761972, 2765145, 2801745, 2801963, 2819291, 2819327, 2819329, 2819366,
2821179, 2821577, 2825672, 2825674, 2831766, 2785533

CVE-2018-6485, CVE-2021-35942, CVE-2018-11236, CVE-2019-9169, CVE-
2017-1000366, CVE-2016-10196, CVE-2016-10197

This patch resolves the following issues:

The ESXi userworld libcurl library is updated to version 7.78.0.

The glibc package is updated to address CVE-2018-6485, CVE-2021-35942, CVE-2018-11236, CVE-2019-9169, CVE-2017-1000366.

The ESXi userworld OpenSSL library is updated to version openssl- 1.0.2za.

The jansson library is updated to version 2.10.

The libarchive library is updated to version 3.3.1.

The Libxml2 library is updated to version 2.9.12.

The Sqlite library is updated to version 3.34.1.

The Libevent library is updated to address CVE-2016-10196 and CVE-2016-10197.

The following VMware Tools ISO images are bundled with ESXi 650-202110001:

windows.iso: VMware Tools 11.3.0 supports Windows 7 SP1 or Windows Server 2008 R2 SP1 and later.

linux.iso: VMware Tools 10.3.23 ISO image for Linux OS with glibc 2.11 or later.

The following VMware Tools ISO images are available for download:

VMware Tools 10.0.12:

winPreVista.iso: for Windows 2000, Windows XP, and Windows 2003.

linuxPreGLibc25.iso: for Linux OS with a glibc version less than 2.5.

VMware Tools 11.0.6:

windows.iso: for Windows Vista (SP2) and Windows Server 2008 Service Pack 2 (SP2).

solaris.iso: VMware Tools image 10.3.10 for Solaris.

darwin.iso: Supports Mac OS X versions 10.11 and later.

Follow the procedures listed in the following documents to download VMware Tools for platforms not bundled with ESXi:

VMware Tools 11.3.0 Release Notes

Earlier versions of VMware Tools

What Every vSphere Admin Must Know About VMware Tools

VMware Tools for hosts provisioned with Auto Deploy

Updating VMware Tools

ESXi-6.5.0-20211001001s-no-tools

Profile Name

ESXi-6.5.0-20211001001s-no-tools

Build

Vendor

Release Date

Acceptance Level

For build information, see the top of the page.

VMware, Inc.

October 12, 2021

PartnerSupported

Affected Hardware

N/A

Affected Software

N/A

Affected VIBs

PRs Fixed

Related CVE numbers

VMware_bootbank_esx-base_6.5.0-3.166.18677441
VMware_bootbank_vsanhealth_6.5.0-3.166.18596722
VMware_bootbank_vsan_6.5.0-3.166.18596718
VMware_bootbank_esx-tboot_6.5.0-3.166.18677441
VMW_bootbank_vmkusb_0.1-1vmw.650.3.166.18677441

2761972, 2765145, 2801745, 2801963, 2819291, 2819327, 2819329, 2819366,
2821179, 2821577, 2825672, 2825674, 2831766, 2785533

CVE-2018-6485, CVE-2021-35942, CVE-2018-11236, CVE-2019-9169, CVE-
2017-1000366, CVE-2016-10196, CVE-2016-10197

This patch resolves the following issues:

The ESXi userworld libcurl library is updated to version 7.78.0.

The glibc package is updated to address CVE-2018-6485, CVE-2021-35942, CVE-2018-11236, CVE-2019-9169, CVE-2017-1000366.

The ESXi userworld OpenSSL library is updated to version openssl- 1.0.2za.

The jansson library is updated to version 2.10.

The libarchive library is updated to version 3.3.1.

The Libxml2 library is updated to version 2.9.12.

The Sqlite library is updated to version 3.34.1.

The Libevent library is updated to address CVE-2016-10196 and CVE-2016-10197.

The following VMware Tools ISO images are bundled with ESXi 650-202110001:

windows.iso: VMware Tools 11.3.0 supports Windows 7 SP1 or Windows Server 2008 R2 SP1 and later.

linux.iso: VMware Tools 10.3.23 ISO image for Linux OS with glibc 2.11 or later.

The following VMware Tools ISO images are available for download:

VMware Tools 10.0.12:

winPreVista.iso: for Windows 2000, Windows XP, and Windows 2003.

linuxPreGLibc25.iso: for Linux OS with a glibc version less than 2.5.

VMware Tools 11.0.6:

windows.iso: for Windows Vista (SP2) and Windows Server 2008 Service Pack 2 (SP2).

solaris.iso: VMware Tools image 10.3.10 for Solaris.

darwin.iso: Supports Mac OS X versions 10.11 and later.

Follow the procedures listed in the following documents to download VMware Tools for platforms not bundled with ESXi:

VMware Tools 11.3.0 Release Notes

Earlier versions of VMware Tools

What Every vSphere Admin Must Know About VMware Tools

VMware Tools for hosts provisioned with Auto Deploy

Updating VMware Tools

Known Issues from Previous Releases

To view a list of previous known issues, click here.

Copyright © Broadcom


