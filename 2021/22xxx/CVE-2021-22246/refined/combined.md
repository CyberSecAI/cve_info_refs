=== Content from gitlab.com_15480492_20250115_094448.html ===


[Skip to content](#content-body)
GitLab
[Next](https://next.gitlab.com)

* Menu

  + [Why GitLab](https://about.gitlab.com/why-gitlab)
  + [Pricing](https://about.gitlab.com/pricing)
  + [Contact Sales](https://about.gitlab.com/sales)
  + [Explore](/explore)
* [Why GitLab](https://about.gitlab.com/why-gitlab)
* [Pricing](https://about.gitlab.com/pricing)
* [Contact Sales](https://about.gitlab.com/sales)
* [Explore](/explore)

* [Sign in](/users/sign_in?redirect_to_referer=yes)
* [Get free trial](/users/sign_up)

* [cves](/gitlab-org/cves/-/tree/master)
* [2021](/gitlab-org/cves/-/tree/master/2021)
* [**CVE-2021-22246.json**](/gitlab-org/cves/-/blob/master/2021/CVE-2021-22246.json)

Find file

[Blame](/gitlab-org/cves/-/blame/master/2021/CVE-2021-22246.json)
[Permalink](/gitlab-org/cves/-/blob/5dbfc725a86e6d942deec9c36b7f06f5b8d912fc/2021/CVE-2021-22246.json "Go to permalink <kbd class='flat ml-1' aria-hidden=true>y</kbd>")

* [![ GitLab Bot 's avatar](/uploads/-/system/user/avatar/1786152/avatar.png?width=64 " GitLab Bot ")](/gitlab-bot)

  Aug 19, 2021

  [5cd09edf](/gitlab-org/cves/-/commit/5cd09edf15324051f1d01870d62fa43c772f7368)
  [Publishing 0 updated advisories and 2 new advisories](/gitlab-org/cves/-/commit/5cd09edf15324051f1d01870d62fa43c772f7368)
  繚
  5cd09edf

  [ GitLab Bot ](/gitlab-bot) authored Aug 19, 2021

  5cd09edf

  [Publishing 0 updated advisories and 2 new advisories](/gitlab-org/cves/-/commit/5cd09edf15324051f1d01870d62fa43c772f7368)
  [ GitLab Bot ](/gitlab-bot) authored Aug 19, 2021

Loading



=== Content from gitlab.com_87aed561_20250115_094449.html ===


[Skip to content](#content-body)
GitLab
[Next](https://next.gitlab.com)

* Menu

  + [Why GitLab](https://about.gitlab.com/why-gitlab)
  + [Pricing](https://about.gitlab.com/pricing)
  + [Contact Sales](https://about.gitlab.com/sales)
  + [Explore](/explore)
* [Why GitLab](https://about.gitlab.com/why-gitlab)
* [Pricing](https://about.gitlab.com/pricing)
* [Contact Sales](https://about.gitlab.com/sales)
* [Explore](/explore)

* [Sign in](/users/sign_in?redirect_to_referer=yes)
* [Get free trial](/users/sign_up)

# Evil webhook forces connections to last forever, DoS

**[HackerOne report #1029269](https://hackerone.com/reports/1029269)** by `afewgoats` on 2020-11-08, assigned to [@dcouture](/dcouture "Dominic Couture"):

[Report](#report) | [Attachments](#attachments) | [How To Reproduce](#how-to-reproduce)

## Report

##### Summary

Gitlab Webhooks do not properly obey timeouts, so connections can be forced to last forever.

A malicious webhook receiver can respond in a way such that:

1. The HTTP(S) connection between the Gitlab sidekiq process and the webhook server NEVER ends
2. Large amounts of data are kept in memory
3. Giant webhook responses are stored potentially causing processes to be killed due to out-of-memory

It's not just project webhooks. Any use of `Gitlab::HTTP.post` such as system webhooks and integrations (e.g. Mattermost) are affected.

##### Steps to reproduce

A test webhook receiver is attached. It can be run with `node never-end.js` and serves on port 3000. If you want to use it, you may need to [enable local webhooks](https://docs.gitlab.com/ee/security/webhooks.html#allowlist-for-local-requests). Instead, you can use my server at <https://gitlab.example.com/never-end/> but **please redact all mention of my domain gitlab.example.com before publishing**.

First, create a project.

###### 1. Never ending request

Create a webhook for the project with URL pointing to: <https://gitlab.example.com/never-end/gitlab>

Trigger the webhook (e.g. by creating a new issue if that was selected as a trigger). Don't use the webhook Test button as it times out after 60 seconds, so isn't as badly affected.

The webhook server replies by sending 1 byte every 3.21 seconds. The read timeout isn't hit because data is still being transferred, albeit very slowly.

See that:

1. The sidekiq job never finishes (see /admin/background\_jobs and screenshot)
2. A TCP connection from the sidekiq process to the webhook server is maintained forever. This can be seen with netstat or tcpdump.

The webhook can be triggered multiple times to have multiple concurrent connections.

###### 2. Several hundred MB of data, then never ending request

Change webhook URL to: <https://gitlab.example.com/never-end/mega/gitlab>

Now trigger the webhook and monitor the memory used by the sidekiq process. It will increase dramatically and stay high forever.

The webhook server replies by sending ~530MB of data and then 1 byte every 3.21 seconds. The read timeout isn't hit because data is still being transferred, albeit very slowly.

The webhook can be triggered multiple times to have multiple concurrent connections and further increase the memory wastage.

###### 3. Several hundred MB of data, then request ends

This isn't timeout related, but more to do with storing large webhook responses.

Change webhook URL to: <https://gitlab.example.com/never-end/mega/exit/gitlab>

This time, the request will end successfully after a few seconds. The response size is ~530MB. It is stored in postgres in the webhook\_logs table.

Go to edit the webhook and scroll down to find the request in "Recent Deliveries". Click "View details" (link to hook\_logs) and it tries to show the full HTTP body. Puma worker is probably killed due to running out of memory.

By making several simultaneous requests to view the hook logs, my computer pretty much set on fire with all CPU cores at 100% so I had to force reboot it e.g.:

```
for i in `seq 1 15`; do curl &#39;https://GITLABSERVER/gitlab-instance-027a8c7d/monitoring/hooks/1/hook_logs/3&#39; \
  -H &#39;cookie: ...ENTER COOKIE...&#39; \
  --compressed \
  --insecure &gt; /dev/null &amp;; done
```

When I increased the response size of this endpoint to 666MB I got this error in postgres, but possibly just due to my setup:

```
ERROR:  invalid memory alloc request size 1073741824
STATEMENT:  INSERT INTO &#34;web_hook_logs&#34; (&#34;web_hook_id&#34;, &#34;trigger&#34;, &#34;url&#34;, &#34;request_headers&#34;, &#34;request_data&#34;, &#34;response_headers&#34;, &#34;response_body&#34;, &#34;response_status&#34;, &#34;execution_
duration&#34;, &#34;created_at&#34;, &#34;updated_at&#34;) VALUES (1, &#39;issue_hooks&#39;, &#39;http://192.168.4.20:3000/mega/exit&#39;, &#39;---
     Content-Type: application/json
     X-Gitlab-Event: Issue Hook
...
```

##### Examples

Most testing was done on my docker instance.

I added a webhook to <https://gitlab.com/afewgoatstest/mytestproject/> (not public) and launched a request: there is currently a never-ending connection from 34.74.90.72 to my server. I didn't try the /mega versions which send hundreds of MB on gitlab.com to avoid disrupting service.

##### What is the current *bug* behavior?

There is no overall timeout on the webhook connection except when using the Test Webhook function. (Also accepts huge response data)

##### What is the expected *correct* behavior?

After 10 seconds (or some other reasonable timeout), webhook requests timeout even if data is still being transferred.

##### Relevant logs and/or screenshots

Stack trace where the sidekiq process gets stuck:

```
lib/gitlab/http.rb:44:in `perform_request&#39;
app/services/web_hook_service.rb:87:in `make_request&#39;
app/services/web_hook_service.rb:39:in `execute&#39;
app/workers/web_hook_worker.rb:16:in `perform&#39;
lib/gitlab/metrics/sidekiq_middleware.rb:18:in `block in call&#39;
lib/gitlab/metrics/transaction.rb:61:in `run&#39;
lib/gitlab/metrics/sidekiq_middleware.rb:18:in `call&#39;
lib/gitlab/sidekiq_middleware/duplicate_jobs/strategies/until_executing.rb:32:in `perform&#39;
lib/gitlab/sidekiq_middleware/duplicate_jobs/duplicate_job.rb:40:in `perform&#39;
lib/gitlab/sidekiq_middleware/duplicate_jobs/server.rb:8:in `call&#39;
lib/gitlab/sidekiq_middleware/worker_context.rb:9:in `wrap_in_optional_context&#39;
lib/gitlab/sidekiq_middleware/worker_context/server.rb:17:in `block in call&#39;
lib/gitlab/application_context.rb:54:in `block in use&#39;
lib/gitlab/application_context.rb:54:in `use&#39;
lib/gitlab/application_context.rb:21:in `with_context&#39;
lib/gitlab/sidekiq_middleware/worker_context/server.rb:15:in `call&#39;
lib/gitlab/sidekiq_status/server_middleware.rb:7:in `call&#39;
lib/gitlab/sidekiq_versioning/middleware.rb:9:in `call&#39;
lib/gitlab/sidekiq_middleware/admin_mode/server.rb:8:in `call&#39;
lib/gitlab/sidekiq_middleware/instrumentation_logger.rb:7:in `call&#39;
lib/gitlab/sidekiq_middleware/batch_loader.rb:7:in `call&#39;
lib/gitlab/sidekiq_middleware/extra_done_log_metadata.rb:7:in `call&#39;
lib/gitlab/sidekiq_middleware/request_store_middleware.rb:10:in `block in call&#39;
lib/gitlab/with_request_store.rb:17:in `enabling_request_store&#39;
lib/gitlab/with_request_store.rb:10:in `with_request_store&#39;
lib/gitlab/sidekiq_middleware/request_store_middleware.rb:9:in `call&#39;
lib/gitlab/sidekiq_middleware/server_metrics.rb:35:in `call&#39;
lib/gitlab/sidekiq_middleware/monitor.rb:8:in `block in call&#39;
lib/gitlab/sidekiq_daemon/monitor.rb:49:in `within_job&#39;
lib/gitlab/sidekiq_middleware/monitor.rb:7:in `call&#39;
lib/gitlab/sidekiq_logging/structured_logger.rb:18:in `call&#39;
```

`perform_request` calls HTTParty which is probably where the issue is.

The Mattermost integration stack trace is similar:

```
lib/gitlab/http.rb:44:in `perform_request&#39;
app/models/project_services/slack_service.rb:55:in `post&#39;
app/models/project_services/slack_service.rb:45:in `notify&#39;
app/models/project_services/chat_notification_service.rb:94:in `execute&#39;
lib/gitlab/metrics/instrumentation.rb:160:in `block in execute&#39;
lib/gitlab/metrics/method_call.rb:27:in `measure&#39;
lib/gitlab/metrics/instrumentation.rb:160:in `execute&#39;
app/workers/project_service_worker.rb:13:in `perform&#39;
lib/gitlab/metrics/sidekiq_middleware.rb:18:in `block in call&#39;
...
```

##### Output of checks

This bug happens on GitLab.com and in docker.

###### Results of GitLab environment info

Using gitlab-ee docker image

```
System information
System:
Proxy:          no
Current User:   git
Using RVM:      no
Ruby Version:   2.6.6p146
Gem Version:    2.7.10
Bundler Version:1.17.3
Rake Version:   12.3.3
Redis Version:  5.0.9
Git Version:    2.28.0
Sidekiq Version:5.2.9
Go Version:     unknown

GitLab information
Version:        13.5.3-ee
Revision:       b9d194b6b91
Directory:      /opt/gitlab/embedded/service/gitlab-rails
DB Adapter:     PostgreSQL
DB Version:     11.9
URL:            https://gitlab.example.com
HTTP Clone URL: https://gitlab.example.com/some-group/some-project.git
SSH Clone URL:  git@gitlab.example.com:some-group/some-project.git
Elasticsearch:  no
Geo:            no
Using LDAP:     no
Using Omniauth: yes
Omniauth Providers:

GitLab Shell
Version:        13.11.0
Repository storage paths:
- default:      /var/opt/gitlab/git-data/repositories
GitLab Shell path:              /opt/gitlab/embedded/service/gitlab-shell
Git:            /opt/gitlab/embedded/bin/git
```

#### Impact

Denial of service.

Can keep never-ending HTTP(S) connections open, deplete connection pool, reserve memory, slow things down, kill process due to out-of-memory, maybe kill server.

The attacker can be either:

1. Someone who can add webhooks to a project (or system webhook or integration)
2. The host of a server which has a webhook pointed to it

## Attachments

**Warning:** Attachments received through HackerOne, please exercise caution!

* [Screenshot\_from\_2020-11-07\_21-29-32.png](https://h1.sec.gitlab.net/a/c240f541-1aed-45a0-964c-a9adc394b8b5/Screenshot_from_2020-11-07_21-29-32.png)
* [Screenshot\_from\_2020-11-07\_21-36-51.png](https://h1.sec.gitlab.net/a/3511bf95-d3b8-498f-9224-6d17b9902e0a/Screenshot_from_2020-11-07_21-36-51.png)
* [never-end.js](https://h1.sec.gitlab.net/a/bd21edae-5d2f-4647-8b47-f0073befb0df/never-end.js)

## How To Reproduce

Please add [reproducibility information](https://about.gitlab.com/handbook/engineering/security/#reproducibility-on-security-issues) to this section:

1. Host the `never-end.js` script on a server
2. Create a webhook that sends a request to `http://server:3000/`, or `http://server:3000/mega` or `http://server:3000/mega/exit` depending on the scenario you're testing
3. Observe sidekiq jobs stuck forever and resource consumption

Edited Feb 23, 2021 by [Dominic Couture](/dcouture)

Assignee
Loading

Time tracking
Loading

Confidentiality

Confidentiality controls have moved to the issue actions menu () at the top of the page.


