=== Content from docs.vmware.com_edd5010b_20250115_175008.html ===
VMware ESXi 7.0 Update 3c Release Notes

ESXi 7.0 Update 3c | 27 JAN 2022  | ISO Build 19193900

Check for additions and updates to these release notes.

What's in the Release Notes

The release notes cover the following topics:

What's New

Earlier Releases of ESXi 7.0

Patches Contained in this Release

Product Support Notices

Resolved Issues

Known Issues

Known Issues from Previous Releases

IMPORTANT: VMware removed ESXi 7.0 Update 3, 7.0 Update 3a and 7.0 Update 3b from all sites on November 19, 2021 due to an upgrade-impacting issue. Build

19193900 for ESXi 7.0 Update 3c ISO replaces build 18644231, 18825058, and 18905247 for ESXi 7.0 Update 3, 7.0 Update 3a, and 7.0 Update 3b respectively. To make

sure you run a smooth upgrade to vSphere 7.0 Update 3c, see VMware knowledge base articles 86447 and 87327.

What's New

If your source system contains the ESXi 7.0 Update 2 release (build number 17630552) or later builds with Intel drivers, before upgrading to ESXi 7.0 Update 3c,

see the Resolved Issues and Known Issues sections, and the VMware vCenter Server 7.0 Update 3c Release Notes.

ESXi 7.0 Update 3c delivers bug and security fixes documented in the Resolved Issues section, and VMware knowledge base articles 86255, 86158,

85982, 86283, and 86100.

For new features in the rolled back releases, see this list:

vSphere Memory Monitoring and Remediation, and support for snapshots of PMem VMs: vSphere Memory Monitoring and Remediation collects data and

provides visibility of performance statistics to help you determine if your application workload is regressed due to Memory Mode. vSphere 7.0 Update 3 also

adds support for snapshots of PMem VMs. For more information, see vSphere Memory Monitoring and Remediation.

Extended support for disk drives types: Starting with vSphere 7.0 Update 3, vSphere Lifecycle Manager validates the following types of disk drives and storage

device configurations:

• HDD (SAS/SATA)

• SSD (SAS/SATA)

• SAS/SATA disk drives behind single-disk RAID-0 logical volumes

For more information, see Cluster-Level Hardware Compatibility Checks.

Use vSphere Lifecycle Manager images to manage a vSAN stretched cluster and its witness host: Starting with vSphere 7.0 Update 3, you can use vSphere

Lifecycle Manager images to manage a vSAN stretched cluster and its witness host. For more information, see Using vSphere Lifecycle Manager Images to

Remediate vSAN Stretched Clusters.

vSphere Cluster Services (vCLS) enhancements: With vSphere 7.0 Update 3, vSphere admins can configure vCLS virtual machines to run on specific datastores

by configuring the vCLS VM datastore preference per cluster. Admins can also define compute policies to specify how the vSphere Distributed Resource

Scheduler (DRS) should place vCLS agent virtual machines (vCLS VMs) and other groups of workload VMs.

Improved interoperability between vCenter Server and ESXi versions: Starting with vSphere 7.0 Update 3, vCenter Server can manage ESXi hosts from the

previous two major releases and any ESXi host from version 7.0 and 7.0 updates. For example, vCenter Server 7.0 Update 3 can manage ESXi hosts of versions

6.5, 6.7 and 7.0, all 7.0 update releases, including later than Update 3, and a mixture of hosts between major and update versions.

New VMNIC tag for NVMe-over-RDMA storage traffic: ESXi 7.0 Update 3 adds a new VMNIC tag for NVMe-over-RDMA storage traffic. This VMkernel port

setting enables NVMe-over-RDMA traffic to be routed over the tagged interface. You can also use the ESXCLI command esxcli network ip interface tag add -i

<interface name> -t NVMeRDMA to enable the NVMeRDMA VMNIC tag.

NVMe over TCP support: vSphere 7.0 Update 3 extends the NVMe-oF suite with the NVMe over TCP storage protocol to enable high performance and

parallelism of NVMe devices over a wide deployment of TCP/IP networks.

Zero downtime, zero data loss for mission critical VMs in case of Machine Check Exception (MCE) hardware failure: With vSphere 7.0 Update 3, mission critical

VMs protected by VMware vSphere Fault Tolerance can achieve zero downtime, zero data loss in case of Machine Check Exception (MCE) hardware failure,

because VMs fallback to the secondary VM, instead of failing. For more information, see How Fault Tolerance Works.

Micro-second level time accuracy for workloads: ESXi 7.0 Update 3 adds the hardware timestamp Precision Time Protocol (PTP) to enable micro-second level

time accuracy. For more information, see Use PTP for Time and Date Synchronization of a Host.

Improved ESXi host timekeeping configuration:  ESXi 7.0 Update 3 enhances the workflow and user experience for setting an ESXi host timekeeping

configuration. For more information, see Editing the Time Configuration Settings of a Host.

Earlier Releases of ESXi 7.0

New features, resolved, and known issues of ESXi are described in the release notes for each release. Release notes for earlier releases of ESXi 7.0 are:

VMware ESXi 7.0, ESXi 7.0 Update 2d Release Notes

VMware ESXi 7.0, ESXi 7.0 Update 2c Release Notes

VMware ESXi 7.0, ESXi 7.0 Update 2a Release Notes

VMware ESXi 7.0, ESXi 7.0 Update 2 Release Notes

VMware ESXi 7.0, ESXi 7.0 Update 1d Release Notes

VMware ESXi 7.0, ESXi 7.0 Update 1c Release Notes

VMware ESXi 7.0, ESXi 7.0 Update 1b Release Notes

VMware ESXi 7.0, ESXi 7.0 Update 1a Release Notes

VMware ESXi 7.0, ESXi 7.0 Update 1 Release Notes

VMware ESXi 7.0, ESXi 7.0b Release Notes

For internationalization, compatibility, and open source components, see the VMware vSphere 7.0 Release Notes.

Patches Contained in This Release

This release of ESXi 7.0 Update 3c delivers the following patches:

Build Details

Download Filename:

VMware-ESXi-7.0U3c-19193900-depot

Build:

Download Size:

md5sum:

sha256checksum:

19193900

395.8 MB

e39a951f4e96e92eae41c94947e046ec

20cdcd6fd8f22f5f8a848b45db67316a3ee630b31a152312f4beab737f2b3cdc

Host Reboot Required:

Virtual Machine Migration or
Shutdown Required:

Yes

Yes

For a table of build numbers and versions of VMware ESXi, see VMware knowledge base article 2143832.

IMPORTANT:

Starting with vSphere 7.0, VMware uses components for packaging VIBs along with bulletins. The ESXi and esx-update bulletins depend on each other. Always

include both in a single ESXi host patch baseline or include the rollup bulletin in the baseline to avoid failure during host patching.

When patching ESXi hosts by using VMware Update Manager from a version prior to ESXi 7.0 Update 2, it is strongly recommended to use the rollup bulletin in

the patch baseline. If you cannot use the rollup bulletin, be sure to include all of the following packages in the patching baseline. If the following packages are not

included in the baseline, the update operation fails:

VMware-vmkusb_0.1-1vmw.701.0.0.16850804 or higher

VMware-vmkata_0.1-1vmw.701.0.0.16850804 or higher

VMware-vmkfcoe_1.0.0.2-1vmw.701.0.0.16850804 or higher

VMware-NVMeoF-RDMA_1.0.1.2-1vmw.701.0.0.16850804 or higher

Rollup Bulletin

This rollup bulletin contains the latest VIBs with all the fixes after the initial release of ESXi 7.0.

Bulletin ID

ESXi70U3c-19193900

Image Profiles

Category

Bugfix

Severity

Critical

VMware patch and update releases contain general and critical image profiles. Application of the general release image profile applies to new bug fixes.

Image Profile Name

ESXi70U3c-19193900-standard

ESXi70U3c-19193900-no-tools

ESXi Image

Name and Version

ESXi70U3c-19193900

Release Date

27 JAN 2022

Category

Detail

Bugfix

Bugfix image

For information about the individual components and bulletins, see the Product Patches page and the Resolved Issues section.

Patch Download and Installation

In vSphere 7.x, the Update Manager plug-in, used for administering vSphere Update Manager, is replaced with the Lifecycle Manager plug-in. Administrative operations

for vSphere Update Manager are still available under the Lifecycle Manager plug-in, along with new capabilities for vSphere Lifecycle Manager.

The typical way to apply patches to ESXi 7.x hosts is by using the vSphere Lifecycle Manager. For details, see About vSphere Lifecycle Manager and vSphere Lifecycle

Manager Baselines and Images.

You can also update ESXi hosts without using the Lifecycle Manager plug-in, and use an image profile instead. To do this, you must manually download the patch

offline bundle ZIP file after you log in to VMware Customer Connect. From the Select a Product drop-down menu, select ESXi (Embedded and Installable) and from

the Select a Version drop-down menu, select 7.0. For more information, see the Upgrading Hosts by Using ESXCLI Commands and the VMware ESXi Upgrade guide.

Product Support Notices

Deprecation of localos accounts: Support for use of localos accounts as an identity source is deprecated. VMware plans to discontinue support for use of the

local operating system as an identity source. This functionality will be removed in a future release of vSphere.

The cURL version in ESXi650-202110001 and ESXi670-202111001 is later than the cURL version in ESXi 7.0 Update 3c: The cURL version in ESXi 7.0 Update

3c is 7.77.0, while ESXi650-202110001 and ESXi670-202111001 have the newer fixed version 7.78.0. As a result, if you upgrade from ESXi650-202110001 or

ESXi670-202111001 to ESXi 7.0 Update 3c, cURL 7.7.0 might expose your system to the following vulnerabilities:

CVE-2021-22926: CVSS 7.5

CVE-2021-22925: CVSS 5.3

CVE-2021-22924: CVSS 3.7

CVE-2021-22923: CVSS 5.3

CVE-2021-22922: CVSS 6.5

cURL version 7.78.0 comes with a future ESXi 7.x release.

Merging the lpfc and brcmnvmefc drivers: Starting with vSphere 7.0 Update 3c, the brcmnvmefc driver is no longer available. The NVMe over Fibre Channel

functionality previously delivered with the brcmnvmefc driver is now included in the lpfc driver.

Deprecation of RDMA over Converged Ethernet (RoCE) v1: VMware intends in a future major vSphere release to discontinue support for the network protocol

RoCE v1. You must migrate drivers that rely on the RoCEv1 protocol to RoCEv2. In addition, you must migrate paravirtualized remote direct memory access

(PVRDMA) network adapters for virtual machines and guest operating systems to an adapter that supports RoCEv2.

Deprecation of SD and USB devices for the ESX-OSData partition: The use of SD and USB devices for storing the ESX-OSData partition, which consolidates the

legacy scratch partition, locker partition for VMware Tools, and core dump destinations, is being deprecated. SD and USB devices are supported for boot bank

partitions. For warnings related to the use of SD and USB devices during ESXi 7.0 Update 3c update or installation, see VMware Knowledge Based Article 85615.

For more information, see VMware knowledge base article 85685.

Resolved Issues

The resolved issues are grouped as follows.

Miscellaneous Issues

Networking Issues

Installation, Upgrade and Migration Issues

Security Issues

vSphere Client Issues

Storage Issues

vSAN Issues

Virtual Machine Management Issues

Resolved Issues from Previous Releases

Miscellaneous Issues

NEW: In the vSphere Client, you might see the alarm Host connection and power state on xxx to change from green to red

Due to a rare issue with handling Asynchronous Input/Output (AIO) calls, hostd and vpxa services on an ESXi host might fail and trigger alarms in the vSphere

Client. In the backtrace, you see errors such as:

#0 0x0000000bd09dcbe5 in __GI_raise (sig=6) at ../nptl/sysdeps/unix/sysv/linux/raise.c:56

#1 0x0000000bd09de05b in __GI_abort () at abort.c:90

#2 0x0000000bc7d00b65 in Vmacore::System::SignalTerminateHandler (info=, ctx=) at bora/vim/lib/vmacore/posix/defSigHandlers.cpp:62

#3 <signal called="" handler="">

#4 NfcAioProcessCloseSessionMsg (closeMsg=0xbd9280420, session=0xbde2c4510) at bora/lib/nfclib/nfcAioServer.c:935

#5 NfcAioProcessMsg (session=session@entry=0xbde2c4510, aioMsg=aioMsg@entry=0xbd92804b0) at bora/lib/nfclib/nfcAioServer.c:4206

#6 0x0000000bd002cc8b in NfcAioGetAndProcessMsg (session=session@entry=0xbde2c4510) at bora/lib/nfclib/nfcAioServer.c:4324

#7 0x0000000bd002d5bd in NfcAioServerProcessMain (session=session@entry=0xbde2c4510, netCallback=netCallback@entry=0 '\000') at

bora/lib/nfclib/nfcAioServer.c:4805

#8 0x0000000bd002ea38 in NfcAioServerProcessClientMsg (session=session@entry=0xbde2c4510, done=done@entry=0xbd92806af "") at bora/lib/nfclib/nfcAioServer.c:5166

This issue is resolved in this release. The fix makes sure the AioSession object works as expected.

A very rare issue with NVIDIA vGPU-powered virtual machines might cause ESXi hosts to fail with a purple diagnostic screen

In very rare conditions, ESXi hosts with NVIDIA vGPU-powered virtual machines might intermittently fail with a purple diagnostic screen with a kernel panic error.

The issue might affect multiple ESXi hosts, but not at the same time. In the backlog, you see kernel reports about heartbeat timeouts against CPU for x seconds

and the stack informs about a P2M cache.

This issue is resolved in this release.

ESXi hosts with virtual machines with Latency Sensitivity enabled might randomly become unresponsive due to CPU starvation

When you enable Latency Sensitivity on virtual machines, some threads of the Likewise Service Manager (lwsmd), which sets CPU affinity explicitly, might

compete for CPU resources on such virtual machines. As a result, you might see the ESXi host and the hostd service to become unresponsive.

This issue is resolved in this release. The fix makes sure lwsmd does not set CPU affinity explicitly.

In very rare cases, the virtual NVME adapter (VNVME) retry logic in ESXi 7.0 Update 3 might potentially cause silent data corruption

The VNVME retry logic in ESXi 7.0 Update 3 has an issue that might potentially cause silent data corruption. Retries rarely occur and they can potentially, not

always, cause data errors. The issue affects only ESXi 7.0 Update 3.

This issue is resolved in this release.

ESXi hosts might fail with a purple diagnostic screen during shutdown due to stale metadata

In rare cases, when you delete a large component in an ESXi host, followed by a reboot, the reboot might start before all metadata of the component gets

deleted. The stale metadata might cause the ESXi host to fail with a purple diagnostic screen.

This issue is resolved in this release. The fix makes sure no pending metadata remains before a reboot of ESXi hosts.

Virtual desktop infrastructure (VDI) might become unresponsive due to a race condition in the VMKAPI driver

Event delivery to applications might delay indefinitely due to a race condition in the VMKAPI driver. As a result, the virtual desktop infrastructure in some

environments, such as systems using NVIDIA graphic cards, might become unresponsive or lose connection to the VDI client.

This issue is resolve in this release.

ESXi hosts might fail with a purple diagnostic screen due to issues with ACPI Component Architecture (ACPICA) semaphores

Several issues in the implementation of ACPICA semaphores in ESXi 7.0 Update 3 and earlier can result in VMKernel panics, typically during boot. An issue in the

semaphore implementation can cause starvation, and on several call paths the VMKernel might improperly try to acquire an ACPICA semaphore or to sleep

within ACPICA while holding a spinlock. Whether these issues cause problems on a specific machine depends on details of the ACPI firmware of the machine.

These issues are resolved in this release. The fix involves a rewrite of the ACPICA semaphores in ESXi, and correction of the code paths that try to enter ACPICA

while holding a spinlock.

ESXi hosts might fail with a purple diagnostic screen when I/O operations run on a software iSCSI adapter

I/O operations on a software iSCSI adapter might cause a rare race condition inside the iscsi_vmk driver. As a result, ESXi hosts might intermittently fail with a

purple diagnostic screen.

This issue is resolved in this release.

Networking Issues

If you use a vSphere Distributed Switch (VDS) of version earlier than 6.6 and change the LAG hash algorithm, ESXi hosts might fail with a purple diagnostic

screen

If you use a VDS of version earlier than 6.6 on a vSphere 7.0 Update 1 or later system, and you change the LAG hash algorithm, for example from L3 to L2

hashes, ESXi hosts might fail with a purple diagnostic screen.

This issue is resolved in this release.

You see packet drops for virtual machines with VMware Network Extensibility (NetX) redirection enabled

In vCenter Server advanced performance charts, you see an increasing number of packet drop count for all virtual machines that have NetX redirection enabled.

However, if you disable NetX redirection, the count becomes 0.

This issue is resolved in this release.

An ESXi host might fail with a purple diagnostic screen during booting due to incorrect CQ to EQ mapping in an Emulex FC HBA

In rare cases, incorrect mapping of completion queues (CQ) when the total number of I/O channels of a Emulex FC HBA is not an exact multiple of the number of

event queues (EQ), might cause booting of an ESXi host to fail with a purple diagnostic screen. In the backtrace, you can see an error in the lpfc_cq_create()

method.

This issue is resolved in this release. The fix ensures correct mapping of CQs to EQs.

ESXi hosts might fail with a purple diagnostic screen due to memory allocation issue in the UNIX domain sockets

During internal communication between UNIX domain sockets, a heap allocation might occur instead of cleaning ancillary data such as file descriptors. As a

result, in some cases, the ESXi host might report an out of memory condition and fail with a purple diagnostic screen with #PF Exception 14 and errors similar to

UserDuct_ReadAndRecvMsg().

This issue is resolved in this release. The fix cleans ancillary data to avoid buffer memory allocations.

NTP optional configurations do not persist on ESXi host reboot

When you set up optional configurations for NTP by using ESXCLI commands, the settings might not persist after the ESXi host reboots.

This issue is resolved in this release. The fix makes sure that optional configurations are restored into the local cache from ConfigStore during ESXi host bootup.

When you change the LACP hashing algorithm in systems with vSphere Distributed Switch of version 6.5.0, multiple ESXi hosts might fail with a purple

diagnostic screen

In systems with vSphere Distributed Switch of version 6.5.0 and ESXi hosts of version 7.0 or later, when you change the LACP hashing algorithm, this might

cause an unsupported LACP event error due to a temporary string array used to save the event type name. As a result, multiple ESXi hosts might fail with a

purple diagnostic screen.

This issue is resolved in this release. To avoid facing the issue, in vCenter Server systems of version 7.0 and later make sure you use a vSphere Distributed Switch

version later than 6.5.0.

Installation, Upgrade and Migration Issues

Remediation of clusters that you manage with vSphere Lifecycle Manager baselines might take long

Remediation of clusters that you manage with vSphere Lifecycle Manager baselines might take long after updates from ESXi 7.0 Update 2d and earlier to a

version later than ESXi 7.0 Update 2d.

This issue is resolved in this release.

After updating to ESXi 7.0 Update 3, virtual machines with physical RDM disks fail to migrate or power-on on destination ESXi hosts

In certain cases, for example virtual machines with RDM devices running on servers with SNMP, a race condition between device open requests might lead to

failing vSphere vMotion operations.

This issue is resolved in this release. The fix makes sure that device open requests are sequenced to avoid race conditions. For more information, see VMware

knowledge base article 86158.

After upgrading to ESXi 7.0 Update 2d and later, you see an NTP time sync error

In some environments, after upgrading to ESXi 7.0 Update 2d and later, in the vSphere Client you might see the error Host has lost time synchronization.

However, the alarm might not indicate an actual issue.

This issue is resolved in this release. The fix replaces the error message with a log function for backtracing but prevents false alarms.

Security Issues

Update to OpenSSL

The OpenSSL package is updated to version openssl-1.0.2zb.

Update to the Python package

The Python package is updated to address CVE-2021-29921.

You can connect to port 9080 by using restricted DES/3DES ciphers

With the OPENSSL command openssl s_client -cipher <CIPHER> -connect localhost:9080 you can connect to port 9080 by using restricted DES/3DES ciphers.

This issue is resolved in this release. You cannot connect to port 9080 by using the following ciphers: DES-CBC3-SHA, EDH-RSA-DES-CBC3-SHA, ECDHE-RSA-

DES-CBC3-SHA, and AECDH-DES-CBC3-SHA.

The following VMware Tools ISO images are bundled with ESXi 7.0 Update 3c:

windows.iso: VMware Tools 11.3.5 supports Windows 7 SP1 or Windows Server 2008 R2 SP1 and later.

linux.iso: VMware Tools 10.3.23 ISO image for Linux OS with glibc 2.11 or later.

The following VMware Tools ISO images are available for download:

VMware Tools 11.0.6:

windows.iso: for Windows Vista (SP2) and Windows Server 2008 Service Pack 2 (SP2).

VMware Tools 10.0.12:

winPreVista.iso: for Windows 2000, Windows XP, and Windows 2003.

linuxPreGLibc25.iso: supports Linux guest operating systems earlier than Red Hat Enterprise Linux (RHEL) 5, SUSE Linux Enterprise Server (SLES) 11,

Ubuntu 7.04, and other distributions with glibc version earlier than 2.5.

solaris.iso: VMware Tools image 10.3.10 for Solaris.

darwin.iso: Supports Mac OS X versions 10.11 and later.

Follow the procedures listed in the following documents to download VMware Tools for platforms not bundled with ESXi:

VMware Tools 11.3.5 Release Notes

Earlier versions of VMware Tools

What Every vSphere Admin Must Know About VMware Tools

VMware Tools for hosts provisioned with Auto Deploy

Updating VMware Tools

vSphere Client Issues

Virtual machines appear as inaccessible in the vSphere Client and you might see some downtime for applications

In rare cases, hardware issues might cause an SQlite DB corruption that makes multiple VMs become inaccessible and lead to some downtime for applications.

This issue is resolved in this release.

Storage Issues

Virtual machine operations fail with an error for insufficient disk space on datastore

A new datastore normally has a high number of large file block (LFB) resources and a lesser number of small file block (SFB) resources. For workflows that

consume SFBs, such as virtual machine operations, LFBs convert to SFBs. However, due to a delay in updating the conversion status, newly converted SFBs

might not be recognized as available for allocation. As a result, you see an error such as Insufficient disk space on datastore when you try to power on, clone, or

migrate a virtual machine.

This issue is resolved in this release.

vSphere Virtual Volume snapshot operations might fail on the source volume or the snapshot volume on Pure storage

Due to an issue that allows the duplication of the unique ID of vSphere Virtual Volumes, virtual machine snapshot operations might fail, or the source volume

might get deleted. The issue is specific to Pure storage and affects Purity release lines 5.3.13 and earlier, 6.0.5 and earlier, and 6.1.1 and earlier.

This issue is resolved in this release.

vSAN Issues

You might see vSAN health errors for cluster partition when data-in-transit encryption is enabled

In the vSphere Client, you might see vSAN health errors such as vSAN cluster partition or vSAN object health when data-in-transit encryption is enabled. The issue

occurs because when a rekey operation starts in a vSAN cluster, a temporary resource issue might cause key exchange between peers to fail.

This issue is resolved in this release.

Virtual Machine Management Issues

A race condition between live migration operations might cause the ESXi host to fail with a purple diagnostic screen

In environments with VMs of 575 GB or more reserved memory that do not use Encrypted vSphere vMotion, a live migration operation might race with another

live migration and cause the ESXi host to fail with a purple diagnostic screen.

This issue is resolved in this release. However, in very rare cases, the migration operation might still fail, regardless that the root cause for the purple diagnostic

screen condition is fixed. In such cases, retry the migration when no other live migration is in progress on the source host, or enable Encrypted vSphere vMotion

on the virtual machines.

Resolved Issues from Previous Releases

Networking Issues

RDMA traffic by using the iWARP protocol might not complete

RDMA traffic by using the iWARP protocol on Intel x722 cards might time out and not complete.

This issue is resolved in this release.

Installation, Upgrade and Migration Issues

The /locker partition might be corrupted when the partition is stored on a USB or SD device

Due to the I/O sensitivity of USB and SD devices, the VMFS-L locker partition on such devices that stores VMware Tools and core dump files might get

corrupted.

This issue is resolved in this release. By default, ESXi loads the locker packages to the RAM disk during boot.

ESXi hosts might lose connectivity after brcmfcoe driver upgrade on Hitachi storage arrays

After an upgrade of the brcmfcoe driver on Hitachi storage arrays, ESXi hosts might fail to boot and lose connectivity.

This issue is resolved in this release.

After upgrading to ESXi 7.0 Update 2, you see excessive storage read I/O load

ESXi 7.0 Update 2 introduced a system statistics provider interface that requires reading the datastore stats for every ESXi host on every 5 min. If a datastore is

shared by multiple ESXi hosts, such frequent reads might cause a read latency on the storage array and lead to excessive storage read I/O load.

This issue is resolved in this release.

Virtual Machine Management Issues

Virtual machines with enabled AMD Secure Encrypted Virtualization-Encrypted State (SEV-ES) cannot create Virtual Machine Communication Interface (VMCI)

sockets

Performance and functionality of features that require VMCI might be affected on virtual machines with enabled AMD SEV-ES, because such virtual machines

cannot create VMCI sockets.

This issue is resolved in this release.

Virtual machines might fail when rebooting a heavily loaded guest OS

In rare cases, when a guest OS reboot is initiated outside the guest, for example from the vSphere Client, virtual machines might fail, generating a VMX dump.

The issue might occur when the guest OS is heavily loaded. As a result, responses from the guest to VMX requests are delayed prior to the reboot. In such cases,

the vmware.log file of the virtual machines includes messages such as: I125: Tools: Unable to send state change 3: TCLO error. E105: PANIC: NOT_REACHED

bora/vmx/tools/toolsRunningStatus.c:953.

This issue is resolved in this release.

Miscellaneous Issues

Asynchronous read I/O containing a SCATTER_GATHER_ELEMENT array of more than 16 members with at least 1 member falling in the last partial block of a

file might lead to ESXi host panic

In rare cases, in an asynchronous read I/O containing a SCATTER_GATHER_ELEMENT array of more than 16 members, at least 1 member might fall in the last partial block

of a file. This might lead to corrupting VMFS memory heap, which in turn causes ESXi hosts to fail with a purple diagnostic screen.

This issue is resolved in this release.

If a guest OS issues UNMAP requests with large size on thin provisioned VMDKs, ESXi hosts might fail with a purple diagnostic screen

ESXi 7.0 Update 3 introduced an uniform UNMAP granularity for VMFS and SEsparse snapshots, and set the maximum UNMAP granularity reported by VMFS to

2GB. However, in certain environments, when the guest OS makes a trim or unmap request of 2GB, such a request might require the VMFS metadata transaction

to do lock acquisition of more than 50 resource clusters. VMFS might not handle such requests correctly. As a result, an ESXi host might fail with a purple

diagnostic screen. VMFS metadata transaction requiring lock actions on greater then 50 resource clusters is rare and can only happen on aged datastores. The

issue impacts only thin-provisioned VMDKs. Thick and eager zero thick VMDKs are not impacted.

Along with the purple diagnostic screen, in the /var/run/log/vmkernel file you see errors such as:

2021-10-20T03:11:41.679Z cpu0:2352732)@BlueScreen: NMI IPI: Panic requested by another PCPU. RIPOFF(base):RBP:CS [0x1404f8(0x420004800000):0x12b8:0xf48] (Src

0x1, CPU0)

2021-10-20T03:11:41.689Z cpu0:2352732)Code start: 0x420004800000 VMK uptime: 11:07:27:23.196

2021-10-20T03:11:41.697Z cpu0:2352732)Saved backtrace from: pcpu 0 Heartbeat NMI

2021-10-20T03:11:41.715Z cpu0:2352732)0x45394629b8b8:[0x4200049404f7]HeapVSIAddChunkInfo@vmkernel#nover+0x1b0 stack: 0x420005bd611e

This issue is resolved in this release.

The hostd service might fail due to a time service event monitoring issue

An issue in the time service event monitoring service, which is enabled by default, might cause the hostd service to fail. In the vobd.log file, you see errors such

as:

2021-10-21T18:04:28.251Z: [UserWorldCorrelator] 304957116us: [esx.problem.hostd.core.dumped] /bin/hostd crashed (1 time(s) so far) and a core file may have been

created at /var/core/hostd-zdump.000. This may have caused connections to the host to be dropped.

2021-10-21T18:04:28.251Z: An event (esx.problem.hostd.core.dumped) could not be sent immediately to hostd; queueing for retry. 2021-10-21T18:04:32.298Z:

[UserWorldCorrelator] 309002531us: [vob.uw.core.dumped] /bin/hostd(2103800) /var/core/hostd-zdump.001

2021-10-21T18:04:36.351Z: [UserWorldCorrelator] 313055552us: [vob.uw.core.dumped] /bin/hostd(2103967) /var/core/hostd-zdump.002.

This issue is resolved in this release.

Known Issues

The known issues are grouped as follows.

Networking Issues

Security Issues

Known Issues from Earlier Releases

Networking Issues

Stale NSX for vSphere properties in vSphere Distributed Switch 7.0 (VDS) or ESXi 7.x hosts might fail host updates

If you had NSX for vSphere with VXLAN enabled on a vSphere Distributed Switch (VDS) of version 7.0 and migrated to NSX-T Data Center by using NSX V2T

migration, stale NSX for vSphere properties in the VDS or some hosts might prevent ESXi 7.x hosts updates. Host update fails with a platform configuration error.

Workaround: Upload the CleanNSXV.py script to the /tmp dir in vCenter Server. Log in to the appliance shell as a user with super administrative privileges (for

example, root) and follow these steps:

1. Run CleanNSXV.py by using the command PYTHONPATH=$VMWARE_PYTHON_PATH python /tmp/CleanNSXV.py --user <vc_admin_user> --password <passwd>. The

<vc_admin_user> parameter is a vCenter Server user with super administrative privileges and <passwd> parameter is the user password.

For example:

PYTHONPATH=$VMWARE_PYTHON_PATH python /tmp/CleanNSXV.py --user 'administrator@vsphere.local' --password 'Admin123'

2. Verify if the following NSX for vSphere properties, com.vmware.netoverlay.layer0 and com.vmware.net.vxlan.udpport, are removed from the ESXi hosts:

1. Connect to a random ESXi host by using an SSH client.

2. Run the command net-dvs -l | grep "com.vmware.netoverlay.layer0\|com.vmware.net.vxlan.udpport".

If you see no output, then the stale properties are removed.

To download the CleanNSXV.py script and for more details, see VMware knowledge base article 87423.

Security Issues

The cURL version in ESXi650-202110001 and ESXi670-202111001 is later than the cURL version in ESXi 7.0 Update 3c

The cURL version in ESXi 7.0 Update 3c is 7.77.0, while ESXi650-202110001 and ESXi670-202111001 have the newer fixed version 7.78.0. As a result, if you

upgrade from ESXi650-202110001 or ESXi670-202111001 to ESXi 7.0 Update 3c, cURL 7.7.0 might expose your system to the following vulnerabilities:

CVE-2021-22926: CVSS 7.5

CVE-2021-22925: CVSS 5.3

CVE-2021-22924: CVSS 3.7

CVE-2021-22923: CVSS 5.3

CVE-2021-22922: CVSS 6.5

Workaround: None. cURL version 7.78.0 comes with a future ESXi 7.x release.

Known Issues from Earlier Releases

To view a list of previous known issues, click here.

Copyright © Broadcom



=== Content from kb.vmware.com_36b2f2fb_20250115_175009.html ===


search

cancel

Search

### ASync BOM patching procedure to patch ESXi 7.0.U1E for VCF 4.1.x and VCF 4.2.x, and 7.0.U2E for 4.3.x environments

book
#### Article ID: 313492

calendar\_today
#### Updated On:

#### Products

VMware Cloud Foundation

Show More
Show Less

#### Issue/Introduction

[VMSA-2022-0004](https://www.vmware.com/security/advisories/VMSA-2022-0004.html) details numerous vulnerabilities in VMware ESXi.
As part of the response, VMware released "back in time" patches for VMware ESXi 7.0U1 and VMware ESX 7.0U2.
This KB documents the process should customers running VMware Cloud Foundation 4.1.x, 4.2.x or 4.3.x wish to update their ESXi hosts to these releases.

[Release Notes](https://docs.vmware.com/en/VMware-vSphere/7.0/rn/vsphere-esxi-70u1e.html) for VMware ESXi 7.0U1e
[Release Notes](https://docs.vmware.com/en/VMware-vSphere/7.0/rn/vsphere-esxi-70u2e-release-notes.html)  for VMware ESXi 7.0U2e

**NOTE: This KB is not applicable for VCF on Dell EMC VxRail Release**

Symptoms:
As documented in [VMSA-2022-0004](https://www.vmware.com/security/advisories/VMSA-2022-0004.html) , all versions of the VMware ESXi 7.0 prior to 7.0 Update 3c are affected by the vulnerabilities listed in the advisory.
Since the VMware Cloud Foundation(VCF) 4.x versions, prior to VCF 4.4, bundle impacted releases of VMware ESXi, the VCF versions VCF 4.1.x, VCF 4.2.x and VCF 4.3.x releases, are similarly impacted by the vulnerabilities listed in the advisory.

#### Environment

VMware Cloud Foundation 4.3.x
VMware Cloud Foundation 4.2.x
VMware Cloud Foundation 4.1

#### Resolution

#### **Guidance Steps:**

1. Verify the VCF release to be able to apply ESXi Async patch.

| **VCF Release** | **Upgrade Steps** |
| --- | --- |
| VCF 4.1.0 | Upgrade to VCF 4.1.0.1, Apply [KB 87050](https://kb.vmware.com/s/article/87050)  and then move to Step 2. |
| VCF 4.1.0.1 | Apply [KB 87050](https://kb.vmware.com/s/article/87050) and then move to Step 2. |
| VCF 4.2.0 | Upgrade to VCF 4.2.1, Apply [KB 87050](https://kb.vmware.com/s/article/87050)  and then apply [KB 88287](https://kb.vmware.com/s/article/88287). |
| VCF 4.2.1 | Apply [KB 87050](https://kb.vmware.com/s/article/87050)  and then apply [KB 88287](https://kb.vmware.com/s/article/88287). |
| VCF 4.3.0 | Upgrade to VCF 4.3.1.1 as documented in [VCF 4.3.1.1 Release Notes](https://docs.vmware.com/en/VMware-Cloud-Foundation/4.3.1/rn/vmware-cloud-foundation-431-release-notes/index.html) and apply [KB 88287](https://kb.vmware.com/s/article/88287). |
| VCF 4.3.1 | Upgrade to VCF 4.3.1.1 as documented in [VCF 4.3.1.1 Release Notes](https://docs.vmware.com/en/VMware-Cloud-Foundation/4.3.1/rn/vmware-cloud-foundation-431-release-notes/index.html) and apply [KB 88287](https://kb.vmware.com/s/article/88287). |
| VCF 4.3.1.1 | Apply [KB 88287](https://kb.vmware.com/s/article/88287). |

2. Apply ESXi fix patch async outside of VCF.

    Release Notes:
    For VCF 4.1.x and 4.2.x, refer [VMware ESXi 7.0 Update 1e](https://docs.vmware.com/en/VMware-vSphere/7.0/rn/vsphere-esxi-70u1e.html)
    For VCF 4.3.x, refer [VMware ESXi 7.0 Update 2e](https://docs.vmware.com/en/VMware-vSphere/7.0/rn/vsphere-esxi-70u2e-release-notes.html)

    Using vSphere product documentation apply ESXi patch directly without using SDDC Manager.

    **Note: After ESXi hosts are async patched. Please make sure to cleanup the baselines and ISOs on vSphere UI which were created as part of the upgrade through vSphere. If not deleted, the future upgrades from VCF LCM are prone to failures.**
3. Update the SDDC Manager Inventory.

Following steps can be used to update SDDC Manager inventory:
3.1 Download the tar file from attachments.
3.2 SCP the file to SDDC Manager in "/tmp" directory.
3.3 SSH to SDDC Manager using "vcf" user and switch to root using "su" command.
3.4 On the SDDC manager, cd  to "/tmp" directory and extract the tar file.

Extract the tar file on the SDDC Manager
root@sddc-manager [~]# cd /tmp
root@sddc-manager [/tmp]# tar xvf lcm-tools-prod.tar.gz

3.5 The executable script can be found in the bin directory.

usage: bin/inventory-sync <SDDC SSO USER> <SDDC SSH USER>
Once the above command is executed with the appropriate parameters, the script will prompt as below. Please provide the credentials when prompted.

Enter SDDC Manager SSH Password:
Enter SDDC Manager Root User's Password:
Enter SDDC Manager SSO Password:

Below is a sample run:
---
vcf@sddc-manager [ /tmp ]$ bin/inventory-sync [[email protected]](/cdn-cgi/l/email-protection) vcf
2022-02-18 16:50:19.130 [WARN ] Cloud Sleuth not configured.
2022-02-18 16:50:20.254 [INFO ] VCF Async Patch Tool - Version: 4.4.0-vcf4400RELEASE-19311902
2022-02-18 16:50:20.308 [INFO ] Log file is generated at /home/vcf/tmp/bin/async\_patch\_tool.log
Enter SDDC Manager SSH Password:
Enter SDDC Manager Root User's Password:
Enter SDDC Manager SSO Password:
2022-02-18 16:50:41.284 [INFO ] Performing global inventory sync.
2022-02-18 16:50:41.284 [INFO ] Validating inventory sync input spec
2022-02-18 16:50:41.308 [INFO ] Performing inventory sync for entities [VCENTER, ESXI, NSXT\_CLUSTER]
2022-02-18 16:50:41.308 [INFO ] Constructing version diff for vcenters
2022-02-18 16:50:42.767 [INFO ] Retrieved 1 vcenter(s) from the inventory.
2022-02-18 16:50:43.311 [INFO ] Creating new session on vcenter-1.vrack.vsphere.local for [[email protected]](/cdn-cgi/l/email-protection)
2022-02-18 16:50:43.821 [INFO ] Get the current appliance system version
---

3.6 After the inventory-sync finishes executing, the SDDC manager inventory will be in sync with all the hosts that were updated.

4. Steps to enable future upgrades from VCF (one time activity per SDDC manager instance).

   | **Patch Applied** | **Recommended Upgrade to ESXi Version** | **Corresponding VCF Version** |
   | --- | --- | --- |
   | 7.0Update 1e | 7.0 Update 3c | VCF 4.4 |
   | 7.0Update 2e | 7.0 Update 3c | VCF 4.4 |

For offline customers, upload the VCF 4.4 patch bundles following [VCF 4.4 release documentation](https://docs.vmware.com/en/VMware-Cloud-Foundation/4.4/vcf-lifecycle/GUID-C82BB0D4-FD52-4CC9-A6BB-3A6CF35E380E.html).

**4.1 Get access token:**

Login to SDDC Manager via SSH and run following command

curl '<SDDC Manager FQDN>/v1/tokens' -i -X POST -H 'Content-Type: application/json' -H 'Accept: application/json' -d '{"username" : "<SSO User ID>","password" : "<SSO Password>"}'

<SDDC Manager FQDN > -   Fully qualified domain name of SDDC manager.
<SSO User ID> - SSO user id of SDDC VM
<SSO Password> - SSO password for SDDC VM

**Example :**

Request:
curl 'http://sddc-manager.vrack.vsphere.local/v1/tokens' -i -X POST -H 'Content-Type: application/json' -H 'Accept: application/json' -d '{"username" : "[[email protected]](/cdn-cgi/l/email-protection)","password" : "password@123"}'

Response:{"accessToken":"eyJhbG...bWluaXN","refreshToken":{"id":"8702601b-cc1d-4ed2-acc4-8ccf8f849999"}}

**4.2 Enable forward upgrades by following below steps based on VCF Version**

#### [Steps for VCF 4.1.xSteps for VCF 4.2.x](#forward_upgrade_42)[Steps for VCF 4.3.x](#forward_upgrade_43)

#### **Forward upgrade path for customers on VCF 4.1.x and VCF 4.2.x:**

To enable forward upgrades, please follow below steps:

1. Create the API request payload:

* Open a vi editor with file name request.json

vi request.json

* Create a file request.json with below contents:

{
    "forceUpdate": true,
    "versionAliasesForBundleComponentTypes": [ {
        "bundleComponentType": "ESX\_HOST",
        "versionAliases": [ {
            "aliases": ["7.0.1-19324898"],
            "version": "7.0.2-18426014"
        } ]
    } ]
}

2. Trigger the PUT api call using the request.json payload and the output for the API call is as below.

API: curl -k '<SDDC Manager FQDN>/v1/system/settings/version-aliases' -X PUT -d @request.json  -H 'Content-Type:application/json'  -H 'Authorization: Bearer <AUTH TOKEN>'

<SDDC Manager FQDN >  -   Fully qualified domain name of SDDC manager.
<AUTH TOKEN> -   Access Token retrieved from Step 4.1.

Sample Output:

{
    "elements": [{
            "bundleComponentType": "ESX\_HOST",
            "versionAliases": [{
                "version": "7.0.2-18426014",
                "aliases": [
                    "7.0.1-19324898"
                ]
            }]
        }
    ]
}

#### **Forward upgrade path for customers on VCF 4.3.x:**

To enable forward upgrades, please follow below steps:

1. Create the API request payload:

* Open a vi editor with file name request.json

vi request.json

* Create a file request.json with below contents:

{
    "forceUpdate": true,
    "versionAliasesForBundleComponentTypes": [ {
        "bundleComponentType": "ESX\_HOST",
        "versionAliases": [ {
            "aliases": ["7.0.2-19290878"],
            "version": "7.0.2-18426014"
        } ]
    } ]
}

2. Trigger the PUT api call using the request.json payload and the output for the API call is as below.

API: curl -k '<SDDC Manager FQDN>/v1/system/settings/version-aliases' -X PUT -d @request.json  -H 'Content-Type:application/json'  -H 'Authorization: Bearer <AUTH TOKEN>'

<SDDC Manager FQDN >  -   Fully qualified domain name of SDDC manager.
<AUTH TOKEN> -   Access Token retrieved from Step 4.1.

Sample Output:
{
    "elements": [{
            "bundleComponentType": "ESX\_HOST",
            "versionAliases": [{
                "version": "7.0.2-18426014",
                "aliases": [
                    "7.0.2-19290878"
                ]
            }]
        }
    ]
}

**Note:**
VI domain creation will still be based on the VCF release BOM. For example: VI domain creation on VCF 4.2.1 will be based on the ESXi version(7.0.1-17551050) i.e [4.2.1 release BOM](https://docs.vmware.com/en/VMware-Cloud-Foundation/4.2.1/rn/VMware-Cloud-Foundation-421-Release-Notes.html#swversions). Follow the Guidance Steps above to patch ESXi for a new VI.

#### Attachments

lcm-tools-prod.tar
get\_app

#### Feedback

thumb\_up
Yes

thumb\_down
No

Powered by
[![Wolken Software](https://cdn.wolkenservicedesk.com/wolken-footer-image.png)](https://www.wolkensoftware.com/)



=== Content from www.vmware.com_b8aa0dcc_20250115_175002.html ===


Menu

* [Products](https://www.broadcom.com/products/)
* [Solutions](https://www.broadcom.com/solutions)
* [Support and Services](https://www.broadcom.com/support)
* [Company](https://www.broadcom.com/company/about-us/)
* [How To Buy](https://www.broadcom.com/how-to-buy/#sales)

* Log in

  [Log In](/c/portal/login)
  [Register](https://profile.broadcom.com/web/registration)

[Register](https://profile.broadcom.com/web/registration)
[Login](/c/portal/login)

VMSA-2022-0004:VMware ESXi, Workstation, and Fusion updates address multiple security vulnerabilities

Product/Component

VMware Cloud Foundation

2 more products

List of Products

3 Products

* VMware Cloud Foundation
* VMware Desktop Hypervisor
* VMware vSphere ESXi

Notification Id

23619

Last Updated

13 February 2022

Initial Publication Date

13 February 2022

Status

CLOSED

Severity

CRITICAL

CVSS Base Score

5.3-8.4

WorkAround

Affected CVE

CVE-2021-22040,CVE-2021-22041,CVE-2021-22042,CVE-2021-22043,CVE-2021-22050

             Advisory ID: VMSA-2022-0004   CVSSv3 Range: 5.3-8.4   Issue Date:2022-02-15   Updated On: 2022-02-15 (Initial Advisory)   CVE(s): CVE-2021-22040, CVE-2021-22041, CVE-2021-22042, CVE-2021-22043, CVE-2021-22050   Synopsis: VMware ESXi, Workstation, and Fusion updates address multiple security vulnerabilities (CVE-2021-22040, CVE-2021-22041, CVE-2021-22042, CVE-2021-22043, CVE-2021-22050)

 [RSS Feed](https://www.vmware.com/security/advisories/VMSA-2022-0004.xml)

 Download PDF

 Download Text File

Share this page on social media:

##### **1. Impacted Products**

* VMware ESXi
* VMware Workstation Pro / Player (Workstation)
* VMware Fusion Pro / Fusion (Fusion)
* VMware Cloud Foundation (Cloud Foundation)

##### **2. Introduction**

Multiple vulnerabilities in VMware ESXi, Workstation, and Fusion were privately reported to VMware. Updates are available to remediate these vulnerabilities in affected VMware products.

The individual vulnerabilities documented on this VMSA have severity Important/Moderate but combining these issues may result in higher severity, hence the severity of this VMSA is at severity level Critical.

##### **3a. Use-after-free vulnerability in XHCI USB controller (CVE-2021-22040)**

**Description**

VMware ESXi, Workstation, and Fusion contain a use-after-free vulnerability in the XHCI USB controller.VMware has evaluated the severity of this issue to be in the [Important severity range](https://www.vmware.com/support/policies/security_response.html) with a maximum CVSSv3 base score of [8.4](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:L/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H).

**Known Attack Vectors**

A malicious actor with local administrative privileges on a virtual machine may exploit this issue to execute code as the virtual machine's VMX process running on the host.

**Resolution**

To remediate CVE-2021-22040 apply the patches listed in the 'Fixed Version' column of the 'Response Matrix' found below.

**Workarounds**

Workarounds for CVE-2021-22040 have been listed in the 'Workarounds' column of the 'Response Matrix' below.

**Additional Documentation**

A supplemental blog post was created for additional clarification. Please see: <https://via.vmw.com/vmsa-2022-0004-qna>.

**Notes**

**[1]** VMware recommends taking ESXi670-202201001 released on January 25, 2022 over ESXi670-202111101-SG released on November 23, 2021 since ESXi670-202201001 also resolves non-security related issues (documented in <https://docs.vmware.com/en/VMware-vSphere/6.7/rn/esxi670-202201001.html>).

**Acknowledgements**

VMware would like to thank Wei of Kunlun Lab working with the 2021 Tianfu Cup Pwn Contest for reporting this issue to us.

##### **3b. Double-fetch vulnerability in UHCI USB controller (CVE-2021-22041)**

**Description**

VMware ESXi, Workstation, and Fusion contain a double-fetch vulnerability in the UHCI USB controller. VMware has evaluated the severity of this issue to be in the [Important severity range](https://www.vmware.com/support/policies/security_response.html) with a maximum CVSSv3 base score of [8.4](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:L/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H).

**Known Attack Vectors**

A malicious actor with local administrative privileges on a virtual machine may exploit this issue to execute code as the virtual machine's VMX process running on the host.

**Resolution**

To remediate CVE-2021-22041 apply the patches listed in the 'Fixed Version' column of the 'Response Matrix' found below.

**Workarounds**

Workarounds for CVE-2021-22041 have been listed in the 'Workarounds' column of the 'Response Matrix' below.

**Additional Documentation**

A supplemental blog post was created for additional clarification. Please see: <https://via.vmw.com/vmsa-2022-0004-qna>.

**Notes**

Successful exploitation of this issue requires an isochronous USB endpoint to be made available to the virtual machine.

**[1]** VMware recommends taking ESXi670-202201001 released on January 25, 2022 over ESXi670-202111101-SG released on November 23, 2021 since ESXi670-202201001 also resolves non-security related issues (documented in <https://docs.vmware.com/en/VMware-vSphere/6.7/rn/esxi670-202201001.html>).

**Acknowledgements**

VMware would like to thank VictorV of Kunlun Lab working with the 2021 Tianfu Cup Pwn Contest for reporting this issue to us.

**Response Matrix: - 3a & 3b**

| Product | Version | Running On | CVE Identifier | CVSSv3 | Severity | Fixed Version | Workarounds | Additional Documentation |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| ESXi | 7.0 U3 | Any | CVE-2021-22040, CVE-2021-22041 | [8.4](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:L/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H) | important | [ESXi70U3c-19193900](https://docs.vmware.com/en/VMware-vSphere/7.0/rn/vsphere-esxi-70u3c-release-notes.html) | [KB87349](https://kb.vmware.com/s/article/87349) | [FAQ](https://via.vmw.com/vmsa-2022-0004-qna) |
| ESXi | 7.0 U2 | Any | CVE-2021-22040, CVE-2021-22041 | [8.4](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:L/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H) | important | [ESXi70U2e-19290878](https://docs.vmware.com/en/VMware-vSphere/7.0/rn/vsphere-esxi-70u2e-release-notes.html) | [KB87349](https://kb.vmware.com/s/article/87349) | [FAQ](https://via.vmw.com/vmsa-2022-0004-qna) |
| ESXi | 7.0 U1 | Any | CVE-2021-22040, CVE-2021-22041 | [8.4](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:L/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H) | important | [ESXi70U1e-19324898](https://docs.vmware.com/en/VMware-vSphere/7.0/rn/vsphere-esxi-70u1e.html) | [KB87349](https://kb.vmware.com/s/article/87349) | [FAQ](https://via.vmw.com/vmsa-2022-0004-qna) |
| ESXi | 6.7 | Any | CVE-2021-22040, CVE-2021-22041 | [8.4](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:L/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H) | important | [[1] ESXi670-202111101-SG](https://docs.vmware.com/en/VMware-vSphere/6.7/rn/esxi670-202111001.html) | [KB87349](https://kb.vmware.com/s/article/87349) | [FAQ](https://via.vmw.com/vmsa-2022-0004-qna) |
| ESXi | 6.5 | Any | CVE-2021-22040, CVE-2021-22041 | [8.4](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:L/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H) | important | [ESXi650-202202401-SG](https://docs.vmware.com/en/VMware-vSphere/6.5/rn/esxi650-202202001.html) | [KB87349](https://kb.vmware.com/s/article/87349) | [FAQ](https://via.vmw.com/vmsa-2022-0004-qna) |
| Fusion | 12.x | OS X | CVE-2021-22040, CVE-2021-22041 | [8.4](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:L/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H) | important | [12.2.1](https://docs.vmware.com/en/VMware-Fusion/12.2.1/rn/VMware-Fusion-1221-Release-Notes.html) | [KB87349](https://kb.vmware.com/s/article/87349) | [FAQ](https://via.vmw.com/vmsa-2022-0004-qna) |
| Workstation | 16.x | Any | CVE-2021-22040, CVE-2021-22041 | [8.4](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:L/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H) | important | [16.2.1](https://docs.vmware.com/en/VMware-Workstation-Player/16.2.1/rn/VMware-Workstation-1621-Player-Release-Notes.html) | [KB87349](https://kb.vmware.com/s/article/87349) | [FAQ](https://via.vmw.com/vmsa-2022-0004-qna) |

**Impacted Product Suites that Deploy Response Matrix 3a & 3b Components:**

| Product | Version | Running On | CVE Identifier | CVSSv3 | Severity | Fixed Version | Workarounds | Additional Documentation |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| Cloud Foundation (ESXi) | 4.x | Any | CVE-2021-22040, CVE-2021-22041 | [8.4](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:L/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H) | important | [KB87646 (4.4)](https://kb.vmware.com/s/article/87646) | [KB87349](https://kb.vmware.com/s/article/87349) | [FAQ](https://via.vmw.com/vmsa-2022-0004-qna) |
| Cloud Foundation (ESXi) | 3.x | Any | CVE-2021-22040, CVE-2021-22041 | [8.4](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:L/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H) | important | [3.11](https://docs.vmware.com/en/VMware-Cloud-Foundation/3.11/rn/VMware-Cloud-Foundation-311-Release-Notes.html) | [KB87349](https://kb.vmware.com/s/article/87349) | [FAQ](https://via.vmw.com/vmsa-2022-0004-qna) |

##### **3c. ESXi settingsd unauthorized access vulnerability (CVE-2021-22042)**

**Description**

VMware ESXi contains an unauthorized access vulnerability due to VMX having access to settingsd authorization tickets. VMware has evaluated the severity of this issue to be in the [Important severity range](https://www.vmware.com/support/policies/security_response.html) with a maximum CVSSv3 base score of [8.2](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:L/AC:L/PR:H/UI:N/S:C/C:H/I:H/A:H).

**Known Attack Vectors**

A malicious actor with privileges within the VMX process only, may be able to access settingsd service running as a high privileged user.

**Resolution**

To remediate CVE-2021-22042 apply the patches listed in the 'Fixed Version' column of the 'Response Matrix' found below.

**Workarounds**

None.

**Additional Documentation**

A supplemental blog post was created for additional clarification. Please see: <https://via.vmw.com/vmsa-2022-0004-qna>.

**Notes**

None.

**Acknowledgements**

VMware would like to thank Wei of Kunlun Lab working with the 2021 Tianfu Cup Pwn Contest for reporting this issue to us.

##### **3d. ESXi settingsd TOCTOU vulnerability (CVE-2021-22043)**

**Description**

VMware ESXi contains a TOCTOU (Time-of-check Time-of-use) vulnerability that exists in the way temporary files are handled. VMware has evaluated the severity of this issue to be in the [Important severity range](https://www.vmware.com/support/policies/security_response.html) with a maximum CVSSv3 base score of [8.2](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:L/AC:L/PR:H/UI:N/S:C/C:H/I:H/A:H).

**Known Attack Vectors**

A malicious actor with access to settingsd, may exploit this issue to escalate their privileges by writing arbitrary files.

**Resolution**

To remediate CVE-2021-22043 apply the patches listed in the 'Fixed Version' column of the 'Response Matrix' found below.

**Workarounds**

None.

**Additional Documentation**

A supplemental blog post was created for additional clarification. Please see: <https://via.vmw.com/vmsa-2022-0004-qna>.

**Notes**

None.

**Acknowledgements**

VMware would like to thank Wei of Kunlun Lab working with the 2021 Tianfu Cup Pwn Contest for reporting this issue to us.

**Response Matrix: - 3c & 3d**

| Product | Version | Running On | CVE Identifier | CVSSv3 | Severity | Fixed Version | Workarounds | Additional Documentation |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| ESXi | 7.0 U3 | Any | CVE-2021-22042, CVE-2021-22043 | [8.2](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:L/AC:L/PR:H/UI:N/S:C/C:H/I:H/A:H) | important | [ESXi70U3c-19193900](https://docs.vmware.com/en/VMware-vSphere/7.0/rn/vsphere-esxi-70u3c-release-notes.html) | None | [FAQ](https://via.vmw.com/vmsa-2022-0004-qna) |
| ESXi | 7.0 U2 | Any | CVE-2021-22042, CVE-2021-22043 | [8.2](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:L/AC:L/PR:H/UI:N/S:C/C:H/I:H/A:H) | important | [ESXi70U2e-19290878](https://docs.vmware.com/en/VMware-vSphere/7.0/rn/vsphere-esxi-70u2e-release-notes.html) | None | [FAQ](https://via.vmw.com/vmsa-2022-0004-qna) |
| ESXi | 7.0 U1 | Any | CVE-2021-22042, CVE-2021-22043 | [8.2](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:L/AC:L/PR:H/UI:N/S:C/C:H/I:H/A:H) | important | [ESXi70U1e-19324898](https://docs.vmware.com/en/VMware-vSphere/7.0/rn/vsphere-esxi-70u1e.html) | None | [FAQ](https://via.vmw.com/vmsa-2022-0004-qna) |
| ESXi | 6.7 | Any | CVE-2021-22042, CVE-2021-22043 | N/A | N/A | Unaffected | N/A | N/A |
| ESXi | 6.5 | Any | CVE-2021-22042, CVE-2021-22043 | N/A | N/A | Unaffected | N/A | N/A |

**Impacted Product Suites that Deploy Response Matrix 3c & 3d Components:**

| Product | Version | Running On | CVE Identifier | CVSSv3 | Severity | Fixed Version | Workarounds | Additional Documentation |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| Cloud Foundation (ESXi) | 4.x | Any | CVE-2021-22042, CVE-2021-22043 | [8.2](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:L/AC:L/PR:H/UI:N/S:C/C:H/I:H/A:H) | important | [KB87646 (4.4)](https://kb.vmware.com/s/article/87646) | None | [FAQ](https://via.vmw.com/vmsa-2022-0004-qna) |
| Cloud Foundation (ESXi) | 3.x | Any | CVE-2021-22042, CVE-2021-22043 | N/A | N/A | Unaffected | N/A | N/A |

##### **3e. ESXi slow HTTP POST denial of service vulnerability (CVE-2021-22050)**

**Description**

ESXi contains a slow HTTP POST denial-of-service vulnerability in rhttpproxy. VMware has evaluated the severity of this issue to be in the [Moderate severity range](https://www.vmware.com/support/policies/security_response.html) with a maximum CVSSv3 base score of [5.3](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:L).

**Known Attack Vectors**

A malicious actor with network access to ESXi may exploit this issue to create a denial-of-service condition by overwhelming rhttpproxy service with multiple requests.

**Resolution**

To remediate CVE-2021-22050 apply the patches listed in the 'Fixed Version' column of the 'Response Matrix' found below.

**Workarounds**

None.

**Additional Documentation**

A supplemental blog post was created for additional clarification. Please see: <https://via.vmw.com/vmsa-2022-0004-qna>.

**Notes**

**[1]** VMware recommends taking ESXi670-202201001 released on January 25, 2022 over ESXi670-202111101-SG released on November 23, 2021 since ESXi670-202201001 also resolves non-security related issues (documented in <https://docs.vmware.com/en/VMware-vSphere/6.7/rn/esxi670-202201001.html>).

**Acknowledgements**

VMware would like to thank George Noseevich (@webpentest) and Sergey Gerasimov of SolidLab LLC for reporting this issue to us.

**Response Matrix**

| Product | Version | Running On | CVE Identifier | CVSSv3 | Severity | Fixed Version | Workarounds | Additional Documentation |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| ESXi | 7.0 | Any | CVE-2021-22050 | [5.3](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:L) | moderate | [ESXi70U3c-19193900](https://docs.vmware.com/en/VMware-vSphere/7.0/rn/vsphere-esxi-70u3c-release-notes.html) | None | [FAQ](https://via.vmw.com/vmsa-2022-0004-qna) |
| ESXi | 6.7 | Any | CVE-2021-22050 | [5.3](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:L) | moderate | [[1] ESXi670-202111101-SG](https://docs.vmware.com/en/VMware-vSphere/6.7/rn/esxi670-202111001.html) | None | [FAQ](https://via.vmw.com/vmsa-2022-0004-qna) |
| ESXi | 6.5 | Any | CVE-2021-22050 | [5.3](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:L) | moderate | [ESXi650-202110101-SG](https://docs.vmware.com/en/VMware-vSphere/6.5/rn/esxi650-202110001.html) | None | [FAQ](https://via.vmw.com/vmsa-2022-0004-qna) |

**Impacted Product Suites that Deploy Response Matrix 3e Components:**

| Product | Version | Running On | CVE Identifier | CVSSv3 | Severity | Fixed Version | Workarounds | Additional Documentation |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| Cloud Foundation (ESXi) | 4.x | Any | CVE-2021-22050 | [5.3](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:L) | moderate | [KB87646 (4.4)](https://kb.vmware.com/s/article/87646) | None | [FAQ](https://via.vmw.com/vmsa-2022-0004-qna) |
| Cloud Foundation (ESXi) | 3.x | Any | CVE-2021-22050 | [5.3](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:L) | moderate | [3.11](https://docs.vmware.com/en/VMware-Cloud-Foundation/3.11/rn/VMware-Cloud-Foundation-311-Release-Notes.html) | None | [FAQ](https://via.vmw.com/vmsa-2022-0004-qna) |

##### **4. References**

**VMware ESXi 7.0 ESXi70U3c-19193900**
<https://my.vmware.com/group/vmware/patch>
<https://docs.vmware.com/en/VMware-vSphere/7.0/rn/vsphere-esxi-70u3c-release-notes.html>

**VMware ESXi 7.0 ESXi70U2e-19290878**
<https://my.vmware.com/group/vmware/patch>
<https://docs.vmware.com/en/VMware-vSphere/7.0/rn/vsphere-esxi-70u2e-release-notes.html>

**VMware ESXi 7.0 ESXi70U1e-19324898**
<https://my.vmware.com/group/vmware/patch>
<https://docs.vmware.com/en/VMware-vSphere/7.0/rn/vsphere-esxi-70u1e.html>

**VMware ESXi 6.7 ESXi670-202111101-SG**
<https://my.vmware.com/group/vmware/patch>
<https://docs.vmware.com/en/VMware-vSphere/6.7/rn/esxi670-202111001.html>

**VMware ESXi 6.5 ESXi650-202202401-SG**
<https://my.vmware.com/group/vmware/patch>
<https://docs.vmware.com/en/VMware-vSphere/6.5/rn/esxi650-202202001.html>

**VMware ESXi 6.5 ESXi650-202110101-SG**
<https://my.vmware.com/group/vmware/patch>
<https://docs.vmware.com/en/VMware-vSphere/6.5/rn/esxi650-202110001.html>

**VMware Cloud Foundation 4.4**
Downloads and Documentation:
<https://docs.vmware.com/en/VMware-Cloud-Foundation/4.4/rn/VMware-Cloud-Foundation-44-Release-Notes.html>

**VMware Cloud Foundation 3.11**
Downloads and Documentation:
<https://docs.vmware.com/en/VMware-Cloud-Foundation/3.11/rn/VMware-Cloud-Foundation-311-Release-Notes.html>

**VMware Workstation Player 16.2.1**
<https://www.vmware.com/go/downloadplayer>
<https://docs.vmware.com/en/VMware-Workstation-Player/index.html>

**VMware Fusion 12.2.1**
Downloads and Documentation:
<https://www.vmware.com/go/downloadfusion>
<https://docs.vmware.com/en/VMware-Fusion/index.html>

**Mitre CVE Dictionary Links:**
<https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-22040>
<https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-22041>
<https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-22042>
<https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-22043>
<https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-22050>

**FIRST CVSSv3 Calculator:**
CVE-2021-22040: <https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:L/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H>
CVE-2021-22041: <https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:L/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H>
CVE-2021-22042: <https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:L/AC:L/PR:H/UI:N/S:C/C:H/I:H/A:H>
CVE-2021-22043: <https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:L/AC:L/PR:H/UI:N/S:C/C:H/I:H/A:H>
CVE-2021-22050: <https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:L>

##### **5. Change Log**

**2022-02-15 VMSA-2022-0004**
Initial security advisory.

##### **6. Contact**

E-mail list for product security notifications and announcements:

[https://lists.vmware.com/mailman/listinfo/security-announce](https://lists.vmware.com/cgi-bin/mailman/listinfo/security-announce)

This Security Advisory is posted to the following lists:

[[email protected]](/cdn-cgi/l/email-protection#0271676177706b767b2f636c6c6d776c6167426e6b7176712c746f756370672c616d6f)

[[email protected]](/cdn-cgi/l/email-protection#9cfee9fbe8eefdeddceff9ffe9eef5e8e5faf3ffe9efb2fff3f1)

[[email protected]](/cdn-cgi/l/email-protection#0d6b78616169647e6e61627e787f684d7e686e61647e797e23627f6a)

E-mail: [[email protected]](/cdn-cgi/l/email-protection#0a796f697f78637e734a7c677d6b786f24696567)

PGP key at:

<https://kb.vmware.com/kb/1055>

VMware Security Advisories

<https://www.vmware.com/security/advisories>

VMware Security Response Policy

<https://www.vmware.com/support/policies/security_response.html>

VMware Lifecycle Support Phases

<https://www.vmware.com/support/policies/lifecycle.html>

VMware Security & Compliance Blog

<https://blogs.vmware.com/security>

Twitter

<https://twitter.com/VMwareSRC>

Copyright 2022 VMware Inc. All rights reserved.

Hidden

#####

×

It appears your Broadcom Products and Services are

supported by one of our certified Support partners

Click below to be redirected to the appropriate Support

Partner Portal to request support

For non-product related issues (Support Portal / Licensing) Click HERE

Continue

#####

×

For **Technical Support** (issues with products or services)

1. Select **Technical** to be redirected to the My Entitlements page
2. Expand the product you require support on
3. Select the case icon from the case column
4. You will be redirected to the appropriate vendor portal where you can raise your technical request

For **Non-Technical Support** (issues with portal access, license keys, software downloads)

1. Select **Non-Technical** to be redirected to Broadcom's case management portal

Technical
Non-Technical

#####

×

# Access Denied

This feature has been disabled by your administrator.

#####

×

To prevent this message from showing again, please enable pop-up blockers for [support.broadcom.com](https://support.broadcom.com/)
or click Continue to proceed.

Continue

Top

* [Products](https://www.broadcom.com/products)
* [Solutions](https://www.broadcom.com/solutions)
* [Support and Services](https://www.broadcom.com/support)
* [Company](https://www.broadcom.com/)
* [How to Buy](https://www.broadcom.com/how-to-buy)

 Copyright © 2005-2024 Broadcom. All Rights Reserved. The term “Broadcom” refers to Broadcom Inc. and/or its subsidiaries.

* [Accessibility](https://www.broadcom.com/company/legal/accessibility)
* [Privacy](https://www.broadcom.com/company/legal/privacy)
* [Supplier Responsibility](https://www.broadcom.com/company/citizenship/supplier-responsibility)
* [Terms of Use](https://www.broadcom.com/company/legal/terms-of-use)
* [Site Map](https://www.broadcom.com/sitemap)



=== Content from docs.vmware.com_fca9f83b_20250115_175005.html ===


[![](/uicontent/images/VMware_by_Broadcom.png)Docs](/)

* ![](/uicontent/images/VMware_by_Broadcom_Gray-Black.png) Docs
  times
* [(current)](/)

* ![](/uicontent/images/nwMyLibrary.png)
  #####

  ![](/uicontent/images/icon-handshake.png)

  ![](/uicontent/images/icon-support.png)

  ![](/uicontent/images/icon-networking-bg.png)
  [VMware Communities](https://communities.vmware.com/)

  ![](/uicontent/images/icon-download-bg.png)

 This site will be decommissioned on January 30th 2025. After that date content will be available at [techdocs.broadcom.com](https://techdocs.broadcom.com/).

![](/uicontent/images/share-mylibrary.svg)

![](/uicontent/images/add-to-library-colorless.svg)

#

![](/uicontent/images/twitter.svg)
![](/uicontent/images/facebook.svg)
![](/uicontent/images/linkedin.svg)
![](/uicontent/images/weibo.svg)

![](/uicontent/images/pdf.svg)

![](/uicontent/images/feedback.svg)

![](/uicontent/images/edit.svg)

![](/uicontent/images/review.svg)

Twitter
Facebook
LinkedIn
微博

| ESXi 7.0 Update 1e | 15 FEB 2022 | Build 19324898  Check for additions and updates to these release notes. |
| --- |

## What's in the Release Notes

**IMPORTANT**: ESXi 7.0 Update 1e delivers fixes for CVE-2021-22040, CVE-2021-22041, CVE-2021-22042, and CVE-2021-22043. If you do not plan to update your environment to ESXi 7.0 Update 3c (build # 19193900), use ESXi 7.0 Update 1e to update your ESXi hosts of version 7.0 Update 1d (build # 17551050) and earlier with these security fixes. The supported update path from ESXi 7.0 Update 1e is to ESXi 7.0 Update 3c or later. The upgrade path from ESXi 6.5.x and ESXi 6.7.x to ESXi 7.0 Update 1e is not supported.

 **Warning**: Updates from ESXi 7.0 Update 1е to 7.0 Update 2, 7.0 Update 2a, 7.0 Update 2c, and 7.0 Update 2d might expose your vSphere system to security vulnerabilities, because this is considered a back-in-time update. For more information, see VMware knowledge base article [67077](https://kb.vmware.com/s/article/67077).

The release notes cover the following topics:

* [What's New](#whatsnew)
* [Earlier Releases of ESXi 7.0](#earlierreleases)
* [Patches Contained in this Release](#patches)
* [Resolved Issues](#resolvedissues)

## What's New

* ESXi 7.0 Update 1е resolves CVE-2021-22040, CVE-2021-22041, CVE-2021-22042, and CVE-2021-22043. For more information on these vulnerabilities and their impact on VMware products, see [VMSA-2022-0004](https://www.vmware.com/security/advisories/VMSA-2022-0004.html).

## Earlier Releases of ESXi 7.0

Features, resolved and known issues of ESXi are described in the release notes for each release. Release notes for earlier releases of ESXi 7.0 are:

* [VMware ESXi 7.0, Patch Release ESXi 7.0 Update 1d](https://docs.vmware.com/en/VMware-vSphere/7.0/rn/vsphere-esxi-70u1d.html)
* [VMware ESXi 7.0, Patch Release ESXi 7.0 Update 1c](https://docs.vmware.com/en/VMware-vSphere/7.0/rn/vsphere-esxi-70u1c.html)
* [VMware ESXi 7.0, Patch Release ESXi 7.0 Update 1b](https://docs.vmware.com/en/VMware-vSphere/7.0/rn/vsphere-esxi-70u1b.html)
* [VMware ESXi 7.0, Patch Release ESXi 7.0 Update 1a](https://docs.vmware.com/en/VMware-vSphere/7.0/rn/vsphere-esxi-70u1a.html)
* [VMware ESXi 7.0, Patch Release ESXi 7.0 Update 1](https://docs.vmware.com/en/VMware-vSphere/7.0/rn/vsphere-esxi-701-release-notes.html)
* [VMware ESXi 7.0, Patch Release ESXi 7.0b](https://docs.vmware.com/en/VMware-vSphere/7.0/rn/esxi70b.html)

For internationalization, compatibility, and open source components, see the [VMware vSphere 7.0 Release Notes](https://docs.vmware.com/en/VMware-vSphere/7.0/rn/vsphere-esxi-vcenter-server-70-release-notes.html).

## Patches Contained in This Release

This release of ESXi 7.0 Update 1e delivers the following patches:

**Build Details**

| **Download Filename**: | VMware-ESXi-7.0U1e-19324898-depot |
| --- | --- |
| **Build**: | 19324898 |
| **Download Size**: | 363.0 MB |
| **md5sum**: | 2f2ab94aba5d8c0c1150729c9855b32d |
| **sha256checksum**: | c5f34e4f544e5982b458ef2218e1880b5e524b27f1d9a1b39c756b736b9eebb6 |
| **Host Reboot Required**: | Yes |
| **Virtual Machine Migration or Shutdown Required**: | Yes |

For a table of build numbers and versions of VMware vCenter Server, see VMware knowledge base article [2143838](https://kb.vmware.com/s/article/2143838).

 **IMPORTANT**:

* Starting with vSphere 7.0, VMware uses components for packaging VIBs along with bulletins. The ESXi and `esx-update` bulletins are dependent on each other. Always include both in a single ESXi host patch baseline or include the rollup bulletin in the baseline to avoid failure during host patching.
* When patching ESXi hosts by using the vSphere Lifecycle Manager from a version earlier than ESXi 7.0 Update 1, it is strongly recommended to use the rollup bulletin in the patch baseline. If you cannot use the rollup bulletin, make sure to include all of the following packages in the patching baseline. If the following packages are not included in the baseline, the update operation fails:

  + VMware-vmkusb\_0.1-1vmw.701.0.0.16850804 or higher
  + VMware-vmkata\_0.1-1vmw.701.0.0.16850804 or higher
  + VMware-vmkfcoe\_1.0.0.2-1vmw.701.0.0.16850804 or higher
  + VMware-NVMeoF-RDMA\_1.0.1.2-1vmw.701.0.0.16850804 or higher

**Components**

| **Component** | **Bulletin ID** | **Category** | **Severity** |
| --- | --- | --- | --- |
| ESXi | ESXi\_7.0.1-0.35.19324898 | Security | Important |
| ESXi Install/Upgrade Component | esx-update\_7.0.1-0.35.19324898 | Security | Important |

**Rollup Bulletin**

This rollup bulletin contains the latest VIBs with all the fixes after the initial release of ESXi 7.0.

| **Bulletin ID** | **Category** | **Severity** |
| --- | --- | --- |
| ESXi70U1e-19324898 | Security | Critical |

**Image Profiles**

VMware patch and update releases contain general and critical image profiles. Application of the general release image profile applies to new bug fixes.

| **Image Profile Name** |
| --- |
| ESXi-7.0U1e-19324898-standard |
| ESXi-7.0U1e-19324898-no-tools |

**ESXi Image**

| Name and Version | Release Date | Category | Detail |
| --- | --- | --- | --- |
| ESXi70U1e-19324898 | 02/15/2022 | Security | Security |

For information about the individual components and bulletins, see the [Product Patches](https://my.vmware.com/group/vmware/patch#search) page and the [Resolved Issues](#resolvedissues) section.

## Patch Download and Installation

In vSphere 7.x, the Update Manager plug-in, used for administering vSphere Update Manager, is replaced with the Lifecycle Manager plug-in. Administrative operations for vSphere Update Manager are still available under the Lifecycle Manager plug-in, along with new capabilities for vSphere Lifecycle Manager.
 The typical way to apply patches to ESXi 7.x hosts is by using the vSphere Lifecycle Manager. For details, see [About vSphere Lifecycle Manager](https://docs.vmware.com/en/VMware-vSphere/7.0/com.vmware.vsphere-lifecycle-manager.doc/GUID-74295A37-E8BB-4EB9-BFBA-47B78F0C570D.html) and [vSphere Lifecycle Manager Baselines and Images](https://docs.vmware.com/en/VMware-vSphere/7.0/com.vmware.vsphere-lifecycle-manager.doc/GUID-9A20C2DA-F45F-4C9B-9D17-A89BCB62E6EF.html).
 You can also update ESXi hosts without using the Lifecycle Manager plug-in, and use an image profile instead. To do this, you must manually download the patch offline bundle ZIP file from the [VMware download](https://my.vmware.com/en/web/vmware/info/slug/datacenter_cloud_infrastructure/vmware_vsphere/7_0) page or the [Product Patches](https://my.vmware.com/group/vmware/patch#search) page and use the `esxcli software profile update` command.
 For more information, see the [Upgrading Hosts by Using ESXCLI Commands](https://docs.vmware.com/en/VMware-vSphere/7.0/com.vmware.esxi.upgrade.doc/GUID-A4301ADA-8E02-459D-BF9D-0AD308DA5325.html) and the [VMware ESXi Upgrade](https://docs.vmware.com/en/VMware-vSphere/7.0/com.vmware.esxi.upgrade.doc/GUID-65B5B313-3DBB-4490-82D2-A225446F4C99.html) guide.

## Resolved Issues

The resolved issues are grouped as follows.

* [ESXi\_7.0.1-0.35.19324898](#esxi_7-0-1-0-35-19324898-resolved)
* [esx-update\_7.0.1-0.35.19324898](#esx-update_7-0-1-0-35-19324898-resolved)
* [ESXi-70U1e-19324898-standard](#esxi-70u1e-19324898-standard-resolved)
* [ESXi-70U1e-19324898-no-tools](#esxi-70u1e-19324898-no-tools-resolved)
* [ESXi Image - ESXi70U1e-19324898](#esxi-image---esxi70u1e-19324898-resolved)

**ESXi\_7.0.1-0.35.19324898**

| **Patch Category** | Security |
| --- | --- |
| **Patch Severity** | Important |
| **Host Reboot Required** | Yes |
| **Virtual Machine Migration or Shutdown Required** | Yes |
| **Affected Hardware** | N/A |
| **Affected Software** | N/A |
| **VIBs Included** | * VMware\_bootbank\_esx-xserver\_7.0.1-0.35.19324898 * VMware\_bootbank\_esx-dvfilter-generic-fastpath\_7.0.1-0.35.19324898 * VMware\_bootbank\_gc\_7.0.1-0.35.19324898 * VMware\_bootbank\_esx-base\_7.0.1-0.35.19324898 * VMware\_bootbank\_crx\_7.0.1-0.35.19324898 * VMware\_bootbank\_native-misc-drivers\_7.0.1-0.35.19324898 * VMware\_bootbank\_cpu-microcode\_7.0.1-0.35.19324898 * VMware\_bootbank\_vsanhealth\_7.0.1-0.35.19324898 * VMware\_bootbank\_vdfs\_7.0.1-0.35.19324898 * VMware\_bootbank\_vsan\_7.0.1-0.35.19324898 |
| **PRs Fixed** | 2903398 |
| **CVE numbers** | CVE-2021-22040, CVE-2021-22041, CVE-2021-22042, CVE-2021-22043 |

The ESXi and esx-update bulletins are dependent on each other. Always include both in a single ESXi host patch baseline or include the rollup bulletin in the baseline to avoid failure during host patching.

 Updates the `esx-dvfilter-generic-fastpath, vsanhealth, vdfs, vsan, esx-base, crx, native-misc-drivers, esx-xserver, gc`and `cpu-microcode` VIBs to resolve the following issues:

* This release resolves CVE-2021-22040, CVE-2021-22041, CVE-2021-22042, and CVE-2021-22043. For more information on these vulnerabilities and their impact on VMware products, see [VMSA-2022-0004](https://www.vmware.com/security/advisories/VMSA-2022-0004.html).
* **PR 2903398: An NSX-T logical switch port, such as the Service Plane Forwarding (SPF) port, might unexpectedly be removed from a vSphere Distributed Switch (VDS)**

  An issue in the sync workflow might cause an NSX-T logical switch port, such as SPF, unexpectedly to drop from a VDS and VMs to lose network connectivity. In the backtrace, you see an error such as `Could not connect SPF port : Not found`.

  This issue is resolved in this release.

**esx-update\_7.0.1-0.35.19324898**

| **Patch Category** | Security |
| --- | --- |
| **Patch Severity** | Important |
| **Host Reboot Required** | Yes |
| **Virtual Machine Migration or Shutdown Required** | Yes |
| **Affected Hardware** | N/A |
| **Affected Software** | N/A |
| **VIBs Included** | * VMware\_bootbank\_loadesx\_7.0.1-0.35.19324898 * VMware\_bootbank\_esx-update\_7.0.1-0.35.19324898 |
| **PRs Fixed** | N/A |
| **CVE numbers** | N/A |

Updates the`loadesx`and `esx-update` VIBs.

**ESXi-70U1e-19324898-standard**

| **Profile Name** | ESXi-70U1e-19324898-standard |
| --- | --- |
| **Build** | For build information, see [Patches Contained in this Release](#patches). |
| **Vendor** | VMware, Inc. |
| **Release Date** | February 15, 2022 |
| **Acceptance Level** | PartnerSupported |
| **Affected Hardware** | N/A |
| **Affected Software** | N/A |
| ****Affected VIBs**** | * VMware\_bootbank\_esx-xserver\_7.0.1-0.35.19324898 * VMware\_bootbank\_esx-dvfilter-generic-fastpath\_7.0.1-0.35.19324898 * VMware\_bootbank\_gc\_7.0.1-0.35.19324898 * VMware\_bootbank\_esx-base\_7.0.1-0.35.19324898 * VMware\_bootbank\_crx\_7.0.1-0.35.19324898 * VMware\_bootbank\_native-misc-drivers\_7.0.1-0.35.19324898 * VMware\_bootbank\_cpu-microcode\_7.0.1-0.35.19324898 * VMware\_bootbank\_vsanhealth\_7.0.1-0.35.19324898 * VMware\_bootbank\_vdfs\_7.0.1-0.35.19324898 * VMware\_bootbank\_vsan\_7.0.1-0.35.19324898 * VMware\_bootbank\_loadesx\_7.0.1-0.35.19324898 * VMware\_bootbank\_esx-update\_7.0.1-0.35.19324898 |
| **PRs Fixed** | 2903398 |
| **Related CVE numbers** | CVE-2021-22040, CVE-2021-22041, CVE-2021-22042, CVE-2021-22043 |

* **This patch updates the following issues:**
  + This release resolves CVE-2021-22040, CVE-2021-22041, CVE-2021-22042, and CVE-2021-22043. For more information on these vulnerabilities and their impact on VMware products, see [VMSA-2022-0004](https://www.vmware.com/security/advisories/VMSA-2022-0004.html).
  + An issue in the sync workflow might cause an NSX-T logical switch port, such as SPF, unexpectedly to drop from a VDS and VMs to lose network connectivity. In the backtrace, you see an error such as `Could not connect SPF port : Not found`.

**ESXi-70U1e-19324898-no-tools**

| **Profile Name** | ESXi-70U1e-19324898-no-tools |
| --- | --- |
| **Build** | For build information, see [Patches Contained in this Release](https://rna.vmware.com/document/preview/html?documentId=2394#patches). |
| **Vendor** | VMware, Inc. |
| **Release Date** | February 15, 2021 |
| **Acceptance Level** | PartnerSupported |
| **Affected Hardware** | N/A |
| **Affected Software** | N/A |
| ****Affected VIBs**** | * VMware\_bootbank\_esx-xserver\_7.0.1-0.35.19324898 * VMware\_bootbank\_esx-dvfilter-generic-fastpath\_7.0.1-0.35.19324898 * VMware\_bootbank\_gc\_7.0.1-0.35.19324898 * VMware\_bootbank\_esx-base\_7.0.1-0.35.19324898 * VMware\_bootbank\_crx\_7.0.1-0.35.19324898 * VMware\_bootbank\_native-misc-drivers\_7.0.1-0.35.19324898 * VMware\_bootbank\_cpu-microcode\_7.0.1-0.35.19324898 * VMware\_bootbank\_vsanhealth\_7.0.1-0.35.19324898 * VMware\_bootbank\_vdfs\_7.0.1-0.35.19324898 * VMware\_bootbank\_vsan\_7.0.1-0.35.19324898 * VMware\_bootbank\_loadesx\_7.0.1-0.35.19324898 * VMware\_bootbank\_esx-update\_7.0.1-0.35.19324898 |
| **PRs Fixed** | 2903398 |
| **Related CVE numbers** | CVE-2021-22040, CVE-2021-22041, CVE-2021-22042, CVE-2021-22043 |

* **This patch updates the following issues:**
  + This release resolves CVE-2021-22040, CVE-2021-22041, CVE-2021-22042, and CVE-2021-22043. For more information on these vulnerabilities and their impact on VMware products, see [VMSA-2022-0004](https://www.vmware.com/security/advisories/VMSA-2022-0004.html).
  + An issue in the sync workflow might cause an NSX-T logical switch port, such as SPF, unexpectedly to drop from a VDS and VMs to lose network connectivity. In the backtrace, you see an error such as `Could not connect SPF port : Not found`.

**ESXi Image - ESXi70U1e-19324898**

| **Name** | ESXi |
| --- | --- |
| **Version** | 70U1e-19324898 |
| **Release Date** | February 15, 2022 |
| **Category** | Security |
| **Affected Components​** | * ESXi * ESXi Install/Upgrade Component |
| **PRs Fixed** | 2903398 |
| **Related CVE numbers** | CVE-2021-22040, CVE-2021-22041, CVE-2021-22042, CVE-2021-22043 |

## Known Issues from Earlier Releases

To view a list of previous known issues, click here.

The earlier known issues are grouped as follows.

* [Installation, Upgrade, and Migration Issues](#installation,-upgrade,-and-migration-issues-earlier-known)
* [Security Features Issues](#security-features-issues-earlier-known)
* [Networking Issues](#networking-issues-earlier-known)
* [Storage Issues](#storage-issues-earlier-known)
* [vCenter Server and vSphere Client Issues](#vcenter-server-and-vsphere-client-issues-earlier-known)
* [Virtual Machine Management Issues](#virtual-machine-management-issues-earlier-known)
* [vSphere HA and Fault Tolerance Issues](#vsphere-ha-and-fault-tolerance-issues-earlier-known)
* [vSphere Lifecycle Manager Issues](#vsphere-lifecycle-manager-issues-earlier-known)
* [Miscellaneous Issues](#miscellaneous-issues-earlier-known)

**Installation, Upgrade, and Migration Issues**

* **The vCenter Upgrade/Migration pre-checks fail with "Unexpected error 87"**

  The vCenter Server Upgrade/Migration pre-checks fail when the Security Token Service (STS) certificate does not contain a Subject Alternative Name (SAN) field. This situation occurs when you have replaced the vCenter 5.5 Single Sign-On certificate with a custom certificate that has no SAN field, and you attempt to upgrade to vCenter Server 7.0. The upgrade considers the STS certificate invalid and the pre-checks prevent the upgrade process from continuing.

  Workaround: Replace the STS certificate with a valid certificate that contains a SAN field then proceed with the vCenter Server 7.0 Upgrade/Migration.
* **Problems upgrading to vSphere 7.0 with pre-existing CIM providers**

  After upgrade, previously installed 32-bit CIM providers stop working because ESXi requires 64-bit CIM providers. Customers may lose management API functions related to CIMPDK, NDDK (native DDK), HEXDK, VAIODK (IO filters), and see errors related to **uwglibc** dependency.
   The syslog reports module missing, "32 bit shared libraries not loaded."

  Workaround: There is no workaround. The fix is to download new 64-bit CIM providers from your vendor.
* **Installation of 7.0 Update 1 drivers on ESXi 7.0 hosts might fail**

  You cannot install drivers applicable to ESXi 7.0 Update 1 on hosts that run ESXi 7.0 or 7.0b.
   The operation fails with an error, such as:
   `VMW_bootbank_qedrntv_3.40.4.0-12vmw.701.0.0.xxxxxxx requires vmkapi_2_7_0_0, but the requirement cannot be satisfied within the ImageProfile. ​
   Please refer to the log file for more details.`

  Workaround: Update the ESXi host to 7.0 Update 1. Retry the driver installation.
* **Upgrades to ESXi 7.x from 6.5x and 6.7.0 by using ESXCLI might fail due to a space limitation**

  Upgrades to ESXi 7.x from 6.5.x and 6.7.0 by using the `esxcli software profile update` or `esxcli software profile install` ESXCLI commands might fail, because the ESXi bootbank might be less than the size of the image profile. In the ESXi Shell or the PowerCLI shell, you see an error such as:
    `[InstallationError]
   The pending transaction requires 244 MB free space, however the maximum supported size is 239 MB.
   Please refer to the log file for more details.`
   The issue also occurs when you attempt an ESXi host upgrade by using the ESXCLI commands `esxcli software vib update` or `esxcli software vib install`.

  Workaround: You can perform the upgrade in two steps, by using the `esxcli software profile update` command to update ESXi hosts to ESXi 6.7 Update 1 or later, and then update to 7.0 Update 1c. Alternatively, you can run an upgrade by using an ISO image and the vSphere Lifecycle Manager.
* **Smart Card and RSA SecurID authentication might stop working after upgrading to vCenter Server 7.0**

  If you have configured vCenter Server for either Smart Card or RSA SecurID authentication, see the VMware knowledge base article at <https://kb.vmware.com/s/article/78057> before starting the vSphere 7.0 upgrade process. If you do not perform the workaround as described in the KB, you might see the following error messages and Smart Card or RSA SecurID authentication does not work.

  "Smart card authentication may stop working. Smart card settings may not be preserved, and smart card authentication may stop working."

  or

  "RSA SecurID authentication may stop working. RSA SecurID settings may not be preserved, and RSA SecurID authentication may stop working."

  Workaround: Before upgrading to vSphere 7.0, see the VMware knowledge base article at <https://kb.vmware.com/s/article/78057>.
* **Upgrading a vCenter Server with an external Platform Services Controller from 6.7u3 to 7.0 fails with VMAFD error**

  When you upgrade a vCenter Server deployment using an external Platform Services Controller, you converge the Platform Services Controller into a vCenter Server appliance. If the upgrade fails with the error `install.vmafd.vmdir_vdcpromo_error_21`, the VMAFD firstboot process has failed. The VMAFD firstboot process copies the VMware Directory Service Database (data.mdb) from the source Platform Services Controller and replication partner vCenter Server appliance.

  Workaround: Disable TCP Segmentation Offload (TSO) and Generic Segmentation Offload (GSO) on the Ethernet adapter of the source Platform Services Controller or replication partner vCenter Server appliance before upgrading a vCenter Server with an external Platform Services Controller. See Knowledge Base article: https://kb.vmware.com/s/article/74678
* **Upgrading vCenter Server using the CLI incorrectly preserves the Transport Security Layer (TLS) configuration for the vSphere Authentication Proxy service**

  If the vSphere Authentication Proxy service (`vmcam`) is configured to use a particular TLS protocol other than the default TLS 1.2 protocol, this configuration is preserved during the CLI upgrade process. By default, vSphere supports the TLS 1.2 encryption protocol. If you must use the TLS 1.0 and TLS 1.1 protocols to support products or services that do not support TLS 1.2, use the TLS Configurator Utility to enable or disable different TLS protocol versions.

  Workaround: Use the TLS Configurator Utility to configure the `vmcam` port. To learn how to manage TLS protocol configuration and use the TLS Configurator Utility, see the *VMware Security* documentation.
* **Smart card and RSA SecurID settings may not be preserved during vCenter Server upgrade**

  Authentication using RSA SecurID will not work after upgrading to vCenter Server 7.0. An error message will alert you to this issue when attempting to login using your RSA SecurID login.

  Workaround: Reconfigure the smart card or RSA SecureID.
* **Migration of vCenter Server for Windows to vCenter Server appliance 7.0 fails with network error message**

  Migration of vCenter Server for Windows to vCenter Server appliance 7.0 fails with the error message `IP already exists in the network`. This prevents the migration process from configuring the network parameters on the new vCenter Server appliance. For more information, examine the log file: `/var/log/vmware/upgrade/UpgradeRunner.log`

  Workaround:

  1. Verify that all Windows Updates have been completed on the source vCenter Server for Windows instance, or disable automatic Windows Updates until after the migration finishes.
  2. Retry the migration of vCenter Server for Windows to vCenter Server appliance 7.0.
* **When you configure the number of virtual functions for an SR-IOV device by using the max\_vfs module parameter, the changes might not take effect**

  In vSphere 7.0, you can configure the number of virtual functions for an SR-IOV device by using the Virtual Infrastructure Management (VIM) API, for example, through the vSphere Client. The task does not require reboot of the ESXi host. After you use the VIM API configuration, if you try to configure the number of SR-IOV virtual functions by using the `max_vfs` module parameter, the changes might not take effect because they are overridden by the VIM API configuration.

  Workaround: None. To configure the number of virtual functions for an SR-IOV device, use the same method every time. Use the VIM API or use the `max_vfs` module parameter and reboot the ESXi host.
* **Upgraded vCenter Server appliance instance does not retain all the secondary networks (NICs) from the source instance**

  During a major upgrade, if the source instance of the vCenter Server appliance is configured with multiple secondary networks other than the VCHA NIC, the target vCenter Server instance will not retain secondary networks other than the VCHA NIC. If the source instance is configured with multiple NICs that are part of DVS port groups, the NIC configuration will not be preserved during the upgrade. Configurations for vCenter Server appliance instances that are part of the standard port group will be preserved.

  Workaround: None. Manually configure the secondary network in the target vCenter Server appliance instance.
* **After upgrading or migrating a vCenter Server with an external Platform Services Controller, users authenticating using Active Directory lose access to the newly upgraded vCenter Server instance**

  After upgrading or migrating a vCenter Server with an external Platform Services Controller, if the newly upgraded vCenter Server is not joined to an Active Directory domain, users authenticating using Active Directory will lose access to the vCenter Server instance.

  Workaround: Verify that the new vCenter Server instance has been joined to an Active Directory domain. See Knowledge Base article: https://kb.vmware.com/s/article/2118543
* **Migrating a vCenter Server for Windows with an external Platform Services Controller using an Oracle database fails**

  If there are non-ASCII strings in the Oracle events and tasks table the migration can fail when exporting events and tasks data. The following error message is provided: UnicodeDecodeError

  Workaround: None.
* **After an ESXi host upgrade, a Host Profile compliance check shows non-compliant status while host remediation tasks fail**

  The non-compliant status indicates an inconsistency between the profile and the host.

  This inconsistency might occur because ESXi 7.0 does not allow duplicate claim rules, but the profile you use contains duplicate rules. For example, if you attempt to use the Host Profile that you extracted from the host before upgrading ESXi 6.5 or ESXi 6.7 to version 7.0 and the Host Profile contains any duplicate claim rules of system default rules, you might experience the problems.

  Workaround:

  1. Remove any duplicate claim rules of the system default rules from the Host Profile document.
  2. Check the compliance status.
  3. Remediate the host.
  4. If the previous steps do not help, reboot the host.
* **Error message displays in the vCenter Server Management Interface**

  After installing or upgrading to vCenter Server 7.0, when you navigate to the Update panel within the vCenter Server Management Interface, the error message "Check the URL and try again" displays. The error message does not prevent you from using the functions within the Update panel, and you can view, stage, and install any available updates.

  Workaround: None.

**Security Features Issues**

* **Encrypted virtual machine fails to power on when HA-enabled Trusted Cluster contains an unattested host**

  In VMware® vSphere Trust Authority™, if you have enabled HA on the Trusted Cluster and one or more hosts in the cluster fails attestation, an encrypted virtual machine cannot power on.

  Workaround: Either remove or remediate all hosts that failed attestation from the Trusted Cluster.
* **Encrypted virtual machine fails to power on when DRS-enabled Trusted Cluster contains an unattested host**

  In VMware® vSphere Trust Authority™, if you have enabled DRS on the Trusted Cluster and one or more hosts in the cluster fails attestation, DRS might try to power on an encrypted virtual machine on an unattested host in the cluster. This operation puts the virtual machine in a locked state.

  Workaround: Either remove or remediate all hosts that failed attestation from the Trusted Cluster.
* **Migrating or cloning encrypted virtual machines across vCenter Server instances fails when attempting to do so using the vSphere Client**

  If you try to migrate or clone an encrypted virtual machine across vCenter Server instances using the vSphere Client, the operation fails with the following error message: "The operation is not allowed in the current state."

  Workaround: You must use the vSphere APIs to migrate or clone encrypted virtual machines across vCenter Server instances.

**Networking Issues**

* **Reduced throughput in networking performance on Intel 82599/X540/X550 NICs**

  The new queue-pair feature added to ixgben driver to improve networking performance on Intel 82599EB/X540/X550 series NICs might reduce throughput under some workloads in vSphere 7.0 as compared to vSphere 6.7.

  Workaround: To achieve the same networking performance as vSphere 6.7, you can disable the queue-pair with a module parameter. To disable the queue-pair, run the command:

  `# esxcli system module parameters set -p "QPair=0,0,0,0..." -m ixgben`

  After running the command, reboot.
* **One or more I/O devices do not generate interrupts when the AMD IOMMU is in use**

  If the I/O devices on your ESXi host provide more than a total of 512 distinct interrupt sources, some sources are erroneously assigned an interrupt-remapping table entry (IRTE) index in the AMD IOMMU that is greater than the maximum value. Interrupts from such a source are lost, so the corresponding I/O device behaves as if interrupts are disabled.

  Workaround: Use the ESXCLI command `esxcli system settings kernel set -s iovDisableIR -v true` to disable the AMD IOMMU interrupt remapper. Reboot the ESXi host so that the command takes effect.
* **High throughput virtual machines may experience degradation in network performance when Network I/O Control (NetIOC) is enabled**

  Virtual machines requiring high network throughput can experience throughput degradation when upgrading from vSphere 6.7 to vSphere 7.0 with NetIOC enabled.

  Workaround: Adjust the `ethernetx.ctxPerDev` setting to enable multiple worlds.
* **IPv6 traffic fails to pass through VMkernel ports using IPsec**

  When you migrate VMkernel ports from one port group to another, IPv6 traffic does not pass through VMkernel ports using IPsec.

  Workaround: Remove the IPsec security association (SA) from the affected server, and then reapply the SA. To learn how to set and remove an IPsec SA, see the *vSphere Security* documentation.
* **Higher ESX network performance with a portion of CPU usage increase**

  ESX network performance may increase with a portion of CPU usage.

  Workaround: Remove and add the network interface with only 1 rx dispatch queue. For example:

  `esxcli network ip interface remove --interface-name=vmk1`

  `esxcli network ip interface add --interface-name=vmk1 --num-rxqueue=1`
* **VM might lose Ethernet traffic after hot-add, hot-remove or storage vMotion**

  A VM might stop receiving Ethernet traffic after a hot-add, hot-remove or storage vMotion. This issue affects VMs where the uplink of the VNIC has SR-IOV enabled. PVRDMA virtual NIC exhibits this issue when the uplink of the virtual network is a Mellanox RDMA capable NIC and RDMA namespaces are configured.

  Workaround: You can hot-remove and hot-add the affected Ethernet NICs of the VM to restore traffic. On Linux guest operating systems, restarting the network might also resolve the issue. If these workarounds have no effect, you can reboot the VM to restore network connectivity.
* **Change of IP address for a VCSA deployed with static IP address requires that you create the DNS records in advance**

  With the introduction of the DDNS, the DNS record update only works for VCSA deployed with DHCP configured networking. While changing the IP address of the vCenter server via VAMI, the following error is displayed:

  The specified IP address does not resolve to the specified hostname.

  Workaround: There are two possible workarounds.

  1. Create an additional DNS entry with the same FQDN and desired IP address. Log in to the VAMI and follow the steps to change the IP address.
  2. Log in to the VCSA using ssh. Execute the following script:

     `./opt/vmware/share/vami/vami_config_net`

     Use option 6 to change the IP adddress of eth0. Once changed, execute the following script:

     `./opt/likewise/bin/lw-update-dns`

     Restart all the services on the VCSA to update the IP information on the DNS server.
* **It may take several seconds for the NSX Distributed Virtual Port Group (NSX DVPG) to be removed after deleting the corresponding logical switch in NSX Manager.**

  As the number of logical switches increases, it may take more time for the NSX DVPG in vCenter Server to be removed after deleting the corresponding logical switch in NSX Manager. In an environment with 12000 logical switches, it takes approximately 10 seconds for an NSX DVPG to be deleted from vCenter Server.

  Workaround: None.
* **Hostd runs out of memory and fails if a large number of NSX Distributed Virtual port groups are created.**

  In vSphere 7.0, NSX Distributed Virtual port groups consume significantly larger amounts of memory than opaque networks. For this reason, NSX Distributed Virtual port groups can not support the same scale as an opaque network given the same amount of memory.

  Workaround:To support the use of NSX Distributed Virtual port groups, increase the amount of memory in your ESXi hosts. If you verify that your system has adequate memory to support your VMs, you can directly increase the memory of `hostd` using the following command.

  `localcli --plugin-dir /usr/lib/vmware/esxcli/int/ sched group setmemconfig --group-path host/vim/vmvisor/hostd --units mb --min 2048 --max 2048`

  Note that this will cause `hostd` to use memory normally reserved for your environment's VMs. This may have the affect of reducing the number of VMs your ESXi host can support.
* **DRS may incorrectly launch vMotion if the network reservation is configured on a VM**

  If the network reservation is configured on a VM, it is expected that DRS only migrates the VM to a host that meets the specified requirements. In a cluster with NSX transport nodes, if some of the transport nodes join the transport zone by NSX-T Virtual Distributed Switch (N-VDS), and others by vSphere Distributed Switch (VDS) 7.0, DRS may incorrectly launch vMotion. You might encounter this issue when:

  + The VM connects to an NSX logical switch configured with a network reservation.
  + Some transport nodes join transport zone using N-VDS, and others by VDS 7.0, or, transport nodes join the transport zone through different VDS 7.0 instances.

  Workaround: Make all transport nodes join the transport zone by N-VDS or the same VDS 7.0 instance.
* **When adding a VMkernel NIC (vmknic) to an NSX portgroup, vCenter Server reports the error "Connecting VMKernel adapter to a NSX Portgroup on a Stateless host is not a supported operation. Please use Distributed Port Group instead."**
  + For stateless ESXi on Distributed Virtual Switch (DVS), the vmknic on a NSX port group is blocked. You must instead use a Distributed Port Group.
  + For stateful ESXi on DVS, vmknic on NSX port group is supported, but vSAN may have an issue if it is using vmknic on a NSX port group.

  Workaround: Use a Distributed Port Group on the same DVS.
* **Enabling SRIOV from vCenter for QLogic 4x10GE QL41164HFCU CNA might fail**

  If you navigate to the **Edit Settings** dialog for physical network adapters and attempt to enable SR-IOV, the operation might fail when using QLogic 4x10GE QL41164HFCU CNA. Attempting to enable SR-IOV might lead to a network outage of the ESXi host.

  Workaround: Use the following command on the ESXi host to enable SRIOV:

  `esxcfg-module`
* **vCenter Server fails if the hosts in a cluster using Distributed Resource Scheduler (DRS) join NSX-T networking by a different Virtual Distributed Switch (VDS) or combination of NSX-T Virtual Distributed Switch (NVDS) and VDS**

  In vSphere 7.0, when using NSX-T networking on vSphere VDS with a DRS cluster, if the hosts do not join the NSX transport zone by the same VDS or NVDS, it can cause vCenter Server to fail.

  Workaround: Have hosts in a DRS cluster join the NSX transport zone using the same VDS or NVDS.

**Storage Issues**

* **VMFS datastores are not mounted automatically after disk hot remove and hot insert on HPE Gen10 servers with SmartPQI controllers**

  When SATA disks on HPE Gen10 servers with SmartPQI controllers without expanders are hot removed and hot inserted back to a different disk bay of the same machine, or when multiple disks are hot removed and hot inserted back in a different order, sometimes a new local name is assigned to the disk. The VMFS datastore on that disk appears as a snapshot and will not be mounted back automatically because the device name has changed.

  Workaround: None. SmartPQI controller does not support unordered hot remove and hot insert operations.
* **ESXi might terminate I/O to NVMeOF devices due to errors on all active paths**

  Occasionally, all active paths to NVMeOF device register I/O errors due to link issues or controller state. If the status of one of the paths changes to Dead, the High Performance Plug-in (HPP) might not select another path if it shows high volume of errors. As a result, the I/O fails.

  Workaround: Disable the configuration option **/Misc/HppManageDegradedPaths** to unblock the I/O.
* **VOMA check on NVMe based VMFS datastores fails with error**

  VOMA check is not supported for NVMe based VMFS datastores and will fail with the error:

  ```
  ERROR: Failed to reserve device. Function not implemented
  ```

  Example:

  ```
  # voma -m vmfs -f check -d /vmfs/devices/disks/: <partition#>
  Running VMFS Checker version 2.1 in check mode
  Initializing LVM metadata, Basic Checks will be done

  Checking for filesystem activity
  Performing filesystem liveness check..|Scanning for VMFS-6 host activity (4096 bytes/HB, 1024 HBs).
  ERROR: Failed to reserve device. Function not implemented
  Aborting VMFS volume check
  VOMA failed to check device : General Error
  ```

  Workaround: None. If you need to analyse VMFS metadata, collect it using the `-l` option, and pass to VMware customer support. The command for collecting the dump is:

  ```
  voma -l -f dump -d /vmfs/devices/disks/:<partition#>
  ```
* **Using the VM reconfigure API to attach an encrypted First Class Disk to an encrypted virtual machine might fail with error**

  If an FCD and a VM are encrypted with different crypto keys, your attempts to attach the encrypted FCD to the encrypted VM using the `VM reconfigure API` might fail with the error message:

  ```
  Cannot decrypt disk because key or password is incorrect.
  ```

  Workaround: Use the `attachDisk API` rather than the `VM reconfigure API` to attach an encrypted FCD to an encrypted VM.
* **ESXi host might get in non responding state if a non-head extent of its spanned VMFS datastore enters the Permanent Device Loss (PDL) state**

  This problem does not occur when a non-head extent of the spanned VMFS datastore fails along with the head extent. In this case, the entire datastore becomes inaccessible and no longer allows I/Os.

  In contrast, when only a non-head extent fails, but the head extent remains accessible, the datastore heartbeat appears to be normal. And the I/Os between the host and the datastore continue. However, any I/Os that depend on the failed non-head extent start failing as well. Other I/O transactions might accumulate while waiting for the failing I/Os to resolve, and cause the host to enter the non responding state.

  Workaround: Fix the PDL condition of the non-head extent to resolve this issue.
* **After recovering from APD or PDL conditions, VMFS datastore with enabled support for clustered virtual disks might remain inaccessible**

  You can encounter this problem only on datastores where the clustered virtual disk support is enabled. When the datastore recovers from an All Paths Down (APD) or Permanent Device Loss (PDL) condition, it remains inaccessible. The VMkernel log might show multiple `SCSI3 reservation conflict`  messages similar to the following:

  `2020-02-18T07:41:10.273Z cpu22:1001391219)ScsiDeviceIO: vm 1001391219: SCSIDeviceCmdCompleteCB:2972: Reservation conflict retries 544 for command 0x45ba814b8340 (op: 0x89) to device "naa.624a9370b97601e346f64ba900024d53"`

  The problem can occur because the ESXi host participating in the cluster loses SCSI reservations for the datastore and cannot always reacquire them automatically after the datastore recovers.

  Workaround: Manually register the reservation using the following command:

  `vmkfstools -L registerkey /vmfs/devices/disks/<device name>`

  where the `<device name>` is the name of the device on which the datastore is created.
* **Virtual NVMe Controller is the default disk controller for Windows 10 guest operating systems**

  The Virtual NVMe Controller is the default disk controller for the following guest operating systems when using Hardware Version 15 or later:

  Windows 10
   Windows Server 2016
   Windows Server 2019

  Some features might not be available when using a Virtual NVMe Controller. For more information, see <https://kb.vmware.com/s/article/2147714>

  **Note**: Some clients use the previous default of LSI Logic SAS. This includes ESXi host client and PowerCLI.

  Workaround: If you need features not available on Virtual NVMe, switch to VMware Paravirtual SCSI (PVSCSI) or LSI Logic SAS. For information on using VMware Paravirtual SCSI (PVSCSI), see <https://kb.vmware.com/s/article/1010398>
* **After an ESXi host upgrade to vSphere 7.0, presence of duplicate core claim rules might cause unexpected behavior**

  Claim rules determine which multipathing plugin, such as NMP, HPP, and so on, owns paths to a particular storage device. ESXi 7.0 does not support duplicate claim rules. However, the ESXi 7.0 host does not alert you if you add duplicate rules to the existing claim rules inherited through an upgrade from a legacy release. As a result of using duplicate rules, storage devices might be claimed by unintended plugins, which can cause unexpected outcome.

  Workaround: Do not use duplicate core claim rules. Before adding a new claim rule, delete any existing matching claim rule.
* **A CNS query with the compliance status filter set might take unusually long time to complete**

  The CNS QueryVolume API enables you to obtain information about the CNS volumes, such as volume health and compliance status. When you check the compliance status of individual volumes, the results are obtained quickly. However, when you invoke the CNS QueryVolume API to check the compliance status of multiple volumes, several tens or hundreds, the query might perform slowly.

  Workaround: Avoid using bulk queries. When you need to get compliance status, query one volume at a time or limit the number of volumes in the query API to 20 or fewer. While using the query, avoid running other CNS operations to get the best performance.
* **A VMFS datastore backed by an NVMe over Fabrics namespace or device might become permanently inaccessible after recovering from an APD or PDL failure**

  If a VMFS datastore on an ESXi host is backed by an NVMe over Fabrics namespace or device, in case of an all paths down (APD) or permanent device loss (PDL) failure, the datastore might be inaccessible even after recovery. You cannot access the datastore from either the ESXi host or the vCenter Server system.

  Workaround: To recover from this state, perform a rescan on a host or cluster level. For more information, see [Perform Storage Rescan](https://docs.vmware.com/en/VMware-vSphere/7.0/com.vmware.vsphere.storage.doc/GUID-D0595EB3-D20C-4951-88EF-5AFB0BF2398D.html).
* **Deleted CNS volumes might temporarily appear as existing in the CNS UI**

  After you delete an FCD disk that backs a CNS volume, the volume might still show up as existing in the CNS UI. However, your attempts to delete the volume fail. You might see an error message similar to the following:
   `The object or item referred to could not be found`.

  Workaround: The next full synchronization will resolve the inconsistency and correctly update the CNS UI.
* **Attempts to attach multiple CNS volumes to the same pod might occasionally fail with an error**

  When you attach multiple volumes to the same pod simultaneously, the attach operation might occasionally choose the same controller slot. As a result, only one of the operations succeeds, while other volume mounts fail.

  Workaround: After Kubernetes retries the failed operation, the operation succeeds if a controller slot is available on the node VM.
* **Under certain circumstances, while a CNS operation fails, the task status appears as successful in the vSphere Client**

  This might occur when, for example, you use an incompliant storage policy to create a CNS volume. The operation fails, while the vSphere Client shows the task status as successful.

  Workaround: The successful task status in the vSphere Client does not guarantee that the CNS operation succeeded. To make sure the operation succeeded, verify its results.
* **Unsuccessful delete operation for a CNS persistent volume might leave the volume undeleted on the vSphere datastore**

  This issue might occur when the CNS Delete API attempts to delete a persistent volume that is still attached to a pod. For example, when you delete the Kubernetes namespace where the pod runs. As a result, the volume gets cleared from CNS and  the CNS query operation does not return the volume. However, the volume continues to reside on the datastore and cannot be deleted through the repeated CNS Delete API operations.

  Workaround: None.

**vCenter Server and vSphere Client Issues**

* **Vendor providers go offline after a PNID change​**

  When you change the vCenter IP address (PNID change), the registered vendor providers go offline.

  Workaround: Re-register the vendor providers.
* **Cross vCenter migration of a virtual machine fails with an error**

  When you use cross vCenter vMotion to move a VM's storage and host to a different vCenter server instance, you might receive the error `The operation is not allowed in the current state.`

  This error appears in the UI wizard after the Host Selection step and before the Datastore Selection step, in cases where the VM has an assigned storage policy containing host-based rules such as encryption or any other IO filter rule.

  Workaround: Assign the VM and its disks to a storage policy without host-based rules. You might need to decrypt the VM if the source VM is encrypted. Then retry the cross vCenter vMotion action.
* **Storage Sensors information in Hardware Health tab shows incorrect values on vCenter UI, host UI, and MOB**

  When you navigate to **Host > Monitor > Hardware Health > Storage Sensors** on vCenter UI, the storage information displays either incorrect or unknown values. The same issue is observed on the host UI and the MOB path “runtime.hardwareStatusInfo.storageStatusInfo” as well.

  Workaround: None.
* **vSphere UI host advanced settings shows the current product locker location as empty with an empty default**

  vSphere UI host advanced settings shows the current product locker location as empty with an empty default. This is inconsistent as the actual product location `symlink` is created and valid. This causes confusion to the user. The default cannot be corrected from UI.

  Workaround: User can use the esxcli command on the host to correct the current product locker location default as below.

  1. Remove the existing Product Locker Location setting with: `"esxcli system settings advanced remove -o ProductLockerLocation"`

  2. Re-add the Product Locker Location setting with the appropriate default:

  2.a.  If the ESXi is a full installation, the default value is `"/locker/packages/vmtoolsRepo" export PRODUCT_LOCKER_DEFAULT="/locker/packages/vmtoolsRepo"`

  2.b. If the ESXi is a PXEboot configuration such as autodeploy, the default value is: "`/vmtoolsRepo" export PRODUCT_LOCKER_DEFAULT="/vmtoolsRepo"`

  Run the following command to automatically figure out the location: `export PRODUCT_LOCKER_DEFAULT=`readlink /productLocker``

  Add the setting: `esxcli system settings advanced add -d "Path to VMware Tools repository" -o ProductLockerLocation -t string -s $PRODUCT_LOCKER_DEFAULT`

  You can combine all the above steps in step 2 by issuing the single command:

  `esxcli system settings advanced add -d "Path to VMware Tools repository" -o ProductLockerLocation -t string -s `readlink /productLocker``
* **Linked Software-Defined Data Center (SDDC) vCenter Server instances appear in the on-premises vSphere Client if a vCenter Cloud Gateway is linked to the SDDC.**

  When a vCenter Cloud Gateway is deployed in the same environment as an on-premises vCenter Server, and linked to an SDDC, the SDDC vCenter Server will appear in the on-premises vSphere Client. This is unexpected behavior and the linked SDDC vCenter Server should be ignored. All operations involving the linked SDDC vCenter Server should be performed on the vSphere Client running within the vCenter Cloud Gateway.

  Workaround: None.

**Virtual Machine Management Issues**

* **Virtual machines with enabled AMD Secure Encrypted Virtualization-Encrypted State (SEV-ES) cannot create Virtual Machine Communication Interface (VMCI) sockets**

  Performance and functionality of features that require VMCI might be affected on virtual machines with enabled AMD SEV-ES, because such virtual machines cannot create VMCI sockets.

  Workaround: None.
* **The postcustomization section of the customization script runs before the guest customization**

  When you run the guest customization script for a Linux guest operating system, the `precustomization` section of the customization script that is defined in the customization specification runs before the guest customization and the `postcustomization` section runs after that. If you enable Cloud-Init in the guest operating system of a virtual machine, the `postcustomization`section runs before the customization due to a known issue in Cloud-Init.

  Workaround: Disable Cloud-Init and use the standard guest customization.
* **Group migration operations in vSphere vMotion, Storage vMotion, and vMotion without shared storage fail with error**

  When you perform group migration operations on VMs with multiple disks and multi-level snapshots, the operations might fail with the error `com.vmware.vc.GenericVmConfigFault Failed waiting for data. Error 195887167. Connection closed by remote host, possibly due to timeout.`

  Workaround: Retry the migration operation on the failed VMs one at a time.
* **Deploying an OVF or OVA template from a URL fails with a 403 Forbidden error**

  The URLs that contain an HTTP query parameter are not supported. For example, `http://webaddress.com?file=abc.ovf` or the Amazon pre-signed S3 URLs.

  Workaround: Download the files and deploy them from your local file system.
* **Importing or deploying local OVF files containing non-ASCII characters in their name might fail with an error**

  When you import local `.ovf` files containing non-ASCII characters in their name, you might receive `400 Bad Request Error`. When you use such `.ovf` files to deploy a virtual machine in the vSphere Client, the deployment process stops at 0%. As a result, you might receive `400 Bad Request Error`or `500 Internal Server Error`.

  Workaround:

  1. Remove the non-ASCII characters from the `.ovf` and `.vmdk` file names.
     + To edit the .`ovf` file, open it with a text editor.
     + Search the non-ASCII `.vmdk` file name and change it to ASCII.
  2. Import or deploy the saved files again.
* **The third level of nested objects in a virtual machine folder is not visible**

  Perform the following steps:

  1. Navigate to a data center and create a virtual machine folder.
  2. In the virtual machine folder, create a nested virtual machine folder.
  3. In the second folder, create another nested virtual machine, virtual machine folder, vApp, or VM Template.

  As a result, from the VMs and Templates inventory tree you cannot see the objects in the third nested folder.

  Workaround: To see the objects in the third nested folder, navigate to the second nested folder and select the VMs tab.

**vSphere HA and Fault Tolerance Issues**

* **VMs in a cluster might be orphaned after recovering from storage inaccessibility such as a cluster wide APD**

  Some VMs might be in orphaned state after cluster wide APD recovers, even if HA and VMCP are enabled on the cluster.

  This issue might be encountered when the following conditions occur simultaneously:

  + All hosts in the cluster experience APD and do not recover until VMCP timeout is reached.
  + HA primary initiates failover due to APD on a host.
  + Power on API during HA failover fails due to one of the following:
    - APD across the same host
    - Cascading APD across the entire cluster
    - Storage issues
    - Resource unavailability
  + FDM unregistration and VCs steal VM logic might initiate during a window where FDM has not unregistered the failed VM and VC's host synchronization responds that multiple hosts are reporting the same VM. Both FDM and VC unregister the different registered copies of the same VM from different hosts, causing the VM to be orphaned.

  Workaround: You must unregister and reregister the orphaned VMs manually within the cluster after the APD recovers.

  If you do not manually reregister the orphaned VMs, HA attempts failover of the orphaned VMs, but it might take between 5 to 10 hours depending on when APD recovers.

  The overall functionality of the cluster is not affected in these cases and HA will continue to protect the VMs. This is an anomaly in what gets displayed on VC for the duration of the problem.

**vSphere Lifecycle Manager Issues**

* **You cannot enable NSX-T on a cluster that is already enabled for managing image setup and updates on all hosts collectively**

  NSX-T is not compatible with the vSphere Lifecycle Manager functionality for image management. When you enable a cluster for image setup and updates on all hosts in the cluster collectively, you cannot enable NSX-T on that cluster. However, you can deploy NSX Edges to this cluster.

  Workaround: Move the hosts to a new cluster that you can manage with baselines and enable NSX-T on that new cluster.
* **vSphere Lifecycle Manager and vSAN File Services cannot be simultaneously enabled on a vSAN cluster in vSphere 7.0 release**

  If vSphere Lifecycle Manager is enabled on a cluster, vSAN File Services cannot be enabled on the same cluster and vice versa. In order to enable vSphere Lifecycle Manager on a cluster, which has VSAN File Services enabled already, first disable vSAN File Services and retry the operation. Please note that if you transition to a cluster that is managed by a single image, vSphere Lifecycle Manager cannot be disabled on that cluster.

  Workaround: None.
* **ESXi 7.0 hosts cannot be added to а cluster that you manage with a single image by using vSphere Auto Deploy**

  Attempting to add ESXi hosts to а cluster that you manage with a single image by using the "Add to Inventory" workflow in vSphere Auto Deploy fails. The failure occurs because no patterns are matched in an existing Auto Deploy ruleset. The task fails silently and the hosts remain in the Discovered Hosts tab.

  Workaround:

  1. Remove the ESXi hosts that did not match the ruleset from the Discovered Hosts tab.
  2. Create a rule or edit an existing Auto Deploy rule, where the host target location is a cluster managed by an image.
  3. Reboot the hosts.

  The hosts are added to the cluster that you manage by an image in vSphere Lifecycle Manager.
* **When a hardware support manager is unavailable, vSphere High Availability (HA) functionality is impacted**

  If hardware support manager is unavailable for a cluster that you manage with a single image, where a firmware and drivers addon is selected and vSphere HA is enabled, the vSphere HA functionality is impacted. You may experience the following errors.

  + Configuring vSphere HA on a cluster fails.
  + Cannot complete the configuration of the vSphere HA agent on a host: `Applying HA VIBs on the cluster encountered a failure.`
  + Remediating vSphere HA fails: `A general system error occurred: Failed to get Effective Component map.`
  + Disabling vSphere HA fails: Delete Solution task failed. `A general system error occurred: Cannot find hardware support package from depot or hardware support manager.`

  Workaround:

  + If the hardware support manager is temporarily unavailable, perform the following steps.
  1. Reconnect the hardware support manager to vCenter Server.
  2. Select a cluster from the Hosts and Cluster menu.
  3. Select the Configure tab.
  4. Under Services, click vSphere Availability.
  5. Re-enable vSphere HA.
  + If the hardware support manager is permanently unavailable, perform the following steps.
  1. Remove the hardware support manager and the hardware support package from the image specification
  2. Re-enable vSphere HA.
  3. Select a cluster from the Hosts and Cluster menu.
  4. Select the Updates tab.
  5. Click Edit .
  6. Remove the firmware and drivers addon and click Save.
  7. Select the Configure tab.
  8. Under Services, click vSphere Availability.
  9. Re-enable vSphere HA.
* **I/OFilter is not removed from a cluster after a remediation process in vSphere Lifecycle Manager**

  Removing I/OFilter from a cluster by remediating the cluster in vSphere Lifecycle Manager, fails with the following error message: `iofilter XXX already exists`. Тhe iofilter remains listed as installed.

  Workaround:

  1. Call IOFilter API `UninstallIoFilter_Task` from the vCenter Server managed object (IoFilterManager).
  2. Remediate the cluster in vSphere Lifecycle Manager.
  3. Call IOFilter API `ResolveInstallationErrorsOnCluster_Task` from the vCenter Server managed object (IoFilterManager) to update the database.
* **While remediating a vSphere HA enabled cluster in vSphere Lifecycle Manager, adding hosts causes a vSphere HA error state**

  Adding one or multiple ESXi hosts during a remediation process of a vSphere HA enabled cluster, results in the following error message: `Applying HA VIBs on the cluster encountered a failure.`

  Workaround: Аfter the cluster remediation operation has finished, perform one of the following tasks.

  + Right-click the failed ESXi host and select *Reconfigure for vSphere HA*.
  + Disable and re-enable vSphere HA for the cluster.
* **While remediating a vSphere HA enabled cluster in vSphere Lifecycle Manager, disabling and re-enabling vSphere HA causes a vSphere HA error state**

  Disabling and re-enabling vSphere HA during remediation process of a cluster, may fail the remediation process due to vSphere HA health checks reporting that hosts don't have vSphere HA VIBs installed. You may see the following error message: `Setting desired image spec for cluster failed`.

  Workaround: Аfter the cluster remediation operation has finished, disable and re-enable vSphere HA for the cluster.
* **Checking for recommended images in vSphere Lifecycle Manager has slow performance in large clusters**

  In large clusters with more than 16 hosts, the recommendation generation task could take more than an hour to finish or may appear to hang. The completion time for the recommendation task depends on the number of devices configured on each host and the number of image candidates from the depot that vSphere Lifecycle Manager needs to process before obtaining a valid image to recommend.

  Workaround: None.
* **Checking for hardware compatibility in vSphere Lifecycle Manager has slow performance in large clusters**

  In large clusters with more than 16 hosts, the validation report generation task could take up to 30 minutes to finish or may appear to hang. The completion time depends on the number of devices configured on each host and the number of hosts configured in the cluster.

  Workaround: None
* **Incomplete error messages in non-English languages are displayed, when remediating a cluster in vSphere Lifecycle Manager**

  You can encounter incomplete error messages for localized languages in the vCenter Server user interface. The messages are displayed, after a cluster remediation process in vSphere Lifecycle Manager fails. For example, your can observe the following error message.

   The error message in English language: `Virtual machine 'VMC on DELL EMC -FileServer' that runs on cluster 'Cluster-1' reported an issue which prevents entering maintenance mode: Unable to access the virtual machine configuration: Unable to access file[local-0] VMC on Dell EMC - FileServer/VMC on Dell EMC - FileServer.vmx`

   The error message in French language: `La VM « VMC on DELL EMC -FileServer », située sur le cluster « {Cluster-1} », a signalé un problème empêchant le passage en mode de maintenance : Unable to access the virtual machine configuration: Unable to access file[local-0] VMC on Dell EMC - FileServer/VMC on Dell EMC - FileServer.vmx`

  Workaround: None.
* **Importing an image with no vendor addon, components, or firmware and drivers addon to a cluster which image contains such elements, does not remove the image elements of the existing image**

  Only the ESXi base image is replaced with the one from the imported image.

  Workaround: After the import process finishes, edit the image, and if needed, remove the vendor addon, components, and firmware and drivers addon.
* **When you convert a cluster that uses baselines to a cluster that uses a single image, a warning is displayed that vSphere HA VIBs will be removed**

  Converting a vSphere HA enabled cluster that uses baselines to a cluster that uses a single image, may result a warning message displaying that `vmware-fdm` component will be removed.

  Workaround: This message can be ignored. The conversion process installs the `vmware-fdm` component.
* **If vSphere Update Manager is configured to download patch updates from the Internet through a proxy server, after upgrade to vSphere 7.0 that converts Update Manager to vSphere Lifecycle Manager, downloading patches from VMware patch repository might fail**

  In earlier releases of vCenter Server you could configure independent proxy settings for vCenter Server and vSphere Update Manager. After an upgrade to vSphere 7.0, vSphere Update Manager service becomes part of the vSphere Lifecycle Manager service. For the vSphere Lifecycle Manager service, the proxy settings are configured from the vCenter Server appliance settings. If you had configured Update Manager to download patch updates from the Internet through a proxy server but the vCenter Server appliance had no proxy setting configuration, after a vCenter Server upgrade to version 7.0, the vSphere Lifecycle Manager fails to connect to the VMware depot and is unable to download patches or updates.

  Workaround: Log in to the vCenter Server Appliance Management Interface, https://*vcenter-server-appliance-FQDN-or-IP-address*:5480, to configure proxy settings for the vCenter Server appliance and enable vSphere Lifecycle Manager to use proxy.

**Miscellaneous Issues**

* **When applying a host profile with version 6.5 to a ESXi host with version 7.0, the compliance check fails**

  Applying a host profile with version 6.5 to a ESXi host with version 7.0, results in Coredump file profile reported as not compliant with the host.

  Workaround: There are two possible workarounds.

  1. When you create a host profile with version 6.5, set an advanced configuration option VMkernel.Boot.autoCreateDumpFile to false on the ESXi host.
  2. When you apply an existing host profile with version 6.5, add an advanced configuration option VMkernel.Boot.autoCreateDumpFile in the host profile, configure the option to a fixed policy, and set value to false.
* **The Actions drop-down menu does not contain any items when your browser is set to language different from English**

  When your browser is set to language different from English and you click the **Switch to New View**button from the virtual machine **Summary**tab of the vSphere Client inventory, the **Actions**drop-down menu in the **Guest OS**panel does not contain any items.

  Workaround: Select the **Actions**drop-down menu on the top of the virtual machine page.
* **Mellanox ConnectX-4 or ConnectX-5 native ESXi drivers might exhibit minor throughput degradation when Dynamic Receive Side Scaling (DYN\_RSS) or Generic RSS (GEN\_RSS) feature is turned on**

  Mellanox ConnectX-4 or ConnectX-5 native ESXi drivers might exhibit less than 5 percent throughput degradation when DYN\_RSS and GEN\_RSS feature is turned on, which is unlikely to impact normal workloads.

  Workaround: You can disable DYN\_RSS and GEN\_RSS feature with the following commands:

  `# esxcli system module parameters set -m nmlx5_core -p "DYN_RSS=0 GEN_RSS=0"`

  `# reboot`
* **RDMA traffic between two VMs on the same host might fail in PVRDMA environment**

  In a vSphere 7.0 implementation of a PVRDMA environment, VMs pass traffic through the HCA for local communication if an HCA is present. However, loopback of RDMA traffic does not work on qedrntv driver.  For instance, RDMA Queue Pairs running on VMs that are configured under same uplink port cannot communicate with each other.

  In vSphere 6.7 and earlier, HCA was used for local RDMA traffic if SRQ was enabled. vSphere 7.0 uses HCA loopback with VMs using versions of PVRDMA that have SRQ enabled with a minimum of HW v14 using RoCE v2.

  The current version of Marvell FastLinQ adapter firmware does not support loopback traffic between QPs of the same PF or port.

  Workaround: Required support is being added in the out-of-box driver certified for vSphere 7.0. If you are using the inbox qedrntv driver, you must use a 3-host configuration and migrate VMs to the third host.
* **Unreliable Datagram traffic QP limitations in qedrntv driver**

  There are limitations with the Marvell FastLinQ qedrntv RoCE driver and Unreliable Datagram (UD) traffic. UD applications involving bulk traffic might fail with qedrntv driver. Additionally, UD QPs can only work with DMA Memory Regions (MR). Physical MRs or FRMR are not supported. Applications attempting to use physical MR or FRMR along with UD QP fail to pass traffic when used with qedrntv driver. Known examples of such test applications are `ibv_ud_pingpong` and `ib_send_bw`.

  Standard RoCE and RoCEv2 use cases in a VMware ESXi environment such as iSER, NVMe-oF (RoCE) and PVRDMA are not impacted by this issue. Use cases for UD traffic are limited and this issue impacts a small set of applications requiring bulk UD traffic.

  Marvell FastLinQ hardware does not support RDMA UD traffic offload. In order to meet the VMware PVRDMA requirement to support GSI QP, a restricted software only implementation of UD QP support was added to the qedrntv driver. The goal of the implementation is to provide support for control path GSI communication and is not a complete implementation of UD QP supporting bulk traffic and advanced features.

  Since UD support is implemented in software, the implementation might not keep up with heavy traffic and packets might be dropped. This can result in failures with bulk UD traffic.

  Workaround: Bulk UD QP traffic is not supported with qedrntv driver and there is no workaround at this time. VMware ESXi RDMA (RoCE) use cases like iSER, NVMe, RDMA and PVRDMA are unaffected by this issue.
* **Servers equipped with QLogic 578xx NIC might fail when frequently connecting or disconnecting iSCSI LUNs**

  If you trigger QLogic 578xx NIC iSCSI connection or disconnection frequently in a short time, the server might fail due to an issue with the qfle3 driver. This is caused by a known defect in the device's firmware.

  Workaround: None.
* **ESXi might fail during driver unload or controller disconnect operation in Broadcom NVMe over FC environment**

  In Broadcom NVMe over FC environment, ESXi might fail during driver unload or controller disconnect operation and display an error message such as: `@BlueScreen: #PF Exception 14 in world 2098707:vmknvmeGener IP 0x4200225021cc addr 0x19`

  Workaround: None.
* **ESXi does not display OEM firmware version number of i350/X550 NICs on some Dell servers**

  The inbox ixgben driver only recognizes firmware data version or signature for i350/X550 NICs. On some Dell servers the OEM firmware version number is programmed into the OEM package version region, and the inbox ixgben driver does not read this information. Only the 8-digit firmware signature is displayed.

  Workaround: To display the OEM firmware version number, install async ixgben driver version 1.7.15 or later.
* **X710 or XL710 NICs might fail in ESXi**

  When you initiate certain destructive operations to X710 or XL710 NICs, such as resetting the NIC or manipulating VMKernel's internal device tree, the NIC hardware might read data from non-packet memory.

  Workaround: Do not reset the NIC or manipulate vmkernel internal device state.
* **NVMe-oF does not guarantee persistent VMHBA name after system reboot**

  NVMe-oF is a new feature in vSphere 7.0. If your server has a USB storage installation that uses vmhba30+ and also has NVMe over RDMA configuration, the VMHBA name might change after a system reboot. This is because the VMHBA name assignment for NVMe over RDMA is different from PCIe devices. ESXi does not guarantee persistence.

  Workaround: None.
* **Backup fails for vCenter database size of 300 GB or greater**

  If the vCenter database size is 300 GB or greater, the file-based backup will fail with a timeout. The following error message is displayed: `Timeout! Failed to complete in 72000 seconds`

  Workaround: None.
* **A restore of vCenter Server 7.0 which is upgraded from vCenter Server 6.x with External Platform Services Controller to vCenter Server 7.0 might fail**

  When you restore a vCenter Server 7.0 which is upgraded from 6.x with External Platform Services Controller to vCenter Server 7.0, the restore might fail and display the following error: `Failed to retrieve appliance storage list`

  Workaround: During the first stage of the restore process, increase the storage level of the vCenter Server 7.0. For example if the vCenter Server 6.7 External Platform Services Controller setup storage type is small, select storage type large for the restore process.
* **Enabled SSL protocols configuration parameter is not configured during a host profile remediation process**

  `Enabled SSL protocols` configuration parameter is not configured during a host profile remediation and only the system default protocol `tlsv1.2` is enabled. This behavior is observed for a host profile with version 7.0 and earlier in a vCenter Server 7.0 environment.

  Workaround: To enable TLSV 1.0 or TLSV 1.1 SSL protocols for SFCB, log in to an ESXi host by using SSH, and run the following ESXCLI command: `esxcli system wbem -P <protocol_name>`
* **Unable to configure Lockdown Mode settings by using Host Profiles**

  Lockdown Мode cannot be configured by using a security host profile and cannot be applied to multiple ESXi hosts at once. You must manually configure each host.

  Workaround: In vCenter Server 7.0, you can configure Lockdown Mode and manage Lockdown Mode exception user list by using a security host profile.
* **When a host profile is applied to a cluster, Enhanced vMotion Compatibility (EVC) settings are missing from the ESXi hosts**

  Some settings in the VMware config file `/etc/vmware/config` are not managed by Host Profiles and are blocked, when the config file is modified. As a result, when the host profile is applied to a cluster, the EVC settings are lost, which causes loss of EVC functionalities. For example, unmasked CPUs can be exposed to workloads.

  Workaround: Reconfigure the relevant EVC baseline on cluster to recover the EVC settings.
* **Using a host profile that defines a core dump partition in vCenter Server 7.0 results in an error**

  In vCenter Server 7.0, configuring and managing a core dump partition in a host profile is not available. Attempting to apply a host profile that defines a core dump partition, results in the following error: `No valid coredump partition found.`

  Workaround: None. In vCenter Server 7.0., Host Profiles supports only file-based core dumps.
* **If you run the ESXCLI command to unload the firewall module, the hostd service fails and ESXi hosts lose connectivity**

  If you automate the firewall configuration in an environment that includes multiple ESXi hosts, and run the ESXCLI command `esxcli network firewall unload` that destroys filters and unloads the firewall module, the hostd service fails and ESXi hosts lose connectivity.

  Workaround: Unloading the firewall module is not recommended at any time. If you must unload the firewall module, use the following steps:

  1. Stop the hostd service by using the command:
      `/etc/init.d/hostd stop.`
  2. Unload the firewall module by using the command:
      `esxcli network firewall unload.`
  3. Perform the required operations.
  4. Load the firewall module by using the command:
      `esxcli network firewall load.`
  5. Start the hostd service by using the command:
      `/etc/init.d/hostd start.`
* **vSphere Storage vMotion operations might fail in a vSAN environment due to an unauthenticated session of the Network File Copy (NFC) manager**

  Migrations to a vSAN datastore by using vSphere Storage vMotion of virtual machines that have at least one snapshot and more than one virtual disk with different storage policy might fail. The issue occurs due to an unauthenticated session of the NFC manager because the Simple Object Access Protocol (SOAP) body exceeds the allowed size.

  Workaround: First migrate the VM home namespace and just one of the virtual disks. After the operation completes, perform a disk only migration of the remaining 2 disks.
* **Changes in the properties and attributes of the devices and storage on an ESXi host might not persist after a reboot**

  If the device discovery routine during a reboot of an ESXi host times out, the jumpstart plug-in might not receive all configuration changes of the devices and storage from all the registered devices on the host. As a result, the process might restore the properties of some devices or storage to the default values after the reboot.

  Workaround: Manually restore the changes in the properties of the affected device or storage.
* **If you use a beta build of ESXi 7.0, ESXi hosts might fail with a purple diagnostic screen during some lifecycle operations**

  If you use a beta build of ESXi 7.0, ESXi hosts might fail with a purple diagnostic screen during some lifecycle operations such as unloading a driver or switching between ENS mode and native driver mode. For example, if you try to change the ENS mode, in the backtrace you see an error message similar to: `case ENS::INTERRUPT::NoVM_DeviceStateWithGracefulRemove hit BlueScreen: ASSERT bora/vmkernel/main/dlmalloc.c:2733` This issue is specific for beta builds and does not affect release builds such as ESXi 7.0.

  Workaround: Update to ESXi 7.0 GA.
* **vSAN secure disk wipe not supported on NVMe devices with multiple namespaces**

  If a NVME device has multiple namespaces configured, vSAN secure disk wipe operations are not supported.
   If a new namespace is configured while a secure wipe operation is in progress, for example in VMware Tanzu Architecture for Dell EMC VxRail or the Google Cloud Platform, the data on the newly added namespace might be erased, so vSAN secure disk wipe operations are supported only for NVME devices with a single namespace.

  Workaround: None
* **HTTP requests from certain libraries to vSphere might be rejected**

  The HTTP reverse proxy in vSphere 7.0 enforces stricter standard compliance than in previous releases. This might expose pre-existing problems in some third-party libraries used by applications for SOAP calls to vSphere.

  If you develop vSphere applications that use such libraries or include applications that rely on such libraries in your vSphere stack, you might experience connection issues when these libraries send HTTP requests to VMOMI. For example, HTTP requests issued from vijava libraries can take the following form:

  ```
  POST /sdk HTTP/1.1
  SOAPAction
  Content-Type: text/xml; charset=utf-8
  User-Agent: Java/1.8.0_221
  ```

  The syntax in this example violates an HTTP protocol header field requirement that mandates a colon after SOAPAction. Hence, the request is rejected in flight.

  Workaround: Developers leveraging noncompliant libraries in their applications can consider using a library that follows HTTP standards instead. For example, developers who use the vijava library can consider using the latest version of the yavijava library instead.
* **Editing an advanced options parameter in a host profile and setting a value to false, results in setting the value to true**

  When attempting to set a value to `false` for an advanced option parameter in a host profile, the user interface creates a non-empty string value. Values that are not empty are interpreted as `true` and the advanced option parameter receives a `true` value in the host profile.

  Workaround: There are two possible workarounds.

  + Set the advanced option parameter to `false` on a reference ESXi host and copy settings from this host in Host Profiles.
     Note: The host must be compliant with the host profile before modifying the advanced option parameter on the host.
  + Set the advanced option parameter to `false` on a reference ESXi host and create a host profile from this host. Then copy the host profile settings from the new host profile to the existing host profile.
* **You might see a dump file when using Broadcom driver lsi\_msgpt3, lsi\_msgpt35 and lsi\_mr3**

  When using the lsi\_msgpt3, lsi\_msgpt35 and lsi\_mr3 controllers, there is a potential risk to see dump file lsuv2-lsi-drivers-plugin-util-zdump. There is an issue when exiting the storelib used in this plugin utility. There is no impact on ESXi operations, you can ignore the dump file.

  Workaround: You can safely ignore this message. You can remove the lsuv2-lsi-drivers-plugin with the following command:

   `esxcli software vib remove -n lsuv2-lsiv2-drivers-plugin`
* **You might see reboot is not required after configuring SR-IOV of a PCI device in vCenter, but device configurations made by third party extensions might be lost and require reboot to be re-applied.**

  In ESXi 7.0, SR-IOV configuration is applied without a reboot and the device driver is reloaded. ESXi hosts might have third party extensions perform device configurations that need to run after the device driver is loaded during boot. A reboot is required for those third party extensions to re-apply the device configuration.

  Workaround: You must reboot after configuring SR-IOV to apply third party device configurations.

To collapse the list of previous known issues, click here.

check-circle-line

exclamation-circle-line

close-line

![Scroll to top icon](/uicontent/images/scroll_top.svg)

![](/uicontent/images/feedback.svg)

             [![VMware](https://www.vmware.com/content/dam/vmwaredesigns/scrapercontent/resources/logos/vmware-logo-grey.svg "VMware")](https://www.vmware.com/)

Resources

* [Blogs](https://blogs.vmware.com/)
* [Careers](https://www.broadcom.com/jobs)
* [Communities](https://community.broadcom.com/)
* [Customer Stories](https://www.vmware.com/resources/customers)

* [News and Stories](https://news.broadcom.com/)
* [Topics](https://www.vmware.com/topics/)
* [Trust Center](https://www.vmware.com/info/trust-center/)

Support

* [Broadcom Support](https://support.broadcom.com/)
* [Documentation](https://docs.vmware.com)
* [Hands-On Labs](https://www.vmware.com/resources/hands-on-labs)
* [Licensing](https://www.broadcom.com/licensing)

* [Twitter](https://twitter.com/VMware)
* [YouTube](https://www.youtube.com/user/vmwaretv)
* [Facebook](https://www.facebook.com/vmware)
* [LinkedIn](https://www.linkedin.com/company/vmware/mycompany/)
* [Contact Sales](https://go-vmware.broadcom.com/contact-us)

---

  Copyright © 2005-2024 Broadcom. All Rights Reserved. The term "Broadcom" refers to Broadcom Inc. and/or its subsidiaries. [Accessibility](https://www.broadcom.com/company/legal/accessibility "Accessibility") [Privacy](https://www.broadcom.com/privacy "Privacy") [Supplier Responsibility](https://www.broadcom.com/company/citizenship/supplier-responsibility "Supplier Responsibility") [Terms Of Use](https://www.broadcom.com/company/legal/terms-of-use "Terms Of Use")

#####

×

Share on Social Media?

×

#####

×

exclamation-circle-line

check-circle-line

 :

#####

×

exclamation-circle-line

|

check-circle-line

exclamation-circle-line

x

#####

×

"
"?

×



=== Content from docs.vmware.com_2868c3bb_20250115_175011.html ===
VMware ESXi 7.0 Update 2e Release Notes

ESXi 7.0 Update 2e | 15 FEB 2022 | Build 19290878

Check for additions and updates to these release notes.

What's in the Release Notes

IMPORTANT: ESXi 7.0 Update 2e delivers fixes for CVE-2021-22040, CVE-2021-22041, CVE-2021-22042, and CVE-2021-22043. If you do not plan to update your

environment to ESXi 7.0 Update 3c (build # 19193900), use ESXi 7.0 Update 2e to update your ESXi hosts of version 7.0 Update 2d (build # 18538813) and earlier with

these security fixes.

The supported update path from ESXi 7.0 Update 2e is to ESXi 7.0 Update 3c or later. The upgrade path from ESXi 6.5.x and ESXi 6.7.x to ESXi 7.0 Update 2e is not

supported. For supported upgrade paths from ESXi 6.5.x and 6.7.x to ESXi 7.x versions, see VMware knowledge base article 67077.

The release notes cover the following topics:

What's New

Earlier Releases of ESXi 7.0

Patches Contained in this Release

Resolved Issues

Known Issues from Previous Releases

What's New

ESXi 7.0 Update 2е resolves CVE-2021-22040, CVE-2021-22041, CVE-2021-22042, and CVE-2021-22043. For more information on these vulnerabilities and their

impact on VMware products, see VMSA-2022-0004.

Earlier Releases of ESXi 7.0

New features, resolved, and known issues of ESXi are described in the release notes for each release. Release notes for earlier releases of ESXi 7.0 are:

VMware ESXi 7.0, Patch Release ESXi 7.0 Update 2d

VMware ESXi 7.0, Patch Release ESXi 7.0 Update 2c

VMware ESXi 7.0, Patch Release ESXi 7.0 Update 2a

VMware ESXi 7.0, Patch Release ESXi 7.0 Update 2

VMware ESXi 7.0, Patch Release ESXi 7.0 Update 1d

VMware ESXi 7.0, Patch Release ESXi 7.0 Update 1c

VMware ESXi 7.0, Patch Release ESXi 7.0 Update 1b

VMware ESXi 7.0, Patch Release ESXi 7.0 Update 1a

VMware ESXi 7.0, Patch Release ESXi 7.0 Update 1

VMware ESXi 7.0, Patch Release ESXi 7.0b

For internationalization, compatibility, and open source components, see the VMware vSphere 7.0 Release Notes.

Patches Contained in This Release

This release of ESXi 7.0 Update 2e delivers the following patches:

Build Details

Download Filename:

VMware-ESXi-7.0U2e-19290878-depot

Build:

Download Size:

md5sum:

sha256checksum:

19290878

391.0 MB

a866ad94d04be2fcadc385b31b1fc804

95ec97007c3c021ed6d4380502c74ac2d6cade11460c5c0d6f19cc5959ca7f01

Host Reboot Required:

Virtual Machine Migration or
Shutdown Required:

Yes

Yes

For a table of build numbers and versions of VMware vCenter Server, see VMware knowledge base article 2143838.

Components

Component

Bulletin

ESXi Component - core ESXi VIBs

ESXi_7.0.2-0.30.19290878

Category

Severity

Security

Important

ESXi Install/Upgrade Component

esx-update_7.0.2-0.30.19290878

Security

Important

IMPORTANT:

To download the ESXi 7.0 Update 2e patch offline depot ZIP file from VMware Customer Connect, you must navigate to Products and Accounts > Product

Patches. From the Select a Product drop-down menu, select ESXi (Embedded and Installable) and from the Select a Version drop-down menu, select 7.0.

Starting with vSphere 7.0, VMware uses components for packaging VIBs along with bulletins. The ESXi and esx-update bulletins are dependent on each other.

Always include both in a single ESXi host patch baseline or include the rollup bulletin in the baseline to avoid failure during host patching.

When patching ESXi hosts by using VMware Update Manager from a version prior to ESXi 7.0 Update 2, it is strongly recommended to use the rollup bulletin in

the patch baseline. If you cannot use the rollup bulletin, be sure to include all of the following packages in the patching baseline. If the following packages are not

included in the baseline, the update operation fails:

VMware-vmkusb_0.1-1vmw.701.0.0.16850804 or higher

VMware-vmkata_0.1-1vmw.701.0.0.16850804 or higher

VMware-vmkfcoe_1.0.0.2-1vmw.701.0.0.16850804 or higher

VMware-NVMeoF-RDMA_1.0.1.2-1vmw.701.0.0.16850804 or higher

Rollup Bulletin

This rollup bulletin contains the latest VIBs with all the fixes after the initial release of ESXi 7.0.

Bulletin ID

ESXi70U2e-19290878

Image Profiles

Category

Security

Severity

Critical

VMware patch and update releases contain general and critical image profiles. Application of the general release image profile applies to new bug fixes.

Image Profile Name

ESXi-7.0U2e-19290878-standard

ESXi-7.0U2e-19290878-no-tools

ESXi Image

Name and Version

ESXi70U2e-19290878

Release Date

02/15/2022

Category

Security

Detail

Security image

For information about the individual components and bulletins, see the Product Patches page and the Resolved Issues section.

Patch Download and Installation

In vSphere 7.0.x, the Update Manager plug-in, used for administering vSphere Update Manager, is replaced with the Lifecycle Manager plug-in. Administrative

operations for vSphere Update Manager are still available under the Lifecycle Manager plug-in, along with new capabilities for vSphere Lifecycle Manager.

The typical way to apply patches to ESXi 7.0.x hosts is by using the vSphere Lifecycle Manager. For details, see About vSphere Lifecycle Manager and vSphere

Lifecycle Manager Baselines and Images.

You can also update ESXi hosts without using the Lifecycle Manager plug-in, and use an image profile instead. To do this, you must manually download the patch

offline bundle ZIP file from the Product Patches page and use the esxcli software profile update command.

For more information, see the Upgrading Hosts by Using ESXCLI Commands and the VMware ESXi Upgrade guide.

Resolved Issues

The resolved issues are grouped as follows.

ESXi_7.0.2-0.30.19290878

esx-update_7.0.2-0.30.19290878

ESXi-70U2e-19290878-standard

ESXi-70U2e-19290878-no-tools

ESXi Image - ESXi70U2e-19290878

ESXi_7.0.2-0.30.19290878

Patch Category

Patch Severity

Host Reboot Required

Security

Important

Yes

Virtual Machine Migration or Shutdown Required Yes

Affected Hardware

Affected Software

N/A

N/A

VIBs Included

PRs Fixed

CVE numbers

VMware_bootbank_esx-dvfilter-generic-fastpath_7.0.2-0.30.19290878
VMware_bootbank_cpu-microcode_7.0.2-0.30.19290878
VMware_bootbank_clusterstore_7.0.2-0.30.19290878
VMware_bootbank_vdfs_7.0.2-0.30.19290878
VMware_bootbank_crx_7.0.2-0.30.19290878
VMware_bootbank_vsan_7.0.2-0.30.19290878
VMware_bootbank_gc_7.0.2-0.30.19290878
VMware_bootbank_native-misc-drivers_7.0.2-0.30.19290878
VMware_bootbank_esx-base_7.0.2-0.30.19290878
VMware_bootbank_esx-xserver_7.0.2-0.30.19290878
VMware_bootbank_vsanhealth_7.0.2-0.30.19290878

N/A

CVE-2021-22040, CVE-2021-22041, CVE-2021-22042, CVE-2021-22043

The ESXi and esx-update bulletins are dependent on each other. Always include both in a single ESXi host patch baseline or include the rollup bulletin in the baseline to

avoid failure during host patching.

Updates the esx-dvfilter-generic-fastpath, vsanhealth, vdfs, vsan, esx-base, crx, native-misc-drivers, esx-xserver, gc and cpu-microcode VIBs to resolve the following

issue:

This release resolves CVE-2021-22040, CVE-2021-22041, CVE-2021-22042, and CVE-2021-22043. For more information on these vulnerabilities and their impact

on VMware products, see VMSA-2022-0004.

esx-update_7.0.2-0.30.19290878

Patch Category

Patch Severity

Host Reboot Required

Virtual Machine Migration or Shutdown
Required

Affected Hardware

Affected Software

VIBs Included

PRs Fixed

CVE numbers

Updates the loadesx and esx-update VIBs.

ESXi-70U2e-19290878-standard

Security

Important

Yes

Yes

N/A

N/A

N/A

N/A

VMware_bootbank_esx-update_7.0.2-0.30.19290878
VMware_bootbank_loadesx_7.0.2-0.30.19290878

Profile Name

Build

Vendor

Release Date

Acceptance Level

Affected Hardware

Affected Software

Affected VIBs

ESXi-70U2e-19290878-standard

For build information, see Patches Contained in this Release.

VMware, Inc.

February 15, 2022

PartnerSupported

N/A

N/A

VMware_bootbank_esx-dvfilter-generic-fastpath_7.0.2-0.30.19290878
VMware_bootbank_cpu-microcode_7.0.2-0.30.19290878
VMware_bootbank_clusterstore_7.0.2-0.30.19290878
VMware_bootbank_vdfs_7.0.2-0.30.19290878
VMware_bootbank_crx_7.0.2-0.30.19290878
VMware_bootbank_vsan_7.0.2-0.30.19290878
VMware_bootbank_gc_7.0.2-0.30.19290878
VMware_bootbank_native-misc-drivers_7.0.2-0.30.19290878
VMware_bootbank_esx-base_7.0.2-0.30.19290878
VMware_bootbank_esx-xserver_7.0.2-0.30.19290878
VMware_bootbank_vsanhealth_7.0.2-0.30.19290878
VMware_bootbank_esx-update_7.0.2-0.30.19290878
VMware_bootbank_loadesx_7.0.2-0.30.19290878

PRs Fixed

N/A

Related CVE numbers

CVE-2021-22040, CVE-2021-22041, CVE-2021-22042, CVE-2021-22043

This patch updates the following issue:

This release resolves CVE-2021-22040, CVE-2021-22041, CVE-2021-22042, and CVE-2021-22043. For more information on these vulnerabilities and their

impact on VMware products, see VMSA-2022-0004.

ESXi-70U2e-19290878-no-tools

Profile Name

Build

Vendor

Release Date

Acceptance Level

Affected Hardware

Affected Software

Affected VIBs

ESXi-70U2e-19290878-no-tools

For build information, see Patches Contained in this Release.

VMware, Inc.

February 15, 2022

PartnerSupported

N/A

N/A

VMware_bootbank_esx-dvfilter-generic-fastpath_7.0.2-0.30.19290878
VMware_bootbank_cpu-microcode_7.0.2-0.30.19290878
VMware_bootbank_clusterstore_7.0.2-0.30.19290878
VMware_bootbank_vdfs_7.0.2-0.30.19290878
VMware_bootbank_crx_7.0.2-0.30.19290878
VMware_bootbank_vsan_7.0.2-0.30.19290878
VMware_bootbank_gc_7.0.2-0.30.19290878
VMware_bootbank_native-misc-drivers_7.0.2-0.30.19290878
VMware_bootbank_esx-base_7.0.2-0.30.19290878
VMware_bootbank_esx-xserver_7.0.2-0.30.19290878
VMware_bootbank_vsanhealth_7.0.2-0.30.19290878
VMware_bootbank_esx-update_7.0.2-0.30.19290878
VMware_bootbank_loadesx_7.0.2-0.30.19290878

PRs Fixed

N/A

Related CVE numbers

CVE-2021-22040, CVE-2021-22041, CVE-2021-22042, CVE-2021-22043

This patch updates the following issue:

This release resolves CVE-2021-22040, CVE-2021-22041, CVE-2021-22042, and CVE-2021-22043. For more information on these vulnerabilities and their

impact on VMware products, see VMSA-2022-0004.

ESXi Image - ESXi70U2e-19290878

Name

Version

Release Date

Category

ESXi

70U2e-19290878

February 15, 2022

Security

Affected Components

ESXi
ESXi Install/Upgrade Component

PRs Fixed

N/A

Related CVE numbers

CVE-2021-22040, CVE-2021-22041, CVE-2021-22042, CVE-2021-
22043

Known Issues from Earlier Releases

To view a list of previous known issues, click here.

Copyright © Broadcom


