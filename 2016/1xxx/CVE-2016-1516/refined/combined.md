=== Content from lists.debian.org_45dca5eb_20250125_022232.html ===


---

[[Date Prev](msg00029.html)][[Date Next](msg00031.html)]
[[Thread Prev](msg00029.html)][[Thread Next](msg00031.html)]
[[Date Index](maillist.html#00030)]
[[Thread Index](threads.html#00030)]

# [SECURITY] [DLA 1438-1] opencv security update

---

* *To*: debian-lts-announce@lists.debian.org
* *Subject*: [SECURITY] [DLA 1438-1] opencv security update
* *From*: Thorsten Alteholz <debian@alteholz.de>
* *Date*: Sun, 22 Jul 2018 12:43:13 +0200 (CEST)
* *Message-id*: <[[üîé]](/msgid-search/alpine.DEB.2.02.1807221242430.8224%40jupiter.server.alteholz.net)¬†[alpine.DEB.2.02.1807221242430.8224@jupiter.server.alteholz.net](msg00030.html)>
* *Mail-followup-to*: debian-lts@lists.debian.org
* *Reply-to*: debian-lts@lists.debian.org

---

```

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA512

Package        : opencv
Version        : 2.4.9.1+dfsg-1+deb8u2
CVE ID         : CVE-2016-1516 CVE-2017-12597 CVE-2017-12598
                 CVE-2017-12599 CVE-2017-12601 CVE-2017-12603
                 CVE-2017-12604 CVE-2017-12605 CVE-2017-12606
                 CVE-2017-12862 CVE-2017-12863 CVE-2017-12864
                 CVE-2017-14136 CVE-2017-17760 CVE-2017-1000450
                 CVE-2018-5268 CVE-2018-5269

```
Early versions of opencv have problems while reading data, which might
result in either buffer overflows, out-of bounds errors or integer
overflows.
```

Further assertion errors might happen due to incorrect integer cast.

For Debian 8 "Jessie", these problems have been fixed in version
2.4.9.1+dfsg-1+deb8u2.

We recommend that you upgrade your opencv packages.

Further information about Debian LTS security advisories, how to apply
these updates to your system and frequently asked questions can be
found at: <https://wiki.debian.org/LTS>

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.12 (GNU/Linux)

iQJ8BAEBCgBmBQJbVF/BXxSAAAAAAC4AKGlzc3Vlci1mcHJAbm90YXRpb25zLm9w
ZW5wZ3AuZmlmdGhob3JzZW1hbi5uZXQ2MjAxRkJGRkRCQkRFMDc4MjJFQUJCOTY5
NkZDQUMwRDM4N0I1ODQ3AAoJEJb8rA04e1hHu9EP/1uFEZvO8uMBHIPD0znbYvfQ
JG9cK9d5lvw5h2qB6BHp/bMXLtncvhOC64TqzjIGmSiD9kBxMLt8dM/orSEmDUjJ
CIJuWksSyc6iSnu3QSldPaSgqJHeeYW6LXOUhlb7DjiLv4TAtmW7w8QEp52GVzOB
9VUE5ohJ4Q5K+10SUId8Sy3cfnIqVnaTHz18K2YdM4L4/r9Mmo64nwlUSrlIaC3l
XgtqV3LenSN64nTDF/vYyXYJ57ZCh3TW863Fo2BjujR4or41IxVU2Cue2HjVYzmU
TskdUUC6S/46rZNb/WVjDK/D1kKAnPiZAzV3prvmA2zqXKCgfuVfDIj3iLoDYszC
lqm5egFjWj77pVtWcJA06g6HNhE30YkjKcRXnVY4vo37Mnj29CUMfK+QMhqc8Caz
gvOEX+pRlNa3bn2djeVt+6FG2y9BaTQZKRmLY6EFLD3CwUsGfuSmZdTY6iVNlbBT
xXOBf9EtHpv4KeZwBJy/rDS8Rbab199/Kia75rPF+KTPl0uhdHSQQ/l2Yt4MTrX4
/VEBw89MVYf6dgXcm9U0jbC0GNWUHqqaFvfrny7iBILSWeN/TIcSw4wn/bWoq2Xe
Q2Wh3r2Lz31OkeLesStiNa5XaFgxrLQzOMmRL9c0sxa+evqQ2Px+XUj2HHz3hcFS
WlYC4YXevRjDXceNhob4
=CtOL
-----END PGP SIGNATURE-----

```

---



=== Content from lists.debian.org_378ac449_20250125_022232.html ===


---

[[Date Prev](msg00027.html)][[Date Next](msg00029.html)]
[[Thread Prev](msg00027.html)][[Thread Next](msg00029.html)]
[[Date Index](maillist.html#00028)]
[[Thread Index](threads.html#00028)]

# [SECURITY] [DLA 2799-1] opencv security update

---

* *To*: debian-lts-announce <debian-lts-announce@lists.debian.org>
* *Subject*: [SECURITY] [DLA 2799-1] opencv security update
* *From*: Markus Koschany <apo@debian.org>
* *Date*: Sat, 30 Oct 2021 16:30:18 +0200
* *Message-id*: <[[üîé]](/msgid-search/13652034ef3d4dd328ceb778296b2d5f6a5d6d08.camel%40debian.org)¬†[13652034ef3d4dd328ceb778296b2d5f6a5d6d08.camel@debian.org](msg00028.html)>
* *Mail-followup-to*: debian-lts@lists.debian.org
* *Reply-to*: debian-lts@lists.debian.org

---

```
-------------------------------------------------------------------------
Debian LTS Advisory DLA-2799-1                debian-lts@lists.debian.org
<https://www.debian.org/lts/security/>                      Markus Koschany
October 29, 2021                              <https://wiki.debian.org/LTS>
-------------------------------------------------------------------------

Package        : opencv
Version        : 2.4.9.1+dfsg1-2+deb9u1
CVE ID         : CVE-2016-1516 CVE-2017-12597 CVE-2017-12598 CVE-2017-12599
                 CVE-2017-12601 CVE-2017-12603 CVE-2017-12604 CVE-2017-12605
                 CVE-2017-12606 CVE-2017-12862 CVE-2017-12863 CVE-2017-12864
                 CVE-2017-17760 CVE-2017-1000450 CVE-2018-5268 CVE-2018-5269
                 CVE-2019-14493 CVE-2019-15939
Debian Bug     : 886282 885843 875342 872044 872043

Several security vulnerabilities have been discovered in OpenCV, the Open
Computer Vision Library. Buffer overflows, NULL pointer dereferences and
out-of-bounds write errors may lead to a denial-of-service or other
unspecified impact.

For Debian 9 stretch, these problems have been fixed in version
2.4.9.1+dfsg1-2+deb9u1.

We recommend that you upgrade your opencv packages.

For the detailed security status of opencv please refer to
its security tracker page at:
<https://security-tracker.debian.org/tracker/opencv>

Further information about Debian LTS security advisories, how to apply
these updates to your system and frequently asked questions can be
found at: <https://wiki.debian.org/LTS>

```

**Attachment:
[signature.asc](pgpEofXZdTfKj.pgp)**

*Description:* This is a digitally signed message part

---



=== Content from wiki.debian.org_8d48714b_20250126_051956.html ===

[![Debian](https://www.debian.org/Pics/openlogo-50.png)](https://www.debian.org "Debian Homepage")

[Wiki](/FrontPage "Debian Wiki Homepage")

[Login](/LTS?action=login)

* [FrontPage](/FrontPage)
* [RecentChanges](/RecentChanges)
* [FindPage](/FindPage)
* [HelpContents](/HelpContents)
* [LTS](/LTS)

Search:

[![Debian](https://www.debian.org/Pics/openlogo-50.png)](https://www.debian.org "Debian Homepage")
[Wiki](/FrontPage "Debian Wiki Homepage")/

* [Login](/LTS?action=login)
* Comments
* [Info](/LTS?action=info)
* [Attachments](/LTS?action=AttachFile)
* More Actions:
  Raw Text
  Print View
  Render as Docbook
  Delete Cache
  ------------------------
  Check Spelling
  Like Pages
  Local Site Map
  ------------------------
  Rename Page
  Delete Page
  ------------------------
  Subscribe User
  ------------------------
  Remove Spam
  Revert to this revision
  Package Pages
  ------------------------
  Load
  Save
  SlideShow

# * [LTS](/LTS)

[Translation(s)](/DebianWiki/EditorGuide#translation): [English](/LTS) - [Deutsch](/de/LTS) - [Espa√±ol](/es/LTS) - [Fran√ßais](/fr/LTS) - [Italiano](/it/LTS) - [ÌïúÍµ≠Ïñ¥](/ko/LTS) - [Portugu√™s](/pt/LTS) - [Portugu√™s (Brasil)](/pt_BR/LTS) - [–†—É—Å—Å–∫–∏–π](/ru/LTS)

# Debian Long Term Support

Debian Long Term Support (LTS) is a project to extend the lifetime of all Debian stable releases to (at least) 5 years. Debian LTS is not handled by the Debian Security and Release teams, but by a separate group of volunteers and companies interested in making it a success.

Thus the Debian LTS team takes over security maintenance of the various releases once the Debian Security team stops its work.

![/!\](/htdocs/debwiki/img/alert.png "/!\")

LTS ([last modified 2024-08-15 02:46:47](/LTS?action=info))

* Debian [privacy policy](https://www.debian.org/legal/privacy), Wiki [team](/Teams/DebianWiki), [bugs](https://bugs.debian.org/wiki.debian.org) and [config](https://salsa.debian.org/debian/wiki.debian.org).
* Powered by [MoinMoin](https://moinmo.in/ "This site uses the MoinMoin Wiki software.") and [Python](https://moinmo.in/Python "MoinMoin is written in Python."), with hosting provided by [Metropolitan Area Network Darmstadt](https://www.man-da.de/).



=== Content from arxiv.org_130916d3_20250125_022231.html ===
Summoning Demons

The Pursuit of Exploitable Bugs in Machine Learning

Rock Stevens
Sanghyun Hong

Octavian Suciu
Michael Hicks
University of Maryland, College Park

Andrew Ruef
Tudor Dumitras,

7
1
0
2

n
a
J

7
1

]

R
C
.
s
c
[

1
v
9
3
7
4
0
.
1
0
7
1
:
v
i
X
r
a

ABSTRACT
Governments and businesses increasingly rely on data ana-
lytics and machine learning (ML) for improving their com-
petitive edge in areas such as consumer satisfaction, threat
intelligence, decision making, and product eÔ¨Éciency. Howe-
ver, by cleverly corrupting a subset of data used as input
to a target‚Äôs ML algorithms, an adversary can perturb out-
comes and compromise the eÔ¨Äectiveness of ML technology.
While prior work in the Ô¨Åeld of adversarial machine learning
has studied the impact of input manipulation on correct ML
algorithms, we consider the exploitation of bugs in ML im-
In this paper, we characterize the attack
plementations.
surface of ML programs, and we show that malicious inputs
exploiting implementation bugs enable strictly more power-
ful attacks than the classic adversarial machine learning te-
chniques. We propose a semi-automated technique, called
steered fuzzing, for exploring this attack surface and for di-
scovering exploitable bugs in machine learning programs,
in order to demonstrate the magnitude of this threat. As
a result of our work, we responsibly disclosed Ô¨Åve vulne-
rabilities, established three new CVE-IDs, and illuminated
a common insecure practice across many machine learning
systems. Finally, we outline several research directions for
further understanding and mitigating this threat.

Keywords
Machine learning, vulnerability research, application secu-
rity, vulnerability exploitation, fuzzing

INTRODUCTION

1.
Governments and businesses increasingly employ data ana-
lytics to improve their competitive edge. For example, the
United States Environmental Protection Agency has ou-
tlined its vision for leveraging machine learning (ML) to im-
prove their everyday operations [12]. IBM oÔ¨Äers businesses
a platform for conducting sentiment analysis to gauge their
eÔ¨Äectiveness within a target audience [15]. OpenDNS uses
ML to automate protection against known and emerging

threats [25]. Machine learning allows these organizations to
extrapolate trends from massive data sets, of often uncertain
provenance.

However, ingesting unÔ¨Åltered, public information into data
analytic engines also introduces a threat, as miscreants can
corrupt eventual inputs to ML algorithms to bias their ou-
tputs. Cretu et al. [7] discussed the importance of ‚Äúcas-
ting out demons,‚Äù or sanitizing the training datasets for safe
machine learning ingestion. Research on adversarial ma-
chine learning [16, 19, 1, 3, 13] has explored various attacks
against ML algorithms, with a focus on skewing their ou-
tputs through malicious perturbations to the input data.

In this paper, we discuss another attack vector: ML algori-
thm implementations. Like all software, ML algorithm im-
plementations have bugs and some of these bugs could aÔ¨Äect
learning tasks. Thus, attacks can construct malicious inputs
to ML algorithm implementations that exploit these bugs.
Indeed, such attacks can be more powerful than traditional
adversarial machine learning techniques. For example, a me-
mory corruption vulnerability could allow an adversary to
corrupt the entire feature matrix, not just the entries that
correspond to adversary-controlled inputs. More generally,
bugs in the cost function, minimization algorithm, model re-
presentation, prediction or clustering steps, could allow an
adversary to arbitrarily skew learning outcomes or to initiate
a denial of service attack.

While considerable eÔ¨Äorts have been devoted to discovering
software vulnerabilities and mitigating the impact of explo-
its, these generally focus on bugs that allow the adversary
to subvert the targeted system, e.g. by executing arbitrary
code or by achieving privilege escalation. In contrast, adver-
saries attacking an ML system are interested in bugs that
allow them to induce mispredictions, misclustering, or to
suppress outputs. Such logic bugs are diÔ¨Écult to discover
using existing tools.

As a Ô¨Årst step toward understanding and mitigating this
threat, we characterize the attack surface of ML programs,
which derives from a general architecture that many ML al-
gorithms share, and we identify decision points whose out-
come we may corrupt. We discuss how bugs around those
decision points could be exploited and the potential outco-
mes of these exploits. We also propose a semi-automated
technique called steered fuzzing for Ô¨Ånding and exploiting
ML implementation bugs. We wrap important decision po-

1

ints from the ML architecture with instrumented code to
convert a logical failure of the algorithm (e.g. mispredic-
tion) into a crash that can be detected by a fuzzing tool [20],
which generates test cases and records program exceptions
on these inputs. We then apply a coverage-based fuzzing
tool, American Fuzzy Lop [36], to summon demons, i.e. to
automatically discover inputs that mislead the ML algori-
thms by exploiting bugs in their implementation.

We utilize this
technique to discover attacks against
OpenCV [4] and Malheur [31], two open source ML imple-
mentations. As an example, we started fuzzing with a seed
image that was recognized as having a face by OpenCV.
We added a logic branch that crashed on non-recognition.
Steered fuzzing then proceeded to generate a mutant input
that was clearly still a face, and yet was not recognized.
This exploit relies on a bug in the rendering library used by
OpenCV, which allows for incorrect rendering of input ima-
ges. In total, we found seven bugs: three in OpenCV, two in
Malheur, one in Scikit-Learn, and one in libarchive (used
by Malheur). Of these, three were assigned a CVE-ID; only
one was not exploitable.

In summary, this paper makes three contributions. First,
we explore the attack surface of ML implementations, as
compared to ML algorithms, highlighting potential attack
vectors and impact on various components within these sys-
tems. Second, we introduce a novel technique for exploiting
ML bugs to corrupt classiÔ¨Åcation outcomes and the data
provided to ML systems from benign sources. This techni-
que is possible through steered fuzzing, which expands upon
existing fuzzing techniques for discovering bugs in software
applications. Finally, we discover several new ML implemen-
tation bugs in important open-source software; our work has
led to these bugs being patched.

2. PROBLEM STATEMENT
We consider an exploit to be a piece of code aiming to su-
bvert the intended functionality of software. Limiting our
scope to machine learning, an exploit would be designed
with the goal of corrupting the outputs of programs or to
inhibit their operation. Such exploitable bugs may be pre-
sent either in the core implementation of the ML algorithm
or in libraries used for feature extraction or model represen-
tation.

In terms of impact, we distinguish between three possible
outcomes of successful exploits. First, an exploit that causes
speciÔ¨Åc instances to be assigned an incorrect label achieves
mispredictions. SpeciÔ¨Åcally, an exploit targeting the training
phase results in poisoning of the classiÔ¨Åer, while one during
testing could allow evasion. Similarly, an exploit targeting
a clustering algorithm may cause inputs to be placed in di-
Ô¨Äerent clusters, resulting in misclustering. Because machine
learning systems are often utilized as black boxes, it may be
diÔ¨Écult to detect that the system has been compromised by
using one of these exploits, as they typically have no other
side eÔ¨Äects besides skewing the learned model and cause the
ML system to fail silently. An exploit may also result in de-
nial of service, e.g. by stopping data ingestion prematurely
or by crashing the application to prevent it from providing
any output. While easier to detect, such exploits may ren-
der the system temporarily unusable. Finally, a successful

exploit could enable code execution capabilities for the atta-
cker. This category of exploits is arguably the most powerful
since it gives the adversary full control over the underlying
machine.

In this paper, we address the problem of discovering explo-
itable vulnerabilities in machine learning implementations.
The goals of our work are: (i) to provide a general ML archi-
tectural description, discussing possible attack vectors and
their impact on diÔ¨Äerent system components; (ii) to develop
a semi-automated technique for discovering ML vulnerabili-
ties by exploring this attack surface; and (iii) to demonstrate
the magnitude of this threat by discussing several real vul-
nerabilities we discovered in popular ML systems.

Non-goals: We do not address limitations of machine lear-
ning algorithms (the area of study in adversarial machine
learning). Instead, we aim to unearth implementation bugs
as an orthogonal attack vector against ML systems. Addi-
tionally, we do not aim to develop a fully automated techni-
que for identifying these bugs. Instead, by relying on steered
code instrumentation and program output manipulation, we
are able to bootstrap existing fuzzing tools in order to di-
scover bugs.

2.1 Threat Model
We consider an adversary who aims to subvert the execution
of machine learning algorithms by exploiting bugs in algo-
rithm implementations. We assume that the adversary has
access to the program‚Äôs source code. We also assume that
the adversary controls some of the program‚Äôs inputs, but
is unable to prevent benign users from providing additional
inputs. These assumptions are realistic in many settings;
for example, the machine learning techniques proposed for
malware classiÔ¨Åcation or clustering [33, 28, 10, 31] operate
on inputs that come from many sources, including possible
adversaries. Additionally, much ML software is open source,
e.g., OpenCV [4] and Scikit-Learn [27].

In searching for exploitable bugs, the adversary does not
pursue the usual goals of vulnerability exploitation, e.g.,
gaining control over remote hosts, achieving privilege es-
calation, escaping sandboxes, etc. Instead, the adversary‚Äôs
goal is to corrupt the outputs of machine learning programs
using silent failures.

From a spectrum-of-control perspective, arbitrary code exe-
cution exploits represent the strongest means for achieving
the adversary‚Äôs goal, as such an exploit permits an adversary
to manipulate all aspects of the target system. However, the
adversary may achieve her goals with less powerful logical
exploits, e.g., targeting memory corruption bugs that allow
modifying data in memory but do not enable code execu-
tion or bugs that trigger loss of precision in Ô¨Çoating point
computations. Denial of service attacks could also prove
beneÔ¨Åcial for the attacker and may, for example, be conduc-
ted by inducing early termination of the ML processing. In
some settings, the weaker attacks may be more attractive
as they could allow the adversary to remain stealthy and
bypass defense mechanisms.

3. ATTACKING ML IMPLEMENTATIONS

2

Figure 1: General architecture of ML systems.

To begin exploring the vulnerabilities of machine learning
applications, we must Ô¨Årst understand their attack surface.
Enumerating the components of ML applications that an
attacker may target allows us to reason about where the
bugs may be and what impact they may have. We then bu-
ild on this understanding and expand upon existing fuzzing
techniques to create exploits for these bugs to induce mis-
classiÔ¨Åcations (false negatives or false positives), incorrect
clustering results, and denial of service.

3.1 Machine Learning Architecture
Machine learning algorithms vary in structure and design.
The two main categories of learning algorithms are supervi-
sed and unsupervised. In supervised learning, the algorithm
receives a set of labeled examples and computes a predictive
model that Ô¨Åts the training examples. The predictive model
is then used to classify new, unlabeled samples. In contrast,
unsupervised learning relies solely on unlabeled examples
with the goal of Ô¨Ånding clusters of similar samples. While
there may not be a generic representation that Ô¨Åts all al-
gorithms, some of the most popular supervised techniques
are variations of iterative minimization algorithms. For un-
supervised learning, clustering is one of the most prevalent
classes of algorithms. Figure 1 presents the general Ô¨Çow of
a learning algorithm, highlighting the key particularities of
each phase. In this setting, the input samples are transfor-
med into a feature matrix representation that serves as the
input to the classiÔ¨Åer. A common, but optional, practice is
to normalize the features prior to feeding them to the al-
gorithm. This involves feature scaling and standardization.
In the training phase, the (normalized) features are applied
onto the current model in order to obtain the perceived pre-
diction. The predictions are compared to the actual class
labels using a cost function. The cost function output quan-
tiÔ¨Åes the distance between the current model and the ground
truth. The model is then updated to reduce the cost through
a minimization algorithm. This iterative process is repeated
until the model becomes a suÔ¨Éciently accurate representa-
tion of the ground truth. Upon convergence, the model is
used to predict new class labels. In the testing phase, the

unlabeled samples are transformed using the same feature
extraction and normalization processes. The predicted class
labels are obtained using the prediction function over the
In clustering, the algorithm Ô¨Årst performs
trained model.
feature extraction and normalization. Using a distance me-
tric, the algorithm groups the samples into clusters that re-
Ô¨Çect the similarity between them.

3.2 From Architecture to Attack Surface
We now discuss how attacks on each component in Figure 1
may impact the overall functioning of the system. Table 1
summarizes the vectors and impact of attacks against the
system components. A successful attack against one com-
ponent may have ripple eÔ¨Äects to others, either directly by
transferring corrupted outputs to inputs, or indirectly via
in-memory data structure corruption.

Feature extraction. Feature extraction is the backbone for
the integrity of the system. Every attack from an external
source must exploit vulnerabilities in this component as it
is the sole communication port between the internal com-
ponents and the external environment. An attack targeting
the feature extraction component results in a corruption of
the information passed downstream.

Within the feature extraction component itself, an attack
can target the input parsing algorithms and/or the integrity
checks performed on the feature representation. As shown in
Section 4, such an exploit could result in mispredictions, mis-
clustering, arbitrary code execution or DoS. It is not always
straightforward to deÔ¨Åne what is allowable input, or an al-
lowable representation thereof. For example, in an image
classiÔ¨Åcation setting, a reasonable assumption would be to
consider any renderable image as legitimate inputs. Howe-
ver, as detailed in Section 4.1, we found that most images
that cause crashes in the OpenCV library are actually valid
from a rendering perspective.

Prediction. Attacks are also possible against the prediction
component, directly inÔ¨Çuencing the labels predicted by the

3

Component

Exploitation Techniques

Feature Extraction

InsuÔ¨Écient integrity checks

Prediction
Cost Function

OverÔ¨Çow / UnderÔ¨Çow, NaN, Loss of Precision
OverÔ¨Çow / UnderÔ¨Çow, NaN, Loss of Precision
Minimization Algorithm OverÔ¨Çow / UnderÔ¨Çow, NaN, Loss of Precision

Model Representation
Clustering

Loss of Precision
OverÔ¨Çow / UnderÔ¨Çow, NaN, Loss of Precision

Impact
Poisoning / Evasion / Misclustering,
Code execution, DoS
Poisoning / Evasion
Poisoning, DoS
Poisoning, DoS
Poisoning / Evasion
Misclustering

Table 1: Attack surface of ML algorithms.

algorithm. This could occur in both the training and the
testing stages. For example, the attack could exploit bugs
related to Ô¨Çoating point overÔ¨Çow, Ô¨Çoating point underÔ¨Çow,
or the use of not-a-number (NaN) values. ML implementa-
tions typically compute logarithms and square roots. This
makes them particularly susceptible to bugs caused by NaNs
(potentially the result of an overÔ¨Çow or insuÔ¨Écient consis-
tency checks). The eÔ¨Äects of a NaN propagate throughout
the remainder of the computation and can result in poiso-
ning or classiÔ¨Åer evasion.

Cost function and minimization algorithm. The cost
function computation and the minimization algorithm are
iteratively applied in the training phase. A bug could re-
sult in incorrect cost estimates or model updates that cause
the decision boundary to be shifted away from the optimal
value. Additionally, a denial of service could be obtained if
the model update does not trigger the termination condition
in the iterative algorithm. If the cost function consistently
results in a NaN for the training examples, the minimiza-
tion algorithm stagnates indeÔ¨Ånitely without updating the
model.

Model representation. The model representation could
cause poisoning or evasion through loss of precision. Since
the training and the testing phases of algorithms are typi-
cally performed separately, the model has to be stored and
transferred from the one to the other. As discussed in Sec-
tion 4, casting between Ô¨Çoat and long types can skew the
model away, resulting in inaccurate predictions for the un-
labeled samples.

Clustering. In clustering, the algorithm itself or the dis-
tance metric can be manipulated using the same attack vec-
tors as for supervised learning. This could result in a de-
nial of service or misclustering. In complete misclustering,
the clusters are completely misrepresented, while in selective
misclustering the attack might result in a particular sample
being placed in a diÔ¨Äerent cluster.

3.3 Discovery Methods
Fuzzing [20] is a popular method for bug discovery. A fuz-
zing tool tests a program using randomly generated inputs,
which are often invalid or unexpected by the implementa-
tion, and records program exceptions or failures. In security,
fuzzing has been employed to identify crashes that are indi-
cative of memory safety errors in application. This technique
has obvious applications to discovering one class of bug in
machine learning systems‚Äîcrashes‚Äîbut can we use fuzzing
to Ô¨Ånd bugs that silently corrupt the system‚Äôs outputs? In
this section, we use OpenCV as a running example while
describing our bug discovery methodology.

4

Our use case is, in one sense, a natural Ô¨Åt for general pur-
pose fuzzing because we can have a single program that runs
on some input (i.e. an image) to produce some output (i.e.
a text classiÔ¨Åcation of that image). However, we have to
ensure that we separate and identify both bug types of inte-
rest: crashes and silent corruption. To do this we introduce
a technique we call steered fuzzing.

We use American Fuzzy Lop (AFL) [36] to instrument and
fuzz-test machine learning programs. AFL was designed and
is commonly used for Ô¨Ånding crashes due to parsing failures,
so the AFL loop involves running an application on multiple
inputs and creating a report if an input causes a crash. AFL
utilizes a genetic algorithm to generate inputs while maxi-
mizing the code coverage and has heuristics to discriminate
between unique crashes and duplicates. We want to capita-
lize on AFL‚Äôs ability to maximize code coverage while also
Ô¨Ånding crashing inputs.

A steered fuzzing workÔ¨Çow begins with a test case with a
known outcome; for example, when analyzing OpenCV, we
start with an image that contains a human face. The three
outputs from the program under test might be: crash, ne-
gative prediction, (e.g. no face found) or positive prediction
(e.g. face found). The default behavior with the initial test
case is to Ô¨Ånd a face. Our fuzzing should mutate the image
to change the output of the program under test to negative
prediction while avoiding crash.

When we are searching for such logical failures, we do not
care about inputs that produce crashes when OpenCV at-
tempts to parse the image (although there is a disturbingly
large number of these inputs). The Ô¨Årst part of our ste-
ered fuzzing technique brackets the parsing regions of the
program in a handler for the SIGSEV signal. The handler
simply exit‚Äôs the program when a segmentation violation
occurs. This prevents the crash and obscures it from AFL,
which then believes that the application exited normally.

We then re-enable crashes in the application and check the
outcome of important decision points in the ML algorithm.
For example, we check the result of the face detection step,
which corresponds to the outcome of the prediction phase
from Figure 1. If the system failed to Ô¨Ånd a face, we induce
a crash by manually dereferencing an invalid pointer.
In
this way, AFL recognizes when it has changed the output of
the program to no face found without any change to AFL
itself. Similarly, we can instrument the outputs of each of
the components described in Section 3.2, to check for the
presence of exploitable ML bugs.

4. RESULTS
We search for exploitable ML bugs in the OpenCV [4] com-
puter vision library and in the Malheur [31] library for ana-
lyzing and clustering malware behavior. We select these
libraries because they are open source and they are widely
adopted.

tion attacks already exist against video monitoring software
that law enforcement organizations use to read license pla-
tes and issuing Ô¨Ånes [24]; one can understand the ramiÔ¨Åca-
tions of a DoS attack against similar applications or even
autonomous driving vehicles using similar computer vision
software.

OpenCV provides its users with a common framework for
computer vision applications and can process still imagery,
live streaming video, and previously recorded video clips.
For example, businesses can use computer vision and ma-
chine learning to reinforce physical security systems [32]. In
such a scenario, an adversary may wish to thwart physical
security through attacking the machine learning application
itself.

Malheur is a security tool that performs analysis on malware
reports that were recorded within sandboxed environments.
Malheur can cluster the reports to determine which samples
likely belong to the same malware family; these malware
reports can be raw text Ô¨Åles or compressed Ô¨Åle archives.
Malheur relies on libarchive to extract the malware re-
ports from the Ô¨Åle archives. An adversary that desires to
delay analysis of their malware may target Malheur through
crafted Ô¨Åle archives and corrupt in-memory data. Data cor-
ruption will cause misclustering and allow the adversary to
accomplish their goal.

As a result of our research, we responsibly disclosed Ô¨Åve vul-
nerabilities (to which three were assigned CVE-IDs). The
libarchive and Malheur system maintainers patched two
of the vulnerabilities; as of January 27, 2016, the OpenCV
maintainers acknowledged three vulnerabilities and would
address the issues in future releases. These vulnerabilities
still exist in the current version of OpenCV. Table 2 sum-
marizes the vulnerabilities we found and their impact.

4.1 Discovery Results

OpenCV. We discovered bugs in OpenCV‚Äôs image pro-
cessing library, and we identiÔ¨Åed various conditions under
which a valid JPG would cause an algorithm to terminate.
Two vulnerabilities (CVE-2016-1516 and CVE-2016-1517)
exist in the feature extraction / selection portion of the ML
attack surface in Figure 1 and cause memory corruption
In
when freeing a matrix allocated for image processing.
both CVEs, heap corruptions overÔ¨Çow Ô¨Åelds in the matrix
object and allow illegal access to memory locations when ma-
trix objects are deallocated. Many examples exist in which
an adversary can exploit similar vulnerabilities in image pro-
cessing code and achieve remote code execution on a vic-
tim‚Äôs system [14, 8, 21]. Our third vulnerability exists in
OpenCV‚Äôs custom image rendering library.
Its improper
handling of Ô¨Åle artifacts and partial rendering of particular
JPG images prevent consistent image classiÔ¨Åcation.

During the steered fuzzing phase, these vulnerabilities and
inconsistencies served as the basis for crafting legitimate in-
put images that evade facial recognition detection. When
used as-is, these images induce denial-of-service (DoS) cra-
shes against OpenCV. DoS crashes in such an application
require an administrator or operator to manually intervene
to bring the system back online. Proof-of-concept SQL injec-

A potentially viable defense against these crash-inducing
images starts with Ô¨Årst Ô¨Åltering input based on a render-
check using the Python Image Library (PIL) [6] and the
code snippet in Listing 1:

Listing 1: Image render-check using PIL

from PIL import Image
def

i s i m a g e o k ( f i l e n a m e ) :
try :

Image . open ( f i l e n a m e ) . l o a d ( )
return True

except :

return F a l s e

Of the 3197 images we found that induce crashes in
OpenCV, PIL only allows 7 images to bypass this Ô¨Ålter,
resulting in a 0.0022% false negative rate. Yahoo! Flickr‚Äôs
proprietary image rendering solution allows 6 crash-inducing
images through. These crash-inducing images are publicly
available for viewing.1

Malheur. We discovered a critical bug within libarchive
as used by Malheur. The vulnerability was issued CVE-
2016-1541 [9] and was patched in libarchive 3.2.0 on May
1, 2016. This vulnerability aÔ¨Äected every version of Linux
and OS X, given that libarchive is pre-packaged in these
operating systems for handling various Ô¨Åle archives. A suc-
cessful exploit would allow an attacker to achieve arbitrary
code execution by exploiting the inconsistent handling of
.tar.gz compressed archives. This vulnerability occurs wi-
thin the feature extraction block within Figure 1, given that
the function inherently relies upon libarchive for attaining
data. Once an attacker achieves arbitrary code execution,
they have unlimited inÔ¨Çuence over the classiÔ¨Åcation of the
ML application. This bug could trigger another bug in Ma-
lheur‚Äôs feature matrix extraction/selection and was patched
on March 6, 2016 [30]. A third vulnerability in Malheur
results in loss of precision when casting a double value to
Ô¨Çoat. Section 4.2 explores the impact of corrupting the fea-
ture matrix in greater depth.

Scikit-Learn and NumPy. We also discovered a loss of
precision vulnerability in the popular SkLearn Python fra-
mework and the underlying NumPy library that, when ex-
ploited, results in mispredictions. Due to the fact that AFL
is not compatible with Python, we manually inspected the
source code during bug discovery.

4.2 Steered Fuzzing Results

1https://www.Ô¨Çickr.com/gp/138669175@N07/L53K8e

5

Vulnerability
Heap Corruption in Fea-
ture Extraction
Heap Corruption in Fea-
ture Extraction
Inconsistent rendering in
Feature Extraction
Heap Corruption in Fea-
ture Extraction
Heap Corruption in Fea-
ture Extraction
Loss of Precision in Fea-
ture Extraction
Loss of Precision in Mo-
del Representation

Application CVE-ID
OpenCV

CVE-2016-1516

Exploited Impact
(cid:88)

Arbitrary code execution via double free

OpenCV

CVE-2016-1517

OpenCV

n/a

Malheur

CVE-2016-1541

Malheur

GitHub patch

Malheur

n/a

Scikit-Learn

n/a

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

Denial of service attack via corrupt chunks
and segfault
Partial rendering of JPG Ô¨Åles results in eva-
sion
Arbitrary code execution on all Linux and
OS X systems via corrupted archive
Memory corruption via unsafe bounds che-
cking results in misclustering
Loss of precision results in misclustering

Loss of precision results in mispredictions

Table 2: Summary of ML Hunter Ô¨Åndings.

OpenCV. An attacker can exploit OpenCV‚Äôs inconsistent
rendering of images to induce silent failures and thwart fa-
cial detection within the prediction block of the ML attack
surface in Figure 1. To begin, AFL utilizes a seed image with
a shoulder-up picture of a person. The source code snippet
that performs facial recognition2 is a prime candidate for
injecting the the logic branch (Listing 2) which allows us
to induce a crash when the picture of the face is no longer
detected.

Listing 2: Logic branch injection for facial detection

i f ( f a c e s . s i z e ( ) == 0 ) {

‚àó ( ( i n t ‚àó ) 0 xdeadbea7 ) =
0 x d e a d b e e f ;

}

After 10.1 million permutations of the seed image, AFL
crafted Figure 2. This image is incorrectly rendered by
OpenCV, as seen on the left, but is clearly renderable by
Google Photos, as seen on the right. In Ô¨Åve out of Ô¨Åve tri-
als, this method successfully recreated photos that exercise
this rendering bug. As these images are correctly formatted
JPEG Ô¨Åles, they bypass the PIL render-check described in
Listing 1. In contrast to existing techniques for crafting ad-
versarial samples that evade detection [34, 3, 2, 19, 1], our
attack does not depend on the learned model and succeeds
from the Ô¨Årst attempt. This represents a new attack vector
against machine learning, illustrating how bugs in ML code
can provide a substantial advantage to the attacker.

Figure 2: OpenCV incorrectly rendering a picture.

sed on degree of induced skew. The vulnerable line of code3
uses the variable j which is dependent on user-provided in-
put. Thus, steered fuzzing can craft a corrupted archive Ô¨Åle
to traverse the heap and stomp over existing values in the
feature matrix as shown in Listing 3.

Listing 3: Example of directed heap corruption

i f
( ( ( unsigned long)& t [ j ‚àí1] >
( unsigned long)& fv‚àí>v a l [ 0 ] ) &&
( ( unsigned long)& t [ j ‚àí1] <
( unsigned long )
( ( unsigned long)& fv‚àí>v a l [ 0 ] +
( unsigned long ) fv‚àí>mem) ) ) {

‚àó ( ( in t ‚àó ) 0 xdeadbea7 ) =
0 x d e a d b e e f ;

}

Malheur. Building on Malheur‚Äôs inability to handle cor-
rupted archive Ô¨Åles, discussed previously, the steered fuzzing
technique can corrupt Malheur‚Äôs feature matrix and induce
silent failures in prediction results. Thus, this vulnerabi-
lity impacts all aspects of clustering within the generalized
attack surface in Figure 1; an attacker can corrupt the in-
memory data for unlabeled samples, tamper with the in-
memory feature matrix, and aÔ¨Äect the clustering results ba-

2https://github.com/opencv/opencv/blob/a8e5d1d9fdae18
3762d4e06a7e25473d10ef1974/samples/cpp/facedetect.cpp
#L202

As this is a heap corruption vulnerability, we performed
our proof-of-concept (PoC) exploit with address space la-
yout randomization turned oÔ¨Ä. An attacker can couple our
PoC exploit with ASLR bypass [11] techniques using ano-
ther information disclosure exploit to Ô¨Ånd the desired oÔ¨Äset.
Additionally, our exploit uses a Ô¨Åle archive; the exploitation
success varies among operating systems and architectures as
expected.

3https://github.com/rieck/malheur/blob/75ffd2498e964aa
7d09782bf5a0d31afde36585f/src/fvec.c#L382!

6

Expanding upon this example, an adversary can inject ad-
ditional logic branches to control the degree in which the
corrupted Ô¨Åle impacts the feature matrix. Given enough
time, AFL can generate inputs that increasingly skew the
clustering of benign Ô¨Åles the adversary did not craft.

This represents a second attack vector that provides new
capabilities for adversaries. Unlike prior attacks proposed
in the adversarial machine learning literature, this attack
introduces the ability to manipulate the in-memory repre-
sentation of inputs not provided by the adversary. From an
adversarial perspective, this attack provides the opportunity
to miscluster benign samples, or other malicious samples, to
obfuscate the attacker‚Äôs own malicious sample. An adver-
sary can achieve this by inducing false negatives (more steal-
thy and desired) or false positives (junk reports). Again, this
attack requires only one malicious sample and succeeds from
the Ô¨Årst attempt, owing to the bug. The libarchive 3.2.0
and Malheur GitHub patch rendered this bug unexploitable,
as corrupted archives are rejected on ingest.

We also discovered a bug in Malheur that we could not ex-
ploit. During feature normalization in Malheur, both func-
tions4 fvec_norm1() and fvec_norm2() return a value of
type double but it is then normalized to a float. Using
steered fuzzing, an attacker can discover instances where
type casting from double to float yields a discrepancy.

Listing 4: Example of discovering precision loss

i f
( f l o a t )

( abs ( ( double )

( f ‚àí>v a l [ i ] / s ) ‚àí

( f ‚àí>v a l [ i ] / s ) ) > e p s i l o n ) {
‚àó ( ( i n t ‚àó ) 0 xdeadbea7 ) = 0 x d e a d b e e f ;

}

The epsilon in Listing 4 represents the loss of precision
an adversary wishes to induce. In this particular instance,
steered fuzzing did not discover any value of epsilon that
caused misclustering. While our attack was unsuccessful,
this approach could be a viable attack vector elsewhere.

We discovered that such a vulnerability is present within
the Scikit-Learn machine learning library for Python and
its underlying reliance on NumPy. When deÔ¨Åning ndar-
ray objects from Python lists without explicitly specifying
a data type, the library infers the resulting data type accor-
ding to undocumented heuristics. The NumPy arrays are
used by Scikit-learn during both training and testing of the
Linear Regression algorithm. This attack forces NumPy to
set the ndarray data type as object, which preserves the
underlying data type of each element. The Scikit-learn sa-
nity checks ensure that the training and testing data types
match. Because both the training and the testing arrays
are of type object, the arrays pass the Scikit-learn checks.
Our proof-of-concept (PoC) code places Python float and
long values in the arrays before that data is ingested by
the Scikit-learn module. When the input numbers are very
large, this results in a loss of precision from casting. Speci-
Ô¨Åcally, the PoC shows how the regression model coeÔ¨Écients
are drastically changed when using float instead of long
4https://github.com/rieck/malheur/blob/75ffd2498e964aa
7d09782bf5a0d31afde36585f/src/fmath.c#L37

7

in the training dataset; this results in mispredictions at tes-
ting. In absence of a fuzzing tool for Python, we discovered
this bug by manually inspecting the Scikit source code, stee-
red by the attack surface guidelines. These bugs illustrate a
third attack vector that potentially enables new adversarial
capabilities.

Disclosure experience.. Three of the vulnerabilities we
discovered were assigned new CVE-IDs (CVE-2016-1516,
CVE-2016-1517,and CVE-2016-1541), as they enabled arbi-
trary code execution or denial of service attacks. Disclosing
these vulnerabilities allowed us to draw an interesting com-
parison between the community‚Äôs reaction to these bugs and
the ML-speciÔ¨Åc bugs we introduce in this paper. In parti-
cular, the bugs that led to misprediction, misclustering, or
model divergence‚Äîincluding a Malheur memory corruption
bug that allows the adversary to control the feature matrix,
but not to inject arbitrary code‚Äîdid not receive CVE nu-
mbers. Many of these bugs were labeled WONTFIX. This
emphasizes the fact that ML bugs are currently a misunder-
stood threat.

5. RELATED WORK
This section presents prior work on fuzzing and adversarial
machine learning. Adversarial machine learning research fo-
cuses on crafting adversarial samples. The key distinction
in our work is that we exploit bugs in machine learning code
that give the adversary an advantage in conducting these
attacks.

InsuÔ¨Écient input sanitization is a common cause of explo-
itable bugs [17]. Fuzzing is an automated technique that
allows developers to test how gracefully their application
handle various valid and invalid input [20, 23]. Fuzzers as-
sist developers with isolating potentially buggy code and can
play a critical role in identifying locations in need of input
sanitization.

The Ô¨Åeld of adversarial machine learning has developed seve-
ral methods for attacking ML systems, typically by querying
ML models. Barreno et al. [1] proposed a general classiÔ¨Åca-
tion system for these attacks. Integrity attacks allow hostile
input into a system and availability attacks prevent benign
input from entering a system. Concept drift [35] is a pheno-
menon that occurs within machine learning systems as the
prediction becomes less accurate over time due to unfore-
seen changes. Identifying concept drift, whether sudden or
gradual, can be diÔ¨Écult in the presence of noise.
Ideally,
machine learning systems should combine robustness to no-
ise and sensitivity to concept drift. Adversarial drift [19]
describes intentionally induced concept drift in an eÔ¨Äort to
decrease the classiÔ¨Åcation accuracy. Biggio et al. [3] descri-
bed a threat model in which an attacker desires to conceal
malicious input in an eÔ¨Äort to evade detection without ne-
gatively impacting the classiÔ¨Åcation of legitimate samples.
According to Biggio, an attacker may wish to inject mali-
cious input to subvert the clustering process, rendering the
resulting knowledge useless. The adversarial classiÔ¨Åer re-
verse engineering [16] describes techniques for learning suÔ¨É-
cient information about a classiÔ¨Åer to instrument adversarial
attacks. This information provides attackers and defenders
with an understanding of how susceptible their system is to

external adversarial inÔ¨Çuence. Newsome et al. [22] intro-
duce a delusive adversary that provides malicious input in
an attempt to obstruct the ML training phase; the attacker
assumes full control over the input and its order.

In an analysis of a neural network trained for image proces-
sing tasks, Szegedy et al.
[34] identiÔ¨Åed that an adversary
can apply a perturbation to an image that is imperceptible
to humans yet it changes the network‚Äôs prediction. Goodfel-
low et al. [13] present a fast method for generating adversa-
rial perturbations to fool an image classiÔ¨Åer. Research from
Cha et al. [5] further explores automated generation of such
perturbations. Utilizing a well-formed seed input, a muta-
tional fuzzer iteratively manipulates the seed to achieve ma-
ximum path traversal in a target program. This technique
can isolate particular sets of input that cause the program
to enter a state that might be of interest for an attacker.

Cretu et al. [7] discuss the process and importance of ‚Äúcas-
ting out demons,‚Äù sanitizing ML training datasets for ano-
maly detection (AD) sensors. AD systems inherently receive
malicious input and anomalous events that may drastically
impact the system‚Äôs tuning and instrumentation. Accoun-
ting for data that may negatively impact the accuracy of the
system‚Äôs classiÔ¨Åer can enhance its overall robustness.

6. DISCUSSION AND FUTURE WORK
In this paper, we focus on attacks against machine learning
systems. However, our threat model has a broader appli-
cability. For example, nation-state adversaries might be
interested in attacking long-running simulations on super-
computers, with the aim of subtly skewing their results. In
high performance computing, outputs are generally diÔ¨Écult
to validate and expensive to re-compute, so it is diÔ¨Écult to
defend against such attacks. Other data analytics systems
may also be susceptible to such attacks.

For some of the bugs that we discovered, it is unclear who is
responsible for Ô¨Åxing them. Should the Malheur maintainers
have to worry about bugs in libarchive in order to preserve
the integrity of their application? Should the architects of
OpenCV sacriÔ¨Åce performance for the sake of handling inva-
lid input that developers did not Ô¨Ålter? As more and more
everyday devices begin to incorporate ML processing, this
ambiguity must be explicitly resolved in order to provide
secure systems.

Section 3 describes a semi-automated approach for disco-
vering bugs in machine learning platforms through catego-
rizing the backtrace of crash-inducing results. Tools such
as !exploitable [18] provide researchers with automated
crash analysis and the likelihood that the crash is explo-
itable. Overlaying the Ô¨Åndings from such a tool on top of
our generalized attack surface could expedite the discovery
phase.

Section 4 explores many techniques that, at Ô¨Årst glance, are
only feasible because the targeted source code is publicly
available. Recently, Papernot et al. [26] proposed model ex-
traction attacks, by building surrogate classiÔ¨Åers that appro-
ximate black-box ML models. A logical next step in expan-
ding our research would be understanding the overlap be-
tween building substitution models of proprietary classiÔ¨Åers

and unique edge cases that result in bugs in the black box
system.

An adversary discovering the possibility of ‚Äúlinchpin values‚Äù
that appear during feature matrix construction would be
another decisive shift towards an attacker‚Äôs inÔ¨Çuence on ML
systems. Linchpin values are consistent ranges of values wi-
thin a feature matrix, that when present, result in a speciÔ¨Åc
classiÔ¨Åcation. Ribeiro et al. [29] proposed a technique for
model explanation by building locally optimal classiÔ¨Åers aro-
und points of interest. Building upon their work, researchers
may apply various analytic techniques to determine if there
are common values or thresholds within a feature matrix
that, when present, always result in a certain classiÔ¨Åcation.
With this information, an attacker could use steered fuzzing
to craft arbitrary input that would guarantee a misclassiÔ¨Å-
cation in the targeted system.

7. CONCLUSIONS
Entities that choose to trust data from unvetted sources su-
bject themselves to a plethora of potential attacks in which
a miscreant only requires minimal control over the entire
dataset. For an attacker that wishes to control the decision-
making process of its competitors or adversaries, this repre-
sents a powerful paradigm shift in attack vectors. We disco-
vered several vulnerabilities within OpenCV and Malheur
that allow an attacker to exploit bugs in underlying depen-
dencies and the applications themselves to gain a marked
advantage in inÔ¨Çuencing or out-right controlling the output
of ML applications.

8. REFERENCES
[1] M. Barreno, B. Nelson, A. D. Joseph, and J. Tygar.
The security of machine learning. Machine Learning,
81(2):121‚Äì148, 2010.

[2] B. Biggio, B. Nelson, and P. Laskov. Support vector
machines under adversarial label noise. In ACML,
pages 97‚Äì112, 2011.

[3] B. Biggio, I. Pillai, S. Rota Bul‚Äòo, D. Ariu, M. Pelillo,
and F. Roli. Is data clustering in adversarial settings
secure? In Proceedings of the 2013 ACM workshop on
ArtiÔ¨Åcial intelligence and security, pages 87‚Äì98. ACM,
2013.

[4] G. Bradski. OpenCV. Dr. Dobb‚Äôs Journal of Software

Tools, 2000.

[5] S. K. Cha, M. Woo, and D. Brumley.

Program-adaptive mutational fuzzing. In 2015 IEEE
Symposium on Security and Privacy, pages 725‚Äì741.
IEEE, 2015.

[6] A. Clark. Python PILLOW, 2015.
[7] G. F. Cretu, A. Stavrou, M. E. Locasto, S. J. Stolfo,

and A. D. Keromytis. Casting out demons: Sanitizing
training data for anomaly sensors. In Security and
Privacy, 2008. SP 2008. IEEE Symposium on, pages
81‚Äì95. IEEE, 2008.

[8] CVEdetails. CVE-2015-4493: Heap-based buÔ¨Äer

overÔ¨Çow in the stagefright::ESDS::parseESDescriptor
function in libstagefright in mozilla Ô¨Årefox bef, 2015.

[9] CVEdetails. Vulnerability note VU#862384, 2016.

[10] G. E. Dahl, J. W. Stokes, L. Deng, and D. Yu.
Large-scale malware classiÔ¨Åcation using random
projections and neural networks. In IEEE

8

International Conference on Acoustics, Speech and
Signal Processing, ICASSP 2013, Vancouver, BC,
Canada, May 26-31, 2013, pages 3422‚Äì3426, 2013.
[11] T. Durden. Bypassing PAX ASLR protection. Phrack

Magazine, 59(9):9‚Äì9, 2002.

[12] Environmental Protection Agency. EPA‚Äôs cross-agency
data analytics and visualization program | toxics
release inventory (TRI) program | us epa, 2015.

[13] I. J. Goodfellow, J. Shlens, and C. Szegedy.

Explaining and harnessing adversarial examples. arXiv
preprint arXiv:1412.6572, 2014.

[14] Google Project Zero. Project zero: Hack the galaxy:
Hunting bugs in the samsung galaxy s6 edge, 2015.

should i trust you?‚Äù: Explaining the predictions of any
classiÔ¨Åer. arXiv preprint arXiv:1602.04938, 2016.
[30] K. Rieck. Fix for problem with corrupt archives., 2016.
[31] K. Rieck, P. Trinius, C. Willems, and T. Holz.

Automatic analysis of malware behavior using machine
learning. Journal of Computer Security, 19(3), 2011.

[32] J. Sandhu. Machine Learning for Smart Home

Security Systems, 2016.

[33] M. G. Schultz, E. Eskin, E. Zadok, and S. J. Stolfo.
Data mining methods for detection of new malicious
executables. In 2001 IEEE Symposium on Security
and Privacy, Oakland, California, USA May 14-16,
2001, pages 38‚Äì49, 2001.

[15] IBM. IBM social sentiment analysis powered by IBM

[34] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna,

D. Erhan, I. Goodfellow, and R. Fergus. Intriguing
properties of neural networks. arXiv preprint
arXiv:1312.6199, 2013.

[35] A. Tsymbal. The problem of concept drift: deÔ¨Ånitions
and related work. Computer Science Department,
Trinity College Dublin, 106, 2004.

[36] M. Zalewski. American Fuzzy Lop, 2015.

analytics ‚Äì india, 2015.

[16] D. Lowd and C. Meek. Adversarial learning. In
Proceedings of the eleventh ACM SIGKDD
international conference on Knowledge discovery in
data mining, pages 641‚Äì647. ACM, 2005.

[17] G. McGraw. Software security: building security in,
volume 1. Addison-Wesley Professional, 2006.

[18] Microsoft. !exploitable crash analyzer - msec debugger

extensions, 2016.

[19] B. Miller, A. Kantchelian, S. Afroz, R. Bachwani,

E. Dauber, L. Huang, M. C. Tschantz, A. D. Joseph,
and J. Tygar. Adversarial active learning. In
Proceedings of the 2014 Workshop on ArtiÔ¨Åcial
Intelligent and Security Workshop, pages 3‚Äì14. ACM,
2014.

[20] B. P. Miller, L. Fredriksen, and B. So. An empirical
study of the reliability of UNIX utilities. Commun.
ACM, 33(12):32‚Äì44, 1990.

[21] National Institute of Standards and Technology. Nvd -

detail, 2015.

[22] J. Newsome, B. Karp, and D. Song. Paragraph:

Thwarting signature learning by training maliciously.
In Recent advances in intrusion detection, pages
81‚Äì105. Springer, 2006.

[23] P. Oehlert. Violating assumptions with fuzzing.
Security & Privacy, IEEE, 3(2):58‚Äì62, 2005.

[24] G. Ollmann. SQL Injection in the Wild, 2013.
[25] OpenDNS. Cyber threat intelligence | OpenDNS, 2015.
[26] N. Papernot, P. D. McDaniel, I. J. Goodfellow, S. Jha,
Z. B. Celik, and A. Swami. Practical black-box attacks
against deep learning systems using adversarial
examples. CoRR, abs/1602.02697, 2016.

[27] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,
R. Weiss, V. Dubourg, J. Vanderplas, A. Passos,
D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. Scikit-learn: Machine learning in
Python. Journal of Machine Learning Research,
12:2825‚Äì2830, 2011.

[28] R. Perdisci, W. Lee, and N. Feamster. Behavioral
clustering of http-based malware and signature
generation using malicious network traces. In
Proceedings of the 7th USENIX Symposium on
Networked Systems Design and Implementation, NSDI
2010, April 28-30, 2010, San Jose, CA, USA, pages
391‚Äì404, 2010.

[29] M. T. Ribeiro, S. Singh, and C. Guestrin. ‚Äù why

9



=== Content from github.com_6515b62d_20250125_022232.html ===

[Skip to content](#start-of-content)

## Navigation Menu

Toggle navigation

[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fopencv%2Fopencv%2Fissues%2F5956)

* Product

  + [GitHub Copilot
    Write better code with AI](https://github.com/features/copilot)
  + [Security
    Find and fix vulnerabilities](https://github.com/features/security)
  + [Actions
    Automate any workflow](https://github.com/features/actions)
  + [Codespaces
    Instant dev environments](https://github.com/features/codespaces)
  + [Issues
    Plan and track work](https://github.com/features/issues)
  + [Code Review
    Manage code changes](https://github.com/features/code-review)
  + [Discussions
    Collaborate outside of code](https://github.com/features/discussions)
  + [Code Search
    Find more, search less](https://github.com/features/code-search)

  Explore
  + [All features](https://github.com/features)
  + [Documentation](https://docs.github.com)
  + [GitHub Skills](https://skills.github.com)
  + [Blog](https://github.blog)
* Solutions

  By company size
  + [Enterprises](https://github.com/enterprise)
  + [Small and medium teams](https://github.com/team)
  + [Startups](https://github.com/enterprise/startups)
  + [Nonprofits](/solutions/industry/nonprofits)
  By use case
  + [DevSecOps](/solutions/use-case/devsecops)
  + [DevOps](/solutions/use-case/devops)
  + [CI/CD](/solutions/use-case/ci-cd)
  + [View all use cases](/solutions/use-case)

  By industry
  + [Healthcare](/solutions/industry/healthcare)
  + [Financial services](/solutions/industry/financial-services)
  + [Manufacturing](/solutions/industry/manufacturing)
  + [Government](/solutions/industry/government)
  + [View all industries](/solutions/industry)

  [View all solutions](/solutions)
* Resources

  Topics
  + [AI](/resources/articles/ai)
  + [DevOps](/resources/articles/devops)
  + [Security](/resources/articles/security)
  + [Software Development](/resources/articles/software-development)
  + [View all](/resources/articles)

  Explore
  + [Learning Pathways](https://resources.github.com/learn/pathways)
  + [White papers, Ebooks, Webinars](https://resources.github.com)
  + [Customer Stories](https://github.com/customer-stories)
  + [Partners](https://partner.github.com)
  + [Executive Insights](https://github.com/solutions/executive-insights)
* Open Source

  + [GitHub Sponsors
    Fund open source developers](/sponsors)
  + [The ReadME Project
    GitHub community articles](https://github.com/readme)
  Repositories
  + [Topics](https://github.com/topics)
  + [Trending](https://github.com/trending)
  + [Collections](https://github.com/collections)
* Enterprise

  + [Enterprise platform
    AI-powered developer platform](/enterprise)
  Available add-ons
  + [Advanced Security
    Enterprise-grade security features](https://github.com/enterprise/advanced-security)
  + [GitHub Copilot
    Enterprise-grade AI features](/features/copilot#enterprise)
  + [Premium Support
    Enterprise-grade 24/7 support](/premium-support)
* [Pricing](https://github.com/pricing)

Search or jump to...

# Search code, repositories, users, issues, pull requests...

Search

Clear

[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)

# Provide feedback

We read every piece of feedback, and take your input very seriously.

Include my email address so I can be contacted

  Cancel

 Submit feedback

# Saved searches

## Use saved searches to filter your results more quickly

Name

Query

To see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).

  Cancel

 Create saved search

[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fopencv%2Fopencv%2Fissues%2F5956)

[Sign up](/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E%2Fvoltron%2Fissues_fragments%2Fissue_layout&source=header-repo&source_repo=opencv%2Fopencv)
Reseting focus

You signed in with another tab or window. Reload to refresh your session.
You signed out in another tab or window. Reload to refresh your session.
You switched accounts on another tab or window. Reload to refresh your session.

Dismiss alert

{{ message }}

[opencv](/opencv)
/
**[opencv](/opencv/opencv)**
Public

* [Notifications](/login?return_to=%2Fopencv%2Fopencv) You must be signed in to change notification settings
* [Fork
  55.9k](/login?return_to=%2Fopencv%2Fopencv)
* [Star
   80.2k](/login?return_to=%2Fopencv%2Fopencv)

* [Code](/opencv/opencv)
* [Issues
  2.6k](/opencv/opencv/issues)
* [Pull requests
  157](/opencv/opencv/pulls)
* [Discussions](/opencv/opencv/discussions)
* [Actions](/opencv/opencv/actions)
* [Projects
  2](/opencv/opencv/projects)
* [Wiki](/opencv/opencv/wiki)
* [Security](/opencv/opencv/security)
* [Insights](/opencv/opencv/pulse)

Additional navigation options

* [Code](/opencv/opencv)
* [Issues](/opencv/opencv/issues)
* [Pull requests](/opencv/opencv/pulls)
* [Discussions](/opencv/opencv/discussions)
* [Actions](/opencv/opencv/actions)
* [Projects](/opencv/opencv/projects)
* [Wiki](/opencv/opencv/wiki)
* [Security](/opencv/opencv/security)
* [Insights](/opencv/opencv/pulse)

# Remote code execution via heap corruption¬†#5956

[New issue](/login?return_to=)[Jump to bottom](#comment-composer-heading)Copy link[New issue](/login?return_to=)[Jump to bottom](#comment-composer-heading)Copy linkClosedClosed[Remote code execution via heap corruption](#top)#5956Copy linkAssignees [![vpisarev](https://avatars.githubusercontent.com/u/2110786?s=64&u=c2e6d7949ad8d5b897066d660e2c403a88c4c59e&v=4)](/vpisarev)Labels[bug](https://github.com/opencv/opencv/issues?q=state%3Aopen%20label%3A%22bug%22)[category: vulnerability](https://github.com/opencv/opencv/issues?q=state%3Aopen%20label%3A%22category%3A%20vulnerability%22)[![@rstevens70](https://avatars.githubusercontent.com/u/2715024?u=dccda0cbba9ea662d7242967ccb06690aa52d799&v=4&size=80)](/rstevens70)
## Description

[![@rstevens70](https://avatars.githubusercontent.com/u/2715024?u=dccda0cbba9ea662d7242967ccb06690aa52d799&v=4&size=48)](/rstevens70)[rstevens70](https://github.com/rstevens70)opened [on Jan 13, 2016](https://github.com/opencv/opencv/issues/5956#issue-126309603)

We've isolated a couple bugs that could allow an attacker to achieve remote code execution on a victim's machine when processing an infected image with OpenCV.

We have more details and would like to responsibly disclose this to a lead developer.

Known vulnerable versions: Linux, OpenCV 3.0.0. Unverified but most likely works on all versions.

## Metadata

### Assignees

* [![@vpisarev](https://avatars.githubusercontent.com/u/2110786?s=64&u=c2e6d7949ad8d5b897066d660e2c403a88c4c59e&v=4)vpisarev](/vpisarev)
### Labels

[bug](https://github.com/opencv/opencv/issues?q=state%3Aopen%20label%3A%22bug%22)[category: vulnerability](https://github.com/opencv/opencv/issues?q=state%3Aopen%20label%3A%22category%3A%20vulnerability%22)
### Type

No type
### Projects

No projects
### Milestone

No milestone

### Relationships

None yet
### Development

No branches or pull requests
## Issue actions

## Footer

¬© 2025 GitHub,¬†Inc.

### Footer navigation

* [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)
* [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)
* [Security](https://github.com/security)
* [Status](https://www.githubstatus.com/)
* [Docs](https://docs.github.com/)
* [Contact](https://support.github.com?tags=dotcom-footer)
* Manage cookies
* Do not share my personal information

You can‚Äôt perform that action at this time.


